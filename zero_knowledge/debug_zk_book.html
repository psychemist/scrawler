<h1>P vs NP and its application to zero knowledge proofs</h1>
<p>Source: https://rareskills.io/post/p-vs-np</p>
<h1>P vs NP and its application to zero knowledge proofs</h1>
<p>The P = NP problem asks: “If we can quickly verify a solution to a problem is correct, can we also quickly compute the solution?” Most researchers believe the answer is no, i.e., P ≠ NP.</p>
<p>By understanding the P vs NP problem, we can see how Zero Knowledge Proofs (ZKPs) fit into the larger field of computer science and comprehend what ZKPs can and cannot do.</p>
<p>It is far easier to “get” Zero Knowledge Proofs by relating them to the P vs NP problem.</p>
<p>This tutorial has three parts:</p>
<ol>
<li>Explaining the P vs NP problem</li>
<li>Expressing problems and solutions as a Boolean formula</li>
<li>P vs NP and how they relate to Zero Knowledge Proofs</li>
</ol>
<h2>Prerequisites</h2>
<p>We assume the reader is familiar with time complexity and big $\mathcal{O}$ notation.</p>
<p>We say an algorithm takes polynomial time if it runs in $\mathcal{O}(nᶜ)$ time or faster, where $n$ is the size of the input and $c$ is a non-negative constant. We may refer to algorithms that run in polynomial time or faster as <em>efficient algorithms</em> because their running time doesn’t grow too quickly with the input size.</p>
<p>We say an algorithm takes <em>exponential time</em> or is <em>expensive</em> if it runs in $\mathcal{O}(cⁿ)$ where $c$ is a constant greater than 1 and $n$ is the size of the input because the algorithm becomes exponentially slow as the size of the input increases.</p>
<h2>Part 1: Explaining the P vs NP problem</h2>
<h3>Problems in P are problems that are both easy to solve and easy to verify solutions for.</h3>
<p>Problems that can be solved in polynomial time and whose solutions can be verified in polynomial time are called problems in P.</p>
<p>Here are some example problems that are easy to compute and verify solutions for. These tasks might be performed by different parties, one doing the computation and another checking the results are valid:</p>
<h4>P Example 1: Sorting a list</h4>
<p>We can efficiently sort a list, and we can efficiently verify that a list is sorted.</p>
<ul>
<li><strong>Solving:</strong> Sorting the list takes $\mathcal{O}(n \log n)$ time (for example, using mergesort), which is polynomial time. Although $n \log n$ is not a polynomial, we know that $\log n &lt; n$ and therefore $(n \log n) &lt; n²$, so our algorithm’s running time is bounded above by some polynomial, which is the requirement for “polynomial time.”</li>
<li><strong>Verifying:</strong> We can verify the list is sorted by traversing the list and checking that each item is greater than its left neighbor, which would take $\mathcal{O}(n)$ time.</li>
</ul>
<h4>P Example 2: Returning the index of a number in a list, if it occurs in the list</h4>
<p>We can efficiently search to see if a number is in a list and then even more efficiently verify that the number is present if we know the index it is in.</p>
<ul>
<li><strong>Solving:</strong> For example, given the list <code style='font-family: Arial'>[8, 2, 1, 6, 7, 3]</code>, we need $\mathcal{O}(n)$ time to determine if the number <code style='font-family: Arial'>7</code> is in the list.</li>
<li><strong>Verifying:</strong> But if we give you the list and say 7 is at index 4, you can verify the number is in the list at that position in $\mathcal{O}(1)$ time. Searching for an item, if we aren’t told its position, takes $\mathcal{O}(n)$ time in the general case since we have to search through the list. If we’re told the supposed location of the item, it takes $\mathcal{O}(1)$ time to verify that the item is, in fact, in the list at that location.</li>
</ul>
<h4>P Example 3: Determining if two nodes in a graph are connected</h4>
<p>We can efficiently determine if two nodes in a graph are connected by using breadth-first search — start at a node, then visit all of its neighbors except nodes we’ve already visited, then search the neighbors of the neighbors, and so forth.</p>
<ul>
<li><strong>Solving:</strong> Discovering the path between nodes using breadth-first search will take $\mathcal{O}(n + e)$ time, where $n$ is the number of nodes in the graph and $e$ is the number of edges. The number of edges $e$ cannot exceed $n^2$, so we can treat $\mathcal{O}(n + e)$ as $\mathcal{O}(n²)$ in the worst case.</li>
<li><strong>Verifying:</strong> We can verify the proposed path is valid in $\mathcal{O}(n)$ time simply by following the proposed path to see if the two points really are connected by that path.</li>
</ul>
<p><strong>In all the examples above, both computing and verifying the solution can be done in polynomial time.</strong></p>
<h4>The Witness</h4>
<p>A <em>witness</em> in computer science is proof that you solved the problem correctly. In the examples above, the witness is the correct answer to the problem. For the examples above, these are things we could use as a witness:</p>
<ol>
<li>The sorted list</li>
<li>The index where a number appears in the list</li>
<li>The path between two nodes in a graph</li>
</ol>
<p>We will see later that a witness does not necessarily have to be the solution to the original problem. It could also be a solution for an alternative representation of the same problem.</p>
<h3>Problems in PSPACE: Not all problems have solutions that can be efficiently verified</h3>
<p>Problems that require exponential resources to solve and verify are called problems in PSPACE. The reason they are called PSPACE is that although they might take exponential time to solve, they don’t necessarily require exponential memory space to run the search.</p>
<p>This class of problems has been researched extensively, yet no efficient algorithm to solve them has been discovered. Many researchers believe no efficient algorithm to solve these problems exists at all. If an efficient solution to these problems could be discovered, it would also be possible to reuse the algorithm to break all modern encryption and fundamentally alter computing as we know it.</p>
<p>Despite significant incentives for finding efficient solutions to these problems, evidence suggests such solutions likely do not exist. These problems are so challenging that you cannot provide easily verifiable proof (witness) even if you solve them correctly.</p>
<h4>Examples of problems in PSPACE</h4>
<h5>PSPACE Example 1: Finding the optimal Chess move</h5>
<p><img alt="Image of a chess board" src="assets/935a00_71d5fe7538a847ccaef0c82b5bea6b57_mv2.jpeg" /></p>
<p>Suppose we ask a powerful computer, “Given this chess board with the pieces in this position, what is the optimal next move?”</p>
<p>The computer responds with “move the black pawn on f4 to f3.”</p>
<p>How can you trust the computer is giving you the correct answer?</p>
<p>There is no efficient way to check—you must do the same work the computer did. This involves doing a full search through all possible future board states. There’s no witness the computer could provide us that would allow us to confirm that “move the black pawn on f4 to f3” is actually the next best move. In this way, the nature of this problem is very different from the examples in the previously discussed: we cannot efficiently verify that the claimed optimal move is the true optimal move.</p>
<p>In this example, the “witness” presented by the computer consists of all potential future game states. However, the massive volume of this data makes it practically impossible to verify the accuracy of the solution efficiently.</p>
<h5>PSPACE Example 2: Determining if regexes (regular expressions) are equivalent</h5>
<p>The two regexes, <code style='font-family: Arial'>a+</code> and <code style='font-family: Arial'>aa*</code>, match the same strings. If a string matches the first regex, it will also match the second, and vice versa.</p>
<p>However, checking if two <em>arbitrary</em> regexes are equivalent takes exponential time to compute. Even if a powerful computer told you they match the same strings, there is no short proof (witness) the computer can give you to show the answers are correct. Similar to the chess example, you’d have to search a very large space of strings to check if the regexes are equivalent, and that will take exponential time.</p>
<p>Both Chess and Regex equivalence have a common characteristic in that they both require exponential resources to find answers and exponential resources to verify the answers.</p>
<h4>Problems even more computationally intensive than PSPACE</h4>
<p>There are problems that are so difficult that they require exponential time and exponential memory to solve, but those problems are usually theoretical and don’t frequently show up in the real world.</p>
<p>An example of such a problem is Checkers with a rule that pieces can never move into a position that recreates an earlier board state. In order to ensure we don’t repeat a board state for a game as we explore the space of possible moves, we have to keep track of all the board states that have already been visited. Since the length of the game can be exponential in the board size, the memory requirements are also exponential.</p>
<h3>Problems in NP: Some problems can be quickly verified but not quickly computed</h3>
<p>If we can quickly verify the solution to a problem, then the problem is in NP. However, finding the solution might require exponential resources.</p>
<p>Any problem whose proposed solution (witness) can be quickly verified as correct is an NP problem. If the problem also has an algorithm for finding the solution in polynomial time, then it is a P problem. All P problems are NP problems, but it is extremely unlikely that all NP problems are also P problems.</p>
<p>Examples of problems in NP. These are explained in more detail below:</p>
<ul>
<li>Computing the solution to a Sudoku puzzle — verifying the proposed solution to a Sudoku puzzle.</li>
<li>Computing the 3-coloring of a map (if it exists) — verifying a proposed 3-coloring of a map.</li>
<li>Finding an assignment to a Boolean formula that results in true — verifying the proposed assignment causes the formula to result in true.</li>
</ul>
<p><strong>Note:</strong> NP stands for non-deterministic polynomial. We won’t get into the jargon about where that name came from; we’re just giving the name so the reader doesn’t mistakenly think it stands for “not polynomial time.”</p>
<h4>Examples of problems in NP</h4>
<h5>NP Example 1: Sudoku</h5>
<p>In the game Sudoku, a player is given a $9 \times 9$ grid with some numbers filled in. The goal is for the player to fill in the rest of the grid with numbers 1-9 such that no number occurs more than once in any row, column, or $3 \times 3$ box (the ones outlined by bold lines). The following images from <a href="https://en.wikipedia.org/wiki/Sudoku">Wikipedia</a> illustrate this. In the first image, we see the 9×9 grid as given to the player. In the second image, we see the player’s solution.</p>
<p><img alt="An incomplete sudoku puzzle" src="assets/935a00_697037a2589d4091a95a9123d3796b4c_mv2.png" /></p>
<p><img alt="A completed sudoku puzzle" src="assets/935a00_2b17812514524ee584f0d9e7c340c73f_mv2.png" /></p>
<p>Given a Sudoku puzzle <em>solution</em>, we can quickly verify the solution is correct simply by looping over the columns, rows, and $3\times 3$ subgrids. The witness can be verified in polynomial time.</p>
<p>However, <em>computing</em> the solution requires significantly more resources — there are an exponential number of combinations to search. For a $9\times 9$ grid, this is not difficult for a computer. However, if we allow the Sudoku puzzle to be arbitrarily large: each side has size $n$, where $n$ is a multiple of 9. In that case, the difficulty of finding the solution grows exponentially with $n$.</p>
<h5>NP Example 2: Three-coloring a map</h5>
<p>Any 2D map of territories can be “colored” with just four colors (see the <a href="https://en.wikipedia.org/wiki/Four_color_theorem">four color theorem</a>). That is, we can assign a unique color (one of four colors) to each territory such that no neighboring territories share the same color. For example, the following image (from <a href="https://en.wikipedia.org/wiki/U.S._state#/media/File:Map_of_USA_with_state_names_2.svg">Wikipedia</a>) shows the United States colored with four colors: pink, green, yellow, and red. Take a moment to look at the verify that no two touching states have been given the same color:</p>
<p><img alt="A map of the United States colored with four colors" src="assets/935a00_80bcbb69d39348819f674827b8c25691_mv2.png" /></p>
<p>The three-coloring problem asks whether a map can be colored using just three colors instead of four. Discovering a three-coloring (if it exists) is a computationally intensive search problem. However, verifying a <em>proposed</em> 3-coloring is easy: loop through each of the regions and check that no neighboring regions have the same color of the territory currently being checked.</p>
<p>It turns out that it is not possible to 3-color the United States.</p>
<p>The reasons a particular map cannot be 3 colored vary, but in the case of the United States, Nevada (the red region in the map below) is surrounded by five territories. We color Nevada with one color, then we must alternate the colors of its neighboring territories. However, when we finish circling the neighbors of Nevada, we will end up with a territory having neighbors with three colors on its boundaries, leaving no valid color for the uncolored territory.</p>
<p><img alt="A map showing Nevada and the surrounding states" src="assets/935a00_5ddfcf6c6b0f4920bdb3624fbab031d9_mv2.jpg" /></p>
<p>Here is a <a href="https://www.youtube.com/watch?v=WlcXoz6tn4g">quick and interesting video about 3-coloring maps</a> if you want to learn more about this problem.</p>
<p>However, it is possible to 3-color Australia:</p>
<p><img alt="A 3 Coloring of Australia" src="assets/935a00_d8396ac3cd15406281b6c83deb2abc71_mv2.jpg" /></p>
<p>Not all maps can be three-colored. Computing a three-coloring for an arbitrary 2D map, if it exists, cannot be done efficiently — it typically requires a brute-force search that may take exponential time.</p>
<p>However, if someone solves a three-coloring, it is easy to verify their solution.</p>
<h3>The relationship between P, NP, and PSPACE</h3>
<h4>Computational resources for each class</h4>
<p>The table below summarizes the computational resources required for each class of problem:</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Compute Time</th>
<th>Verification Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>P</td>
<td>Must be polynomial or better</td>
<td>Must be polynomial or better</td>
</tr>
<tr>
<td>NP</td>
<td>No Requirement</td>
<td>Must be polynomial or better</td>
</tr>
<tr>
<td>PSPACE</td>
<td>No Requirement</td>
<td>No Requirement</td>
</tr>
</tbody>
</table>
<h4>Hierarchy of Difficulty between P, NP, and PSPACE</h4>
<p>Any problem that requires exponential resources to verify the witness for is a PSPACE (or harder problem). If one has exponential resources to verify witnesses for PSPACE problems, that person can trivially compute solutions for any P or NP problem. Therefore, all P and NP problems are a subset of PSPACE problems, as illustrated in the figure below.</p>
<p>In other words, if you have a powerful enough computer to solve or verify a class of problem in the larger circle, you can solve or verify a subset of it:</p>
<p><img alt="Hierarchy of computation complexity classes" src="assets/935a00_9a3130175f2945eb8ae7a4d975b36f55_mv2.jpg" /></p>
<h3>The P vs NP problem</h3>
<p>P is the class of problems that can be solved and verified efficiently, while NP is the class of problems that can be verified efficiently. The “P vs NP” question asks, simply, whether these two classes are the same.</p>
<p>If P = NP, it would mean that whenever we can find an efficient method for verifying a solution, we can also find an efficient method for finding that solution. Remember that finding a solution is always at least as hard as verifying it. (By definition, solving a problem includes finding the correct answer, which means an algorithm that solves the problem is also verifying its answer in the process).</p>
<p>If P = NP is true, that means there is an efficient algorithm for computing Sudoku puzzles (of arbitrary size) and finding if a three coloring exists. It also means there is an efficient algorithm to break most modern cryptographic algorithms.</p>
<p>Currently, it remains unproven whether P and NP are the same set. Although numerous attempts have been made to find efficient algorithms for NP problems, all evidence suggests that such algorithms do not exist.</p>
<p>Similarly, whether efficient solutions or verification mechanisms exist for PSPACE problems remains open. While researchers widely speculate that P ≠ NP and NP ≠ PSPACE, no formal mathematical proof exists for these conclusions. Therefore, despite extensive efforts, efficient solutions for NP problems and efficient verification methods for PSPACE problems remain undiscovered.</p>
<p>For practical purposes, we can assume P ≠ NP and NP ≠ PSPACE. In fact, we implicitly make that assumption when we trust important data to modern cryptography algorithms.</p>
<h2>Part 2: Expressing problems and solutions as Boolean formulas</h2>
<p>What ties P and NP problems together is that their solution can be quickly verified. It would be extremely useful if we could describe all those (P and NP) problems and their solutions in a common language. That is, we want an encoding of the problem that works for problems as diverse as proving a list is sorted, to proving a Sudoku puzzle is solved, to proving we have a three coloring.</p>
<p><strong>Verifying a solution to a problem in NP or P can be accomplished by verifying a solution to a Boolean formula that models the problem.</strong></p>
<p>What we mean by “a Boolean formula that models the problem” will become clear as we describe what we mean by “a solution to a Boolean formula” and looking at some example modeling problems with Boolean formulas.</p>
<h3>Solutions to a Boolean formula</h3>
<p>To express a Boolean formula, we use the operator $¬$ to be the Boolean NOT, $∧$ to be the Boolean AND, and $∨$ to be the Boolean OR. For example, $a ∧ b$ evaluates to true if and only if $a$ and $b$ are both true. $a ∧ ¬b$ evaluates to true if and only if $a$ is true and $b$ is false.</p>
<p>Suppose we have a bunch of Boolean variables $x₁$, $x₂$, $x₃$, $x₄$ and a Boolean formula:</p>
<p>$$<br />
out = (x₁ ∨ ¬x₂ ∨ ¬ x₃) ∧ (¬x₂ ∨ x₃ ∨ x₄) ∧ (x₁ ∨ x₃ ∨ ¬x₄) ∧ (¬x₂ ∨ ¬x₃∨ ¬x₄)<br />
$$</p>
<p>The question is: can we find values for $x₁, x₂, x₃, x₄$ such that $out$ is true? For the above formula, we can. Writing $T$ for true and $F$ for false, we can write our solution as:</p>
<p>$$<br />
\begin{align*}<br />
x₁ = T \<br />
x₂ = F \<br />
x₃ = T \<br />
x₄ = F<br />
\end{align*}<br />
$$</p>
<p>Plugging the solution into the formula yields:</p>
<p>$$<br />
\begin{align*}<br />
x₁ &amp;= T \<br />
x₂ &amp;= F \<br />
x₃ &amp;= T \<br />
x₄ &amp;= F \<br />
out &amp;= (x₁ ∨ ¬x₂ ∨ ¬ x₃) ∧ (¬x₂ ∨ x₃ ∨ x₄) ∧<br />
(x₁ ∨ x₃ ∨ ¬x₄)∧ (¬x₂ ∨ ¬x₃∨ ¬x₄) \<br />
out &amp;= (T ∨ ¬ F ∨ ¬ T) ∧ (¬ F ∨ T ∨ F) ∧<br />
(T ∨ T ∨ ¬ F) ∧ (¬ F ∨ ¬ T ∨ ¬ F) \<br />
&amp;= (T) ∧ (T) ∧ (T) ∧ (T) \<br />
&amp;= T \<br />
\end{align*}<br />
$$</p>
<p>That was easy to verify, but discovering the solution for a very large Boolean formula could require exponential time. Finding a solution to a Boolean formula is an NP problem itself — it may require exponential resources to find the solution, but verifying it can be done in polynomial time.</p>
<p>But we must emphasize: our use of Boolean formulas is not to solve them — only to verify proposed solutions for them.</p>
<h3>All problems in P and NP can be verified by transforming them into boolean formulas and showing a solution to the formula</h3>
<p>In the following examples, the input is a P or NP problem and the output is a Boolean formula. If we know the solution to the original problem, then we will also know the solution to the Boolean formula.</p>
<p>Our intent is to show we know the witness for the original problem — but in Boolean form.</p>
<h4>Example 1: checking if a list is sorted using a Boolean formula</h4>
<p>Consider one-bit binary numbers $A$ and $B$. The following truth table returns true when $A &gt; B$:</p>
<table>
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>A &gt; B</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>The column $A &gt; B$ can be modeled with the expression $A ∧ ¬B$, which returns true in the only row where $A &gt; B$ is one.</p>
<p>Now consider a table that expresses $A = B$:</p>
<table>
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>A = B</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>The column $A = B$ can be modeled with the expression $(A ∧ B) ∨ ¬(A ∨ B)$. $(A ∧ B)$ returns true when $A = 1$ and $B = 1$ and $¬(A ∨ B)$ returns true when $A$ and $B$ are both zero.</p>
<p>The expressions for one bit numbers:</p>
<p>$$<br />
\begin{align*}<br />
A &gt; B &amp;\rightarrow A ∧ ¬B \<br />
A = B &amp;\rightarrow (A ∧ B) ∨ ¬(A ∨ B)<br />
\end{align*}<br />
$$</p>
<p>will come in handy shortly.</p>
<p>Now how do we compare binary numbers of more than one bit?</p>
<h5>Binary comparison by most significant different bit</h5>
<p>Suppose you start from both numbers’ leftmost Most Significant Bit (MSB) and move towards the right Least Significant Bit (LSB). At the first bit, where the two numbers differ:</p>
<p>If $P$ has a value of $1$ at that bit and $Q$ has a value of $0$, then $P ≥ Q$.<br />
The following animation illustrates the algorithm detecting that $P ≥ Q$:</p>
<p><img alt="Algorithm to test if P is greater or equal to Q" src="assets/935a00_4dc4955706b54e778847f861f651e486_mv2.gif" /></p>
<p>If $P = Q$, then all of the bits are equal. $P = Q$ means $P ≥ Q$ is also true:<br />
<img alt="Algorithm to test P = Q" src="assets/935a00_186a696a8b14493a8d6cd17d7f7bfe0d_mv2.gif" /></p>
<p>If P &lt; Q, then we will detect that on the first bit where Q is 1 and P is 0:</p>
<p><img alt="Detecting if P &lt; Q" src="assets/935a00_bba4e16f05d34184945faf33fd9a8c53_mv2.gif" /></p>
<p>Suppose, without loss of generality, that we number the bits in $P$ as $p₄, p₃, p₂, p₁$ and the bits in $Q$ as $q₄, q₃, q₂, q₁$.</p>
<p>If $P ≥ Q$ then one of the following must be true:</p>
<ul>
<li>$p₄ &gt; q₄$</li>
<li>$p₄ = q₄$ and $p₃ &gt; q₃$</li>
<li>$p₄ = q₄$ and $p₃ = q₃$ and $p₂ &gt; q₂$</li>
<li>$p₄ = q₄$ and $p₃ = q₃$ and $p₂ = q₂$ and $(p₁ &gt; q₁ \text{ or } p₁ = q₁)$</li>
</ul>
<p>We can combine the bullet points into a single equation.</p>
<p>$$<br />
\begin{align*}<br />
&amp;((p₄ &gt; q₄)) ∨ \<br />
&amp;((p₄ = q₄) ∧ (p₃ &gt; q₃)) ∨ \<br />
&amp;((p₄ = q₄) ∧ (p₃ = q₃) ∧ (p₂ &gt; q₂)) ∨ \<br />
&amp;((p₄ = q₄) ∧ (p₃ = q₃) ∧ (p₂ = q₂) ∧ ((p₁ &gt; q₁) ∨ (p₁ = q₁)))<br />
\end{align*}<br />
$$</p>
<p>Recall our Boolean expressions that modeled one bit equality and comparison:</p>
<p>$$<br />
\begin{align*}<br />
A &gt; B &amp;== A ∧ ¬B\<br />
A = B &amp;== (A ∧ B) ∨ ¬(A ∨ B)<br />
\end{align*}<br />
$$</p>
<p>We can substitute the expressions for $A &gt; B$ and $A = B$ formula in to the equation above. To avoid a wall of math, we show the operations in video form below:</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/935a00_655f627a117f46ecb0a48deb2640b9a1/1080p/mp4/file.mp4)</p>
<p>The final Boolean formula that expresses $P ≥ Q$, for 4 bits, is:</p>
<p>$$<br />
\begin{align*}<br />
&amp;(p₄ ∧ ¬q₄) ∨ \<br />
&amp;(((p₄ ∧ q₄) ∨ ¬(p₄ ∨ q₄)) ∧ (p₃ ∧ ¬q₃)) ∨ \<br />
&amp;(((p₄ ∧ q₄) ∨ ¬(p₄ ∨ q₄)) ∧ ((p₃ ∧ q₃) ∨ ¬(p₃ ∨ q₃)) ∧ (p₂ ∧ ¬q₂)) ∨ \<br />
&amp;(((p₄ ∧ q₄) ∨ ¬(p₄ ∨ q₄)) ∧ ((p₃ ∧ q₃) ∨ ¬(p₃ ∨ q₃)) ∧ ((p₂ ∧ q₂) ∨ ¬(p₂ ∨ q₂)) ∧ ((p₁ ∧ ¬q₁) ∨ ((p₁ ∧ q₁) ∨ ¬(p₁ ∨ q₁))))<br />
\end{align*}<br />
$$</p>
<p>Let’s call a Boolean expression that compares two binary numbers in the manner described above a<br />
“comparison expression.”</p>
<h5>Checking if a list is sorted</h5>
<p>Given a Boolean formula to compare numbers of a fixed size, we can repeatedly apply the comparison expression to each pair of adjacent elements in the list and combine the comparison expressions using the AND operation. The list is sorted if and only if the AND of all the comparison expressions is true.</p>
<p>Thus, we see that a witness that proves a list is sorted does not have to be the sorted list. It can also be the input to Boolean formula that we created above that results in the formula returning true.</p>
<h4>Example 2: A 3-coloring as a Boolean formula</h4>
<p>Let’s look at our map of Australia again:</p>
<p><img alt="A 3 Coloring of Australia" src="assets/935a00_d8396ac3cd15406281b6c83deb2abc71_mv2.jpg" /></p>
<p>To model the solution as a Boolean formula, the formula needs to encode the following facts:</p>
<ul>
<li>Each territory has one of three colors</li>
<li>Each territory has a different color from its neighbor</li>
</ul>
<p>For example, to say Western Australia is either green, blue, or red, we need to create three variables <code style='font-family: Arial'>WESTERN_AUSTRALIA_GREEN</code>, <code style='font-family: Arial'>WESTERN_AUSTRALIA_BLUE</code>,<code style='font-family: Arial'>WESTERN_AUSTRALIA_RED</code>. To avoid long variable names, let’s call it <code style='font-family: Arial'>WA_G</code>, <code style='font-family: Arial'>WA_B</code>, and <code style='font-family: Arial'>WA_R</code>. Our Boolean formula is then:</p>
<pre style='font-family: Arial'><code class="language-solidity">(WA_G ∧ ¬WA_B ∧ ¬WA_R) ∨ (¬WA_G ∧ WA_B ∧ ¬WA_R) ∨ (¬WA_G ∧ ¬WA_B ∧ WA_R)
</code></pre>
<p>In other words:</p>
<p>“(West Australia is green AND West Australia is NOT blue AND West Australia is NOT red)</p>
<p>OR</p>
<p>(West Australia is NOT green AND West Australia is blue AND West Australia is NOT red)</p>
<p>OR</p>
<p>(West Australia is NOT green AND West Australia is NOT blue AND West Australia is red)”</p>
<p>Coloring the Boolean formula for emphasis:</p>
<p>(WA_G ∧ ¬WA_B ∧ ¬WA_R) ∨ (¬WA_G ∧ WA_B ∧ ¬WA_R) ∨ (¬WA_G ∧ ¬WA_B ∧ WA_R)</p>
<p>Let’s call the formula above the <em>color assignment constraint</em>.</p>
<p>We use Boolean variables to encode the assigned color of a territory. Since a Boolean variable can only hold two values, but a territory can be one of three colors, each territory is assigned three Boolean variables, one for each color. If a territory is assigned a particular color, the corresponding variable is set to true and the others are set to false.</p>
<p>The above formula is true if and only if the territory of Western Australia is assigned exactly one color.</p>
<h5>Neighboring color constraint</h5>
<p>Next, we want to write a formula which expresses that WA has a different color than it’s neighbor. We make three variables for SA (South Australia) like we did for WA. Now, our formula simply says, “for each color, WA and SA are not both that color”. This is equivalent to saying “WA and SA are not the same color.” Let’s use the variable names <code style='font-family: Arial'>SA_G</code>, <code style='font-family: Arial'>SA_B</code>, and <code style='font-family: Arial'>SA_R</code> to refer to the color assignment of South Australia (which neighbors Western Australia). We use the formula below to express that they have different colors:</p>
<p>¬(WA_G ∧ SA_G) ∧ ¬(WA_B ∧ SA_B) ∧ ¬(WA_R ∧ SA_R)</p>
<p>In other words:</p>
<p>it is <strong>NOT</strong> the case that (West Australia is green AND South Australia is green)</p>
<p><strong>AND</strong></p>
<p>it is <strong>NOT</strong> the case that (West Australia is blue AND South Australia is blue)</p>
<p><strong>AND</strong></p>
<p>it is <strong>NOT</strong> the case that (West Australia is red AND South Australia is red)”</p>
<p>The above formula will be satisfied if and only if Western Australia and South Australia were not assigned the same color. Let’s call the formula above the <em>boundary constraint</em>.</p>
<p>We need to apply the color assignment constraint to each territory and the different color constraint to each pair of neighbors, then AND all the constraints together.</p>
<h5>A formula to model a 3-coloring of Australia</h5>
<p>We now show the final Boolean formula that verifies a valid 3 coloring for Australia. Here are the territories labelled:</p>
<p><img alt="3-coloring of Australia labelled with colors for the territories" src="assets/935a00_824140c195b64b20bb5351d8d54464d8_mv2.jpg" /></p>
<p>First, we assign each territory a variable name:</p>
<ul>
<li><code style='font-family: Arial'>WA</code> = West Australia</li>
<li><code style='font-family: Arial'>SA</code> = South Australia</li>
<li><code style='font-family: Arial'>NT</code> = Northern Territory</li>
<li><code style='font-family: Arial'>Q</code> = Queensland</li>
<li><code style='font-family: Arial'>NSW</code> = New South Wales</li>
<li><code style='font-family: Arial'>V</code> = Victoria</li>
</ul>
<h6>Color constraints: Each of the six territories has exactly one color:</h6>
<ol>
<li>(WA_G ∧ ¬WA_B ∧ ¬WA_R) ∨ (¬WA_G ∧ WA_B ∧ ¬WA_R) ∨ (¬WA_G ∧ ¬WA_B ∧ WA_R)</li>
<li>(SA_G ∧ ¬SA_B ∧ ¬SA_R) ∨ (¬SA_G ∧ SA_B ∧ ¬SA_R) ∨ (¬SA_G ∧ ¬SA_B ∧ SA_R)</li>
<li>(NT_G ∧ ¬NT_B ∧ ¬NT_R) ∨ (¬NT_G ∧ NT_B ∧ ¬NT_R) ∨ (¬NT_G ∧ ¬NT_B ∧ NT_R)</li>
<li>(Q_G ∧ ¬Q_B ∧ ¬Q_R) ∨ (¬Q_G ∧ Q_B ∧ ¬Q_R) ∨ (¬Q_G ∧ ¬Q_B ∧ Q_R)</li>
<li>(NSW_G ∧ ¬NSW_B ∧ ¬NSW_R) ∨ (¬NSW_G ∧ NSW_B ∧ ¬NSW_R) ∨ (¬NSW_G ∧ ¬NSW_B ∧ NSW_R)</li>
<li>(V_G ∧ ¬V_B ∧ ¬V_R) ∨ (¬V_G ∧ V_B ∧ ¬V_R) ∨ (¬V_G ∧ ¬V_B ∧ V_R)</li>
</ol>
<h6>Boundary constraints: Each neighboring territory does not share a color</h6>
<p>Next, we iterate through the boundaries and compute a boundary constraint for those neighbors. The video below shows the algorithm in action. We show the final set of formulas for the boundary conditions after the video:</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/935a00_d942bd31ee0d4e0087a0c3fe5ec8b75a/1080p/mp4/file.mp4)</p>
<ol start="7">
<li>Western Australia and South Australia:</li>
</ol>
<p>¬(WA_G ∧ SA_G) ∧ ¬(WA_B ∧ SA_B) ∧ ¬(WA_R ∧ SA_R)</p>
<ol start="8">
<li>Western Australia and Northern Territory:</li>
</ol>
<p>¬(WA_G ∧ NT_G) ∧ ¬(WA_B ∧ NT_B) ∧ ¬(WA_R ∧ NT_R)</p>
<ol start="9">
<li>Northern Territory and South Australia:</li>
</ol>
<p>¬(NT_G ∧ SA_G) ∧ ¬(NT_B ∧ SA_B) ∧ ¬(NT_R ∧ SA_R)</p>
<ol start="10">
<li>Northern Territory and Queensland:</li>
</ol>
<p>¬(NT_G ∧ Q_G) ∧ ¬(NT_B ∧ Q_B) ∧ ¬(NT_R ∧ Q_R)</p>
<ol start="11">
<li>South Australia and Queensland:</li>
</ol>
<p>¬(SA_G ∧ Q_G) ∧ ¬(SA_B ∧ Q_B) ∧ ¬(SA_R ∧ Q_R)</p>
<ol start="12">
<li>South Australia and New South Wales:</li>
</ol>
<p>¬(SA_G ∧ NSW_G) ∧ ¬(SA_B ∧ NSW_B) ∧ ¬(SA_R ∧ NSW_R)</p>
<ol start="13">
<li>South Australia and Victoria:</li>
</ol>
<p>¬(SA_G ∧ V_G) ∧ ¬(SA_B ∧ V_B) ∧ ¬(SA_R ∧ V_R)</p>
<ol start="14">
<li>Queensland and New South Wales:</li>
</ol>
<p>¬(Q_G ∧ NSW_G) ∧ ¬(Q_B ∧ NSW_B) ∧ ¬(Q_R ∧ NSW_R)</p>
<ol start="15">
<li>New South Wales and Victoria:</li>
</ol>
<p>¬(NSW_G ∧ V_G) ∧ ¬(NSW_B ∧ V_B) ∧ ¬(NSW_R ∧ V_R)</p>
<p>We create a Boolean formula by taking the Boolean AND of the 15 formulas mentioned above.. Having an assignment to the variables that results in the result of the Boolean expression being true is equivalent to having a valid 3-coloring of Australia.</p>
<p>In other words, if we know a valid 3-coloring for Australia, then we also know an assignment to the Boolean formula constructed above.</p>
<h3>The Boolean Formula must be constructable in polynomial time</h3>
<p>We only need to take a polynomial number of steps to construct this Boolean formula for 3-coloring. Specifically, we need:</p>
<ul>
<li>3 color constraints per territory</li>
<li>At most N neighboring color constraints per territory</li>
</ul>
<p>It is a requirement that the steps taken to construct the Boolean formula be done in polynomial time. If it takes an exponential number of steps, then the Boolean formula will be exponentially large, and the witness will be exponentially large — and not verifiable in polynomial time.</p>
<h3>Summary of using Boolean expressions to model problems and proposed solutions</h3>
<p>All problems in P and NP can be expressed as a Boolean formula that outputs true if we know the corresponding variable assignment (witness), which encodes a correct solution to the original problem.</p>
<p>Now that we have a standard language for efficiently demonstrating a solution to a problem, we are one step closer to a standard method for demonstrating that we have a solution to a problem — without revealing the solution, i.e., Zero Knowledge Proofs.</p>
<h2>Part 3: P vs NP and ZK Proofs</h2>
<h3>How P = NP relates to ZK Proofs</h3>
<p>The “knowledge” in Zero Knowledge Proofs refers to knowledge of the witness.</p>
<p>ZK proofs are concerned with the verification aspect of computation. That is, given that you have found a Sudoku solution or a 3-coloring of a map, can you give someone evidence (witness) that would allow them to efficiently verify that your solution is correct?</p>
<p>ZK proofs seek to demonstrate that you know the witness without revealing it.</p>
<h3>ZKPs only work with P or NP problems. They are not usable for problems which we can’t verify efficiently.</h3>
<p>If we don’t have a mechanism to efficiently prove regexes are equivalent, or that a certain move in chess is optimal, then ZK proofs cannot magically enable us to produce such an efficient proof.</p>
<p>For both P and NP problems, the verification of the solution can be done efficiently. ZK enables verifying the solution is valid while concealing the details of the computation. Furthermore, ZK cannot help you discover a solution to a Sudoku puzzle or discover a 3-coloring of a map. However, it can help you prove to another party that you have a solution, if you already computed it.</p>
<h3>The connection between P vs NP and Zero Knowledge Proofs</h3>
<p><strong>All problems with solutions that can be quickly verified can be converted into a Boolean formula.</strong></p>
<p>Being able to convert any problem into a Boolean formula is not a cheat code for finding the answer efficiently. Solving an arbitrary Boolean expression is an NP problem, and finding a solution to it may be difficult.</p>
<p>The size of the Boolean formula is important. Going back to our chess example, if you try to model every state with a Boolean formula, then the size of your formula will be exponentially large. Therefore, the only feasible problems are NP or P, which have reasonably sized Boolean formulas that model them.</p>
<p><strong>In the ZK literature, we often refer to Boolean formulas as Boolean circuits.</strong></p>
<p>Creating a zero knowledge proof for a problem boils down to translating the problem to a circuit, as demonstrated when proving a three-coloring for Australia or validating a sorted list. Then, you prove you have valid input to the circuit (the witness), which ultimately transforms into zero knowledge proof.</p>
<p>The ability to efficiently verify a solution to a problem is a prerequisite for creating a zero knowledge proof that you have a solution. One must be able to construct a Boolean circuit to model the solution efficiently. However, for problems like determining optimal Chess moves, which belong to PSPACE, this approach results in exponentially large circuits, making them impractical.</p>
<p>In conclusion, zero knowledge proofs are feasible only for problems within P and NP, where efficient solution verification is possible. Without efficient verification, creating a zero knowledge proof for a problem becomes infeasible.</p>
<h2>Learn more</h2>
<p>Please see the <a href="https://rareskills.io/zk-book">RareSkills ZK Book</a> for more topics in zero knowledge proofs.</p>
<h2>Technicalities</h2>
<p>Some concepts have been simplified in this article to make them as understandable as possible to someone seeing them for the first time. The information presented here is sufficient to explain what ZK proofs can and cannot do. For those interested in pursuing the subject further, here are some clarifications:</p>
<ul>
<li>A chess board of a fixed size $(8 \times 8)$ cannot be assigned a difficulty level because the difficulty of the problem cannot be expressed as $\mathcal{O}(f(n))$. Technically, we say chess of an arbitrary size is PSPACE. It might be confusing to think of a $10 \times 10$ chess board, but one can simply specify that the extra spaces don’t have any pieces in them at the starting position.</li>
<li>Chess has a lesser known rule that if no capture move has taken place and no pawn has moved for the last 50 moves, then a player can call a draw. This places a bound on the search space that puts it in PSPACE. If this rule is removed, then this version of chess is in EXPSPACE — a category of problems that requires exponential time and exponential memory size to compute.</li>
<li>Some NP problems can be solved in <em>sub exponential time</em>, but for practical purposes, they take exponential time to solve. For example $\mathcal{O}(2^{\sqrt{n}})$ is technically sub-exponential, but it is still exponentially difficult.</li>
<li>Very powerful heuristics for finding solutions to some NP problems exist. Although three coloring takes exponential time to solve, many instances of the problem of reasonable size can be solved quickly. For example, <a href="https://www.cs.ubc.ca/~hoos/SATLIB/benchm.html">here are benchmark problems of maps with 200 territories and valid 3 colorings</a>. Clever algorithms are able to find the solution without exploring an exponentially large search space. However, for any heuristics designed to speed up solving an NP problem, it is possible to create a pathological instance of problem that is designed to exploit the heuristic and make it worthless. Nevertheless, the heuristics work well for the typical instance of a realistic example of the problem.</li>
</ul>
<p><em>Originally Published April 10, 2024</em></p>
<div style='page-break-after: always;'></div>

<h1>Arithmetic Circuits for ZK</h1>
<p>Source: https://rareskills.io/post/arithmetic-circuit</p>
<h1>Arithmetic Circuits for ZK</h1>
<p>In the context of zero-knowledge proofs, an arithmetic circuit is a system of equations that models a problem in NP.</p>
<p>A key point from our article on <a href="https://rareskills.io/post/p-vs-np">P vs NP</a> is that any solution to a problem in P or NP can be verified by modeling the problem as a Boolean circuit.</p>
<p>Then, we convert our solution for the original problem to a set of values for the Boolean variables (called the witness) that results in the Boolean circuit returning true.</p>
<p>This article builds on the one linked above, so please read that first.</p>
<h2>Arithmetic circuits as an alternative to Boolean circuits</h2>
<p>One disadvantage of using a Boolean circuit to represent a solution to a problem is that it can be verbose when representing arithmetic operations, such as addition or multiplication.</p>
<p>For example, if we want to express $a + b = c$ where $a = 8, b = 4, c = 12$ we must transform $a$, $b$, and $c$ into binary numbers. Each bit in the binary number will correspond to a distinct Boolean variable. In this example, let’s assume we need 4 bits to encode $a$, $b$, and $c$, where $a₀$ represents the Least Significant Bit (LSB), and $a₃$ represents the Most Significant Bit (MSB) of number $a$, as shown below:</p>
<ul>
<li><code style='font-family: Arial'>a₃, a₂, a₁, a₀</code></li>
<li>$a = 1000$</li>
<li><code style='font-family: Arial'>b₃, b₂, b₁, b₀</code></li>
<li>$b = 0100$</li>
<li><code style='font-family: Arial'>c₃, c₂, c₁, c₀</code></li>
<li>$c = 1100$</li>
</ul>
<p>(You don’t need to know how to convert a number to binary for now, we will explain the method later in the article).</p>
<p>Once we have $a$, $b$ and $c$ written in binary, we can write a Boolean circuit whose inputs are all the binary digits $(a₀, a₁, …, c₂, c₃)$. Our goal is to write such a Boolean circuit, such that the circuit outputs true if and only if $a + b = c$.</p>
<p>This turns out to be more complicated than expected, as demonstrated by the large circuit below, which models $a + b = c$ in binary. For brevity, we do not show the derivation. We only show the formula to illustrate how verbose such a circuit can be:</p>
<pre style='font-family: Arial'><code class="language-solidity">((a₄ ∧ b₄ ∧ c₄) ∨ (¬a₄ ∧ ¬b₄ ∧ c₄) ∨ (¬a₄ ∧ b₄ ∧ ¬c₄) ∨ (a₄ ∧ ¬b₄ ∧ ¬c₄)) ∧

((a₃ ∧ b₃ ∧ ((a₂ ∧ b₂) ∨ (b₂ ∧ (a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀)))) ∨
 (¬a₃ ∧ ¬b₃ ∧ ((a₂ ∧ b₂) ∨ (b₂ ∧ (a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀)))) ∨
 (¬a₃ ∧ b₃ ∧ ¬((a₂ ∧ b₂) ∨ (b₂ ∧ (a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀)))) ∨
 (a₃ ∧ ¬b₃ ∧ ¬((a₂ ∧ b₂) ∨ (b₂ ∧ (a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀))))) ∧

((a₂ ∧ b₂ ∧ ((a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀))) ∨
 (¬a₂ ∧ ¬b₂ ∧ ((a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀))) ∨
 (¬a₂ ∧ b₂ ∧ ¬((a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀)))) ∨
 (a₂ ∧ ¬b₂ ∧ ¬((a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀))))) ∧

((a₁ ∧ b₁ ∧ c₀) ∨ (¬a₁ ∧ ¬b₁ ∧ c₀) ∨ (¬a₁ ∧ b₁ ∧ ¬c₀) ∨ (a₁ ∧ ¬b₁ ∧ ¬c₀)) ∧

((a₀ ∧ b₀ ∧ c₀) ∨ (¬a₀ ∧ ¬b₀ ∧ c₀) ∨ (¬a₀ ∧ b₀ ∧ ¬c₀) ∨ (a₀ ∧ ¬b₀ ∧ ¬c₀)) ∧

¬ ((a₄ ∧ b₄) ∨
     (b₄ ∧ (a₃ ∧ b₃) ∨ (b₃ ∧ (a₂ ∧ b₂) ∨ (b₂ ∧ (a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀))) ∨
     (a₃ ∧ (a₂ ∧ b₂) ∨ (b₂ ∧ (a₁ ∧ b₁) ∨ (b₁ ∧ c₀) ∨ (a₁ ∧ c₀))))
</code></pre>
<p>The point is, if we are restricted to Boolean inputs and basic Boolean operations (AND, OR, NOT), constructing circuits can quickly become complicated and tedious for basic problems, especially when they involve arithmetic.</p>
<p>In contrast, it would be simpler to directly represent numbers inside a circuit. Instead of modeling addition with a Boolean formula, we directly use addition and multiplication on those numbers.</p>
<p>This article demonstrates that it is also possible to model any problem in P or NP with an <em>arithmetic circuit</em>.</p>
<h2>Arithmetic Circuits</h2>
<p>An arithmetic circuit is a system of equations using only addition, multiplication, and equality. Like a Boolean circuit, it checks that a proposed set of inputs is valid, but doesn’t compute a solution.</p>
<p>The following is our first example of an arithmetic circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">6 = x₁ + x₂
9 = x₁x₂
</code></pre>
<p>We say a Boolean circuit is <em>satisfied</em> if we have an assignment to the input variables that results in an output of true. Similarly, an arithmetic circuit is satisfied if there is an assignment to the variables such that all the equations hold true.</p>
<p>For example, the circuit above is satisfied by x₁ = 3, x₂ = 3 because both equations in the circuit hold true. Conversely, the circuit is not satisfied by <code style='font-family: Arial'>x₁ = 1, x₂ = 6</code> because the equation <code style='font-family: Arial'>9 = x₁x₂</code> is not true.”</p>
<p>So, we can think of an arithmetic circuit interchangeably with the set of equations in the circuit. A set of inputs “satisfies the circuit” if and only if those inputs make <em>all</em> the equations true.</p>
<h2>Notation and Terminology</h2>
<p>Variables in an arithmetic circuit are referred to as <strong>signals</strong> because <a href="https://docs.circom.io">Circom</a>, the programming language we will use to write ZK Proofs, refers to them as such.</p>
<p>To express equality, we will use the <code style='font-family: Arial'>===</code> operator. We use this notation because Circom uses it to state that two signals hold equal value, so we may as well get accustomed to seeing it.</p>
<p>We emphasize that the <code style='font-family: Arial'>===</code> is asserting the left-hand side and right-hand side are equal. For example, in the following circuit:</p>
<p><code style='font-family: Arial'>c === a + b</code></p>
<p><strong>we are not adding <code style='font-family: Arial'>a</code> to <code style='font-family: Arial'>b</code> and assigning the result to <code style='font-family: Arial'>c</code></strong>. <strong>Instead, we assume that the values <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, and <code style='font-family: Arial'>c</code> are provided as inputs, and we are asserting a relationship between them holds. This has the effect of <em>constraining</em> the sum of <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code> to be <code style='font-family: Arial'>c</code>.</strong></p>
<p>Think of the <code style='font-family: Arial'>c === a + b</code> as being completely equivalent to <code style='font-family: Arial'>assertEq(c, a + b)</code>. Similarly, the expression <code style='font-family: Arial'>a + b === c * d</code> is completely equivalent to <code style='font-family: Arial'>assertEq(a + b, c * d)</code>. In essence, verifying these equations in a circuit involves checking if certain conditions (constraints) are satisfied. The agent proving the validity of their witness can assign any values to signals. However, their proof (witness) will only be considered valid if all the constraints are met.</p>
<p>For example, if an agent wishes to prove:</p>
<pre style='font-family: Arial'><code class="language-solidity">a === b + c + 3
a * u === x * y
</code></pre>
<p>they must supply <code style='font-family: Arial'>(a, b, c, u, x, y)</code> from <em>outside the circuit</em> and assign them to the signals in the circuit.</p>
<p>Remember, the code above is equivalent to:</p>
<pre style='font-family: Arial'><code class="language-solidity">assertEq(a, b + c + 3)
assertEq(a * u, x * y)
</code></pre>
<p>A useful mental model for the arithmetic circuit is that all signals are treated as inputs without outputs.</p>
<p>To drive the point home, we supply a visualization in the following video. All of the signals are inputs, and <code style='font-family: Arial'>===</code> is used to check instead of assign.</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/706568_f4fb9d3d127c4735a718deffbd9fed70/1080p/mp4/file.mp4)</p>
<p>The circuit in the video could have been written as:</p>
<pre style='font-family: Arial'><code class="language-solidity">z + y === x
x + y === u
</code></pre>
<p>with no change in meaning.</p>
<p>The arithmetic circuit <code style='font-family: Arial'>x === x + 1</code> does not mean increment <code style='font-family: Arial'>x</code>. It is an arithmetic circuit with no solution because x cannot be equal to <code style='font-family: Arial'>x + 1</code>. Thus, it is impossible to satisfy the constraint.</p>
<h3>Interpreting Arithmetic Circuits</h3>
<p>Consider the following circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">x₁(x₁ - 1) === 0
x₁x₂ === x₁
</code></pre>
<p>The first constraint <code style='font-family: Arial'>x₁(x₁ - 1) === 0</code> restricts the possible values x₁ to only 0 or 1. Any other value for <code style='font-family: Arial'>x₁</code> would not satisfy this constraint.</p>
<p>In the second constraint <code style='font-family: Arial'>x₁x₂ === x₁</code> we have two possible scenarios:</p>
<ul>
<li>If <code style='font-family: Arial'>x₁ = 1</code>, then <code style='font-family: Arial'>x₂</code> must also be 1, or the second constraint cannot be satisfied. If <code style='font-family: Arial'>x₁ = 1</code> and <code style='font-family: Arial'>x₂ ≠ 1</code>, then the second equation becomes <code style='font-family: Arial'>1 * x₂ === 1</code> which can only be satisfied by <code style='font-family: Arial'>x₂ = 1</code>, which creates a conflict.</li>
<li>If <code style='font-family: Arial'>x₁ = 0</code>, then <code style='font-family: Arial'>x₂</code> can have any value because <code style='font-family: Arial'>0x₂ === 0</code> is trivial to satisfy.</li>
</ul>
<p>The following assignments to <code style='font-family: Arial'>(x₁, x₂)</code> are all valid witnesses:</p>
<ul>
<li>$(x₁, x₂) = (1, 1)$</li>
<li>$(x₁, x₂) = (0, 2)$</li>
<li>$(x₁, x₂) = (0, 1337)$</li>
<li>$(x₁, x₂) = (0, 404)$</li>
</ul>
<p>Remember, a system of equation can have many solutions. Similarly, an arithmetic circuit can also have many solutions. Usually though, we’re only interested in verifying a given solution. We don’t need to find all solutions for an arithmetic circuit.</p>
<h3>Boolean vs Arithmetic Circuit</h3>
<p>The table below shows how Boolean circuits and arithmetic circuits differ, but keep in mind they serve the same purpose of validating a witness:</p>
<table>
<thead>
<tr>
<th>Boolean Circuit</th>
<th>Arithmetic Circuit</th>
</tr>
</thead>
<tbody>
<tr>
<td>Variables are 0, 1</td>
<td>Signals hold numbers</td>
</tr>
<tr>
<td>The only operations are AND, OR, NOT</td>
<td>The only operations are addition and multiplication</td>
</tr>
<tr>
<td>Satisfied when the output is true</td>
<td>Satisfied when the left hand side equals the right hand side for all equations (there is no output)</td>
</tr>
<tr>
<td>Witness is an assignment to the Boolean variables that satisfies the Boolean circuit</td>
<td>Witness is an assignment to the signals that satisfies all the equality constraints</td>
</tr>
</tbody>
</table>
<p>Aside from the convenience of using fewer variables in some circumstances, arithmetic circuits and Boolean circuits are tools that accomplish the same job — proving you have a witness to a problem in NP.</p>
<h3>Returning to the initial example a + b = c</h3>
<p>Let’s revisit our example above: writing a <em>Boolean</em> circuit to represent the equation <code style='font-family: Arial'>a + b = c</code>, where we’re given <code style='font-family: Arial'>c = 12</code>. For a Boolean circuit, we need to encode <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, and <code style='font-family: Arial'>c</code> in binary, which requires 4 bits each (in this example). In total, we have 12 inputs to the circuit. By comparison, the arithmetic circuit only requires 3 inputs: <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, and <code style='font-family: Arial'>c</code>. The reduction in the number of inputs and the overall circuit size is why we prefer using arithmetic circuits for ZK applications.</p>
<h3>Similarities between systems of equations and arithmetic circuits</h3>
<p>Boolean circuits always have one expression that returns true or false if the witness is satisfied.</p>
<p>For example, if we have a set of signals $x$, $y$, and $z$, and we wish to constrain the sum of $x$ and $y$ to be $5$, then we need a separate equation for that. Any way we wish to constrain z would have its own separate equation.</p>
<p>To demonstrate arithmetic circuits and Boolean circuits are equivalent, we will later show that any Boolean circuit can be transformed into an arithmetic circuit. This shows they can be used interchangeably for the purpose of demonstrating an agent has a witness to a problem in P or NP.</p>
<h3>All P problems are a subset of NP problems</h3>
<p>As discussed in the previous <a href="https://rareskills.io/post/p-vs-np">chapter on P vs NP</a>, <strong>all P problems are a subset of NP problems</strong> in terms of the computation requirements for validating a witness, so we will only refer to NP problems going forward, with the understanding that this includes P.</p>
<p>Our conclusion is that if any solution to a problem in NP can be modeled with a Boolean circuit, then any solution to a problem in NP (or P) can be modeled with an arithmetic circuit.</p>
<p>But before we demonstrate their equivalence, we will provide examples of modeling the solutions to problems in NP so we get an intuition for how arithmetic circuits are used.</p>
<h2>Examples of Arithmetic circuits</h2>
<p>In our first example, we redo our 3-coloring problem for Australia. In the second, we demonstrate how to use an arithmetic circuit to prove that a list is sorted.</p>
<h3>Example 1: Modeling 3-coloring with an Arithmetic Circuit</h3>
<p>When we used a Boolean circuit to model a 3-coloring, each territory had 3 Boolean variables – one for each color – indicating whether the country had been assigned that color. We then added constraints to force each territory to have exactly one color (color constraints) and constraints to enforce that adjacent territories did not get the same color (boundary constraints).</p>
<p>It’s easier to model this problem using arithmetic circuits because we can assign a single signal to each territory with the possible values $\set{1, 2, 3}$ to model their colors, instead of three Boolean variables. We can arbitrarily assign colors to the numbers, such as <code style='font-family: Arial'>blue = 1</code>, <code style='font-family: Arial'>red = 2</code>, and <code style='font-family: Arial'>green = 3</code>.</p>
<p>For each territory, we write the single color constraint as:</p>
<pre style='font-family: Arial'><code class="language-solidity">0 === (1 - x) * (2 - x) * (3 - x)
</code></pre>
<p>to enforce that each territory has exactly one color. The constraint above can only be satisfied if <code style='font-family: Arial'>x</code> is 1, 2, or 3.</p>
<p><strong>3-Coloring Australia</strong></p>
<p><img alt="3 coloring of Australia" src="assets/706568_b649d43396ef43cd954f4beb61dc1bc6_mv2.jpg" /></p>
<p>Recall that Australia has six territories:</p>
<ul>
<li><code style='font-family: Arial'>WA</code> = West Australia</li>
<li><code style='font-family: Arial'>SA</code> = South Australia</li>
<li><code style='font-family: Arial'>NT</code> = Northern Territory</li>
<li><code style='font-family: Arial'>Q</code> = Queensland</li>
<li><code style='font-family: Arial'>NSW</code> = New South Wales</li>
<li><code style='font-family: Arial'>V</code> = Victoria</li>
</ul>
<p>Saying <code style='font-family: Arial'>WA = 1</code> is equivalent to saying “Color West Australia Blue.” Similarly, <code style='font-family: Arial'>WA = 2</code> means “red” was assigned to West Australia, and <code style='font-family: Arial'>WA = 3</code> means “green” was assigned.</p>
<p>Our color constraint (constraining each territory to be blue, red or green) for each territory becomes:</p>
<pre style='font-family: Arial'><code class="language-solidity">1) 0 === (1 - WA) * (2 - WA) * (3 - WA)
2) 0 === (1 - SA) * (2 - SA) * (3 - SA)
3) 0 === (1 - NT) * (2 - NT) * (3 - NT)
4) 0 === (1 - Q) * (2 - Q) * (3 - Q)
5) 0 === (1 - NSW) * (2 - NSW) * (3 - NSW)
6) 0 === (1 - V) * (2 - V) * (3 - V)
</code></pre>
<p>We now want to enforce that neighboring territories do not have the same color. One way to accomplish this is to multiply the signals of the neighboring territory and ensure that the product is an “acceptable” one. Consider the following table for neighboring territories <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code>:</p>
<table>
<thead>
<tr>
<th>x</th>
<th>y</th>
<th>product</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>6</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>3</td>
<td>2</td>
<td>6</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>9</td>
</tr>
</tbody>
</table>
<p>If two signals (neighboring territories) have the same number (color), then their product will be one of $\set{1,4,9}$, the red numbers above. If <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> are constrained to be 1, 2, or 3, and <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> are not equal to each other, then the product <code style='font-family: Arial'>xy</code> will be one of $\set{2, 3, 6}$. Therefore, if <code style='font-family: Arial'>xy = 2</code> or <code style='font-family: Arial'>xy = 3</code> or <code style='font-family: Arial'>xy = 6</code>, we accept the assignment because it means the two neighbors have different colors.</p>
<p>For each neighboring territory <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code>, we can use the following constraint to enforce that they are not equal to each other:</p>
<pre style='font-family: Arial'><code class="language-solidity">0 === (2 - xy) * (3 - xy) * (6 - xy)
</code></pre>
<p>The above equation is satisfied if and only if the product <code style='font-family: Arial'>xy</code> is equal to 2, 3, or 6.</p>
<p>The boundary constraints are created by iterating through the borders and applying the boundary constraints between each pair of neigboring territories as the video below illustrates:</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/706568_71747f743e8e49c0955fa5de2f827ab4/1080p/mp4/file.mp4)</p>
<p>We now show the boundary constraints:</p>
<pre style='font-family: Arial'><code class="language-solidity">Western Australia and South Australia:
7) 0 === (2 - WA * SA) * (3 - WA * SA) * (6 - WA * SA)

Western Australia and Northern Territory
8) 0 === (2 - WA * NT) * (3 - WA * NT) * (6 - WA * NT)

Northern Territory and South Australia
9) 0 === (2 - NT * SA) * (3 - NT * SA) * (6 - NT * SA)

Northern Territory and Queensland
10) 0 === (2 - NT * Q) * (3 - NT * Q) * (6 - NT * Q)

South Australia and Queensland
11) 0 === (2 - SA * Q) * (3 - SA * Q) * (6 - SA * Q)

South Australia and New South Wales
12) 0 === (2 - SA * NSW) * (3 - SA * NSW) * (6 - SA * NSW)

South Australia and Victoria
13) 0 === (2 - SA * V) * (3 - SA * V) * (6 - SA * V)

Queensland and New South Wales
14) 0 === (2 - Q * NSW) * (3 - Q * NSW) * (6 - Q * NSW)

New South Wales and Victoria
15) 0 === (2 - NSW * V) * (3 - NSW * V) * (6 - NSW * V)
</code></pre>
<p>By combining the two, we see the complete arithmetic circuit for proving we have a valid 3-coloring for Australia:</p>
<pre style='font-family: Arial'><code class="language-solidity">// color constraints
0 === (1 - WA) * (2 - WA) * (3 - WA)
0 === (1 - SA) * (2 - SA) * (3 - SA)
0 === (1 - NT) * (2 - NT) * (3 - NT)
0 === (1 - Q) * (2 - Q) * (3 - Q)
0 === (1 - NSW) * (2 - NSW) * (3 - NSW)
0 === (1 - V) * (2 - V) * (3 - V)

// boundary constraints
0 === (2 - WA * SA) * (3 - WA * SA) * (6 - WA * SA)
0 === (2 - WA * NT) * (3 - WA * NT) * (6 - WA * NT)
0 === (2 - NT * SA) * (3 - NT * SA) * (6 - NT * SA)
0 === (2 - NT * Q) * (3 - NT * Q) * (6 - NT * Q)
0 === (2 - SA * Q) * (3 - SA * Q) * (6 - SA * Q)
0 === (2 - SA * NSW) * (3 - SA * NSW) * (6 - SA * NSW)
0 === (2 - SA * V) * (3 - SA * V) * (6 - SA * V)
0 === (2 - Q * NSW) * (3 - Q * NSW) * (6 - Q * NSW)
0 === (2 - NSW * V) * (3 - NSW * V) * (6 - NSW * V)
</code></pre>
<p>We have 15 constraints just like the Boolean circuit, <strong>but 1/3rd as many variables (signals).</strong> Instead of 3 Boolean variables for each territory, we have one signal for each territory. For larger circuits, this reduction in complexity and space can be substantial.</p>
<h3>Example 2: Proving a List is Sorted</h3>
<p>Given a list of numbers <code style='font-family: Arial'>[a₁, a₂, ..., aₙ]</code>, we say the list is “sorted” if <code style='font-family: Arial'>aₙ ≥ aₙ₋₁ ≥ … a₃ ≥ a₂ ≥ a₁</code>. In other words, going from the end to the beginning, the numbers are non-increasing.</p>
<p>Our goal is to write an arithmetic circuit that verifies the list is sorted.</p>
<p>To do this, we need an arithmetic circuit that expresses <code style='font-family: Arial'>a ≥ b</code> for two signals. This turns out to be more complicated than it would seem at first glance because arithmetic circuits only allow for equality, addition, and multiplication, not comparison.</p>
<p>But let’s say we had such a “greater than or equal to” circuit – call it <code style='font-family: Arial'>GTE(a,b)</code>. Then we would construct the circuits for comparing each pair of consecutive list elements: <code style='font-family: Arial'>GTE(aₙ, aₙ₋₁), ..., GTE(a₃, a₂), GTE(a₂, a₁)</code>, and if all of them are satisfied, then the list is sorted.</p>
<p>To compare two decimal numbers without the $≥$ operator, we first need an arithmetic circuit that validates a proposed binary representation for the number, so we first go on a small detour about binary numbers.</p>
<h3>Prerequisite: Binary encoding</h3>
<p>We write binary numbers with the subscript 2. For example, 11₂ is 3 and 101₂ is 5. Each of the 1s and 0s is called a bit. We say the left-most bit is the most significant bit (MSB) and the right-most bit is the least significant bit (LSB).</p>
<p>As we will show shortly, during conversion to decimal, the most significant bit is multiplied by the largest coefficient and the least significant bit is multiplied by the smallest coefficient. So if we write a four bit binary number as <code style='font-family: Arial'>b₃b₂b₁b₀</code>, <code style='font-family: Arial'>b₃</code> is the MSB and <code style='font-family: Arial'>b₀</code> is the LSB.</p>
<p>The video below illustrate the conversion of 1101₂ to 13:</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/706568_e4cf36f8de2d401b94370b279f411b4b/720p/mp4/file.mp4)</p>
<p>As shown in the video, a four bit binary number can be converted to a decimal number <code style='font-family: Arial'>v</code> with the following formula:</p>
<p><code style='font-family: Arial'>v = 8b₃ + 4b₂ + 2b₁ + b₀</code></p>
<p>This could also be written as:</p>
<p><code style='font-family: Arial'>v = 2³b₃ + 2²b₂ + 2¹b₁ + 2⁰b₀</code></p>
<p>For example, 1001₂ = 9, 1010₂ = 10, and so forth. For a general <code style='font-family: Arial'>n</code> bit binary number, the conversion is:</p>
<p><code style='font-family: Arial'>v = 2ⁿ⁻¹b₃ + ... + 2¹b₁ + 2⁰b₀</code></p>
<p>We omit the discussion on how to convert a decimal number to binary. For now, if the reader wishes to convert to binary, they can use the built-in <code style='font-family: Arial'>bin</code> function from Python:</p>
<pre style='font-family: Arial'><code class="language-solidity">&gt;&gt;&gt; bin(3)
'0b11'
&gt;&gt;&gt; bin(9)
'0b1001'
&gt;&gt;&gt; bin(10)
'0b1010'
&gt;&gt;&gt; bin(1337)
'0b10100111001'
&gt;&gt;&gt; bin(404)
'0b110010100'
</code></pre>
<p>We can create an arithmetic circuit that asserts “<code style='font-family: Arial'>v</code> is a decimal number with a four bit binary representation <code style='font-family: Arial'>b₃</code>, <code style='font-family: Arial'>b₂</code>, <code style='font-family: Arial'>b₁</code>, <code style='font-family: Arial'>b₀</code>” by using the following circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">8b₃ + 4b₂ + 2b₁ + b₀ === v

// force the &quot;bits&quot; to be zero or one
b₀(b₀ - 1) === 0
b₁(b₁ - 1) === 0
b₂(b₂ - 1) === 0
b₃(b₃ - 1) === 0
</code></pre>
<p>The signals <code style='font-family: Arial'>b₃, b₂, b₁, b₀</code> are constrained to be the binary representation of <code style='font-family: Arial'>v</code>. If <code style='font-family: Arial'>b₃, b₂, b₁, b₀</code> aren’t binary, or aren’t the binary representation of <code style='font-family: Arial'>v</code>, then the circuit cannot be satisfied.</p>
<p>Observe that <strong>there is no satisfying assignment to the signals <code style='font-family: Arial'>(v, b₃, b₂, b₁, b₀)</code> where <code style='font-family: Arial'>v &gt; 15</code>.</strong> That is, if we set <code style='font-family: Arial'>b₃, b₂, b₁, b₀</code> to all 1, the highest the constraints allow, then the sum will be 15. It is not possible to add to anything higher. In ZK, this is sometimes called a <em>range check</em> on <code style='font-family: Arial'>v</code>. Not only does the circuit above demonstrate the binary representation of <code style='font-family: Arial'>v</code>, it also forces <code style='font-family: Arial'>v &lt; 16</code>.</p>
<p>We can generalize this to the following circuit which constrains $v &lt; 2^n$ and also gives us the binary representation of <code style='font-family: Arial'>v</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">2ⁿ⁻¹bₙ₋₁ +...+ 2²b₂ + 2¹b₁ + b₀ === v
b₀(b₀ - 1) === 0
b₁(b₁ - 1) === 0
//...
bₙ₋₁(bₙ₋₁ - 1) === 0
</code></pre>
<p><strong>Saying a number <code style='font-family: Arial'>v</code> is encoded with at most <code style='font-family: Arial'>n</code> bits is equivalent to saying $v &lt; 2^n$.</strong></p>
<p>To get a sense for how $2^n$ changes as a function of $n$, consider the following table:</p>
<table>
<thead>
<tr>
<th>n bits</th>
<th>max value (binary)</th>
<th>max value (decimal)</th>
<th>2ⁿ (decimal)</th>
<th>2ⁿ (binary)</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>11₂</td>
<td>3</td>
<td>4</td>
<td>100</td>
</tr>
<tr>
<td>3</td>
<td>111₂</td>
<td>7</td>
<td>8</td>
<td>1000</td>
</tr>
<tr>
<td>4</td>
<td>1111₂</td>
<td>15</td>
<td>16</td>
<td>10000</td>
</tr>
<tr>
<td>5</td>
<td>11111₂</td>
<td>31</td>
<td>32</td>
<td>100000</td>
</tr>
</tbody>
</table>
<p>Note that the number $2^n$ in binary requires 1 more bit to store than the value $2^n – 1$. By constraining the number of bits a number is encoded with to $n$ bits, it forces that number to be less than $2^n$.</p>
<p>It’s helpful to remember the relationship between powers of $2$ and the number of bits required to store them.</p>
<ul>
<li>$2^n$ requires $n + 1$ bits to store. For example, $2^0=1_2$, $2^1 = 10_2$, $2^2=100_2$, $2^3=1000_2$ and so forth.</li>
<li>$2^{n-1}$ is half of $2^n$ and requires $n$ bits to store</li>
<li>$2^n − 1$ requires $n$ bits to store. It is the maximum value we can store with $n$ bits, when all the bits are set to $1$.</li>
</ul>
<p>If we take a number $n$ and compute $2^n$, we get an $n + 1$ bit number with the most significant bit being 1, and the rest zero. $n = 3$ in the examples below:</p>
<p>$$<br />
2^n=\underbrace{1000}_{n+1\space bits}<br />
$$</p>
<p>$2^{n-1}$ is the same as $2^n / 2$. Since it is written as 2 to some power, it still has the same “shape” of a binary number with an MSB of 1 and the rest zero, but it will require $n$ bits to encode it instead of $n + 1$ bits.</p>
<p>$$<br />
2^{n-1}=\underbrace{100}_{n\space bits}<br />
$$</p>
<p>$2^n −1$ is an $n$ bit number with all the bits set to one.</p>
<p>$$<br />
2^n-1=\underbrace{111}_{n\space bits}<br />
$$</p>
<h3>Compute ≥ in binary</h3>
<p>If we are working with binary numbers of a fixed size, $n$ bits, the number $2^{n-1}$ is special because we can easily assert an $n$ bit binary number is greater than or equal to $2^{n-1}$ — or less than it. We call $2^{n-1}$ the “midpoint.” The video below illustrates how to compare the size of an $n$ bit number to $2^{n-1}$:</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/706568_adae25cac0e6414ab0643a5792a2ed52/1080p/mp4/file.mp4)</p>
<p>By checking the most significant bit of an $n$ bit number, we can tell if that number is greater than or equal to $2^{n-1}$ or less than $2^{n-1}$.</p>
<p>If we compute $2^{n-1} + \Delta$ and look at the most significant bit of that sum, we can quickly tell if $\Delta$ is positive or negative. If $\Delta$ is negative, then $2^{n-1} + \Delta$ must be less than $2^{n-1}$.</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/706568_6b61fecfedb64a888f6538bc91707f40/1080p/mp4/file.mp4)</p>
<h3>Detecting if $u \ge v$</h3>
<p>If we replace $\Delta$ with $u – v$ then the most significant bit of $2^{n-1} + (u – v)$ tells us if $u ≥ v$ or $u &lt; v$.</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/706568_ea57bc6fb8c5493686c3dc4cf9123c72/1080p/mp4/file.mp4)</p>
<h4>Preventing overflow in $2^{n-1} + (u – v)$</h4>
<p>If we restrict $u$ and $v$ to be represented with at most $n – 1$ bits, while $2^{n-1}$ is represented with $n$ bits, then underflow and overflow cannot occur. When both $u$ and $v$ are represented with at most $n – 1$ bits, the maximum absolute value of $|u – v|$ is an $n – 1$ bit number.</p>
<p>We see that $2^{n-1} + (u – v)$ cannot underflow in this case, because $2^{n-1}$ is at least 1 bit larger than $|u – v|$.</p>
<p>Now consider the overflow case. Without loss of generality, for $n = 4$, i.e. four bit numbers, the midpoint is $2^{n-1} = 2^{4-1} = 8$ or $1000_2$. The maximum value $|u – v|$ can hold in this case, as a 3-bit number, is $111_2$. Adding $1000_2 + 111_2$ results in $1111_2$, which is not an overflow.</p>
<h3>Summary of the arithmetic circuit for $u ≥ v$, when $u$ and $v$ are $n – 1$ bit numbers</h3>
<ul>
<li>We constrain $u$ and $v$ to be at most $n – 1$ bit numbers.</li>
<li>We create an arithmetic circuit that encodes the binary representation of $2^{n-1} + (u – v)$ using $n$ bits.</li>
<li>If the most significant bit of $2^{n-1} + (u – v)$ is 1, then $u \geq v$ and vice versa.</li>
</ul>
<p>The final arithmetic circuit to check if $u \geq v$ is as follows. We fix $n = 4$ which means $u$ and $v$ must be constrained to be 3-bit numbers. The interested reader can generalize this to other values of $n$:</p>
<pre style='font-family: Arial'><code class="language-solidity">// u and v are represented with at most 3 bits:
2²a₂ + 2¹a₁ + a₀ === u
2²b₂ + 2¹b₁ + b₀ === v

// 0 1 constraints for aᵢ, bᵢ
a₀(a₀ - 1) === 0
a₁(a₁ - 1) === 0
a₂(a₂ - 1) === 0
b₀(b₀ - 1) === 0
b₁(b₁ - 1) === 0
b₂(b₂ - 1) === 0

// 2ⁿ⁻¹ + (u - v) binary representation
2³ + (u - v) === 8c₃ + 4c₂ + 2c₁ + c₀

// 0 1 constraints for cᵢ
c₀(c₀ - 1) === 0
c₁(c₁ - 1) === 0
c₂(c₂ − 1) === 0
c₃(c₃ − 1) === 0

// Check that the MSB is 1
c₃ === 1
</code></pre>
<h3>Asserting a list is sorted</h3>
<p>Now that we have an arithmetic circuits to compare pairs of signals, we repeat this circuit for each sequential pair in the list and verify it is sorted.</p>
<h2>Summary of examples</h2>
<p>We’ve shown how we can create an arithmetic circuit modeling the solution to the problems from the previous chapter.</p>
<p>We can now generalize this to say we can model any problem in NP using an arithmetic circuit.</p>
<h2>How a Boolean circuit can be modeled with an arithmetic circuit</h2>
<p>Any Boolean circuit can be modeled using an arithmetic circuit. This means we can define a process for converting a Boolean circuit B into an arithmetic circuit A, such that a set of inputs that satisfy B can be translated into a set of signals that satisfy A. Below, we outline the key components of this process and walk through an example of converting a specific Boolean circuit into an arithmetic circuit.</p>
<p>Suppose we have the following Boolean formula: <code style='font-family: Arial'>out = (x ∧ ¬ y) ∨ z</code>. This formula is true if (<code style='font-family: Arial'>x</code> is true AND <code style='font-family: Arial'>y</code> is false) OR <code style='font-family: Arial'>z</code> is true.</p>
<p>We encode <code style='font-family: Arial'>x</code>, <code style='font-family: Arial'>y</code>, and <code style='font-family: Arial'>z</code> as arithmetic circuit signals and constrain them to have values 0 or 1.</p>
<p>The following arithmetic circuit can only be satisfied if <code style='font-family: Arial'>x</code>, <code style='font-family: Arial'>y</code>, and <code style='font-family: Arial'>z</code> are each 0 or 1.</p>
<pre style='font-family: Arial'><code class="language-solidity">x(x - 1) === 0
y(y - 1) === 0
z(z - 1) === 0
</code></pre>
<p>Now let’s show how to map Boolean circuit operators to arithmetic circuit operators, assuming that the input variables have been constrained to be 0 or 1.</p>
<h3>AND gate</h3>
<p>We translate the Boolean AND <code style='font-family: Arial'>t = u ∧ v</code> into an arithmetic circuit as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">u(u - 1) === 0
v(v - 1) === 0
t === uv
</code></pre>
<p><code style='font-family: Arial'>t</code> will only be 1 if both <code style='font-family: Arial'>u</code> and <code style='font-family: Arial'>v</code> are 1, hence this arithmetic circuit models an AND gate. Because of the constraints <code style='font-family: Arial'>u(u - 1) = 0</code> and <code style='font-family: Arial'>v(v - 1) = 0</code>, <code style='font-family: Arial'>t</code> can only be 0 or 1.</p>
<h3>NOT gate</h3>
<p>We translate the Boolean NOT <code style='font-family: Arial'>t = ¬u</code> into an arithmetic circuit as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">u(u - 1) === 0
t === 1 - u
</code></pre>
<p><code style='font-family: Arial'>t</code> is 1 when <code style='font-family: Arial'>u</code> is 0 and vice versa. Because of the constraint <code style='font-family: Arial'>u(u - 1) === 0</code>, <code style='font-family: Arial'>t</code> can only be 0 or 1.</p>
<h3>OR gate</h3>
<p>We translate the Boolean OR <code style='font-family: Arial'>t === u ∨ v</code> into an arithmetic circuit as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">u(u - 1) === 0
v(v - 1) === 0
t === u + v - uv
</code></pre>
<p>To see why it models the OR gate, consider the following table:</p>
<table>
<thead>
<tr>
<th>u</th>
<th>v</th>
<th>u + v</th>
<th>uv</th>
<th>t (u + v – uv)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>If <code style='font-family: Arial'>u</code> or <code style='font-family: Arial'>v</code> are 1, then <code style='font-family: Arial'>t</code> will be at least 1. To prevent <code style='font-family: Arial'>t</code> from equaling 2 (which is an invalid output from a Boolean operator), we subtract <code style='font-family: Arial'>uv</code>, which will be 1 when both <code style='font-family: Arial'>u</code> and <code style='font-family: Arial'>v</code> are 1.</p>
<p>Observe that with all the gates above, we do not need to apply a constraint <code style='font-family: Arial'>t(t - 1) === 0</code>. The output <code style='font-family: Arial'>t</code> is implicitly constrained to be 0 or 1 because there is no assignment to the inputs that could result in a value 2 or greater for <code style='font-family: Arial'>t</code>.</p>
<h3>Transforming <code style='font-family: Arial'>out = (x ∧ ¬ y) ∨ z</code> into an arithmetic circuit</h3>
<p>Now that we’ve seen how to translate all the allowed operations of Boolean circuits into arithmetic circuits, let’s look at an example of converting a Boolean circuit into an arithmetic one.</p>
<h3>Create the 0 1 constraints</h3>
<pre style='font-family: Arial'><code class="language-solidity">x(x - 1) === 0
y(y - 1) === 0
z(z - 1) === 0
</code></pre>
<h3>Replace <code style='font-family: Arial'>¬ y</code> with the the arithmetic circuit for NOT</h3>
<p>out = (x ∧ ¬ y) ∨ z</p>
<p>out = (x ∧ (1 – y)) ∨ z</p>
<h3>Replace <code style='font-family: Arial'>∧</code> with the arithmetic circuit for AND</h3>
<p>out = (x ∧ (1 – y)) ∨ z</p>
<p>out = (x(1 – y)) ∨ z</p>
<h3>Replace <code style='font-family: Arial'>∨</code> with the arithmetic circuit for OR</h3>
<p>out = (x(1 – y)) ∨ z</p>
<p>out = (x(1 – y)) + z – (x(1 – y))z</p>
<p>Our final arithmetic circuit for <code style='font-family: Arial'>out = (x ∧ ¬ y) ∨ z</code> is:</p>
<pre style='font-family: Arial'><code class="language-solidity">x(x - 1) === 0
y(y - 1) === 0
z(z - 1) === 0
out === (x(1 - y)) + z - (x(1 - y))z
</code></pre>
<p>If desired, we could simpify the last equation:</p>
<pre style='font-family: Arial'><code class="language-solidity">out === (x(1 - y)) + z - ((x(1 - y))z)
out === x - xy + z - ((x - xy)z)
out === x - xy + z - (xz - xyz)

out === x - xy + z - xz + xyz
</code></pre>
<p>We could also write the arithmetic circuit as follows with no change in meaning:</p>
<pre style='font-family: Arial'><code class="language-solidity">x² === x
y² === y
z² === z
out === x - xy + z - xz + xyz
</code></pre>
<h2>Summary</h2>
<p><strong>If the solution to every problem in NP can be modeled with a Boolean circuit and every Boolean circuit can be transformed into an equivalent arithmetic circuit, then it follows that the solution to every problem in NP can be modeled with an arithmetic circuit.</strong></p>
<p>In practice, ZK developers prefer to use arithmetic circuits over Boolean circuits because, as shown in the examples above, they generally require fewer variables to accomplish the same task.</p>
<p>There is no need to calculate a Boolean circuit and then transform it into an arithmetic circuit. We can model the solution to the NP problem with an arithmetic circuit directly.</p>
<h2>Next steps</h2>
<p>We have glossed over two very important details in this article. Some other challenges exist that need to be addressed. For example:</p>
<ul>
<li>We didn’t discuss what datatype we used to store signals for the arithmetic circuit and how we handle overflow during addition or multiplication.</li>
<li>We have no way of expressing the value 2/3 without losing precision. Any fixed point or floating point representation we choose will have rounding issues</li>
</ul>
<p>To handle these problems, arithmetic circuits are calculated over <em><a href="https://rareskills.io/post/finite-fields">finite fields</a>:</em> a branch of mathematics where all addition and multiplication is done modulo a prime number.</p>
<p>Finite field arithmetic has some surprising differences from regular arithmetic introduced by the modulo operator, so the next chapter will explore them in detail.</p>
<h2>Learn more with RareSkills</h2>
<p>Learn more about <a href="https://rareskills.io/zk-book">Zero Knowledge Proofs</a> in our free ZK Book. This tutorial is a chapter in that book.</p>
<h2>Practice Problems</h2>
<ol>
<li>Create an arithmetic circuit that takes signals <code style='font-family: Arial'>x₁</code>, <code style='font-family: Arial'>x₂</code>, …, <code style='font-family: Arial'>xₙ</code> and is satisfied if <em>at least</em> one signal is 0.</li>
<li>Create an arithmetic circuit that takes signals <code style='font-family: Arial'>x₁</code>, <code style='font-family: Arial'>x₂</code>, …, <code style='font-family: Arial'>xₙ</code> and is satsified if all signals are 1.</li>
<li>A bipartite graph is a graph that can be colored with two colors such that no two neighboring nodes share the same color. Devise an arithmetic circuit scheme to show you have a valid witness of a 2-coloring of a graph. Hint: the scheme in this tutorial needs to be adjusted before it will work with a 2-coloring.</li>
<li>Create an arithmetic circuit that constrains <code style='font-family: Arial'>k</code> to be the maximum of <code style='font-family: Arial'>x</code>, <code style='font-family: Arial'>y</code>, or <code style='font-family: Arial'>z</code>. That is, <code style='font-family: Arial'>k</code> should be equal to <code style='font-family: Arial'>x</code> if <code style='font-family: Arial'>x</code> is the maximum value, and same for <code style='font-family: Arial'>y</code> and <code style='font-family: Arial'>z</code>.</li>
<li>Create an arithmetic circuit that takes signals <code style='font-family: Arial'>x₁</code>, <code style='font-family: Arial'>x₂</code>, …, <code style='font-family: Arial'>xₙ</code>, constrains them to be binary, and outputs 1 if <em>at least</em> one of the signals is 1. Hint: this is tricker than it looks. Consider combining what you learned in the first two problems and using the NOT gate.</li>
<li>Create an arithmetic circuit to determine if a signal <code style='font-family: Arial'>v</code> is a power of two (1, 2, 4, 8, etc). Hint: create an arithmetic circuit that constrains another set of signals to encode the binary representation of <code style='font-family: Arial'>v</code>, then place additional restrictions on those signals.</li>
<li>Create an arithmetic circuit that models the <a href="https://en.wikipedia.org/wiki/Subset_sum_problem">Subset sum problem</a>. Given a set of integers (assume they are all non-negative), determine if there is a subset that sums to a given value $k$. For example, given the set $\set{3,5,17,21}$ and $k = 22$, there is a subset $\set{5, 17}$ that sums to $22$. Of course, a subset sum problem does not necessarily have a solution.</li>
</ol>
<p>Hint</p>
<p>Use a “switch” that is 0 or 1 if a number is part of the subset or not.<br />
8. The covering set problem starts with a set $S = \set{1, 2, …, 10}$ and several well-defined subsets of $S$, for example: $\set{1, 2, 3}$, $\set{3, 5, 7, 9}$, $\set{8, 10}$, $\set{5, 6, 7, 8}$, $\set{2, 4, 6, 8}$, and asks if we can take at most $k$ subsets of $S$ such that their union is $S$. In the example problem above, the answer for $k = 4$ is true because we can use $\set{1, 2, 3}$, $\set{3, 5, 7, 9}$, $\set{8, 10}$, $\set{2, 4, 6, 8}$. Note that for each problems, the subsets we can work with are determined at the beginning. We cannot construct the subsets ourselves. If we had been given the subsets $\set{1,2,3}$, $\set{4,5}$ $\set{7,8,9,10}$ then there would be no solution because the number $6$ is not in the subsets.</p>
<p>On the other hand, if we had been given $S = \set{1,2,3,4,5}$ and the subsets $\set{1}, \set{1,2}, \set{3, 4}, \set{1, 4, 5}$ and asked can it be covered with $k = 2$ subsets, then there would be no solution. However, if $k = 3$ then a valid solution would be $\set{1, 2}, \set{3, 4}, \set{1, 4, 5}$.</p>
<p>Our goal is to prove for a given set $S$ and a defined list of subsets of $S$, if we can pick a set of subsets such that their union is $S$. Specifically, the question is if we can do it with $k$ or fewer subsets. We wish to prove we know which $k$ (or fewer) subsets to use by encoding the problem as an arithmetic circuit.</p>
<p><em>Originally Published April 23, 2024</em></p>
<div style='page-break-after: always;'></div>

<h1>Finite Fields and Modular Arithmetic for ZK Proofs</h1>
<p>Source: https://rareskills.io/post/finite-fields</p>
<h1>Finite Fields and Modular Arithmetic for ZK Proofs</h1>
<p><em>This article is the third in a series. We present finite fields in the context of circuits for zero-knowledge proofs. The previous chapters are <a href="https://www.rareskills.io/post/p-vs-np">P vs NP and its Application to Zero Knowledge Proofs</a> and <a href="https://www.rareskills.io/post/arithmetic-circuit">Arithmetic Circuits</a>.</em></p>
<p>In the previous chapter on arithmetic circuits, we pointed out a limitation that we cannot encode the number $2/3$ because it cannot be represented precisely using binary. We also pointed out that we didn’t explicitly have a way to handle overflow.</p>
<p>Both of these issues can be handled seamlessly with a variant of arithmetic, which is popular in general cryptography, called <em>finite fields.</em></p>
<h2>Finite fields</h2>
<p>Given a prime number <code style='font-family: Arial'>p</code>, we can make a finite field with <code style='font-family: Arial'>p</code> elements by taking the set of integers $\set{0, 1, 2, …, p-1}$ and define addition and multiplication to be done modulo $p$. We’ll start by limiting ourselves to fields where the number of elements is a prime.</p>
<p>For example, if the prime number $p$ is $7$, then the elements in the finite field are $\set{0, 1, 2, 3, 4, 5, 6}$. Any number outside this range ($≥ p$ or $&lt; 0$) is always mapped to an "equivalent" number in this range using modulo. The technical word for "equivalent" is <em>congruent</em>.</p>
<p>Modulo calculates the remainder when dividing the number by the prime number. For example, if our modulo is 7, the number 12 is <em>congruent</em> to 5 i.e. $12 \pmod 7 = 5$, and the number 14 is congruent to 0. Similarly, when we add two numbers, say 3 + 5, the resulting sum of 8 is congruent to 1 (8 mod 7 = 1). The animation below illustrates this:</p>
<p><img alt="Number line gif" src="assets/706568_27817a32acf7429ca035667488ebce27_mv2.gif" /></p>
<p>In Python, the calculation shown above can be computed as:</p>
<pre style='font-family: Arial'><code class="language-solidity">p = 7
result = (3 + 5) % p
print(result) # prints 1
</code></pre>
<p>In this chapter, whenever we perform calculations, we’ll express our result as a number in the range of $0…(p-1)$, which is the set of elements in our finite field <code style='font-family: Arial'>p</code>. For example, 2 * 5 = 10, which we “simplify” to 3 (modulo 7).</p>
<p>Note how 3 + 5 "overflowed" the limit of 6. <strong>In finite fields, overflow is not a bad thing, we define the overflowing behavior to be part of the calculation.</strong> In a finite field modulo 7, 5 + 3 is defined to be 1.</p>
<p>Underflows are handled also similarly. For example, $3 – 5 = – 2$, but in modulo 7 we get $5$ because $7 – 2 = 5$.</p>
<h2>How modular arithmetic works</h2>
<p>In a typical programming language, we write addition in a finite field as <code style='font-family: Arial'>(6 + 1) % 7 == 0</code>, but in mathematical notation, we typically say</p>
<p>$$0 = 6 + 1 \pmod 7$$</p>
<p>Or more generally,</p>
<p>$$c = a + b \pmod p$$</p>
<p>where $a$ and $b$ are numbers in the finite field, $c$ is the remainder that maps any number $≥ p$ and $&lt; 0 $ back in the set $\set{0, 1, …, p - 1}$.</p>
<p>The notation $\pmod p$ means <em>all</em> arithmetic is done modulo $p$. For example,</p>
<p>$$a + b = c + d \pmod p$$</p>
<p>Is equivalent (in Python or C) to <code style='font-family: Arial'>(a + b) % p == (c + d) % p</code>.</p>
<p>Multiplication works similarly by multiplying the numbers together, then taking the modulus:</p>
<p>$$3 = 4 × 6 \pmod 7 = 24 \pmod 7 = 3$$</p>
<p>The multiplication operation above can be visualized in two ways:</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/706568_30c0c24e6c344dd786b813c99a76bb50/1080p/mp4/file.mp4)</p>
<p>Or alternatively:</p>
<p>[</p>
<p>](https://video.wixstatic.com/video/706568_011ac2a6b52a48c5a5ffc6221df889a3/1080p/mp4/file.mp4)</p>
<p>We refer to a number in a finite field as an "element."</p>
<h2>$p$ and the order of the field</h2>
<p>The number we take the modulus with, we will call $p$. In all our examples it is a prime number. In the broader field of mathematics, $p$ might not necessarily be prime, but we will only concern ourselves with cases where $p$ is prime.</p>
<p>Because of this restriction, the <em>order</em> of the field is always equal to $p$. The <em>order</em> is the number of elements in the field. The general term for the number we take the modulus with is the <em>characteristic</em> of the field.</p>
<p>For example, if we have $p = 5$, the elements are $\set{0, 1, 2, 3, 4}$. There are 5 elements, so the order of that field is 5.</p>
<h2>The $p$ addition identity</h2>
<p>Any element plus $p$ is the same element. For example, $(3 + 7) \pmod 7 = 3$. Consider the examples in the following animation:</p>
<p><img alt="Number Line animation showing 3 + 7 = 3 (mod 7)" src="assets/706568_bb147f17337d49788e9e74718b23dcd5_mv2.gif" /></p>
<h2>Additive Inverse</h2>
<p>Consider that $5 + (-5) = 0$.</p>
<p>In mathematics, the additive inverse of $a$ is a number $b$ such at $a + b = 0$. Generally, we express the additive inverse of a number by placing a negative sign in front. $-a$ is, by definition, the number that when added with $a$ gives 0.</p>
<h3>General rules of additive inverses</h3>
<ul>
<li>Zero is its own additive inverse.</li>
<li>Every number has exactly one additive inverse</li>
</ul>
<p>These additive inverse rules apply to finite fields also. Although we don’t have elements with negative signs like $-5$, some elements can “behave” like negative numbers with respect to each other using the modulo operator.</p>
<p>In regular arithmetic, $-5$ is the number that, when added to $5$, results in $0$. If our finite field is $p = 7$, then $2$ can be considered to be $-5$ because $(5 + 2) \pmod 7 = 0$. Similarly, $5$ can be considered to be $-2$ because they are each other’s additive inverse. To be precise, $-2$ is congruent to $5$ or $-2 \equiv 5 \pmod 7$.</p>
<p>To compute the additive inverse of an element, simply compute $p – a$ where $a$ is the element we are trying to find the additive inverse of. For example, to find the additive inverse of 14 modulo 23, we compute $23 – 14 = 9$. We can see $14 + 9 \pmod {23} = 0$. $p$ is congruent to zero, so this is equivalent to computing $-5$ as $0 – 5$.</p>
<p>Just like with real numbers:</p>
<ul>
<li>every element in a finite field has exactly one additive inverse</li>
<li>zero is its own additive inverse.</li>
</ul>
<p>The general pattern for additive inverses in a finite field is that the elements in the first half of the finite field are the additive inverses of the elements in the second half, as show in the figure below. Zero is the exception since it is its own additive inverse. The numbers connected by the green line are each other’s additive inverse in the field $p = 7$:</p>
<p><img alt="Image showing the additive inverse relationship" src="assets/706568_b60115959b634536b3ddfc7b8461625b_mv2.jpg" /></p>
<p><strong>Exercise:</strong> Let’s say we pick a $p \geq 3$. Which, if any, non-zero values are their own additive inverse?</p>
<h2>Multiplicative inverse</h2>
<p>Consider that $5 × (1/5) = 1$.</p>
<p>The multiplicative inverse for $a$ is a number $b$ such that $ab = 1$. Every element except zero has a multiplicative inverse. With "regular numbers," a multiplicative inverse is $1 / \text{num}$. For example, the multiplicative inverse of $5$ is $1/5$, and the multiplicative inverse of $19$ is $1/19$.</p>
<p>Although we do not have fractional numbers in finite fields, we can still find pairs of numbers that "behave like" fractions when added or multiplied together.</p>
<p>For example, the multiplicative inverse of $4$ in the finite field $p = 7$ is $2$, because $4 * 2 = 8$, and $8 \pmod 7 = 1$. Thus, $2$ "behaves" like $1/4$ in the finite field, or to be precise, $1/4$ is congruent to $2 \pmod 7$.</p>
<h3>General rules of multiplicative inverses in finite field arithmetic:</h3>
<ul>
<li>0 does not have a multiplicative inverse</li>
<li>1 is its own multiplicative inverse</li>
<li>Every number (except 0) has exactly one multiplicative inverse (which could be itself)</li>
<li>the element of value $(p – 1)$ is its own multiplicative inverse. For example in a finite field of $p = 103$, $1$ and $102$ are their own multiplicative inverses. In a finite field of $p = 23$, $1$ and $22$ are their own multiplicative inverses (the reason why is explained in the upcoming section). Another example: in the finite field modulo 5, 1 is its own inverse, and 4 is its own inverse. $4 \times 4 = 16$, and $16 \pmod 5 = 1$.</li>
</ul>
<h3>Why element of value $p – 1$ is its own multiplicative inverse</h3>
<p>When we multiply $-1$ by itself, we get $1$. Therefore, $-1$ is its own multiplicative inverse for real numbers. The element of value $(p – 1)$ is congruent to $-1$. Therefore, we expect $(p – 1)$ to be its own multiplicative inverse, and indeed that is the case.</p>
<p>As another way of seeing why $p – 1$ is its own multiplicative inverse, consider that $(p – 1)(p – 1) = p² – 2p + 1$. Since $p$ is congruent to $0$, then $p² – 2p + 1$ simplifies to $0^2 – 2*0 + 1 = 1$.</p>
<h4>Solutions for arithmetic circuits over finite fields</h4>
<p>If we consider the following arithmetic circuit over regular numbers, we expect <code style='font-family: Arial'>x = -1</code> to be only satisfying assignment.</p>
<pre style='font-family: Arial'><code class="language-solidity">x * x === 1
x + 1 === 0
</code></pre>
<p>In a finite field, the satisfying assignment is the element congruent to $-1$, or $p – 1$.</p>
<h3>Computing the multiplicative inverse with Fermat’s Little Theorem</h3>
<p><a href="https://en.wikipedia.org/wiki/Fermat%27s_little_theorem">Fermat’s Little Theorem</a> states that</p>
<p>$$<br />
a^{p}=a\pmod p, a\neq0$$</p>
<p>In other words, if you raise a non-zero element to the $p$, you get that element back. Some examples:</p>
<ul>
<li>$3⁵ \pmod 5 = 3$</li>
<li>$4⁷ \pmod 7 = 4$</li>
<li>$14⁵³ \pmod {53} = 14$</li>
</ul>
<p>Now consider if we divide both sides of $aᵖ = a \pmod p$ by $a$ (remember, $a ≠ 0$):</p>
<p>$$a^p=a$$<br />
$$frac{a^p}{a}=frac{a}{a}$$<br />
$$a^{p-1}=1$$</p>
<p>We can write the result as</p>
<p>$$<br />
a(a^{p-2}) = 1$$</p>
<p><strong>This means that $aᵖ⁻² \pmod p$ is the multiplicative inverse of $a$</strong>, since $a$ times $aᵖ⁻²$ is 1.</p>
<p>Some examples:</p>
<ul>
<li>The multiplicative inverse of $3$ in the finite field $p = 7$ is $5$. $3⁷⁻² = 5 \pmod 7$</li>
<li>The multiplicative inverse of 8 in the finite field $p = 11$ is $7$. $8¹¹⁻² = 7 \pmod {11}$</li>
</ul>
<p>The advantage of this approach is we can use the <code style='font-family: Arial'>expmod</code> <a href="https://www.rareskills.io/post/solidity-precompiles">precompile in Ethereum</a> to compute the modular inverse in a smart contract.</p>
<p>In practice, this is not an ideal way to compute multiplicative inverses because raising a number to a large power is computationally expensive. Libraries that compute the multiplicative inverse use more efficient algorithms under the hood. However, when such a library is not a available, and you want a quick and simple solution, and computing a large exponent is not excessively costly, Fermat’s Little Theorem can be used.</p>
<h2>Computing the multiplicative inverse with Python</h2>
<p>Using Python 3.8 or later, we can do <code style='font-family: Arial'>pow(a, -1, p)</code> to compute the multiplicative inverse of <code style='font-family: Arial'>a</code> in the finite field <code style='font-family: Arial'>p</code>. The first argument to <code style='font-family: Arial'>pow</code> is the base, the second is the exponent, and the third is the modulus.</p>
<p>Example:</p>
<pre style='font-family: Arial'><code class="language-solidity">p = 17
print(pow(5, -1, p))
# 7
assert (5 * 7) % p == 1
</code></pre>
<p><strong>Exercise:</strong> Find the multiplicative inverse of 3 modulo 5. There are only 5 possibilities, so try all of them and see which ones work.</p>
<p><strong>Exercise:</strong> What is the multiplicative inverse of 50 in the finite field $p = 51$? You do not need Python to compute this, see the principles described in “General rules of multiplicative inverses.”</p>
<p><strong>Exercise:</strong> Use Python to compute the multiplicative inverse of 288 in the finite field of <code style='font-family: Arial'>p = 311</code>. You can check your work by validating that <code style='font-family: Arial'>(288 * answer) % 311 == 1</code>.</p>
<h2>The addition of multiplicative inverses is consistent with "regular" addition of fractions</h2>
<p>In a finite field of <code style='font-family: Arial'>p = 7</code>, the numbers 2 and 4 are multiplicative inverses of each other because $(2 \times 4) \pmod 7 = 1$. This means that in the finite field $p = 7$, $4$ is congruent to $1/2$ and $2$ is congruent to $1/4$.</p>
<p>We say $4$ is congruent to $1/2$ in the finite field of $p = 7$ because $2 \times \mathsf{mul\_inv}(2) = 1$. The multiplicative inverse of $2$ is $4$ in this finite field, so we can treat $4$ like $1/2$.</p>
<p>With real numbers, if we add $1/2 + 1/2$, we expect to get $1$. The same thing happens in a finite field. Since $4$ "behaves like" $1/2$, we expect $4 + 4 \pmod 7 = 1$, and indeed it does.</p>
<p>We are able to encode the number $5/6$ in a finite field by thinking of it as the operation $5 * (1/6)$, or $5 \times \mathsf{mul\_inv}(6)$.</p>
<p>Consider that $1/2 + 1/3 = 5/6$. If our finite field is $p = 7$, then $1/2$ is the multiplicative inverse of $2$, $1/3$ is the multiplicative inverse of 3, and $5/6$ is $5$ times the multiplicative inverse of 6:</p>
<pre style='font-family: Arial'><code class="language-solidity">p = 7
one_half = pow(2, -1, p)
one_third = pow(3, -1, p)
five_over_six = (pow(6, -1, p) * 5) % p

assert (one_half + one_third) % p == five_over_six
# True
</code></pre>
<p>The general way to compute a "fraction" in a finite field is the numerator times the multiplicative inverse of the denominator, modulo <code style='font-family: Arial'>p</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">def compute_field_element_from_fraction(num, den, p):
    inv_den = pow(den, -1, p)
    return (num * inv_den) % p
</code></pre>
<p>It is not possible to do this when the denominator is a multiple of the <code style='font-family: Arial'>p</code>. For example, $1/7$ cannot be represented in the finite field $p = 7$ because <code style='font-family: Arial'>pow(7, -1, 7)</code> has no solution. The modulo is taking the remainder after division, and the remainder of $7/7$ is zero, or more generally, $7/d$ is zero when $d$ is a multiple of $7$. The multiplicative inverse means we can multiply a number and its inverse together to get $1$, but if one of the numbers is zero, there is nothing we can multiply by zero to get 1.</p>
<p><strong>Exercise:</strong> run <code style='font-family: Arial'>pow(7, -1, 7)</code> in Python. You should see an exception get thrown, <code style='font-family: Arial'>ValueError: base is not invertible for the given modulus</code>. $7$ mod $7$ equals zero. There is nothing we can multiply by zero to get $1$.</p>
<h3>Finite field "division" does not suffer from precision loss</h3>
<p>If we divide <code style='font-family: Arial'>1 / 3</code> in most programming languages, we will suffer from precision loss because $0.\overline{3}$ is not representable in binary.</p>
<p>However, <code style='font-family: Arial'>1 / 3</code> in a finite field is simply the multiplicative inverse of 3.</p>
<p>This means the arithmetic circuit</p>
<pre style='font-family: Arial'><code class="language-solidity">x + y + z === 1;
x === y;
y === z;
</code></pre>
<p>has an exact solution when done in a finite field. This would not be possible to do reliably if we used fixed point or floating point numbers as the data types for our arithmetic circuits (consider that adding 0.33333 + 0.33333 + 0.33333 will result in 0.99999 rather than 1).</p>
<p>The following implementation in Python illustrates the circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">p = 11

# x, y, z have value 1/3
x = pow(3, -1, 11)
y = pow(3, -1, 11)
z = pow(3, -1, 11)

assert x == y;
assert y == z;
assert (x + y + z) % p == 1
</code></pre>
<h2>Finite field elements do not have a traditional notion of "even" or "odd"</h2>
<p>We say a number is "even" if it can be divided by two with no remainder.</p>
<p><strong>In a finite field, any element can be divided by 2 with no remainder.</strong></p>
<p>That is, "dividing by two" is really multiplying by the multiplicative inverse of 2, and that will always result in another field element without "remainder."</p>
<p>However, if we have a binary representation of the field element, then we can check if the element is even or odd if it were cast to an integer. If the least significant bit is 1, then the number is odd (if interpreted as an integer, not a finite field element).</p>
<h2>Finite field library in Python</h2>
<p>Because it can be a bit tedious to keep writing <code style='font-family: Arial'>pow</code> and <code style='font-family: Arial'>% p</code> in Python, the reader may wish to use the <a href="https://pypi.org/project/galois/">galois library</a> instead (finite fields are sometimes called Galois fields, pronounced “Gal-wah”). It can be installed with <code style='font-family: Arial'>python3 -m pip install galois</code>.</p>
<p>Below, we translate the addition of fractions code from the above section $(1/2 + 1/3 = 1/6)$ to use the <code style='font-family: Arial'>galois</code> library instead. The library overwrites the math operators to work in a finite field:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
GF7 = galois.GF(7) # GF7 is a class that wraps 7

one_half = GF7(1) / GF7(2)
one_third = GF7(1) / GF7(3)
five_over_six = GF7(5) / GF7(6)

assert one_half + one_third == five_over_six
</code></pre>
<p>The operation <code style='font-family: Arial'>1 / GF(a)</code> computes the multiplicative inverse of <code style='font-family: Arial'>a</code>.</p>
<p>The <code style='font-family: Arial'>galois</code> library can compute the additive inverse by adding a negative sign in front:</p>
<pre style='font-family: Arial'><code class="language-solidity">negative_two = -GF(2)
assert negative_two + GF(2) == 0
</code></pre>
<h2>The multiplication of fractions is also consistent</h2>
<p>Let’s use a finite field <code style='font-family: Arial'>p = 11</code> for this example.</p>
<p>With regular numbers we know that $1/3 * 1/2 = 1/6$.</p>
<p>Let’s do this same operation in a finite field:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
GF = galois.GF(11)

one_third = GF(1) / GF(3)
one_half = GF(1) / GF(2)
one_sixth = GF(1) / GF(6)

assert one_third * one_half == one_sixth
</code></pre>
<p><strong>Exercise:</strong> use the galois library to check that $3/4 * 1/2 = 3/8$ in the finite field $p = 17$.</p>
<h2>a × b ≠ 0 for all nonzero a, b</h2>
<p>No two elements can be multiplied together to obtain zero in a finite field unless one of the elements is zero itself. This is also true of regular numbers.</p>
<p>To understand this, consider the finite field $p = 7$. To multiply two numbers together and get $0$ as a result, then one of the terms $a$ needs to be a multiple of 7, so that $a \pmod 7$ is zero. However, none of $\set{0, 1, 2, 3, 5, 6}$ are a multiple of 7, so this cannot happen.</p>
<p>We will refer to this fact frequently when we design arithmetic circuit. For example, if we know</p>
<pre style='font-family: Arial'><code class="language-solidity">x₁ * x₂ * ... * xₙ ≠ 0
</code></pre>
<p>Then we can be certain all of variables <code style='font-family: Arial'>x₁, x₂, xₙ</code> are non-zero — even if we don’t know their values.</p>
<p>Here’s how we can use this trick for a realistic arithmetic circuit. Suppose we have signals <code style='font-family: Arial'>x₁, x₂, x₃</code>. We wish to constrain at least one of these signals to have the value 8 without specifying which one does. First we compute the additive inverse of 8 <code style='font-family: Arial'>a_inv(8)</code> for our field. Then we do:</p>
<p><code style='font-family: Arial'>(x₁ + a_inv(8))(x₂ + a_inv(8))(x₃ + a_inv(8)) === 0</code></p>
<p>This could be written as</p>
<p><code style='font-family: Arial'>(x₁ - 8)(x₂ - 8)(x₃ - 8) === 0</code></p>
<p>with the understanding that <code style='font-family: Arial'>-8</code> is the additive inverse of 8 for our finite field.</p>
<p>As long of one of the signals has the value 8, then that term will be zero and the whole expression will multiply to zero. This trick relies on two facts:</p>
<ul>
<li>If all of the terms are non-zero, there is no way for the expression to evaluate to zero</li>
<li>The additive inverse of 8 is unique, and 8 is the uniquely additive inverse for the additive inverse of 8. In other words, there is no value except 8 that results in 8 + inv(8) being zero.</li>
</ul>
<p>Therefore, the arithmetic circuit <code style='font-family: Arial'>(x₁ + a_inv(8))(x₂ + a_inv(8))(x₃ + a_inv(8)) === 0</code> states that at least one of <code style='font-family: Arial'>x₁, x₂, xₙ</code> has the value 8.</p>
<h2>Finite field operations are associative, commutative, and distributive</h2>
<p>Just like with regular math, with modular arithmetic, the associative, commutative, and distributive properties hold, i.e.</p>
<p><strong>associative</strong></p>
<p>$(a + b) + c = a + (b + c) \pmod p$</p>
<p><strong>commutative addition</strong></p>
<p>$(a + b) = (b + a) \pmod p$</p>
<p><strong>commutative multiplication</strong></p>
<p>$ab = ba \pmod p$</p>
<p><strong>distributive</strong></p>
<p>$a(b + c) = ab + ac \pmod p$</p>
<h2>Modular square roots</h2>
<p>In "regular math," perfect square numbers have integer square roots. For example, 25 has a square root of 5, 49 has a square root of 7, and so on.</p>
<h3>Elements in a finite field do not need to be perfect squares to have a square root</h3>
<p>Consider that $5 × 5 \pmod {11} = 3$. This means that the square root of 3 is 5, modulo 11. Because of how finite fields wrap around, finite field elements do not have to be perfect squares to have a square root.</p>
<p>Just like regular square roots, which have two solutions: a positive and negative one, modular square roots in a finite field also have two solutions. The exception is the element 0, which only has 0 as its square root.</p>
<p>In the finite field modulo 11, the following elements have square roots:</p>
<table>
<thead>
<tr>
<th>Element</th>
<th>1st Square Root</th>
<th>2nd Square Root</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>n/a</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>3</td>
<td>5</td>
<td>6</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
<td>9</td>
</tr>
<tr>
<td>5</td>
<td>4</td>
<td>7</td>
</tr>
<tr>
<td>9</td>
<td>3</td>
<td>8</td>
</tr>
</tbody>
</table>
<p><strong>Exercise</strong>: Verify the claimed square roots in the table are correct in the finite field modulo 11.</p>
<p>Observe that the second square root is always the additive inverse of the first square root, just like real numbers.</p>
<p>For example:</p>
<ul>
<li>In regular math, the square roots of $9$ are $3$ and $-3$, where both are additive inverses of each other.</li>
<li>In a finite field of $p = 11$, the square roots of 9 are 3 and 8. 8 is the additive inverse of 3 because $8 + 3 \pmod {11}$ is $0$, just as $3$ is the additive inverse of $-3$.</li>
</ul>
<p>The elements 2, 6, 7, 8, and 10 do not have modular square roots in the finite field $p = 11$. This can be discovered by squaring every element from 0 to 10 inclusive, and seeing that 2, 6, 7, 8, and 10 are never produced.</p>
<pre style='font-family: Arial'><code class="language-solidity">numbers_with_roots = set()
p = 11
for i in range(0, p):
    numbers_with_roots.add(i * i % p)

print(&quot;numbers_with_roots:&quot;, numbers_with_roots)
# numbers_with_roots: {0, 1, 3, 4, 5, 9}
</code></pre>
<p>Note that 3 is not a perfect square, but it does have a square root in this finite field.</p>
<h3>Computing the modular square root</h3>
<p>The modular square root can be computed in Python with the <a href="https://github.com/hellman/libnum">libnum library</a>. Below, we compute the square root of 5 modulo 11. The third argument to the functions <code style='font-family: Arial'>has_sqrtmod_prime_power</code> and <code style='font-family: Arial'>sqrtmod_prime_power</code> can be set to 1 for our purposes.</p>
<pre style='font-family: Arial'><code class="language-solidity"># install libnum with `python -m pip install libnum`

from libnum import has_sqrtmod_prime_power, sqrtmod_prime_power
has_sqrtmod_prime_power(5, 11, 1) # True
list(sqrtmod_prime_power(5, 11, 1)) # [4, 7]
# square roots generally have two solutions. 4 and 7 are the square roots of 5 (mod 11)
</code></pre>
<p>When <code style='font-family: Arial'>p</code> can be written as <code style='font-family: Arial'>4k + 3</code> where <code style='font-family: Arial'>k</code> is an integer, then the modular square root can be computed as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">def mod_sqrt(x, p):
    assert (p - 3) % 4 == 0, &quot;prime not 4k + 3&quot;
    exponent = (p + 1) // 4
    return pow(x, exponent, p) # x ^ e % p
</code></pre>
<p>The function above returns one of the square roots of x modulo p. The other square root can be computed by calculating the additive inverse of the value returned. If the prime number is not of the form <code style='font-family: Arial'>4k + 3</code> then the <a href="https://en.wikipedia.org/wiki/Tonelli–Shanks_algorithm">Tonelli-Shanks algorithm</a> must be used to compute the modular square root (which the libnum library above implements).</p>
<p>*<em>The implication for this is that the arithmetic circuit <code style='font-family: Arial'>x*  x === y</code>may have two solutions.</em>* For example, in a finite field<code style='font-family: Arial'>p = 11</code>, it might seem that the arithmetic circuit<code style='font-family: Arial'>x \* x === 4</code>only admits the value 2 because -2 is not a finite field element. However, that assumption is very wrong! The assignment<code style='font-family: Arial'>x = 9</code>, which is congruent to -2, also satisfies the circuit.</p>
<p><strong>Exercise:</strong> Use the code snippet above to compute the modular square root of 5 in the finite field of <code style='font-family: Arial'>p = 19</code>. The code will only give you one of the answers. How can you compute the other?</p>
<h2>Linear systems of equations in finite fields</h2>
<p>As noted in the previous chapter, an arithmetic circuit is essentially a system of equations. Linear systems of equations in a finite field share a lot of properties with a linear system of equations over regular numbers. However, there are some differences which may initially be unexpected. Since arithmetic circuits are computed over finite fields, we must understand where these surprising deviations may be.</p>
<p>A <a href="https://math.libretexts.org/Bookshelves/Algebra/Algebra_and_Trigonometry_1e_(OpenStax)/11%3A_Systems_of_Equations_and_Inequalities/11.01%3A_Systems_of_Linear_Equations_-_Two_Variables">linear system of equations</a> is a collection of straight-line equations with a set of unknowns (variables) that we try to solve together. To find the unique solution to a system of linear equations, we must find a numerical value for each variable that will satisfy all equations in the system simultaneously.</p>
<p>Linear systems of equations with real numbers either have:</p>
<p>1) <strong>No solution:</strong> which means the two equations represent lines that are parallel in two dimensions, or never cross in three dimensions or higher</p>
<p><img alt="Parallel lines" src="assets/706568_3c9b8369fd064afe93c2a7ac7816bbf7_mv2.gif" /></p>
<p>2) <strong>One solution</strong> which means the lines intersect at one point</p>
<p><img alt="Intersecting lines" src="assets/706568_140802e7d9534b448073e17ea95ff821_mv2.gif" /></p>
<p>3) <strong>Infinite solutions:</strong> if the two equations represent the same line, then there are infinitely many points of intersection, and the linear system of equations has an infinite number of solutions.</p>
<p><img alt="Two lines that are the same" src="assets/706568_a16e84f93c44406486a5a1ee540f281c_mv2.gif" /></p>
<p>Finite field systems of equations also have</p>
<ol>
<li>no solution or</li>
<li>one solution or</li>
<li>$p$ many solutions, i.e. as many solutions as the order of the field</li>
</ol>
<p><strong>However, just because a linear system of equations over real numbers has zero, one, or infinite solutions it <em>does not imply</em> that the same linear system of equations over a finite field will also have zero, one or, <code style='font-family: Arial'>p</code> many solutions.</strong></p>
<p>The reason we emphasize this is because we use arithmetic circuits and an assignment to the signals to encode our solution to a problem in NP. However, because arithmetic circuits are encoded with finite fields, we may end up encoding the problem in a way that does not capture the behavior of the equations we are trying to model.</p>
<p>The following three examples show how the behavior of a system of equations can change when done over a finite field.</p>
<h3>Example 1: A system of equations with one solution over regular numbers may have <code style='font-family: Arial'>p</code> many solutions in a finite field</h3>
<p>For example, we plot:</p>
<p>$$x+2y=1$$<br />
$$6x+y=6$$</p>
<p><img alt="Two lines with a single intersection" src="assets/706568_26aa180e616f434ab16a76673e03b530_mv2.png" /></p>
<p>It has one solution: $(1, 0)$ for real numbers, but over the finite field $p = 11$, it has 11 solutions: $\set{(0, 6), (1, 0), (2, 5), (3, 10), (4, 4), (5, 9), (6, 3), (7, 8), (8, 2), (9, 7), (10, 1)}$.</p>
<p><strong>Don’t assume that a system of equations (arithmetic circuit) that has a single solution in real numbers has a single solution in a finite field!</strong></p>
<p>Below we plot the solutions to the systems of equation over the finite fields to illustrate both equations "intersect everywhere," that is, have the same set of points that satisfy both equations:</p>
<p><img alt="Modular arithmetic plot of the same equations" src="assets/706568_079fc534789a431cbe3e52bf0081012c_mv2.png" /></p>
<p>This might seem extremely counterintuitive — let’s see how it happens. If we solve the original equations:</p>
<p>$$x+2y=1$$<br />
$$6x+y=6$$</p>
<p>for $y$ we get:</p>
<p>$$y = 1/2 – 1/2*x$$<br />
$$y=6-6x$$</p>
<p>$1/2$ is the multiplicative inverse of $2$. In a finite field of $p = 11$, $6$ is the mutiplicative inverse of $2$, i.e. $2 * 6 \pmod {11} = 1$. Therefore, $x + 2y = 1$ and $6x + y = 6$ are actually the same equation in finite field $p = 11$. That is, the equation $y = 1/2 – 1/2x$ when encoded in a finite field is $y = \mathsf{mul\_inv}(2) – \mathsf{mul\_inv}(2)x$ which is $y = 6 – 6x$, which is the same as the other equation in the system.</p>
<h3>Example 2: A system of equations with one solution over regular numbers may have zero solutions in a finite field</h3>
<p>Also counterintuitively, a system of equations with a single solution over real numbers may have no solution in a finite field:</p>
<p>$$x + 2y=1$$<br />
$$7x+3y=2$$</p>
<p><img alt="A different plot over real numbers showing a single intersection" src="assets/706568_c194df52697f4010943ff4e927f206b6_mv2.png" /></p>
<p>Clearly, this system of equations has an intersection point, but over a finite field it has no solution.</p>
<p>Below we show the plot of the two equations in a finite field:</p>
<p><img alt="A plot in a finite field showing no intersection" src="assets/706568_a737888a0bbc4f4e88d4209b47d0911f_mv2.png" /></p>
<p><strong>Exercise:</strong> Write code to bruteforce every combination of <code style='font-family: Arial'>(x, y)</code> over <code style='font-family: Arial'>x = 0..10, y = 0..10</code> to verify the above system has no solution over the finite field of <code style='font-family: Arial'>p = 11</code>.</p>
<p>Why does this system of equations have no solution in finite field $p = 11$?</p>
<p>If we solve</p>
<p>$$x + 2y=1$$<br />
$$7x+3y=2$$</p>
<p>for real numbers, we get the solutions</p>
<p>$$<br />
x = \frac{1}{11},\space y=\frac{5}{11}$$</p>
<p>The fractions above do not have a congruent element in the finite field <code style='font-family: Arial'>p = 11</code>.</p>
<p>Recall that dividing by a number is equivalent to multiplying by its multiplicative inverse. Also, recall the field order (in this case 11) won’t have a multiplicative inverse, because the field order is congruent to 0.</p>
<p>The multiplicative inverse of <code style='font-family: Arial'>a</code> is the value <code style='font-family: Arial'>b</code> such that <code style='font-family: Arial'>a * b = 1</code>. However, if <code style='font-family: Arial'>a = 0</code> (or any value congruent to it) then there is no solution for <code style='font-family: Arial'>b</code>. So, the expressions we wrote for the solutions in the real numbers can’t be translated into elements of our finite field.</p>
<p>Therefore the solutions (x, y) above are not part of the finite field. Hence, in a finite field <code style='font-family: Arial'>p = 11</code>, <code style='font-family: Arial'>x + 2y = 1</code> and <code style='font-family: Arial'>7x + 3y = 2</code> are parallel lines.</p>
<p>To see this from another angle, we could solve the equations for y and get:</p>
<p>$$y = 1/2 – x/2$$<br />
$$y=2/3-7x/3$$</p>
<p>We saw in the previous section that 6 is the multiplicative inverse of 2, so the first equation has a "slope" of 6 in the the finite field. In the second equation, we compute the slope by computing 7 times the multiplicative inverse of 3: <code style='font-family: Arial'>(7 * pow(3, -1, 11)) % 11 = 6</code> . We now show that their slopes are the same in a finite field.</p>
<p>The slope is the coefficient of <code style='font-family: Arial'>x</code> in the form <code style='font-family: Arial'>y = c + bx</code>. For the two equations above, the first slope is <code style='font-family: Arial'>-1/2</code> and the second slope is <code style='font-family: Arial'>-7/3</code>. If we convert both of these fractions to an element in the finite field of <code style='font-family: Arial'>p = 11</code>, we get the same value of 5:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
GF11 = galois.GF(11)

negative_1 = -GF11(1)
negative_7 = -GF11(7)

slope1 = GF11(negative_1) / GF11(2)
slope2 = GF11(negative_7) / GF11(3)

assert slope1 == slope2 # 5 == 5
</code></pre>
<p>The implication of this fact for the arithmetic circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">x + 2 * y === 1
7 * x + 3 * y === 2
</code></pre>
<p>is that the arithmetic circuit does not have a satisfying assignment in a finite field <code style='font-family: Arial'>p = 11</code>.</p>
<h3>Example 3: A system of equations with zero solutions over regular numbers may have <code style='font-family: Arial'>p</code> solutions in a finite field</h3>
<p>The following two formulas plot lines that are parallel and hence have no solution over reals:</p>
<p>$$x + 2y = 3$$<br />
$$4x + 8y = 1$$</p>
<p><img alt="Two parallel lines" src="assets/706568_90da5d94046042f28be6d681d4cb8dce_mv2.png" /></p>
<p>However, over the finite field <code style='font-family: Arial'>p = 11</code>, it has 11 solutions: $\set{(0, 7), (1, 1), (2, 6), (3, 0), (4, 5), (5, 10), (6, 4), (7, 9), (8, 3), (9, 8), (10, 2)}$. Those solutions are plotted below:</p>
<p><img alt="Plot of overlapping lines in a finite field" src="assets/706568_f5d5e95e4b5340a496f3756d67626e15_mv2.png" /></p>
<p><strong>Exercise:</strong> Convert the two equations to their finite field representation and see they are the same.</p>
<p>Suppose we encoded this system of equations as an arithmetic circuit:</p>
<p>$$x + 2y = 3$$<br />
$$4x + 8y = 1$$</p>
<pre style='font-family: Arial'><code class="language-solidity">x + 2 * y === 3
4 * x + 8 * y === 1
</code></pre>
<p>For the finite field we are using, our constraints are redundant even though they "look different." That is, two different looking constraints actually constrain $x$ and $y$ to have the same values.</p>
<h2>Polynomials in finite fields</h2>
<p>In the chapter on Arithmetic circuits, we used the polynomial <code style='font-family: Arial'>x(x - 1) === 0</code> to enforce that <code style='font-family: Arial'>x</code> can only be 0 or 1. This could be written as a polynomial equation. To do so, we fully expand our expression until it’s expressed as separate powers of x, each multiplied by a coefficient. In this case: <code style='font-family: Arial'>x² - x === 0</code>.</p>
<p>The assumption that the polynomial <code style='font-family: Arial'>x² - x === 0</code> only has solutions 0 or 1 in a finite field (as well as with real numbers) is sound in this case. However, in general, one should not assume that the roots of a polynomial over real numbers has the same roots in a finite field. We will show some counterexamples later.</p>
<p>Nevertheless, polynomials in finite fields share a lot of properties with polynomials over real numbers:</p>
<ul>
<li>A polynomial of degree $d$ has at most $d$ roots. The roots of a polynomial $p(x)$ are the values $r$ such that $p(r) = 0$.</li>
<li>If we add two polynomials $p_1$ and $p_2$, the degree of $p_1 + p_2$ will be at most $\max(\deg(p_1), \deg(p_2))$. It’s possible for the degree of $p_1 + p_2$ to be less than $\max(\deg(p_1), \deg(p_2))$. For example, $p_1=x³ + 5x + 7$, and $p_2 = -x³$,</li>
<li>Adding polynomials in a finite field follows associative, commutative, and distributive laws.</li>
<li>If we multiply two polynomials $p_1$ and $p_2$, the roots of the product will be the union of the roots of $p_1$ and $p_2$.</li>
</ul>
<p>Let’s plot $y = x² \pmod {17}$ as an example.</p>
<p><img alt="plot of x^2 mod 17" src="assets/706568_3917656775e4477598b9afad9dea92d7_mv2.png" /></p>
<p>The domain of $x$ are the elements of the finite field, and the output (range) must be a member of the finite field as well. That is, note how all the $x$ and $y$ values lie in the interval of $[0,16]$. A polynomial over a finite field can only have $x$ and $y$ values less than $p$.</p>
<p>The equivalent of $y = -x²$ in finite field $p = 17$ is $y = 16x² \pmod {17}$ since 16 is the additive inverse of 1 in that finite field. The polynomial $y = 16x² \pmod {17}$ is plotted below:</p>
<p><img alt="plot of y = 16x^2 mod 17" src="assets/706568_983e1638b445495ebf2d99ab0d4f7027_mv2.png" /></p>
<h3>Polynomials that do not have roots in real numbers may have roots in a finite field</h3>
<p>Just like our examples above with linear systems of equations, one should not assume that a polynomial with certain roots in real numbers has the same roots in a finite field.</p>
<p>Below, we plot $y = x² + 1$ in the finite field $p = 17$. In real numbers, $y = x² + 1$ has no real roots. But in a finite field, it has two roots at $4$ and $13$, marked in red dots below:</p>
<p><img alt="Plot of y = x^2 + 1 mod 17" src="assets/706568_c1a9510ce859456d8a20fb705c35c604_mv2.png" /></p>
<p>Let’s now explain why $y = x² + 1$ does not have roots in real numbers but does have roots in the finite field $p = 17$. In the finite field $p = 17$, 17 is congruent to zero. So if we plug a value into $x$ such that $x² + 1$ becomes $17$, then the polynomial will output zero, not $17$. We can solve $x² + 1 = 17$ for $x² = 17 – 1 = 16$. In a finite field of $p = 17$, $x² = 16$ has solutions $4$ and $13$. Therefore, $y = x² + 1$ has roots $4$ and $13$ in the finite field $p = 17$.</p>
<h3>Polynomials with real roots may have no roots in a finite field</h3>
<p>Consider the polynomial $y = x² − 5$. We can see it has roots at $\sqrt{5}$ and $-\sqrt{5}$. However, if we plot it over a finite field modulo 17, we can see it never crosses the x-axis:</p>
<p><img alt="Plot of y = x^2 + 5 (mod 17)" src="assets/706568_a87f571b1af9454db2b27d4a9fd3d8f6_mv2.png" /></p>
<p>There are no roots is because $\sqrt{5}$ cannot be represented in a finite field modulo 17. However, in the finite field $p = 11$, then there would be two roots because 5 has a modular square roots in the finite field of $p = 11$.</p>
<h3>Limitations in arithmetic circuits for ZK Proofs</h3>
<p>If we wish to write an arithmetic circuit to show "I know the root of the polynomial $y = x² − 5$" using an arithmetic circuit over a finite field, then we may run into the issue of not being able to encode $\sqrt{5}$. That is, over real numbers, $y = x² − 5$ has a root of $\sqrt{5}$, but this cannot be expressed in some finite fields. Depending on $p$, the arithmetic circuit <code style='font-family: Arial'>x² === 5</code> may have no no satisfying witness.</p>
<h3>Polynomials in finite fields with Python</h3>
<p>When experimenting with arithmetic circuits, it is sometimes helpful to write Python code to simulate them. When our arithmetic circuit has a polynomial form, we can use the <code style='font-family: Arial'>galois</code> library from earlier to test its behavior. The following code examples illustrate how to use this library for working with polynomials.</p>
<h3>Adding polynomials in a finite field</h3>
<p>In the code below, we define two polynomials: <code style='font-family: Arial'>p1 = x² + 2x + 102</code> and <code style='font-family: Arial'>p2 = x² + x + 1</code> and then symbolically add them together to produce <code style='font-family: Arial'>2x² + 3x</code>. Note that the constant coefficient terms add up to zero in the finite field <code style='font-family: Arial'>p = 103</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
GF103 = galois.GF(103) # p = 103

# we define a polynomial x^2 + 2x + 102 mod 103
p1 = galois.Poly([1,2,102], GF103)

print(p1)
# x^2 + 2x + 102

# we define a polynomial x^2 + x + 1 mod 103
p2 = galois.Poly([1,1,1], GF103)

print(p1 + p2)
# 2x^2 + 3x
</code></pre>
<p>The <code style='font-family: Arial'>galois</code> library is intelligent enough to interpret negative integers as additive inverses, as the code below demonstrates:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
GF103 = galois.GF(103) # p = 103

# We can input &quot;-1&quot; as a coefficient, and that will
# automatically be calculated as `p - 1`
# -1 becomes 102 in a field p = 103
p3 = galois.Poly([-1, 1], GF103)
p4 = galois.Poly([-1, 2], GF103)

print(p3)
# 102x + 1
print(p4)
# 102x + 2
</code></pre>
<h3>Multiplying polynomials in a finite field</h3>
<p>We can multiply polynomials together:</p>
<pre style='font-family: Arial'><code class="language-solidity">print(p3 * p4)
# x^2 + 100x + 2
</code></pre>
<p>Note that <code style='font-family: Arial'>p3</code> and <code style='font-family: Arial'>p4</code> are degree 1 polynomials and their product is a degree 2 polynomial.</p>
<h3>Finding the roots of a polynomials in a finite field</h3>
<p>Finding roots of polynomials over finite fields is a separate topic (see the Wikipedia page for algorithms on <a href="https://en.wikipedia.org/wiki/Factorization_of_polynomials_over_finite_fields">factorizing polynomials over finite fields</a>). The <code style='font-family: Arial'>galois</code> library can execute calculate the roots with the <code style='font-family: Arial'>roots</code> function. This may be handy for verifying arithmetic constraints in polynomial form actually create the intended constraints.</p>
<pre style='font-family: Arial'><code class="language-solidity">print((p3 * p4).roots())
# [1, 2]
</code></pre>
<h3><code style='font-family: Arial'>galois</code> library gotchas</h3>
<p>The library silently takes the floor of floating point numbers passed as coefficients:</p>
<pre style='font-family: Arial'><code class="language-solidity"># The galois library will silently convert
# floats into a integer
galois.Poly([2.5], GF103)
# Poly(2, GF(103))
</code></pre>
<p>The library will revert if passed a number greater than or equal to <code style='font-family: Arial'>p</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity"># The follow code fails because we cannot
# have coefficients the order of the field or higher
galois.Poly([103], GF103)
# ValueError: GF(103) arrays must have elements in `0 &lt;= x &lt; 103`, not [103].
</code></pre>
<h2>Learn more with RareSkills</h2>
<p>See our <a href="https://www.rareskills.io/zk-book">ZK Book</a> to learn more about Zero Knowledge Proofs</p>
<h2>Practice problems</h2>
<p>In the problems below, use a finite field <code style='font-family: Arial'>p</code> of <code style='font-family: Arial'>21888242871839275222246405745257275088548364400416034343698204186575808495617</code>. Beware that the <code style='font-family: Arial'>galois</code> library takes a while to initialize a <code style='font-family: Arial'>GF</code> object, <code style='font-family: Arial'>galois.GF(p)</code>, of this size.</p>
<ol>
<li>A dev creates an arithmetic circuit <code style='font-family: Arial'>x * y * z === 0</code> and <code style='font-family: Arial'>x + y + z === 0</code> with the intent of constraining all the signals to be zero. Find a counter example to this where the constraints are satisfied, but not all of <code style='font-family: Arial'>x</code>, <code style='font-family: Arial'>y</code>, and <code style='font-family: Arial'>z</code> are 0.</li>
<li>A dev creates a circuit with the polynomial <code style='font-family: Arial'>x² + 2x + 3 === 11</code> and proves that 2 is a solution. What is the other solution? Hint: write the circuit as <code style='font-family: Arial'>x² + 2x - 8 === 0</code> then factor the polynomial by hand to find the roots. Finally, compute the congruent element of the roots in the finite field to find the other solution.</li>
</ol>
<p><em>Originally Published April 29, 2024</em></p>
<div style='page-break-after: always;'></div>

<h1>Elementary Set Theory for Programmers</h1>
<p>Source: https://rareskills.io/post/set-theory</p>
<h1>Elementary Set Theory for Programmers</h1>
<p>Why another set theory tutorial?</p>
<p>The target audience for this piece is the sort of folks who don’t care about abstract math unless they see a direct use-case for it. They want to get the essential parts they need and move on. This piece caters to that audience.</p>
<p>Our end goal is to understand abstract algebra because abstract algebra has a lot of concepts that can be properly called “useful,” but abstract algebra is heavily dependent on set theory.</p>
<p>Our goal here is to take the most direct path through set theory to abstract algebra such that we understand all the terminology and concepts we will be dealing with.</p>
<p>Engineers are usually not interested in collecting abstractions or proving theorems, they want to ship things while having a sufficiently deep understanding of the topic such that they don’t unintentionally create bugs or inefficiencies. Acquiring knowledge that does not directly aid in this quest is considered a waste of time.</p>
<p>To optimize for this goal, I have deliberately omitted any aspect of Set Theory that isn’t directly useful for understanding the relevant aspects of abstract algebra. Consequently, this resource on set theory is not comprehensive by design.</p>
<h2>Motivation of Set Theory for Zero Knowledge Proofs</h2>
<p>The purpose of this tutorial is to give you a firm grasp of what a <strong>binary operator</strong> is in the context of Set Theory.</p>
<p>The concept of a binary operator lies at the core of <a href="staging.rareskills.io/post/group-theory-and-coding">Group Theory</a>, and Group Theory is used <em>everywhere</em> in Zero Knowledge Proofs.</p>
<h2>Our treatment is not rigorous</h2>
<p>Some mathematicians might be horrified that I don’t discuss the axioms of set theory here. This is a feature, not a bug. If you want a proof for something, ask Google or a chatbot (or better yet, work it out on your own). The concepts discussed here have been rigorously proven to death for over a century.</p>
<p>Set theory is not difficult (at least if you skip the proof writing, set theory proofs can be shockingly hard when doing it for the first time). You probably understand set theory intuitively already, and have almost certainly used sets in your code, perhaps to quickly eliminate duplicates in an array. However, we need to put a language to this intuition and make our intuitive understanding explicit.</p>
<p>Learning abstract math is like learning a human language. You can learn the vocabulary (what the words refer to), and the grammar (how they combine together in a valid way). To use that analogy, we place a heavy emphasis on vocabulary over grammar. There is good reason for this.</p>
<p>If you enter a store in a foreign country and ask the equivalent of “the breads to be buy for me where?” the clerk can help you even though your grammar is horribly off. But if you nail the grammar and don’t know the vocabulary, your knowledge becomes useless. You can form a perfect sentence, but if you can’t succinctly refer to “bread” and “buy” your trip to the store will not be a success (I didn’t invent this analogy, however I can’t remember where I first saw it unfortunately).</p>
<p>Hence, we will barely scratch the grammar (proofs and theorems) and emphasize the vocabulary of abstract math (which is very useful in navigating this foreign country).</p>
<p><strong>Some types of knowledge can only be acquired by experience, not by explanation.</strong></p>
<p>Therefore, you <em>should</em> do the exercises in this text. Don’t worry, you won’t be writing proofs, just to make sure you actually internalized what you just read.</p>
<p>Don’t let the relative brevity of this text fool you. It will take at least an afternoon (or two, maybe three) to work through it if you actually do the exercises. If a certain section doesn’t make sense, consult the internet or a chatbot for alternative explanations of that subtopic.</p>
<h2>Definition of a set</h2>
<p>A set is a well-defined collection of objects. These objects could be <em>anything</em> and the rules we learn from set theory apply to them.</p>
<p>Integers, rational numbers, real numbers, complex numbers, matrices, polynomials, polygons, and many other things all have one thing in common: they are all sets.</p>
<p>There is a well-defined rule that decides if something is a member of the set or not. We won’t get into it, but it is clear that a polygon is not a polynomial, and a polynomial is not a matrix, etc.</p>
<p>Sets are allowed to be empty. We call this the <em>empty set</em>.</p>
<p>By definition, sets do not contain duplicate items. For example, $\set{a, a, b}$ is really just $\set{a, b}$.</p>
<p><strong>Exercise:</strong> Assume you have a proper definition for integers. Create a well-defined set of rational numbers.</p>
<h2>Superset and subsets</h2>
<p>When we look at integers and rational numbers, there seems to be a relationship between some of them. Specifically, all integers are rational numbers, but not all rational numbers are integers. The relationship between them is that integers are a <em>subset</em> of rational numbers. On the flip side, rational numbers are a superset of integers.</p>
<p>A subset does not need to be strictly smaller than the set it belongs to. For example, it is perfectly valid to say that the set of integers is a subset of itself.</p>
<p>The precise definition for the relationship between integers and rational numbers is a <em>proper subset</em>, that is, there exist rational numbers that are not integers.</p>
<p><strong>Exercise:</strong> Define the subset relationship between integers, rational numbers, real numbers, and complex numbers.</p>
<p><strong>Exercise:</strong> Define the relationship between the set of transcendental numbers and the set of complex numbers in terms of subsets. Is it a proper subset?</p>
<h2>Set equality</h2>
<p>Sets are defined to be equal if they contain the same elements, regardless of order in which those elements appear. For example, ${4, 2, 5}$ is the same set as ${2, 5, 4}$. When doing formal proofs for sets, we say that if $A$ is a subset of $B$ and $B$ is a subset of $A$, then $A = B$. Or in more mathy notation: $A = B \iff A \subseteq B$ and $B \subseteq A$. That’s read as $A = B$ if and only if $A$ is a subset of $B$ and $B$ is a subset of $A$.</p>
<h2>Cardinality</h2>
<p>In some of our above examples, there are an infinite number of integers, rational numbers, and other similar sets. However, we can also define sets in a finite way, such as the numbers $\set{0,1,2,3,4,5,6,7,8,9,10}$. The cardinality of the previous set is $11$. If $A = \set{5,9,10}$, then $|A| = 3$, where the two vertical bars around the A mean cardinality.</p>
<p>There are different levels of infinity in set theory. For example, there are infinitely many more real numbers than there are integers. Specifically, we say integers are countably infinite because you can literally count them out. But there is no way to start counting real numbers which are uncountably infinite.</p>
<p><strong>Exercise:</strong> Using the formal definition of equality above, argue that if two finite sets have different cardinality, they cannot be equal. (Demonstrating this for infinite sets is a little trickier, so we skip that).</p>
<h2>Fancy blackboard letters</h2>
<p>Because “integers as a set” and “real numbers as a set” are used so frequently, there is scary looking mathematical shorthand for that.</p>
<ul>
<li>The symbol $\mathbb{N}$ is the set of natural numbers $(1,2,3,…)$. It definitely does not include negative numbers, but whether it includes zero depends on who you are talking to.</li>
<li>The symbol $\mathbb{Z}$ is the set of all integers (because “zahlen” is integer in German)</li>
<li>The symbol $\mathbb{Q}$ is the set of all rational numbers. Rational numbers are a number that can be expressed as the quotient or fraction ⁠$p$ for $q$ as $\frac{p}{q}$⁠ of two integers, a numerator $p$ and a non-zero denominator $q$). From this definition, it’s easy to see where the symbol $\mathbb{Q}$ comes from.</li>
<li>The symbol $\mathbb{R}$ is the set of all real numbers, because R stands for real. Duh.</li>
<li>The symbol $\mathbb{C}$ is the set of all complex numbers for similarly obvious reasons.</li>
</ul>
<p>Sometimes people write $\mathbb{R}^2$ as a vector of two real numbers, so $a \in \mathbb{R}^2$ means $a$ is a 2D vector. I recommend writing it the second way because it is more concise, and also makes you look smarter.</p>
<p><img alt="Math meme about 2D vectors of real numbers" src="assets/935a00_9815078a1a9d44b0ae354484a71d8052_mv2.png" /></p>
<h2>Ordered pairs</h2>
<p>Although sets do not have an inherent order, a new type of data structure called an <em>ordered pair</em> can emerge from sets of sets. For example, $(a, b)$ is an ordered pair while ${a, b<br />
}$ is a set.</p>
<p>We programmers typically think of an ordered pair as a tuple. We say two ordered pairs are equal in the same sense we say two tuples are equal.</p>
<p>How do we create order out of something that is inherently unordered?</p>
<p>The key implementation detail is that we represent $(a, b)$ as a set form as $\set{a, \set{b}}$. We can do this because we can define our set as containing either letters or a set of cardinality one that contains a letter. This is why we can say $(a, b) \neq (b, a)$ because $\set{a, \set{b}} \neq \set{b, \set{a}}$. We will not concern ourselves with this implementation detail any further.</p>
<p>Just like in other programming languages, our ordered pair can be arbitrarily long; for example, $(a,b,c,d)$ is valid. We can also encode and ordered pair holding an ordered pair as $((a, b), c)$, which will be useful later.</p>
<h2>Cartesian product</h2>
<p>Because sets are well-defined, we can define a set such that every element from one set is one part of an ordered pair with an element from another set. For example, if $A = {1,2,3}$ and $B = {x, y, z}$, then the Cartesian product $A \times B$ is the set ${(1, x), (1, y), (1, z), (2, x), …, (3, z)}$. This can also be represented as a table:</p>
<p>$$<br />
A \times B =\space\space<br />
\begin{array}{c|ccc}<br />
&amp; x &amp; y &amp; z \<br />
\hline<br />
1 &amp; (1,x) &amp; (1,y) &amp; (1,z) \<br />
2 &amp; (2,x) &amp; (2,y) &amp; (2,z) \<br />
3 &amp; (3,x) &amp; (3,y) &amp; (3,z)<br />
\end{array}<br />
$$</p>
<p>Cartesian products are not commutative, as the following exercise will demonstrate. Commutative means $B \times A = A \times B$ in the general case.</p>
<p><strong>Exercise:</strong> Compute the Cartesian product of $B \times A$ using the definitions above.</p>
<h2>Subsets of the Cartesian product form a function</h2>
<p>What if we wanted to say we have a function</p>
<p>$$<br />
\begin{align*}<br />
1 \rightarrow y \<br />
2 \rightarrow z \<br />
3 \rightarrow x \<br />
\end{align*}<br />
$$</p>
<p>(I picked this out-of-order example to make it a little more interesting).</p>
<p>We can define a set that defines this mapping. We just take a subset of our Cartesian product above to include $(1, y)$, $(2, z)$, and $(3, x)$.</p>
<p><strong>In set-theoretic terms, a <em>function</em> is a subset of the Cartesian product of the domain set and codomain set.</strong></p>
<p>For our example of $1$ mapping to $y$, $2$ mapping to $z$, and $3$ mapping to $x$, the subset of the Cartesian product is shown in bold below:</p>
<p>$$<br />
{1,2,3} \times {x, y, z} =\space\space<br />
\begin{array}{c|ccc}<br />
&amp; x &amp; y &amp; z \<br />
\hline<br />
1 &amp; (1,x) &amp; \mathbf{(1,y)} &amp; (1,z) \<br />
2 &amp; (2,x) &amp; (2,y) &amp; \mathbf{(2,z)} \<br />
3 &amp; \mathbf{(3,x)} &amp; (3,y) &amp; (3,z) \<br />
\end{array}<br />
$$</p>
<p>Therefore, our function is defined by the set ${(1, y), (2, z), (3, x)}$.</p>
<p>To define a function, we need the set we start from and the set we end at. We take the Cartesian product of these two sets, which results in every possible assignment from the input set to the output set. Then we take the subset of the Cartesian product to define the function as we like. When dealing with infinite sets like integers, we aren’t bothered that we can’t enumerate all the ordered points explicitly.</p>
<h3>Functions are not necessarily computable</h3>
<p>A very important note is that mathematicians rarely concern themselves with <em>computability</em>. A function is a mapping between sets. How that function is <em>computed</em>, if it is even possible to compute with a reasonable computer, is not a concern of most mathematicians.</p>
<p>This is where programmers sometimes get tripped up. They often only think of functions as something that can be efficiently computed with lines of code. While useful, this limits our understanding of the general properties of functions.</p>
<p>The reason I emphasize this is that in zero knowledge proofs, we are going to be dealing with functions that are a lot “higher level” than plugging an argument into a function and getting a return value. We need to be able to appreciate the “big picture” of functions. <em>They are a mapping from one set to another set.</em> And a mapping between sets comes about by taking a subset of their Cartesian product.</p>
<h4>Functions with dissimilar domains and codomains</h4>
<p>Specifically, we are going to be jumping between integers, polynomials, matrices, elliptic curves in one dimension, then elliptic curves in another dimension, and so forth.</p>
<p>You’ll get very dizzy trying to conceptualize this unless you understand at a foundational level that, by Set Theory, we are allowed to define jumps however we like!</p>
<p>Of course, how we map the jump will have a strong effect on its usefulness, if we map everything to zero, that is a valid map, but not very useful.</p>
<p>I want you to understand early that we aren’t doing anything weird when we warp between universes in this way.</p>
<p>At the end of the day, we are allowed to take any two sets we like, create a new set by taking their Cartesian product, taking a subset of that set of ordered pairs, and boom, we have a mapping.</p>
<h4>Axiom of Choice</h4>
<p>If you’re thinking “hey wait a minute, I can just pick a subset and define the function however I like?” you are not alone in wondering this. If you want to go down the rabbit hole, we’ve really been discussing the <a href="https://en.wikipedia.org/wiki/Axiom_of_choice">axiom of choice</a> this whole time, and it has been the subject of controversy despite the definition seeming non-controversial:</p>
<blockquote>
<p>The Cartesian product of a collection of non-empty sets is non-empty.</p>
</blockquote>
<h2>Subsets of the Cartesian product form a function: example</h2>
<p>Let’s define a mapping between non-negative real numbers (zero or greater) and non-negative integers (zero or greater) using the floor function. The floor function simply removes the decimal portion of a number. We can’t show all the real numbers (or integers), but we can create a sketch.</p>
<p>When we do $\mathbb{R}\times\mathbb{Z}$ and take a subset, we simply pick the ordered pairs that correspond to taking the floor of the element from the real numbers. The ordered pairs we do not show in the table are ordered pairs that are not in our subset that defines the mapping. For example, $2$ is not the floor of $500.3$ so that ordered pair $(500.3, 2)$ is not included.</p>
<p>$$<br />
\begin{array}{c|ccccc}<br />
&amp; 1 &amp; 2 &amp; \dots &amp; 499 &amp; 500 \<br />
\hline<br />
1.5 &amp; \color{red}{\mathbf{(1.5, 1)}} &amp; (1.5, 2) &amp; \dots &amp; (1.5, 499) &amp; (1.5, 500) \<br />
2.7772 &amp; (2.7772, 1) &amp; \color{red}{\mathbf{(2.7772, 2)}} &amp; \dots &amp; (2.7772, 499) &amp; (2.7772, 500) \<br />
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \<br />
500.3 &amp; (500.3, 1) &amp; (500.3, 2) &amp; \dots &amp; (500.3, 499) &amp; \color{red}{\mathbf{(500.3, 500)}} \<br />
\end{array}<br />
$$</p>
<p><strong>Exercise:</strong> Define a mapping (function) from integers ${1,2,3,4,5,6}$ to the set ${\text{even}, \text{odd}}$.</p>
<p><strong>Exercise:</strong> Take the Cartesian product of the polygons ${\text{triangle}, \text{square}, \text{pentagon}, \text{hexagon}, \text{heptagon}, \text{octagon}}$ and the set of integers ${0,1,2,…,8}$. Define a mapping such that the polygon maps to an integer representing the number of sides. For example, the ordered pair $(\Box, 4)$ should be in the subset, but $(\bigtriangleup, 7)$ should not be in the subset of the Cartesian product.</p>
<p><strong>Exercise:</strong> Define a mapping between positive integers and positive rational numbers (not the whole thing, obviously). It is possible to perfectly map the integers to rational numbers. Hint: draw a table to construct rational numbers where the columns are the numerators and the rows are the denominators. Struggle with this for at least 15 minutes before looking up the answer.</p>
<h3>Valid and invalid subsets of the Cartesian product.</h3>
<p>There is an important restriction on how we pick our subset. For example, the following subset of the Cartesian product ${1,2,3}$, ${p,q,r}$ is not valid because $1$ maps to $p$ and $1$ maps to $q$. When defining a function with a Cartesian product, the same domain element cannot map to two different codomain elements.</p>
<p>$$<br />
\begin{array}{c|cc}<br />
&amp; p &amp; q &amp; r\<br />
\hline<br />
1&amp;(1,p) &amp; (1,q)\<br />
2&amp;&amp;(2,q)\<br />
3&amp;&amp;&amp;(3,r)\<br />
\end{array}<br />
$$</p>
<h2>A Cartesian product of a set with itself</h2>
<p>It should be no surprise that instead of doing a Cartesian product between $A$ and $B$, you can do a Cartesian product between $A$ and $A$. This is just mapping a set to itself.</p>
<p>This is is the first step of a more abstract form of what we traditionally think of as <em>functions</em> over integers.</p>
<p>For example, $y=x^2$ (over positive integers) can be visualized in set theoretic terms as the subset of $\mathbb{Z}\times\mathbb{Z}$:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}<br />
\hline<br />
&amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp;…\<br />
\hline<br />
1 &amp; \color{red}{\mathbf{(1,1)}} &amp; (1,2) &amp; (1,3) &amp; (1,4) &amp; (1,5) &amp; (1,6) &amp; (1,7) &amp; (1,8) &amp; (1,9) &amp; (1,10) \<br />
\hline<br />
2 &amp; (2,1) &amp; (2,2) &amp; (2,3) &amp; \color{red}{\mathbf{(2,4)}} &amp; (2,5) &amp; (2,6) &amp; (2,7) &amp; (2,8) &amp; (2,9) &amp; (2,10) \<br />
\hline<br />
3 &amp; (3,1) &amp; (3,2) &amp; (3,3) &amp; (3,4) &amp; (3,5) &amp; (3,6) &amp; (3,7) &amp; (3,8) &amp; \color{red}{\mathbf{(3,9)}} &amp; (3,10) \<br />
\hline<br />
…\<br />
\hline<br />
\end{array}<br />
$$</p>
<h2>Set relations</h2>
<p>The phrase “taking a subset of the Cartesian product” is so common that we have a word for it. It is a <em>relation</em>.</p>
<p>A relation can be from a Cartesian product of a set with itself or a set with another set. If we have two sets $A$ and $B$ ($B$ might or might not be equal to $A$ without loss of generality), and we take a subset of $A \times B$, then we say an element $a$ from $A$ is related to an element $b$ from $B$ if there is an ordered pair $(a, b)$ in the subset of $A \times B$.</p>
<p>In the $y=x^2$ example, $2$ from $X$ is related to $4$ from $Y$, but $3$ in $X$ is not related to $6$ from $Y$.</p>
<h2>A “binary operator” in set theoretic terms</h2>
<p>A binary operator is a function from $A \times A \rightarrow A$. Basically, we take every possible pair from $A \times A$ (the Cartesian product of $A$ with itself) and map it to an element in $A$. In other words, it is a set relation between $A \times A$ and $A$.</p>
<p>Here are some basic examples of binary operators:</p>
<ul>
<li><strong>addition of integers</strong> take any two elements from the set integers and add them together, you get another element from the set of integers. Here, we’ve mapped a pair of integers to another integer. For example, $(1, 3)$ maps to $4$.</li>
<li><strong>multiplication of rational numbers</strong> take any two rational numbers, multiply them together, you get another rational number. For example, $(3.5, 2.0)$ maps to $7.0$.</li>
<li><strong>concatenation of strings</strong> take any pair of strings from the set of strings, concatenate them, and the result is another string. For example, $(\text{Rare},\text{Skills})$ maps to $\text{RareSkills}$. Unlike the two examples above, the order of the strings in the pair changes the final string the pair gets mapped to. For example, $(\text{Rare},\text{Skills})$ maps to the string $\text{RareSkills}$ but $(\text{Skills},\text{Rare})$ maps to the string $\text{SkillsRare}$</li>
</ul>
<h3>Constructing a Set-Theoretic Binary Operator</h3>
<p>Let’s use an example of the set $\set{0,1,2}$ with binary operator addition modulo $3$. First we take the set’s Cartesian product with itself (i.e. $A \times A$):</p>
<p>$$<br />
\begin{array}{c|ccc}<br />
&amp; 0 &amp; 1 &amp; 2 \<br />
\hline<br />
0 &amp; (0,0) &amp; (0,1) &amp; (0,2) \<br />
1 &amp; (1,0) &amp; (1,1) &amp; (1,2) \<br />
2 &amp; (2,0) &amp; (2,1) &amp; (2,2) \<br />
\end{array}<br />
$$</p>
<p>Then we take the Cartesian product of this new set of pairs with the original set</p>
<p>$$<br />
\begin{array}{c|ccc}<br />
&amp; 0 &amp; 1 &amp; 2 \<br />
\hline<br />
(0,0) &amp; ((0,0),0) &amp; ((0,0),1) &amp; ((0,0),2) \<br />
(0,1) &amp; ((0,1),0) &amp; ((0,1),1) &amp; ((0,1),2) \<br />
(0,2) &amp; ((0,2),0) &amp; ((0,2),1) &amp; ((0,2),2) \<br />
(1,0) &amp; ((1,0),0) &amp; ((1,0),1) &amp; ((1,0),2) \<br />
(1,1) &amp; ((1,1),0) &amp; ((1,1),1) &amp; ((1,1),2) \<br />
(1,2) &amp; ((1,2),0) &amp; ((1,2),1) &amp; ((1,2),2) \<br />
(2,0) &amp; ((2,0),0) &amp; ((2,0),1) &amp; ((2,0),2) \<br />
(2,1) &amp; ((2,1),0) &amp; ((2,1),1) &amp; ((2,1),2) \<br />
(2,2) &amp; ((2,2),0) &amp; ((2,2),1) &amp; ((2,2),2) \<br />
\end{array}<br />
$$</p>
<p>And then take the subset of that which defines our binary operator addition modulo $3$:</p>
<p>$$<br />
\begin{array}{c|ccc}<br />
&amp; 0 &amp; 1 &amp; 2 \<br />
\hline<br />
(0,0) &amp; \color{red}{\mathbf{((0,0),0)}} &amp; ((0,0),1) &amp; ((0,0),2) \<br />
(0,1) &amp; ((0,1),0) &amp; \color{red}{\mathbf{((0,1),1)}} &amp; ((0,1),2) \<br />
(0,2) &amp; ((0,2),0) &amp; ((0,2),1) &amp; \color{red}{\mathbf{((0,2),2)}} \<br />
(1,0) &amp; ((1,0),0) &amp; \color{red}{\mathbf{((1,0),1)}} &amp; ((1,0),2) \<br />
(1,1) &amp; ((1,1),0) &amp; ((1,1),1) &amp; \color{red}{\mathbf{((1,1),2)}} \<br />
(1,2) &amp; \color{red}{\mathbf{((1,2),0)}} &amp; ((1,2),1) &amp; ((1,2),2) \<br />
(2,0) &amp; ((2,0),0) &amp; ((2,0),1) &amp; \color{red}{\mathbf{((2,0),2)}} \<br />
(2,1) &amp; \color{red}{\mathbf{((2,1),0)}} &amp; ((2,1),1) &amp; ((2,1),2) \<br />
(2,2) &amp; ((2,2),0) &amp; \color{red}{\mathbf{((2,2),1)}} &amp; ((2,2),2) \<br />
\end{array}<br />
$$</p>
<p>Observe that if we add the values inside the “inner tuple” modulo $3$ we get the third number in the tuple. For example, in the bottom row, we see that $2 + 2 = 1 \pmod 3$.</p>
<h2>Functions generally “exist” — but computability is a different story</h2>
<p>Thinking about functions as a subset of a Cartesian product may be a little weird at first — especially since such definitions don’t readily translate to code.</p>
<p>But ZK is heavily influenced by the mathematician definitions, so it’s helpful to have this vocabulary in our back pocket.</p>
<p>It’s helpful to conceptualize functions as a map that takes an element from one set and returns an element in another set.</p>
<p><strong>Exercise:</strong> Pick a subset of ordered pairs that defines $a \times b \pmod 3$.</p>
<p><strong>Exercise:</strong> Define our set $A$ to be the numbers $\set{0,1,2,3,4}$ and our binary operator to be subtraction modulo $5$. Define all the ordered pairs of $A \times A$ in a table, then map that set of ordered pairs to $A$.</p>
<p>A <em>closed</em> binary operator takes any two elements of a set, and outputs another element from the same set. The closed part is important, as it restricts the output to be in the same set.</p>
<p>Specifically, start with a set $A$ and construct a binary operator as follows:</p>
<p>Take a Cartesian product of a set with itself, $A \times A$ and call this set of ordered pairs $P$.</p>
<p>Take the Cartesian product of $P$ with $A$ and take a subset of that such that $P\times A$ is well defined.</p>
<p>Division over integers is not a binary operator because what really happens is we do the $P = (\mathbb{Z} \times \mathbb{Z})$ then take a subset of $P \times \mathbb{Q}$ to get our relation. Division over integers is not closed because it can produce rational numbers.</p>
<p>We are going to deal with binary operators a lot on elliptic curves, integers, polynomials, matrices, etc.</p>
<p>When we can trust that the binary operator will have certain properties, then we can abstract away a lot of implementation detail.</p>
<p>For example, “adding” elliptic curve points is not exactly trivial, and the math is not obvious from the get go.</p>
<p>However, if you know the binary operator is closed, and follows certain properties, then how the “addition” is implemented does not matter! It’s a map that follows certain rules!</p>
<p>Let’s use a slightly more relatable example. If you multiply two square matrices of determinant $1$ together, the determinant of the product matrix will also be $1$. The proof is not something you can quickly work out in your head. But if you model this as a “set of $3\times 3$ matrices with determinant $1$ and binary operator multiply, and multiply is closed” then you suddenly have a lot of functional knowledge of a system you don’t know the implementation details about. You know no matter what you do, you’ll get a determinant $1$ matrix without having to know why.</p>
<p>Having the language to describe a binary operator as “closed” allows you to operate at a higher level and understand the bigger picture of transformations and not get bogged down in the implementation details.</p>
<p>You can reason about operations without understanding how they work! This is <em>extremely helpful</em> when it comes to dealing with very esoteric math such as <a href="https://rareskills.io/post/bilinear-pairing">Elliptic curve bilinear pairings</a>.</p>
<h3>Constructing valid binary operations</h3>
<p>When it comes to binary operators, we are not allowed to take a subset of $A \times A$ before mapping that to $A$. Binary operators must accept <em>all</em> members of set $A$ as its inputs. We of course must take a subset of the ordered pairs between $A \times A$ and $A$ because each pair from $A \times A$ must map to exactly one $A$. The result of a binary operation must have one unambiguous answer.</p>
<h2>Learn more with RareSkills</h2>
<p>The usefulness of the vocabulary from abstract algebra is why our <a href="https://rareskills.io/zk-bootcamp">zero knowledge course</a> does not dodge math. We just make sure we have our essential vocabulary down before we start using it.</p>
<p><em>Originally Published July 25, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>Abstract Algebra</h1>
<p>Source: https://rareskills.io/post/abstract-algebra</p>
<h1>Abstract Algebra</h1>
<p>Abstract Algebra is the study of sets that have one or more operators on that set. For our purposes, we only care about sets where the operator is a binary operator.</p>
<p>Given a set with a binary operator, we can categorize those sets based on how the binary operator behaves, and what elements are allowed (or expected) to be in the set.</p>
<p>Mathematicians have a terminology for every possible behavior of the binary operator on the set. As applied programmers, our primary concern is the Group (from Group Theory) in particular, but let’s work our way there incrementally. The group is just one type of animal in this large zoo. So rather than study the group in isolation, let’s study the group in its larger context of related algebraic structures (i.e. sets with a binary operator).</p>
<p>Abstract algebra is a massive field, but our objective here is to clearly understand what a <em>Group</em> is because that is used everywhere in Zero Knowledge Proofs. We could just give a definition right now:</p>
<p><em>A Group is a set with a binary operator that is closed, associative, has an identity element, and where each element has an inverse.</em></p>
<p>But that terse definition is not very enlightening. It’s more helpful to understand Groups as they relate to the larger field of Abstract Algebra.</p>
<h2>Magma</h2>
<p>A Magma is a set with a closed binary operator. That’s all.</p>
<p>A Magma is definitely something you understand intuitively as a programmer. Now you have a word for it.</p>
<p>For example, consider the set of all positive integers and our binary operator be $x^y$. Note that we don’t allow negative numbers because if $y$ is negative, we get a fraction.</p>
<p>Clearly, the output will be in the space of integers. Our function is a subset of the Cartesian product (relation) of $(\mathbb{Z}\times\mathbb{Z})$ and $\mathbb{Z}$. Specifically, $x^y$ is a function from $(\mathbb{Z}\times\mathbb{Z})$ to $\mathbb{Z}$.</p>
<p>Interestingly, this example is not commutative or associative. You can convince yourself of this by choosing values for <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, and <code style='font-family: Arial'>c</code> in the Python code below.</p>
<pre style='font-family: Arial'><code class="language-solidity">assert a ** (b ** c) != (a ** b) ** c
assert a ** b != b ** a
</code></pre>
<p>But we don’t care. A Magma is one of the least restrictive kinds of <em>algebraic structures</em>. All that matters is that the binary operator is closed. Everything else is fair game.</p>
<p>An <em>algebraic structure</em> is a set with a collection of operations on that set. For our purposes, the only operation we care about is a binary operator.</p>
<h2>Semigroup</h2>
<p>A Semigroup is a Magma where the binary operator must be associative.</p>
<p>All Semigroups are Magmas, but not all Magmas are Semigroups.</p>
<p>In other words, a Semigroup is a set with a binary operator that is closed and associative.</p>
<p>Let our (infinite) set be the set of all possible non-empty strings from the traditional alphabet a, b, c,…, x, y, z. For example, “az”, “programmer”, “zero”, “asdfghjk”, “foo”, and “bar” are all in this set.</p>
<p>Let our binary operator be the concatenation of strings. This is closed, because it produces another string in the set.</p>
<p>Note that if we commute “foo” and “bar”, the output string will not be the same, i.e., “foobar” and “barfoo”. However, that does not matter. Both “foobar” and “barfoo” are members of the set, so the binary operator “concatenate” is closed. Because we have a set with a closed and associative binary operator, the set of all strings under concatenation is a Semigroup.</p>
<p><strong>Exercise:</strong> Work out for yourself that concatenating “foo”, “bar”, “baz” in that order is associative. Remember, associative means $(A \square B) \square C = A \square (B \square C)$, where $\square$ is the Semigroup’s binary operator.</p>
<p><strong>Exercise:</strong> Give an example of a Magma and a Semigroup. The Magma must not be a Semigroup. Don’t use the examples above. This means you must think of a binary operator that is closed but not associative for the Magma, and for the Semigroup, the binary operator must be closed and associative.</p>
<h2>Monoid</h2>
<p>A Monoid is a Semigroup with an identity element.</p>
<p>Awww yes, this is the same Monoid from the “A monad is a monoid in the category of endofunctors.”</p>
<p><img alt="Math meme about monad tutorials" src="assets/935a00_bd9a8814842b4359b63050555a1c1c95_mv2.png" /></p>
<p>If we look at the <a href="https://typelevel.org/cats/typeclasses/monoid.html">monoid documentation</a> in the Cats library for Scala, we see these definitions explicitly:</p>
<pre style='font-family: Arial'><code class="language-solidity">trait Semigroup[A] {
    def combine(x: A, y: A): A
}

trait Monoid[A] extends Semigroup[A] {
    def empty: A
}
</code></pre>
<p>The Cats library simply refers to “identity” as <code style='font-family: Arial'>empty</code> and the binary operator as <code style='font-family: Arial'>combine</code>. The fact that Monoid extends Semigroup shows that a Monoid is a Semigroup with the requirement that it has an “empty” (identity).</p>
<p>The snippet above doesn’t show it, but it is indeed required that combine be associative.</p>
<p>A Semigroup just has a binary operator with no restrictions on it except that it outputs the same type (<code style='font-family: Arial'>A</code>) as the inputs (<code style='font-family: Arial'>x</code>, and <code style='font-family: Arial'>y</code>).</p>
<p>For example, addition over positive integers without zero is a Semigroup, but if you include zero, it becomes a Monoid.</p>
<p>An <em>identity element</em> means you apply a binary operator with the identity element and another element $a$, you get $a$. In the example of addition $8 + 0 = 8$, where $0$ is the identity element. If your binary operator is multiplication, then the identity element would be $1$, since multiplying by $1$ gives the same number back.</p>
<p>If our set is the set of all $2\times 2$ matrices and our binary operator is matrix multiplication, the identity element would be the identity matrix</p>
<p>$$<br />
\begin{bmatrix}<br />
1&amp;0\<br />
0&amp;1\<br />
\end{bmatrix}<br />
$$</p>
<h3>Sets of sets over union and intersection</h3>
<p>Something bizarrely absent from our earlier discussion of sets was the mention of union and intersection of sets. These are binary operators, and now is a good time to introduce them.</p>
<p>If you take the union of two sets $\set{1,2,3,4}$ and $\set{3,4,5,6}$, you get $\set{1,2,3,4,5,6}$. If you take the intersection of $\set{1,2,3,4}$ and $\set{3,4,5,6}$, you get $\set{3, 4}$.</p>
<p>It should be clear that both of these operators are associative.</p>
<p>If we define our domain to be the set of all finite sets of integers, then the binary operators union and intersection are closed because their result is a set of integers.</p>
<p>Set union has an identity element in this domain: the empty set $\set{}$. Take the union of a set with the empty set and you get the original set, i.e. $A \cup \set{} = A$.</p>
<p>Hence, the set of all <em>finite</em> sets of integers over union is a Monoid.</p>
<p>However, in the set of all possible finite sets of integers under intersection ($\cap$), it is a Semigroup — no finite set will work as the identity. But if we expand this set to include $\mathbb{Z}$ itself — that is, our set is ${\text{all finite sets of integers}}\cup\mathbb{{Z}}$ under intersection, then it becomes a Monoid, as $\mathbb{Z}$ is the identity element.</p>
<p>If it feels like we “hacked” the identity element in, we did.</p>
<p>We’ll see later that elliptic curves use a trick like this and include a special point called the “point at infinity” to stay consistent with the algebraic laws. The point is that we need to be very clear what our identity element is if we say a set is a Monoid over some binary operator.</p>
<p>As another example, we could say our set is all positive integers under addition, with the additional element $\text{mug}$. We define $\text{mug} + x = x$ and $x + \text{mug} = x$. As the architects of the systems, we are allowed to make our set consist of whatever we like, and the binary operator behave however we like. However, the binary operator must be closed, associative, and the set must have an identity element for that algebraic data structure to be a Monoid.</p>
<p>If we restrict the domain to be all subsets of $\set{0,1,2,3,4,5}$, then intersection clearly becomes a Monoid because the identity element would be $\set{0,1,2,3,4,5}$, as any set of integers you intersect with it will produce the other set, i.e., $A \cap \set{0,1,2,3,4,5} = A$. For example, $\set{1,3,4} \cap \set{0,1,2,3,4,5} = \set{1,3,4}$.</p>
<p>At this point it should be clear that the category of an algebraic structure for a given binary operator is very sensitive to the domain of the set.</p>
<p><strong>Exercise:</strong> Let our binary operator be the function <code style='font-family: Arial'>min(a,b)</code> over integers. Is this a Magma, Semigroup, or Monoid? What if we restrict the domain to be positive integers (zero or greater)? What about the binary operator <code style='font-family: Arial'>max(a,b)</code> over those two domains?</p>
<p><strong>Exercise:</strong> Let our set be all 3 bit binary numbers (a set of cardinality 8). Let our possible binary operators be bit-wise and, bit-wise or, bit-wise xor, bit-wise nor, bit-wise xnor, and bit-wise nand. Clearly this is closed because the output is a 3 bit binary number. For each binary operator, determine if the set under that binary operator is a Magma, Semigroup, or Monoid.</p>
<h2>Group – The Star of the Show</h2>
<p>A Group is a Monoid in which each element has an inverse.</p>
<p>Or to be explicit, it is a set with four properties</p>
<ol>
<li>The binary operator is closed (Magma)</li>
<li>The binary operator is associative (Semigroup)</li>
<li>The set has an identity element (Monoid)</li>
<li>Every element has an <em>inverse</em></li>
</ol>
<p>That is, for any element $a$ in the set $A$, there exists an $a’$ such that $a \square a’ = i$ where $i$ is the identity element and $\square$ is the binary operator. Spoken more mathematically, that would be:</p>
<p>$$<br />
\forall a \in A \space\space \exists a’ \in A: a\square a’ = i<br />
$$</p>
<p>Here, $\square$ is the binary operator of the set.</p>
<p>It is rather incorrect to say “the set has an inverse.” To be precise, every element has another element in the set that is that element’s inverse.</p>
<p>Using integers with addition, the identity element is zero (because you add zero, you get the same number back), and the inverse of an integer is that integer with the sign flipped (e.g. the inverse of $5$ is $-5$ and the inverse of $-7$ is $7$).</p>
<p>Going back to the domain sensitivity, addition over positive integers is not a group because there can be no inverse elements.</p>
<p>Here is a table to drive the point home</p>
<table>
<thead>
<tr>
<th>set domain</th>
<th>binary operator</th>
<th>algebraic structure</th>
<th>reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>non-zero positive integers</td>
<td>addition</td>
<td>Semigroup</td>
<td>no identity</td>
</tr>
<tr>
<td>positive integers including zero</td>
<td>addition</td>
<td>Monoid</td>
<td>has identity, no inverses</td>
</tr>
<tr>
<td>all integers</td>
<td>addition</td>
<td>Group</td>
<td>every element has an inverse</td>
</tr>
</tbody>
</table>
<p>Note that “inverse” is not meaningful if the set does not have an identity. By definition, applying the binary operator to an element and that element’s inverse results in the identity element.</p>
<p><strong>Exercise:</strong> Why can’t strings under concatenation be a group?</p>
<p><strong>Exercise:</strong> Polynomials under addition satisfy the property of a group. Demonstrate this is the case by showing it matches all the properties that define a group.</p>
<p>Unfortunately, our tutorial must end here, because elementary group theory is the subject of another chapter.</p>
<p>But now you have a lot of context to understand what a group is even though we barely discussed it here!</p>
<h3>A word about commutativity</h3>
<p>None of the algebraic data structures above are required to be commutative. If they are, we say they are abelian over their binary operator. In this context, “abelian” means that the binary operator is commutative, which means the order of the operands does not affect the result.</p>
<p>Abelian means the binary operator is commutative.</p>
<p>But say abelian, you’ll sound smarter.</p>
<p>The technicality is we don’t normally say “addition is abelian” but “the group is abelian over addition.”</p>
<h2>Subsets again</h2>
<p>Let’s tie this all back to what we learned at the beginning. Magmas, Semigroups, Monoids, and Groups are all sets that have a closed binary operator. A binary operator is just a map from all the ordered pairs of the set’s Cartesian product with itself back to itself.</p>
<p>Groups are a subset of Monoids, Monoids are a subset of Semigroups, Semigroups are a subset of Magmas, and Magmas are a subset of sets in general. Every Group is also a Magma or a set, but a Magma is not necessarily a Group.</p>
<p>“Sets” are easy to conceptualize, but when we start talking about groups and other algebraic structures, it’s easy to start getting lost. Groups are very important in our study of cryptography. Just remember:</p>
<p><strong>Groups are sets with a binary operator that follows four rules.</strong></p>
<p>Also, it’s time to free your mind from “addition” and “multiplication” being the primary way of combining things.</p>
<p>We are allowed to take a Cartesian product of a set (which could be anything) with itself then map that set of ordered pairs back to the set. This is a binary operator. If it follows the construction above, then it is closed.</p>
<h2>Math vocabulary doesn’t need to scare us</h2>
<p>Before you began this tutorial, the sentence</p>
<p>“The set of strings over the binary operator string concatenation is a Semigroup or a Monoid depending on the presence or absence of the empty string in the set”</p>
<p>probably didn’t make sense.</p>
<p>You might still have to translate that in your head like most learners of a second language, but you realize it’s actually packing a lot of information into a tiny space.</p>
<p>Could I say that sentence without the mathiness? Of course I could, but it would take me at least 500 words to do it clearly. It’s actually worth understanding what those terms mean. This will save us a lot of trouble in the long run.</p>
<p>What makes it cool is there are a plethora of theorems about Groups that let us make claims about the group <em>without understanding how the binary operator of the group works under the hood.</em> This is somewhat analogous to polymorphism in object-oriented programming or traits in functional languages. They hide implementation details from you and let you focus on the high level. That is powerful.</p>
<p>In the next chapter, you will learn how groups can be “related” to each other through homomorphisms.</p>
<h2>Learn more with RareSkills</h2>
<p>See our <a href="https://rareskills.io/zk-bootcamp">Zero Knowledge Course</a> to learn with our community.</p>
<p>Our <a href="https://rareskills.io/zk-book">ZK Book</a> contains the rest of the ZK tutorials in this series.</p>
<p><em>Originally Published July 25, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>Elementary Group Theory for Programmers</h1>
<p>Source: https://rareskills.io/post/group-theory</p>
<h1>Elementary Group Theory for Programmers</h1>
<p><img alt="Group Theory Hero Image" src="assets/935a00_fd134878f9e449418c68ee94bc24cfd9_mv2.png" /></p>
<p>This article provides several examples of algebraic groups so that you can build an intuition for them.</p>
<p>A group is a set with:</p>
<ul>
<li>a closed binary operator</li>
<li>the binary operator is also associative</li>
<li>an identity element</li>
<li>every element having an inverse</li>
</ul>
<p>We also discussed abelian groups. An abelian group has the additional requirement that the binary operator is commutative.</p>
<p>Now it’s time to discuss groups as a mathematical structure.</p>
<p>One confusing aspect of using integers under addition as an example of a group is that students often respond with, “but can’t you also multiply integers?”</p>
<p>Admittedly, this can be confusing. There are other algebraic structures, like rings and fields, that involve two binary operators. However, for now, think of a group as having one fixed and unchanging binary operator, and from the group’s perspective, any other possible binary operator either does not exist or isn’t a concern.</p>
<p>Here’s where it gets even more confusing. Sometimes binary operators are other binary operators in disguise.</p>
<p>For example, when dealing with groups that only have addition, sometimes writers casually refer to multiplication even though the group does not have that binary operator. What multiplication is in this context is really shorthand for repeating an addition operation a certain number of times.</p>
<h2>Examples of groups</h2>
<p>The best way to get a feel for a group is to see a lot of them. Let’s do that.</p>
<h3>1. The trivial group</h3>
<p>Let our group be a set consisting of the number $\set{0}$ with the binary operator of addition. It is clear that the binary operator is closed and associative</p>
<p>$$<br />
(0 + 0) + 0 = (0 + 0) + 0<br />
$$</p>
<p>and each element has an identity and inverse.</p>
<p>This is not an interesting group, but it is the smallest valid you can create.</p>
<p>Note that a group cannot be empty because by definition it must contain an identity element.</p>
<p>As an exercise for the reader, show that the set $\set{1}$ with the binary operator $\times$ is a group.</p>
<h3>2. Real numbers are not a group under multiplication</h3>
<p>Although reals ($\mathbb{R}$) under multiplication has an identity (the number $1$) and is closed and associative, they do not all have an inverse.</p>
<p>Real numbers are invertible by taking their multiplicative inverse $(1 / n)$, but zero, although a real number, cannot be inverted because $1/0$ is undefined and not a real number.</p>
<p>Strictly speaking, the inverse of $a$, is $b$ such that $ab = 1$. Here we are saying that if $a = 0$ there is no element in the set $b$ such that $ab = 1$.</p>
<p>Positive real numbers excluding zero are a group under multiplication however. Every element has an inverse ($1/n$), and the identity element is 1.</p>
<p><strong>Exercise:</strong> Integers (positive and negative) are not a group under multiplication. Show which of the four requirements (closed, associative, existence of identity, all elements having an inverse) are not satisfied.</p>
<h3>3. $n \times m$ matrices of real numbers are a group under addition</h3>
<p>Let’s work it out:</p>
<p>Matrix addition is closed and associative. If you add an $n × m$ matrix of real numbers with another $n × m$ matrix of real numbers, you get an $n × m$ matrix of real numbers.<br />
The identity matrix is an $n × m$ matrix of all zeros<br />
the inverse of a matrix is that matrix multiplied by -1.<br />
Hey wait, we aren’t allowed to multiply by -1 right?</p>
<p>A group doesn’t require the inverse to be “computable using the group binary operator” only to exist. That is, we compute the inverse as multiplying each element by $-1$ even though multiplication by $-1$ is not a group operation.</p>
<p>If we define our operator for $n \times m$ matrices to be the Hadamard product (element-wise multiplication), this cannot be a group for the same reason discussed above. Specifically, the inverse is computed as the reciprocal of each element in the matrix, and if one of the elements is zero, then the inverse cannot be computed.</p>
<p>If we define our operator to be traditional matrix multiplication over square matrices, this may or may not be a group depending on the set definition, as we will see in section example 5.</p>
<h3>4. The set of 2D points on an euclidean plane under element-wiseaddition is a group</h3>
<p>This is actually a special case of the previous example, but let’s look at it through a different angle.</p>
<p>Consider the set of all real-valued $(x, y)$ points on a standard 2D plane.</p>
<p>Our binary operator is adding points together, for example $(1,1) + (2,2) = (3,3)$.</p>
<p>Our identity element is the origin, because adding with that will result in the same location of the other point.</p>
<p>The inverse of a point is simply the “mirror image” over the origin (with the $x$ and $y$ coordinates negated) because when you add a point to its inverse, they result in the origin.</p>
<h3>5. $n \times n$ matrices of non-zero determinant under multiplication are a group</h3>
<p>By way of review, if a matrix has a non-zero determinant, then it is invertible. When a matrix of non-zero determinant is multiplied by another matrix of non-zero determinant, then the product also has a non-zero determinant. Actually, we can be more specific, if $A$, $B$, and $C$ are square matrices, and $AB = C$, then $\det(A) \times \det(B) = \det(C)$.</p>
<p>Let’s work through the definitions</p>
<ul>
<li>Multiplication of non-zero determinant matrices is closed because you cannot “leave the group” as the product will always have non-zero determinant. Matrix multiplication is associative.</li>
<li>The identity element is the identity matrix (all zeros, except the main diagonal is one).</li>
<li>The inverse is simply the matrix inverse, and matrices with determinant of non-zero are invertible.</li>
</ul>
<h3>6. $n \times n$ matrices of zero determinant under multiplication are a not group</h3>
<p>Remember, a matrix with zero determinant cannot be inverted, so this set cannot have an inverse. In this case, we do not have an identity element because the identity matrix has determinant one. Since we have no identity element, this set and binary operator isn’t even a monoid, it’s a semigroup.</p>
<h3>7. The set of all polynomials of a fixed upper-bounded degree is a group under addition</h3>
<p>If we say “all polynomials with real coefficients of degree at most 7 under addition” this is a valid group.</p>
<ul>
<li>Polynomial addition is closed and associative</li>
<li>The identity is the polynomial $0$ (or $0x^0$ to be precise)</li>
<li>The inverse is the coefficients multiplied by $-1$</li>
<li>We cannot say degree “exactly $7$” because the identity element has degree 0, and wouldn’t be part of the group.</li>
</ul>
<p>Polynomials of a fixed upper-bounded degree under multiplication are not a group, because generally the degree gets larger when you multiply polynomials, hence the operator would not be closed.</p>
<h3>8. Addition modulo a prime number is a group</h3>
<p>Let’s take a prime number $7$.</p>
<p>Here, the identity is still $0$, because you add by $0$ and get the same number back.</p>
<p>In this situation, what would the inverse of $5$ be?</p>
<p>It would just be $2$, because $7 – 5 = 2$, or $5 + 2 \mod 7$ is zero (the identity).</p>
<h3>9. Multiplication modulo a prime number is not a group</h3>
<p>Zero doesn’t have an inverse for this example.</p>
<p>If we omit the number $0$, then we have a group. The identity element is 1, and the inverse of an element $a$ is its modular inverse $a^{-1}$.</p>
<h3>10. A fixed based raised to integer powers under multiplication is a group</h3>
<p>If two integer powers of $b$ are multiplied together, the result is the integer power of the product of the bases. For example, $2^3 \times 2^4 = 2^{3 + 4} = 2^7$. This works for arbitrary bases:</p>
<p>$$<br />
b^x \times b^y = b^{x + y}<br />
$$</p>
<p>The identity element is $b^0$, which is $1$, and the inverse of $b^x$ is $b^{-x}$.</p>
<h2>Finite groups</h2>
<p>As the name suggests, a finite group has a finite number of elements in it. The set of all integers under addition is not finite, but addition of integers modulo a prime number is a finite group.</p>
<p>In zero knowledge proofs, we only use finite groups.</p>
<h2>Order of a group</h2>
<p>The order of a group is the number of elements in it.</p>
<h2>Cyclic groups</h2>
<p>A cyclic group is a group that has an element such that every element in the group can be “generated” by applying the binary operator repeatedly to that element, or to it’s inverse.</p>
<h3>Examples of a cyclic groups</h3>
<h4>Example 1: The group consisting of 0 under addition is a cyclic group</h4>
<p>This is trivially true because $0 + 0 = 0$ generates every element in the group.</p>
<h4>Example 2: Addition modulo 7 is a cyclic group</h4>
<p>If we start with $1$ and add $1$ to itself repeatedly, we will generate every element in the group. For example:</p>
<p>$$<br />
\begin{align*}<br />
&amp;1 + 1 &amp;= 2 \<br />
&amp;1 + 1 + 1 &amp;= 3 \<br />
&amp;1 + 1 + 1 + 1 &amp;= 4 \<br />
&amp;1 + 1 + 1 + 1 + 1 &amp;= 5 \<br />
&amp;1 + 1 + 1 + 1 + 1 + 1 &amp;= 6 \<br />
&amp;1 + 1 + 1 + 1 + 1 + 1 + 1 &amp;= 0 \<br />
&amp;1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 &amp;= 1 \<br />
\end{align*}<br />
$$</p>
<h4>Example 3: Multiplication modulo $7$ is a cyclic group, if we exclude zero</h4>
<p>We have to be careful when picking the generator here, because, for example, $2$ won’t generate the whole group.</p>
<p>$$<br />
\begin{align*}<br />
&amp;2 * 2 &amp;\equiv 4 \pmod 7 \<br />
&amp;2 * 2 * 2 &amp;\equiv 1 \pmod 7 \<br />
&amp;2 * 2 * 2 * 2 &amp;\equiv 2 \pmod 7 \<br />
&amp;2 * 2 * 2 * 2 * 2 &amp;\equiv 4 \pmod 7 \<br />
&amp;…\<br />
\end{align*}<br />
$$</p>
<p>We can see that $2$ can only generate $\set{1,2,4}$. However, if we pick $3$ as the generator, then we can generate the whole group.</p>
<p>$$<br />
\begin{align*}<br />
3 ^ 2 \equiv 2 \pmod 7 \<br />
3 ^ 3 \equiv 6 \pmod 7 \<br />
3 ^ 4 \equiv 4 \pmod 7 \<br />
3 ^ 5 \equiv 5 \pmod 7 \<br />
3 ^ 6 \equiv 1 \pmod 7 \<br />
3 ^ 7 \equiv 3 \pmod 7 \<br />
&amp;…\<br />
\end{align*}<br />
$$</p>
<p>The reason why $3$ works and $2$ does not is because $3$ is a <em><a href="https://en.wikipedia.org/wiki/Primitive_root_modulo_n">primitive root</a></em>.</p>
<p>We can use the galois library to find primitive roots:</p>
<pre style='font-family: Arial'><code class="language-solidity">from galois import GF
GF7 = GF(7)
print(GF.primitive_elements)
# [3, 5]
</code></pre>
<p>From the code output above, we can see that $5$ will also generate all the non-zero elements.</p>
<h3>If a group is cyclic, then it is abelian</h3>
<p>This one is a bit more subtle, but consider this:</p>
<p>In a cyclic group, every element in the group can be generated as $(g + g + … + g)$. Let’s arbitrarily pick an element $r$. Let’s partition this set of additions like so</p>
<p>$$<br />
r = \underbrace{g + g + \dots + g}_\text{m times} + \underbrace{g + g + \dots + g}_\text{n times}<br />
$$</p>
<p>Because of associativity, we can add parenthesis</p>
<p>$$<br />
r = \underbrace{(g + g + \dots + g)}_\text{m times} + \underbrace{(g + g + \dots + g)}_\text{n times}<br />
$$</p>
<p>Let $g$ added to itself $m$ times be $p$ and $g$ added to itself $n$ times be $q$. Therefore,</p>
<p>$$r = p + q$$</p>
<p>By associativity, we can repartition the original equation as follows (note that the $m$ and $n$ switched places!)</p>
<p>$$<br />
r = \underbrace{(g + g + \dots + g)}_\text{n times} + \underbrace{(g + g + \dots + g)}_\text{m times}<br />
$$</p>
<p>And we get back:</p>
<p>$$r = q + p$$</p>
<p>Hence, if the group is cyclic, then the group is abelian.</p>
<p>Note that the converse of this statement isn’t true. Real numbers under addition are an abelian group, but they are not cyclic.</p>
<p>Cyclic groups don’t have to be arithmetic modulo some prime number, but these are the only cyclic groups we will be using in our discussion of zero knowledge proofs.</p>
<h2>The identity element of a group is unique</h2>
<p>A group cannot have two identity elements. Don’t overthink this one, it’s derived by simple contradiction. Let’s say $\square$ is our binary operator and $e$ is our identity element.</p>
<p>Let’s say we have an alternative identity element $e’$. This means the following:</p>
<p>$$a \square a^{-1} = e \text{ and } a \square a^{-1} = e’$$</p>
<p>If we say $e≠e’$, then it must also be true that</p>
<p>$$a \square a^{-1} \neq a \square a^{-1}$$</p>
<p>But that is obviously a contradiction, hence identity elements must be unique.</p>
<h2>The use of groups in zero knowledge proofs</h2>
<p>Understanding group theory for the purpose of zero knowledge proofs is more of an exercise in vocabulary. When we say “inverse” or “identity” we want to you to immediately grasp what those mean.</p>
<p>Additionally, groups help us reason about very complex mathematical objects without understanding how they work. We’ve used familiar examples in this article, but later on we will deal with very unfamiliar objects such as <em>elliptic curves over field extensions</em>. Even though you don’t know what exactly that is, if we tell you it is a group, you already know a lot about what it is.</p>
<h2>Learn more with RareSkills</h2>
<p>This article is part of a series on ZK proofs. See our <a href="https://rareskills.io/zk-book">ZK Book</a> for the full series.</p>
<p><em>Originally Published August 1, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>Homomorphisms</h1>
<p>Source: https://rareskills.io/post/homomorphisms</p>
<h1>Homomorphisms by Example</h1>
<p>A homomorphism between two groups exists if a <em>structure preserving map</em> between the two groups exists.</p>
<p>Suppose we have two algebraic data structures $(A,\square)$ and $(B, \blacksquare)$, where the binary operator of $A$ is $\square$ and the binary operator of $B$ is $\blacksquare$.</p>
<p>A homomorphism from $A$ to $B$ exists if and only if there exists a function $\phi: A\rightarrow B$ such that</p>
<p>$$<br />
\phi(a_i \square a_j)=\phi(a_i)\blacksquare\phi(a_j)\space\space\forall a_i,a_j\in A<br />
$$</p>
<p>In other words, if $a_i \square a_j = a_k$, then $\phi(a_i) \blacksquare \phi(a_j) = \phi(a_k)$.</p>
<p>Note that a homomorphism is one-directional. The function $\phi$ takes elements in $A$ and maps them to elements in $B$. We have no requirements about “going backwards.”</p>
<p>We’ll first show some simple examples, provide some clarification, and then provide more complex examples.</p>
<h2>Simple examples of Homomorphisms</h2>
<h3>All integers under addition to all even integers under addition</h3>
<p>Let $A$ be the set of all integers under addition, and let $B$ be the set of all even integers under addition. It is clear that both $A$ and $B$ are groups.</p>
<p>Let $\phi(x)=2x$</p>
<p>We see that $\phi$ defines a homomorphism from $A$ to $B$ because the following is true for any pair of integers $a_i$ and $a_j$</p>
<p>$$<br />
\begin{align*}<br />
\phi(a_i+a_j)&amp;=\phi(a_i)+\phi(a_j)\<br />
2(a_i+a_j)&amp;=2(a_i)+2(a_j)<br />
\end{align*}<br />
$$</p>
<h3>All strings under concatenation to all integers zero or greater</h3>
<p>For example, let $A$ be the Monoid of all strings (including the empty string) under concatenation, and let $B$ be the Monoid of all integers zero or greater under addition.</p>
<p>There exists a homomorphism from $A$ to $B$ because there exists a function $\phi$ that maps strings to integers zero or greater and preserves the following property</p>
<p>$$<br />
\phi(a_i+a_j)=\phi(a_i)+\phi(a_j)<br />
$$</p>
<p>In this case, $\phi$ is the <em>length</em> of the string. For example:</p>
<p>$$<br />
\begin{align*}<br />
\text{Rare}&amp;\rightarrow 4\<br />
\text{Skills}&amp;\rightarrow 6\<br />
\text{RareSkills}&amp;\rightarrow 10\<br />
\end{align*}<br />
$$</p>
<p>Here, we have</p>
<p>$$<br />
\begin{align*}<br />
\phi(\text{Rare})&amp;=4\<br />
\phi(\text{Skills})&amp;=6\<br />
\phi(\text{RareSkills})&amp;=10\<br />
\phi(\mathsf{concat}(\text{Rare},\text{Skills}))&amp;=\phi(\text{Rare})+\phi(\text{Skills})\<br />
\end{align*}<br />
$$</p>
<h3>All real numbers under addition to all $n\times m$ matrices of real number under addition</h3>
<p>Some homomorphisms may seem rather trivial once you get the hang of them. This is an example of such a homomorphism. In this case, our function $\phi$ simply repeats the real number $n\times m$ times. For example, if $n=3$ and $m=2$, then $\phi(8.8)$ would be:</p>
<p>$$<br />
\begin{bmatrix}<br />
8.8&amp;8.8\<br />
8.8&amp;8.8\<br />
8.8&amp;8.8<br />
\end{bmatrix}<br />
$$</p>
<p>As an example, if $\phi(8.8 + 0.2)=\phi(8.8)+\phi(0.2)$ because</p>
<p>$$<br />
\begin{bmatrix}<br />
9&amp;9\<br />
9&amp;9\<br />
9&amp;9\<br />
\end{bmatrix}=<br />
\begin{bmatrix}<br />
8.8&amp;8.8\<br />
8.8&amp;8.8\<br />
8.8&amp;8.8<br />
\end{bmatrix}+<br />
\begin{bmatrix}<br />
0.2&amp;0.2\<br />
0.2&amp;0.2\<br />
0.2&amp;0.2\<br />
\end{bmatrix}<br />
$$</p>
<h2>Clarifications about Homomorphisms</h2>
<ul>
<li>$\phi$ must work with every possible pair of elements from $A$ (including pairs of the same element). However, it does not need to “access” all the elements of $B$. For example, a homomorphism that maps every element in $A$ to the identity element in $B$ is a valid homomorphism, but not a useful one. It’s called the <em>trivial homomorphism</em>.</li>
<li>If we choose two arbitrary sets with a binary operator, a homomorphism may not necessarily exist.</li>
<li>There may be a homomorphism from $A$ to $B$, but not necessarily from $B$ to $A$.</li>
<li>If there is a homomorphism from $A$ to $B$ and from $B$ to $A$, and $\phi$ is the map from $A$ to $B$, the inverse of $\phi$ may not necessarily be a valid map for the homomorphism from $B$ to $A$</li>
</ul>
<h2>More examples of homomorphisms</h2>
<h3>Integers under addition to integer powers of $b$ under multiplication</h3>
<p>Suppose we have group $A=(\mathbb{Z},+)$ (the set of all integers under addition) and the group $B$, which is the set of all integer powers of $b$ under multiplication, i.e., $B=(b^i\space:\space i\in\mathbb{Z}, \times)$. We can arbitrarily set $b=2$ to make the example easier to think about.</p>
<p>There exists a homomorphism from $A$ to $B$, defined by $\phi(x)=b^x$. By the rules of algebra,</p>
<p>$$<br />
\begin{align*}<br />
\phi(a_i+a_j)&amp;=\phi(a_i)\times\phi(a_j)\<br />
b^{a_i + a_j}&amp;=b^{a_i}b^{a_j}<br />
\end{align*}<br />
$$</p>
<p>To understand why this relationship holds, consider that</p>
<p>$$<br />
b^{a_i}=\underbrace{b\cdot b\cdot\dots\cdot b}_{{a_i} \text{ times}}<br />
$$</p>
<p>$$<br />
b^{a_j}=\underbrace{b\cdot b\cdot\dots\cdot b}_{{a_j} \text{ times}}<br />
$$</p>
<p>$$<br />
b^{a_i + a_j}=\underbrace{b\cdot b\cdot\dots\cdot b}_{{a_i} \text{ times}} \cdot \underbrace{b \cdot b \cdot \dots \cdot b}_{a_j \text{ times}}<br />
$$</p>
<p>$$<br />
b^{a_i +a_j}=\underbrace{b\cdot b\cdot\dots\cdot b\cdot b\cdot b\dots\cdot b}_{{a_i+a_j} \text{ times}}<br />
$$</p>
<h3>Integers under addition to integer powers of $b$ under multiplication modulo a prime number</h3>
<h3>$n\times m$ matrices under addition to integers under addition</h3>
<p>In this case, $\phi$ simply adds up all the elements in a matrix. Why this works is left as an exercise for the reader.</p>
<h3>$2\times2$ matrices of integers under multiplication to integers under multiplication</h3>
<p>There is a homomorphism from the first to the second Monoid because $\phi$ is the <em>determinant</em> of the matrix and the following rule holds:</p>
<p>$$<br />
XY=Z\rightarrow\det(X)\det(Y)=\det(Z)<br />
$$</p>
<p>where $X,Y,Z$ are $2\times2$ integer matrices. Why these are two algebraic data structures are Monoids and not groups are left as an exercise for the reader.</p>
<h3>The group of rational numbers (excluding rational numbers where the denominator is a multiple of $p$) to addition modulo $p$</h3>
<p>This concept was already taught in our article on <a href="https://rareskills.io/post/finite-fields">finite fields</a>, but we didn’t use the term “homomorphism” to describe it.</p>
<p>Let $A$ be the group of all rational numbers whose denominators are not a multiple of $p$, under addition. Let $B$ be the finite field modulo $p$.</p>
<p>There exists a homomorphism from group $A$ to group $B$. $\phi$ is</p>
<p>$$<br />
\phi(x) = \mathsf{numerator}(x)\times\mathsf{modular\_inverse}(\mathsf{denominator}(x)) \pmod p<br />
$$</p>
<p>Or in Python:</p>
<pre style='font-family: Arial'><code class="language-solidity">p = 11
def phi(num, den):
    return num * pow(den, -1, p) % p
</code></pre>
<p>For example:</p>
<ul>
<li>$1/3 + 3/5 = 14/15$</li>
<li>$1/3$ is congruent to $6 \pmod {17}$</li>
<li>$3/5$ is congruent to $4 \pmod {17}$</li>
<li>$4 + 6\equiv10 \pmod {17}$</li>
<li>$14/15\equiv 10 \pmod {17}$</li>
</ul>
<p>Saying $1/3$ is congruent to $6 \pmod {17}$ is equivalent to the statement $\phi(1/3)=6$.</p>
<h3>The Monoid of rational numbers (excluding rational numbers where the denominator is a multiple of $p$) to multiplication modulo $p$ (excluding zero)</h3>
<p>Let’s use the same example as above, but with multiplication. The function $\phi$ remains the same.</p>
<ul>
<li>$1/3 * 3/5 = 1/5$</li>
<li>$1/3$ is congruent to $6 \pmod {17}$</li>
<li>$3/5$ is congruent to $4 \pmod {17}$</li>
<li>$4 \times 6\equiv7 \pmod {17}$</li>
<li>$1/5\equiv 7 \pmod {17}$</li>
</ul>
<h2>Exercises for the reader</h2>
<p>Find a homomorphism for the following pairs of algebraic data structures. If you get stuck (or just don’t want to solve the problem), you can Google the answer or consult with a chatbot.</p>
<ol>
<li>Real numbers under addition to polynomials with real coefficients under addition.</li>
<li>Polynomials with real coefficients to real numbers under addition. Hint: even though this look similar to problem 1, the function $\phi$ will be completely unrelated to the answer for the previous problem.</li>
<li>Positive real numbers greater than zero under multiplication to all real numbers under addition.</li>
</ol>
<h2>Homomorphic Encryption</h2>
<p>If $\phi$ is computationally difficult to invert, then $\phi$ <em>homomorphically encrypts</em> the elements of $A$.</p>
<p>Let $A$ be all integers under addition, and $B$ be the target group and $\blacksquare$ be binary operator of $B$.</p>
<h3>Zero Knowledge Addition, example 1</h3>
<p>Suppose we wish to prove to a verifier that we computed $2 + 3=5$. We would give the verifier $(x, y, 5)$ where $x=\phi(2), y= \phi(3)$ and the verifier check that:</p>
<p>$$<br />
x\blacksquare y \stackrel{?}=\phi(5)<br />
$$</p>
<p>Note that homomorphic encryption implies that the verifier knows the function $\phi$.</p>
<h3>Zero Knowledge Addition, example 2</h3>
<p>A prover makes the claim, “I have two numbers $a$ and $b$, and $b$ is five times $a$.” The prover sends $\phi(a)$ and $\phi(b)$ to the verifier, and the verifier checks that</p>
<p>$$\phi(a) + \phi(a) + \phi(a) + \phi(a) + \phi(a) = \phi(b)$$</p>
<p>Remember, “multiplication” here is not the binary operator, it’s simply shorthand for repeated addition.</p>
<p>In these examples, note that we didn’t say anything about what elements of $B$ are or what $\blacksquare$ is. $B$ can be scary mathematical objects, and $\blacksquare$ can be a scary mathematical operator, but <em>that doesn’t matter</em>.</p>
<p>This is the beauty of abstract algebra: <em>we don’t need to know</em>. As long as it has the properties we care about, we can reason about its behavior even if we know nothing about the implementation.</p>
<h2>Motivation</h2>
<p>Okay cool; we understand groups and homomorphisms, but how does this help us? The reason I went through all the effort explaining this is because I want you to understand the statement below:</p>
<p>“Elliptic curve points in a finite field under addition are a finite cyclic group and integers under addition are homomorphic to this group.”</p>
<p>You probably don’t know what elliptic curve points are or what adding them means, but you do know:</p>
<ol>
<li>The set of elliptic curve points under addition produces another elliptic curve point.</li>
<li>The binary operator that takes two elliptic curve points and returns another elliptic curve point is associative.</li>
<li>The set of elliptic curve points contains an identity element.</li>
<li>The elliptic curve group has an identity element, which is unique.</li>
<li>Each elliptic curve point has an inverse such that adding a point and its inverse produces the identity.</li>
<li>Because the group is cyclic, every elliptic curve point can be generated by repeatedly applying the binary operator to some generator element.</li>
<li>Because the group of elliptic curve points is cyclic, it is also an abelian group.</li>
<li>Because it is a finite group, the order is finite.</li>
<li>Because of the homomorphism, we have a strong idea of how the binary operator for elliptic curve points behaves. We can use the elliptic curve point binary operator to “add integers” in a certain sense.</li>
</ol>
<p>Even though you don’t know what elliptic curve points are, you already know nine things about them!</p>
<p>So whatever these bizarre objects “elliptic curve points” are, you know it behaves like, and has the same properties as the groups we discussed above.</p>
<p>Believe it or not, you are already 90% of the way to comprehending elliptic curves. It’s far easier to make sense of elliptic curves by understanding their similarity to other familiar structures than to try to understand their funky math from the ground up.</p>
<p>This is similar to me telling you that Ethereum uses “Patricia Merkle trees” to store state. You may not know what a “Patricia tree” or a “Merkle tree” is, but you do know:</p>
<ul>
<li>It has a root.</li>
<li>You can probably access elements in logarithmic time, or at least that is the intent.</li>
<li>Something useful is stored in the leaves.</li>
<li>There exists some algorithm to traverse the tree and access a leaf you care about.</li>
</ul>
<p>So when I tell you elliptic curve points under addition form a group, you should already know what to look out for when you learn about that subject.</p>
<p>Again, groups don’t need to be mysterious moon math. You’ve worked with groups intuitively as a programmer. Now you have a concrete word to describe this recurring phenomenon.</p>
<p>It’s far more efficient to say “group” than it is to say “this is a set with a way to combine elements associatively and the elements all have blah blah blah.”</p>
<p>I know this may seem like a huge tangent, but trust me, understanding “homomorphism” enables us to succinctly describe a concept that we will regularly see. It will also come in handy again when we discuss <a href="https://rareskills.io/post/quadratic-arithmetic-program">Quadratic Arithmetic Programs</a>. Homomorphisms appear frequently in the ZK world.</p>
<p>Imagine trying to discuss tree data structures without a word for “roots” or “leaves.” That would be immensely frustrating.</p>
<h2>Summary</h2>
<p>A homomorphism from $A$ to $B$ exists <strong>if and only if</strong> a function $\phi$ exists that takes an element from $A$ and returns and element from $B$ and $\phi(a_i \square a_j)=\phi(a_i)\blacksquare\phi(a_j)$ or all $a_i$ and $a_j$ in $A$, where $\square$ is the binary operator of $A$ and $\blacksquare$ is the binary operator of $B$. <strong>The existence of $\phi$ is sufficient for the homomorphism to exist.</strong></p>
<p>Homomorphisms are not necessarily bidirectional. They are only required to work in one direction, from $A$ to $B$.</p>
<p>If $\phi: A \rightarrow B$ is computationally difficult to invert, then $\phi$ homomorphically encrypts the elements of $A$. That means we can validate claims about computations in $A$ using elements in $B$.</p>
<p>The good news is, we are done with our treatment of abstract algebra, and we now have a strong foundation to move on to <a href="https://rareskills.io/post/elliptic-curve-addition">elliptic curves</a>.</p>
<div style='page-break-after: always;'></div>

<h1>Elliptic Curve Point Addition</h1>
<p>Source: https://rareskills.io/post/elliptic-curve-addition</p>
<h1>Elliptic Curve Point Addition</h1>
<p>This article describes how elliptic curve addition works over real numbers.</p>
<p>Cryptography uses elliptic curves over finite fields, but elliptic curves are easier to conceptualize in a real Cartesian plane. This article is aimed at programmers and tries to strike a balance between getting too math heavy and too hand-wavy.</p>
<h2>Set theoretic definition of elliptic curves</h2>
<p>The <a href="https://www.rareskills.io/post/set-theory">set</a> of points on an elliptic curve form a group under elliptic curve point addition.</p>
<p>Hopefully, if you’ve been following our <a href="https://www.rareskills.io/post/group-theory">group theory introduction</a>, then you actually understood most of this, aside from what “point addition” is. But that’s the beauty of abstract algebra right? You don’t need to know what that is, and you still understand the above sentence.</p>
<p>Elliptic curves are a family of curves which have the formula</p>
<p>$$ y^2 = x^3 + ax + b $$</p>
<p>Depending on what value of a and b you pick, you’ll get a curve that looks like some of the following:</p>
<p><img alt="Elliptic curves" src="assets/935a00_26f928a28e2b424690c1e3df172f783a_mv2.png" /></p>
<p>A point on an elliptic curve is an $(x, y)$ pair that satisfies $y² = x³ + ax + b$ for a given $a$ and $b$.</p>
<p>For example, the point $(3, 6)$ is in the curve $y² = x³ + 9$ because it $6² = 3³ + 9$. In group theoretic terms, $(3, 6)$ is a member of the set defined by $y² = x³ + 9$. Since we are dealing with real numbers, the set has infinite cardinality.</p>
<p>The idea here is we can take two points from this set, do a binary operator, and we will get another point that is also in the set. That is, it is an $(x, y)$ pair that also lies on the curve.</p>
<p><strong>Instead of thinking about elliptic curves as a plot on a graph, think of them as an infinite set of points. Points are in the set if and only if they satisfy the elliptic curve equation.</strong></p>
<p>Once we see these points as a set, looking at them as a group isn’t mysterious. We just take two points, and produce a third according to the rules of a group.</p>
<p>Specifically, to be a group, the set of points needs to have:</p>
<ul>
<li>a binary operator that is closed and associative, i.e. it produces another point in the set</li>
<li>the set must have an identity element $I$</li>
<li>every point in the set must have an inverse such that when the two are combined with the binary operator, the result is $I$</li>
</ul>
<h2>Elliptic Curves form an abelian group under addition</h2>
<p>Although we don’t know how the binary operator works, we do know that it takes two $(x, y)$ points on the curve and returns another point on the curve. Because the operator is closed, we know that the point will in fact be a valid solution to the elliptic curve equation, not a point somewhere else.</p>
<p>We also know that this binary operator is associative (and commutative, per the section heading).</p>
<p>So given three points on the elliptic curve $A$, $B$, and $C$ (or $(x_a, y_a)$, $(x_b, y_b)$, and $(x_c, y_c)$ if you prefer), we know the following is true:</p>
<ul>
<li>$(A ⊕ B) ⊕ C = A ⊕ (B ⊕ C)$</li>
<li>$A ⊕ B = B ⊕ A$</li>
</ul>
<p>I’m using $⊕$, because we know this binary operator is not addition in any normal sense, but a binary operator (again, remember from set theory, a binary operator takes two elements in a set and returns another element in a set, how it does that is not central to the definition).</p>
<p>We also know that there has to be an identity element somewhere. That is, any $(x, y)$ point that falls on the curve is combined with the identity element, the output is the same $(x, y)$ point unchanged.</p>
<p>And because this is a group and not a monoid, every point needs to have an inverse such that $P ⊕ P⁻¹ = I$, where $I$ is the identity element.</p>
<h3>The identity element</h3>
<p>Intuitively, we might think of $(0, 0)$ or $(1, 1)$ being the identity element, since something like that often is in other groups, but you can see in the plots above that those points generally do not land on the curve. Since they don’t belong to the set of points on $y² = x³ + ax + b$, they are not part of the group.</p>
<p>But recall from set theory that we can define binary operators however we like over sets arbitrarily defined. This allows us to add a special element that isn’t technically on the curve but by definition is the identity element.</p>
<p>I like to think of the identity element as "the point that is nowhere" because if you combine nowhere with any real point, nothing changes. Annoyingly, mathematicians call this point, the identity element "the point at infinity."</p>
<p>Hey wait, isn’t this point supposed to satisfy $y² = x³ + ax + b$? Nowhere (or infinity) is not a valid value for $(x, y)$.</p>
<p>Ahh, but remember, we can define sets however we like! We define the set that makes up the elliptic curve as points on the elliptic curve and the nowhere point.</p>
<p>Because binary operators are just subsets of a cartesian product (a relation), and we can define the relation however we like. We can have as many hacky “if statements” in our arithmetic as we please and still follow the group laws.</p>
<h2>Addition is closed.</h2>
<p>Without loss of generality, let’s take the elliptic curve</p>
<p>$$ y² = x³ + 10 $$</p>
<p>To illustrate how lines intersect on elliptic curves, then let’s draw a nearly vertical line $y = 10x$</p>
<p>(It could be 1000x to make it more vertical, but we would get numerical instability as you will see later)</p>
<p>We get the following set of plots.</p>
<p><img alt="Elliptic curve with a line drawn through it" src="assets/935a00_fe30b49a14b448b2a306925812e052f5_mv2.png" /></p>
<p>It turns out, even though it looks like the purple line ($y = 10x$) is rising faster than the blue curve ($y² = x³ + 10$), they will always eventually intersect.</p>
<p>If we zoom out far enough, we can see the intersection. This is true in general.</p>
<p><strong>As long as x is not "perfectly vertical" if it crosses two points on the curve, it will always cross a third.</strong> Two of those points could be the same point if one of the points of intersection is a tangent point.</p>
<p>The "if we intersect two points" is important. If we shift our purple line over to the left so it doesn’t cross the "U-turn" of the elliptic curve, then it will only cross at one point</p>
<p>Another way of understanding it:</p>
<p><strong>If a straight line crosses an elliptic curve at exactly two points, and neither of the intersection points are tangent intersections, then it must be perfectly vertical.</strong></p>
<p>You could work out an algebraic proof from the formulas above, but I think the geometric argument is more intuitive.</p>
<p>I recommend you stop here and draw some elliptic curves and lines and convince yourself of this visually.</p>
<p>Our exception for vertical lines actually causes inverse and identity elements to fall into place beautifully.</p>
<p><strong>The inverse of an elliptic curve point is the negative of the y value of the pair.</strong> That is, the inverse of $(x, y)$ is $(x, -y)$ and vice versa. Drawing a line through such points creates a perfectly vertical line.</p>
<p>The identity element is the "point at infinity" we alluded to earlier is simply the point "way up there" when we draw a vertical line.</p>
<h3>Abelian Group</h3>
<p>The fact that elliptic curve points are a group under our “2 points always result in a 3rd except for the identity” makes its commutative nature obvious.</p>
<p>When we pick two points, there is only one other third point. You can’t get four intersections in an elliptic curve. Since we only have one possible solution, then it is clear that $A ⊕ B = B ⊕ A$.</p>
<h2>Why elliptic curve addition flips over the x axis</h2>
<p>We glossed over a very important detail in the last section, because it really deserves a section on its own.</p>
<p>In it’s current form, it has a bug if we add two points where the intersection happens in the middle.</p>
<p><img alt="3 point intersection through an elliptic curve" src="assets/935a00_cffa7b60afd8486f8cc2f97de8b07f17_mv2.png" /></p>
<p>Using our definitions above, the following must be true</p>
<p>$$<br />
\begin{align*}<br />
A ⊕ B &amp;= C \<br />
A ⊕ C &amp;= B \<br />
B ⊕ C &amp;= A<br />
\end{align*}$$</p>
<p>With a little algebra, we’ll derive a contradiction</p>
<p>$$<br />
\begin{align*}<br />
(B ⊕ C) ⊕ B &amp;= C \<br />
B ⊕ C &amp;= \mathsf{inv}(B) ⊕ C \<br />
B &amp;= \mathsf{inv}(B)<br />
\end{align*}$$</p>
<p>This says $B$ is equal to it’s inverse. But $B$ is not the identity element (which is the only element that can be the inverse of itself), so we have a contradiction.</p>
<p>Thankfully, there is a way to rescue this. Just define point addition to be the third point <em>flipped over the y axis</em>. Again, we are <em>allowed to do this</em> because binary operators can be defined however we like, we just care that our definitions satisfy the group laws.</p>
<p>So the correct way to add elliptic curve points is represented graphically below</p>
<p><img alt="Elliptic curve point addition" src="assets/935a00_47a61dc12ed54c2b9a36415cceea5b54_mv2.png" /></p>
<h2>Formula for Addition</h2>
<p>Using some algebra, and given two points</p>
<p>$$<br />
\begin{align*}<br />
P₁ &amp;= (x₁, y₁) \<br />
P₂ &amp;= (x₂, y₂)<br />
\end{align*}$$</p>
<p>One can derive how to compute $P₃ = (x₃, y₃)$ where $P₃ = P₁ ⊕ P₂$ using the following formula.</p>
<p>$$<br />
\begin{align*}<br />
\lambda &amp;= \frac{y₂ – y₁}{x₂ – x₁} \<br />
x₃ &amp;= \lambda² – x₁ – x₂ \<br />
y₃ &amp;= \lambda(x₁-x₃) – y₁<br />
\end{align*}$$</p>
<h3>Algebraically demonstrating commutativity and associativity</h3>
<p>Because we have a closed form equation, we can prove algebraically that $T⊕U = U⊕T$ given points $T$ and $U$.</p>
<p>We do it as follows</p>
<p>$$\begin{align*}<br />
P &amp;= T ⊕ U \<br />
Q &amp;= U ⊕ T \<br />
P &amp;= Q<br />
\end{align*}$$</p>
<pre style='font-family: Arial'><code class="language-solidity">var('y_t', 'y_u', 'x_t', 'x_u')
lambda_p = (y_u - y_t)/(x_u - x_t)
x_p = lambda_p^2 - x_t - x_u
y_p = (lambda_p*(x_t - x_p) - y_t)

lambda_q = (y_t - y_u)/(x_t - x_u)
x_q = lambda_q^2 - x_u - x_t
y_q = (lambda_q*(x_u - x_q) - y_u)
</code></pre>
<p>Here is a screenshot of running the above code in Jupyter notebook and pretty printing the output. The computer algebra system needs a bit of coaxing, but we can clearly see <code style='font-family: Arial'>x_q == x_p</code> and <code style='font-family: Arial'>y_q == y_p</code>.</p>
<p><img alt="Algebraically demonstrating commutativity and associativity" src="assets/935a00_fb8599e0572c4987b88525005917b394_mv2.png" /></p>
<p>$P = Q$ for all $(x_t, y_t)$ and $(x_u, y_u)$ values. We get a division by zero error if $x_t = x_u$, but this means they are the same point and that is obviously commutative.</p>
<p>We can use similar techniques to demonstrate associativity, but unfortunately this is extremely messy so we refer the interested reader to another <a href="https://www.scirp.org/journal/paperinformation.aspx">proof of associativity</a>.</p>
<h2>Elliptic curves meet the abelian group property</h2>
<p>Let’s see that elliptic curves meet the group property.</p>
<ol>
<li>The binary operator is closed. It either intersects with a 3rd point on the curve or the point at infinity (identity). We are guaranteed to get a third valid point when we intersect two points. The binary operator is associative.</li>
<li>The group has an identity element.</li>
<li>Each point has an inverse.</li>
<li>The group is abelian because A ⊕ B = B ⊕ A</li>
</ol>
<p>A binary operator must accept every possible pair from the set. What if the pair is the same element, i.e. A ⊕ A?</p>
<h2>Point multiplication: adding a point with itself</h2>
<p>Let’s think of this in limit terms. Adding a point to itself is like bringing two points infinitesimally close to each other until they become the same point. When this convergence happens, the slope of the line will lie tangent to the curve.</p>
<p>So adding a point to itself is simply taking the derivative at that point, getting the intersection, then flipping the $y$ axis.</p>
<p>The following image graphically demonstrates $A ⊕ A = 2A$.</p>
<p><img alt="Point multiplication on an elliptic curve" src="assets/935a00_61ed6a7a5ba14b53a95cf5c16e54f3f5_mv2.png" /></p>
<h3>Shortcut for point multiplication</h3>
<p>What if we wanted to compute $1000A$ instead of $2A$? It would seem this is an $\mathcal{O}(n)$ operation, but it isn’t.</p>
<p>Because of associativity, we can write $1000A$ as</p>
<p>$$1000A = 512A ⊕ 256A ⊕ 128A ⊕ 64A ⊕ 32A ⊕ 8A$$</p>
<p>$512A$ (and the other terms) can be computed quickly because 512 is just $A$ doubled 9 times.</p>
<p>So rather than doing 1000 operations, we can do it in 14 (9 to compute 512, caching the intermediate results, then 5 additions).</p>
<p>This is actually an important property when we get to cryptography:</p>
<p><em>We can efficiently multiply an elliptic curve point by a large integer.</em></p>
<h2>Implementation details of addition</h2>
<p>It isn’t too hard to derive the formula for point addition using simple algebra. When we intersect two points, we know the slope and the points that it crossed through, so we can calculate the point of intersection.</p>
<p>I’d rather not do that here because I don’t want to get lost in a bunch of symbolic manipulation.</p>
<p>The whole power of group theory is that we don’t care what that symbolic manipulation looks like. We know that if we do our binary operator on two points, we’ll get another point in our set, and our set follows the group laws.</p>
<p>If you think about it that way, elliptic curves are much easier to understand.</p>
<p>Rather than trying to understand elliptic curves in isolation from the ground up, we study a bunch of other algebraic groups, then transfer that knowledge and intuition to elliptic curves.</p>
<p>Rational numbers under addition are a group. Integers modulo a prime are a group under multiplication. Matrices of non-zero determinant under multiplication are a group.</p>
<p>You do the binary operator, and you get another item in the set. The group has an identity element, and each element has an inverse. Associative law holds. With all that in mind, you shouldn’t care what the operator ⊕ is doing behind the scenes.</p>
<p>In my opinion, if you try to understand elliptic curves math in isolation from first principles of group theory, you’re doing it the hard way. It’s far easier to understand it in the context of its relatives.</p>
<p>That makes for a smoother learning experience.</p>
<h3>Algebraic manipulation is really just associative addition.</h3>
<p>Let $P$ be an elliptic curve point. What happens we do something like this?</p>
<p>$$(a + b)P + cP = aP + (b + c)P$$</p>
<p>At first, it may seem weird that we can do that, because if we try to visualize what is happening with the elliptic curve, we will surely get lost.</p>
<p>Remember, what looks like multiplication above is really just a point getting added with itself repeatedly, so this is what actually happens under the hood when looking at it as a group</p>
<p>$$\underbrace{(a + b)P + cP}_{((a + b + c)P)} = \underbrace{aP + (b + c)P}_{(a + b + c)P}$$</p>
<p>$$<br />
\begin{align*}<br />
(a + b + c)P &amp;= (a + b + c)P \<br />
(aP + bP) + cP &amp;= aP + (bP + cP) \<br />
(a + b)P + cP &amp;= aP + (b + c)P<br />
\end{align*}$$</p>
<p>Scalar "multiplication" isn’t "distributive" the way we would think about normal algebra. It’s just shorthand for rearranging the order in which we add P to itself.</p>
<p>Under the hood, we just added $P$ to itself $(a + b + c)$ times. The order we do it in doesn’t matter because of associativity.</p>
<p>So when you see manipulation like that, our group didn’t suddenly gain a multiplication binary operator, it’s just misleading shorthand.</p>
<h2>Elliptic curves in finite fields</h2>
<p>If we were to do elliptic curves over real numbers for a real application, they would be very numerically unstable because the intersection point could require a lot of decimal places to compute.</p>
<p>So in reality, we do everything with <a href="https://www.rareskills.io/post/finite-fields">modular arithmetic</a>.</p>
<p>But we lose none of the intuition we’ve gained above by using real numbers as a visual example.</p>
<h2>Learn more with RareSkills</h2>
<p>This material is from our zero knowledge course, see there to learn more. This post is part of a series on <a href="rareskills.io/zk-book">Zero Knowledge Proofs</a>.</p>
<p><em>Originally Published September 1, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>Elliptic Curves over Finite Fields</h1>
<p>Source: https://rareskills.io/post/elliptic-curves-finite-fields</p>
<h1>Elliptic Curves over Finite Fields</h1>
<p>What do elliptic curves in finite fields look like?</p>
<p>It’s easy to visualize smooth elliptic curves, but what do elliptic curves over a finite field look like?</p>
<p>The following is a plot of $y² = x³ + 3 \pmod {23}$</p>
<p><img alt="Plot of y² = x³ + 3 \pmod 23" src="assets/935a00_9b8594bdeb9b4eb580847f1d5ffcd6c0_mv2.png" /></p>
<p>Because we only allow integer inputs (more specifically, finite field elements), we are not going to obtain a smooth plot.</p>
<p>Not every $x$ value will result in an integer for the $y$ value when we solve the equation, so no point will be present at that value for $x$. You can see such gaps in the plot above.</p>
<p>Code to generate this plot will be provided later.</p>
<h3>The prime number changes the plot dramatically</h3>
<p>Here are some plots of $y² = x³ + 3$ done over modulo 11, 23, 31, and 41 respectively. The higher the modulus, the more points it holds, and the more complex the plot appears to be.</p>
<p><img alt="Plot of elliptic curves modulo 11, 23" src="assets/935a00_e592040ff7174e81a1f32ed7ed70a150_mv2.png" /></p>
<p><img alt="Plot of elliptic curves modulo 23, 31" src="assets/935a00_382bd8455deb45efba13fdb7d77517b4_mv2.png" /></p>
<p>We established in the previous article that elliptic curve points with the "connect and flip" operation are a group. When we do this over a finite field, it remains a group, but it becomes a cyclic group, which is tremendously useful for our application. Why it is cyclic unfortunately will require some very involved math, so you’ll just have to accept that for now. But this should not be too surprising. We have a finite number of points, so generating each point by carrying out $(x + 1)G, (x + 2)G, … (x + \text{order} – 1)G$ should at least seem plausible.</p>
<p>In the application of cryptography, $p$ needs to be large. In practice, it is over 200 bits. We will revisit this in a later section.</p>
<h2>Background</h2>
<h3>Field element</h3>
<p>We’re going to say "field element" a lot in this article, and hopefully from our previous tutorials (especially on <a href="https://www.rareskills.io/post/finite-fields">finite fields</a>) you are comfortable with that term already. But if not, it is a positive integer that is inside a modulo operation.</p>
<p>That is, if we do addition modulo 11, then the finite field elements from that set is $\set{0,1,…,9,10}$. It isn’t correct to say "integers" even though that’s the data type we will use in our Python examples. You cannot have a negative field element (although integers can be negative) when doing addition modulo a prime number. A "negative" number in a finite field is simply the additive inverse of another number, that is, a number that when summed together results in zero. For example, in a finite field of modulo 11, 4 can be considered "-7" because $7 + 4 \pmod {11}$ is $0$, in a comparative sense to how 7 + (-7) is zero for regular numbers.</p>
<p>Over the field of rational numbers, multiplication has the identity element of 1, and the inverse of a number is simply the numerator and the denominator flipped. For example, 500/303 is the inverse of 303/500 because if you multiply them, you get 1.</p>
<p>In a finite field, the inverse of an element is the number you multiply it with to get the finite field element 1. For example, in modulo 23, 6 is the inverse of 4 because when you multiply them together modulo 23, you get 1. When the order of the field is prime, every number except zero has an inverse.</p>
<h3>Cyclic Groups</h3>
<p>A cyclic <a href="https://www.rareskills.io/group-theory">group</a> is a group such that every element can be computed by starting with a generator element and repeatedly applying the group’s binary operator.</p>
<p>A very simple example is integers modulo 11 under addition. If your generator is 1, and you keep adding the generator to itself, you can generate every element in the group from 0 to 10.</p>
<p>Saying a elliptic curve points form a cyclic group (under elliptic curve addition) means we can represent every number in a finite field as an elliptic curve point and add them together just like we would regular integers in a finite field.</p>
<p>That is,</p>
<p>$5 + 7 \pmod p$ is homomorphic to $5G + 7G$</p>
<p>Where G is the generator of the elliptic curve cyclic group.</p>
<p>This is only true of elliptic curves over finite fields that have a prime number of points, which are the kinds of curves we use in practice. This is something we’ll revisit later.</p>
<h2>BN128 formula</h2>
<p>The BN128 curve, which is used by the Ethereum precompiles to verify ZK proofs, is specified as follows:</p>
<p>$$<br />
p = 21888242871839275222246405745257275088696311157297823662689037894645226208583$$<br />
$$<br />
y² = x³ + 3 \pmod p$$</p>
<p>Here $p$ is the field modulus.</p>
<p>The <code style='font-family: Arial'>field_modulus</code> should not be confused with the curve order, which is the number of points on the curve.</p>
<p>For the bn128 curve, the curve order is as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import curve_order
# 21888242871839275222246405745257275088548364400416034343698204186575808495617
print(curve_order)
</code></pre>
<p>The field modulus is very large, which makes experimenting with it unwieldy. In the next section, we’ll build up an intuition for elliptic curve points in finite fields using the same formula, but with a smaller modulus.</p>
<h2>Generating elliptic curve cyclic group $y² = x³ + 3 \pmod {11}$</h2>
<p>To solve the equation above, and determine which $(x, y)$ points are on the curve, we’ll need to compute $\sqrt{(x³ + 3)} \pmod {11}$.</p>
<h3>Modular square roots</h3>
<p>We use the <a href="https://en.wikipedia.org/wiki/Tonelli%E2%80%93Shanks_algorithm">Tonelli Shanks Algorithm</a> to compute modular square roots. You can read up on how the algorithm works if you are curious, but for now you can treat it as a black box that computes the mathematical square root of a field element over a modulus, or lets you know if the square root does not exist.</p>
<p>For example, the square root of 5 modulo 11 is 4 $(4 \times 4 \mod 11 = 5)$, but there is no square root of 6 modulo 11. (The reader is encouraged to discover this is true via brute force).</p>
<p>Square roots often have two solutions, a positive and a negative one. Although we don’t have numbers with a negative sign in a finite field, we still have a notion of "negative numbers" in the sense of having an inverse.</p>
<p>You can find code online to implement the algorithm described above, but to avoid putting large chunks of code in this tutorial, we will install a Python library instead.</p>
<pre style='font-family: Arial'><code class="language-solidity">python3 -m pip install libnum
</code></pre>
<p>After installing <a href="https://pypi.org/project/libnum/">libnum</a>, we can run the following code to demonstrate its use.</p>
<pre style='font-family: Arial'><code class="language-solidity">from libnum import has_sqrtmod_prime_power, sqrtmod_prime_power

# the functions take arguments# has_sqrtmod_prime_power(n, field_mod, k), where n**k,
# but we aren't interested in powers in modular fields, so we set k = 1
# check if sqrt(8) mod 11 exists
print(has_sqrtmod_prime_power(8, 11, 1))
# False

# check if sqrt(5) mod 11 exists
print(has_sqrtmod_prime_power(5, 11, 1))
# True

# compute sqrt(5) mod 11
print(list(sqrtmod_prime_power(5, 11, 1)))
# [4, 7]

assert (4 ** 2) % 11 == 5
assert (7 ** 2) % 11 == 5

# we expect 4 and 7 to be inverses of each other, because in &quot;regular&quot; math, the two solutions to a square root are sqrt and -sqrt
assert (4 + 7) % 11 == 0
</code></pre>
<p>Now that we know how to compute modular square roots, we can iterate through values of $x$ and compute $y$ from the formula $y² = x³ + b$. Solving for $y$ is just a matter of taking the modular square root of both sides (if it exists) and saving the resulting $(x, y)$ pairs so we can plot them later.</p>
<p>Let’s create a simple plot of an elliptic curve</p>
<p>$$y² = x³ + 3 \pmod {11}$$</p>
<pre style='font-family: Arial'><code class="language-solidity">import libnum
import matplotlib.pyplot as plt

def generate_points(mod):
    xs = []
    ys = []
    def y_squared(x):
        return (x**3 + 3) % mod

    for x in range(0, mod):
        if libnum.has_sqrtmod_prime_power(y_squared(x), mod, 1):
            square_roots = libnum.sqrtmod_prime_power(y_squared(x), mod, 1)

            # we might have two solutions
            for sr in square_roots:
                ys.append(sr)
                xs.append(x)
    return xs, ys

xs, ys = generate_points(11)
fig, (ax1) = plt.subplots(1, 1);
fig.suptitle('y^2 = x^3 + 3 (mod p)');
fig.set_size_inches(6, 6);
ax1.set_xticks(range(0,11));
ax1.set_yticks(range(0,11));
plt.grid()
plt.scatter(xs, ys)
plt.plot();
</code></pre>
<p>The result of the plot is shown below:</p>
<p><img alt="Plot of y² = x³ + 3 (mod 11)" src="assets/935a00_2355ae79d450498eb3ee5b6721634b43_mv2.png" /></p>
<p>Some observations:</p>
<ul>
<li>There won’t be any x or y values greater than or equal to the modulus we use</li>
<li>Just like the real-valued plot, the modular one "appears symmetric"</li>
</ul>
<h2>Elliptic curve point addition</h2>
<p>Even more interestingly, our “connect the dots and flip” operation to compute elliptic curves still works!</p>
<p>But given that we are doing this over a finite field, this should not be surprising. Our formulas over real numbers use the normal field operations of addition and multiplication. Although we use square roots to determine if a point is on the curve, and square roots are not a valid field operator, we do not use square roots to compute the addition and doubling of points.</p>
<p>The reader can verify this by picking two points from the plots above, then plugging them into the code below to add points and seeing they always land on another point (or the point on infinity if the points are inverses of each other). These formulas are taken from the <a href="https://en.wikipedia.org/wiki/Elliptic_curve_point_multiplication">Wikipedia page on elliptic curve point multiplication</a>.</p>
<pre style='font-family: Arial'><code class="language-solidity">def double(x, y, a, p):
    lambd = (((3 * x**2 + a) % p ) *  pow(2 * y, -1, p)) % p
    newx = (lambd**2 - 2 * x) % p
    newy = (-lambd * newx + lambd * x - y) % p
    return (newx, newy)

def add_points(xq, yq, xp, yp, p, a=0):
    if xq == yq == None:
        return xp, yp
    if xp == yp == None:
        return xq, yq

    assert (xq**3 + 3) % p == (yq ** 2) % p, &quot;q not on curve&quot;
    assert (xp**3 + 3) % p == (yp ** 2) % p, &quot;p not on curve&quot;

    if xq == xp and yq == yp:
        return double(xq, yq, a, p)
    elif xq == xp:
        return None, None

    lambd = ((yq - yp) * pow((xq - xp), -1, p) ) % p
    xr = (lambd**2 - xp - xq) % p
    yr = (lambd*(xp - xr) - yp) % p
    return xr, yr
</code></pre>
<p>Here are some visualizations of what "connect and flip" looks like in a finite field:</p>
<p><img alt="examples of EC addition in a finite field" src="assets/elliptic-curve-finite-fields-example.jpg" /></p>
<h2>Every elliptic curve point in a cyclic group has a “number”</h2>
<p>A cyclic group, by definition, can be generated by repeatedly adding the generator to itself.</p>
<p>Let’s use a real example of $y² = x³ + 3 \pmod {11}$ with the generator point being $(4, 10)$.</p>
<p>Using the Python functions above, we can start with the point $(4, 10)$ and generate every point in the group:</p>
<pre style='font-family: Arial'><code class="language-solidity"># for our purposes, (4, 10) is the generator point G
next_x, next_y = 4, 10
print(0, 4, 10)
points = [(next_x, next_y)]
for i in range(1, 12):
    # repeatedly add G to the next point to generate all the elements
    next_x, next_y = add_points(next_x, next_y, 4, 10, 11)
    print(i, next_x, next_y)
    points.append((next_x, next_y))
</code></pre>
<p>The output will be</p>
<pre style='font-family: Arial'><code class="language-solidity">0 4 10
1 7 7
2 1 9
3 0 6
4 8 8
5 2 0
6 8 3
7 0 5
8 1 2
9 7 4
10 4 1
11 None None
12 4 10 # note that this is the same point as the first one
</code></pre>
<p>Observe that $(\text{order} + 1)G = G$. Just like modular addition, when we "overflow", the cycle starts over.</p>
<p>Here, <code style='font-family: Arial'>None</code> means the point at infinity, which is indeed part of the group. Adding the point at infinity to the generator returns the generator, as that is how the identity element is supposed to behave.</p>
<p>We can assign each point a "number" based on how many times we added the generator to itself to arrive at that point.</p>
<p>We can use the following code to plot the curve and assign a number next to it</p>
<pre style='font-family: Arial'><code class="language-solidity">xs11, ys11 = generate_points(11)

fig, (ax1) = plt.subplots(1, 1);
fig.suptitle('y^2 = x^3 + 3 (mod 11)');
fig.set_size_inches(13, 6);

ax1.set_title(&quot;modulo 11&quot;)
ax1.scatter(xs11, ys11, marker='o');
ax1.set_xticks(range(0,11));
ax1.set_yticks(range(0,11));
ax1.grid()

for i in range(0, 11):
    plt.annotate(str(i+1), (points[i][0] + 0.1, points[i][1]), color=&quot;red&quot;);
</code></pre>
<p>The red text can be thought of as starting with the identity element, and how many times we added the generator to it.</p>
<p><img alt="plot of y^2 = x^3 + 3 (mod 11) with the points numbered" src="assets/935a00_3e9b90d38e9c4c4a82b34d138fa9f49c_mv2.png" /></p>
<h3>Point inverses are still vertically symmetric</h3>
<p>Here is an interesting observation: note that points that share the same x-value add up to 12, which corresponds to the identity element $(12 \mod 12 = 0)$. If we add the point $(4, 1)$, which is point 11 in our plot to $(4, 10)$, we will get the point at in infinity, which would be the 12th element in the group.</p>
<h3>The order is not the modulus</h3>
<p>In this example, the order of the group is 12 (total number of elliptic curve points in our group), despite the formula for the elliptic curve being modulo 11. This will be stressed several times, but you should NOT assume that the modulus in the elliptic curve is the group order. However, you can estimate the curve’s order range from the field modulus itself using <a href="https://en.wikipedia.org/wiki/Hasse%27s_theorem_on_elliptic_curves">Hasse’s Theorem</a>.</p>
<h3>If the number of points is prime, then the addition of points behaves like a finite field</h3>
<p>In the plot above, there are 12 points (including 0). Addition modulo 12 is not a finite field because 12 is not prime.</p>
<p>However, if we pick our parameters for the curve carefully, we can create an elliptic curve where the points correspond to elements in a finite field. That is, the order of the curve equals the order of the finite field.</p>
<p>For example, $y^2 = x^3 + 7 \pmod {43}$ creates a curve with 31 points total as can be seen in the plot below:</p>
<p><img alt="elliptic curve with 31 points" src="assets/elliptic-curve-finitie-order-31.jpg" /></p>
<p>When the order of the curve matches the order of the finite field <strong>every operation you do in the finite field has a homomorphic equivalent in the elliptic curve</strong>.</p>
<p>To go from a finite field to an elliptic curve, we we pick one point (arbitrarily) to be the generator, then we multiply the element in the finite field by the generator.</p>
<h2>Multiplication is really repeated addition</h2>
<p>There is no such thing as elliptic curve point multiplication. When we say "scalar multiplication" we really mean repeated addition. You cannot take two elliptic curve points and multiply them together (well, you sort of can with <a href="https://www.rareskills.io/post/bilinear-pairing">bilinear pairings</a>, but that’s something we will get to later).</p>
<p>When we use the Python library to do <code style='font-family: Arial'>multiply(G1, x)</code>, this is really the same as <code style='font-family: Arial'>G1 + G1 + … + G1</code> <code style='font-family: Arial'>x</code> times. Under the hood, we don’t actually self-add that many times, we use some clever shortcuts with point doubling to complete the operation in logarithmic time.</p>
<p>For example, if we wanted to compute 135G, we would really compute the following values efficiently using point doubling, cache them,</p>
<pre style='font-family: Arial'><code class="language-solidity">G, 2G, 4G, 8G, 16G, 32G, 64G, 128G
</code></pre>
<p>…then sum up 128G + 4G + 2G + G = 135G.</p>
<p>When we say <code style='font-family: Arial'>5G + 6G = 11G</code>, we are essentially just adding G to itself 11 times. Using the shortcut illustrated above, we can compute 11G with a logarithmic number of computations, but at the end of the day, it’s just repeated addition.</p>
<h2>Python bn128 library</h2>
<p>The library the EVM implementation pyEVM uses for the elliptic curve precompiles is <code style='font-family: Arial'>py_ecc</code>, and we will be relying on that library heavily. The code below shows what the generator points looks like, and also shows some addition and scalar multiplication.</p>
<p>Here is what a G1 point looks like:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, add, eq, neg

print(G1)
# (1, 2)

print(add(G1, G1))
# (1368015179489954701390400359078579693043519447331113978918064868415326638035, 9918110051302171585080402603319702774565515993150576347155970296011118125764)

print(multiply(G1, 2))
#(1368015179489954701390400359078579693043519447331113978918064868415326638035, 9918110051302171585080402603319702774565515993150576347155970296011118125764)

# 10G + 11G = 21G
assert eq(add(multiply(G1, 10), multiply(G1, 11)), multiply(G1, 21))
</code></pre>
<p>Although the numbers are large and hard to read, we can see adding a point to itself results in the same value as "multiplying" a point by 2. The two points above are clearly the same point. The tuple is still an $(x, y)$ pair, just over a very large domain.</p>
<p>The number printed above is huge for a reason. We do not want attackers to be able to take an elliptic curve point and compute the field element that generated it. If the order of our cyclic group is too small, then the attacker can just brute force it.</p>
<p>Here is a plot of the first 1000 points:</p>
<p><img alt="plot of first 1000 points of bn128" src="assets/935a00_d9bd567f47c247f588061305dc97940e_mv2.png" /></p>
<p>And this is the code to generate the plot above:</p>
<pre style='font-family: Arial'><code class="language-solidity">import matplotlib.pyplot as plt
from py_ecc.bn128 import G1, multiply, neg
import math
import numpy as np
xs = []
ys = []
for i in range(1,1000):
    xs.append(i)
    ys.append(int(multiply(G1, i)[1]))
    xs.append(i)
    ys.append(int(neg(multiply(G1, i))[1]))
plt.scatter(xs, ys, marker='.')
</code></pre>
<p>This may look scary, but the only difference from what we did in the previous section is using a much larger modulus and a different point for the generator.</p>
<h3>Addition in the library</h3>
<p>The <code style='font-family: Arial'>py_ecc</code> library makes point addition convenient for us, and the syntax should be self-explanatory:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, add, eq

# 5 = 2 + 3
assert eq(multiply(G1, 5), add(multiply(G1, 2), multiply(G1, 3)));
</code></pre>
<p>Addition in a finite field is homomorphic to addition among elliptic curve points (when their order is equal). Because of the discrete logarithm, another party can add elliptic curve points together without knowing which field elements generated those points.</p>
<p>At this point, hopefully the reader has a good intuition for adding elliptic curve points together, both theoretically and practically, because modern zero knowledge algorithms rely <em>heavily</em> on this..</p>
<h3>Implementation detail about the homomorphism between modular addition and elliptic curve addition</h3>
<p>We need to make a careful distinction between terminologies here:</p>
<p>The <strong>field modulus</strong> is the modulo we do the curve over. The <strong>curve order</strong> is the number of points on the curve.</p>
<p>If you start with a point $R$ and add the curve order $o$, you will get $R$ back. If you add the field modulus, you will get a different point.</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import curve_order, field_modulus, G1, multiply, eq

x = 5 # chosen randomly
# This passes
assert eq(multiply(G1, x), multiply(G1, x + curve_order))

# This fails
assert eq(multiply(G1, x), multiply(G1, x + field_modulus))
</code></pre>
<p>The implication of this is that <code style='font-family: Arial'>(x + y) mod curve_order == xG + yG</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">x = 2 ** 300 + 21
y = 3 ** 50 + 11

# (x + y) == xG + yG
assert eq(multiply(G1, (x + y)), add(multiply(G1, x), multiply(G1, y)))
</code></pre>
<p>Even though the <code style='font-family: Arial'>x + y</code> operation will clearly "overflow" over the curve order, this doesn’t matter. Just like in a finite field, this is the behavior we expect. The elliptic curve multiplication is implicitly executing the same operation as taking the modulus before doing the multiplication.</p>
<p>In fact, we don’t even need to do the modulus if we only care about positive numbers, the following identity also holds:</p>
<pre style='font-family: Arial'><code class="language-solidity">x = 2 ** 300 + 21
y = 3 ** 50 + 11

assert eq(multiply(G1, (x + y) % curve_order), add(multiply(G1, x), multiply(G1, y)))
</code></pre>
<p>However, if we do the finite math modulo with the wrong number (some number other than the curve order), the equality will break if we "overflow"</p>
<pre style='font-family: Arial'><code class="language-solidity">x = 2 ** 300 + 21
y = 3 ** 50 + 11 # these values are large enough to overflow:

assert eq(multiply(G1, (x + y) % (curve_order - 1)), add(multiply(G1, x), multiply(G1, y))), &quot;this breaks&quot;
</code></pre>
<h4>Encoding rational numbers</h4>
<p>When we take the modulus, we are able to encode a concept of division.</p>
<p>For example, we cannot do the following using regular integers.</p>
<pre style='font-family: Arial'><code class="language-solidity"># this throws an exception
eq(add(multiply(G1, 5 / 2), multiply(G1, 1 / 2), multiply(G1, 3)
</code></pre>
<p>However, in a finite field, 1/2 can be meaningfully computed as the multiplicative inverse of 2. Therefore, 5 / 2 can be encoded as $5 \cdot \mathsf{inv}(2)$.</p>
<p>Here’s how we can do it in Python:</p>
<pre style='font-family: Arial'><code class="language-solidity">five_over_two = (5 * pow(2, -1, curve_order)) % curve_order
one_half = pow(2, -1, curve_order)

# Essentially 5/2 = 2.5# 2.5 + 0.5 = 3
# but we are doing this in a finite field
assert eq(add(multiply(G1, five_over_two), multiply(G1, one_half)), multiply(G1, 3))
</code></pre>
<h4>Associativity</h4>
<p>We know groups are associative, so we expect the following identity to be true in general:</p>
<pre style='font-family: Arial'><code class="language-solidity">x = 5
y = 10
z = 15

lhs = add(add(multiply(G1, x), multiply(G1, y)), multiply(G1, z))

rhs = add(multiply(G1, x), add(multiply(G1, y), multiply(G1, z)))

assert eq(lhs, rhs)
</code></pre>
<p>The reader is encouraged to try different values of <code style='font-family: Arial'>x</code>, <code style='font-family: Arial'>y</code>, and <code style='font-family: Arial'>z</code> for themselves.</p>
<h4>Every element has an inverse</h4>
<p>The <code style='font-family: Arial'>py_ecc</code> library supplies us with the <code style='font-family: Arial'>neg</code> function which will provide the inverse of a given element by flipping it over the x-axis (in a finite field). The library encodes the "point at infinity" as a Python <code style='font-family: Arial'>None</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, neg, is_inf, Z1

# pick a field element
x = 12345678# generate the point
p = multiply(G1, x)

# invert
p_inv = neg(p)

# every element added to its inverse produces the identity elementassert is_inf(add(p, p_inv))

# Z1 is just None, which is the point at infinity
assert Z1 is None

# special case: the inverse of the identity is itself
assert eq(neg(Z1), Z1)
</code></pre>
<p>As is the case with elliptic curves over real numbers, the inverse of an elliptic curve point has the same x value, but the y value is the inverse.</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, neg, multiply

field_modulus = 21888242871839275222246405745257275088696311157297823662689037894645226208583
for i in range(1, 4):
    point = multiply(G1, i)
    print(point)
    print(neg(point))
    print('----')

    # x values are the same
    assert int(point[0]) == int(neg(point)[0])

    # y values are inverses of each other, we are adding y values
    # not ec points
    assert int(point[1]) + int(neg(point)[1]) == field_modulus
</code></pre>
<h5>Every element can be generated by a generator</h5>
<p>When we are dealing with over $2^{200}$ points, this is not possible to verify by brute force. However, consider the fact that <code style='font-family: Arial'>eq(multiply(G1, x), multiply(G1, x + order))</code> is always true. That means we are able to generate up to order points, then it cycles back to where it started.</p>
<h4>What about <code style='font-family: Arial'>optimized_bn128</code>?</h4>
<p>Examining the library, you will see an implementation called optimized_bn128. If you benchmark the execution time, you will see this version runs much quicker, and it is the implementation used by pyEvm. For educational purposes however, it is preferable to use the non-optimized version as it structures the points in a more intuitive way (the usual x, y tuple). The optimized version structures EC points as 3-tuples, which are harder to interpret.</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.optimized_bn128 import G1, multiply, neg, is_inf, Z1
print(G1)
# (1, 2, 1)
</code></pre>
<h2>Basic zero knowledge proofs with elliptic curves</h2>
<p>Consider this rather trivial example:</p>
<p>Claim: "I know two values $x$ and $y$ such that $x + y = 15$"</p>
<p>Proof: I multiply <code style='font-family: Arial'>x</code> by <code style='font-family: Arial'>G1</code> and <code style='font-family: Arial'>y</code> by <code style='font-family: Arial'>G1</code> and give those to you as <code style='font-family: Arial'>A</code> and <code style='font-family: Arial'>B</code>.</p>
<p>Verifier: You multiply 15 by G1 and check that <code style='font-family: Arial'>A + B == 15G1</code>.</p>
<p>Here it is in Python:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, add

# Prover
secret_x = 5
secret_y = 10

x = multiply(G1, 5)
y = multiply(G1, 10)

proof = (x, y, 15)

# verifier
if multiply(G1, proof[2]) == add(proof[0], proof[1]):
    print(&quot;statement is true&quot;)
else:
    print(&quot;statement is false&quot;)
</code></pre>
<p>Although the verifier doesn’t know what <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> are, they can verify that <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> add up to 15 in elliptic curve space, therefore <code style='font-family: Arial'>secret_x</code> and <code style='font-family: Arial'>secret_y</code> add up to 15 as finite field elements.</p>
<p>It is an exercise for the reader to do something more sophisticated, like prove knowledge of a solution to a linear system of equations.</p>
<p>As a (very important) hint, multiplying a number by a constant is the same thing as repeated addition. Repeated addition is the same thing as elliptic curve scalar multiplication. Thus, if x is an elliptic curve point, we can multiply it by a scalar 9 as <code style='font-family: Arial'>multiply(x, 9)</code>. This is consistent with our statement that we cannot multiply elliptic curve points – we are actually multiplying an elliptic curve point by a scalar, not another point.</p>
<p>Can you prove you know $x$ such that $23x = 161$? Can you generalize this to more variables?</p>
<p>As another hint: you (the prover) and the verifier need to agree on the formula in advance, as the verifier will be running the same "structure" of the original formula you claim to know the solution for.</p>
<h3>Security assumptions</h3>
<p>For the above scheme to be secure, we are assuming that if we publish a point such as <code style='font-family: Arial'>multiply(G1, x)</code>, an attacker cannot infer from the $(x, y)$ value created what the original value for $x$ was. This is the discrete logarithm assumption. This is why the prime number we compute the formula over needs to be large, so that the attacker cannot brute force guess.</p>
<p>There are mores sophisticated algorithms, like the <a href="https://en.wikipedia.org/wiki/Baby-step_giant-step">baby step giant step</a> algorithm that can outperform brute force.</p>
<p>Note: The BN128 comes from the assumption that it has 128 bits of security. The elliptic curve is computed in a finite field of 254 bits, but it is believed to have 128 bits of security since there are better algorithms than naive brute force to compute the discrete logarithm.</p>
<h3>True zero knowledge</h3>
<p>We should also point out that our <code style='font-family: Arial'>A + B = 15G</code> example is not truly zero knowledge. If an attacker guesses <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code>, they can verify their guess by comparing the generated elliptic curve points to ours.</p>
<p>The solution to this issue will be addressed in a later chapter.</p>
<h2>Treating elliptic curves over finite fields as a magic black box</h2>
<p>Just like you don’t need to know how a hash function works under the hood to use it, you don’t need to know the implementation details of adding elliptic curve points and multiplying them with a scalar.</p>
<p>However, you do need to know the rules they follow. At the risk of sounding like a broken record at this point, they follow the rules of cyclic groups:</p>
<ul>
<li>adding elliptic curve points is closed: it produces another elliptic curve point</li>
<li>adding elliptic curve points is associative</li>
<li>there exists an identity element</li>
<li>each element has an inverse that when added, produces the identity element</li>
</ul>
<p>As long as you understand this, you can add, multiply, and invert to your heart’s content without doing anything invalid. Each of these operations has a corresponding function in the <code style='font-family: Arial'>py_ecc</code> library.</p>
<p>This is most important thing to remember for this lesson:</p>
<p><strong>Elliptic curves over finite fields homomorphically encrypt addition in a finite field.</strong></p>
<h3>Moon math: how do we know the order of the curve?</h3>
<p>The reader may be wondering how we are able to determine the order of the bn128 curve without counting all the valid solutions to the formula. There are more valid points than any computer can enumerate, so how did we arrive at the curve order?</p>
<p>This is an example of the kind of math we are trying to avoid, because it is quite advanced. It turns out, computing the number of points can be done in polynomial time with <a href="https://en.wikipedia.org/wiki/Schoof%27s_algorithm">Schoof’s Algorithm</a>. You are not expected to understand how this algorithm works, but it is sufficient to know that the algorithm exists. How we arrived at the curve order is not important from an implementation standpoint, we just care that the designers computed it correctly.</p>
<p>The materials here on RareSkills are carefully designed to stay clear of these mathematical minefields.</p>
<h2>Learn more with RareSkills</h2>
<p>This is why our <a href="https://www.rareskills.io/zk-bootcamp">zero knowledge course</a> stresses the basics of abstract algebra so much. Understanding the implementation details of elliptic curves is nightmarishly hard. But understanding the behavior of cyclic groups, while unusual at first, is fully comprehensible to most programmers. Once we understand that, the general behavior of adding elliptic curve points becomes intuitive, despite the operation being hard to visualize.</p>
<p><em>Originally Published September 19, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>Bilinear Pairings in Python, Solidity, and the EVM</h1>
<p>Source: https://rareskills.io/post/bilinear-pairing</p>
<h1>Bilinear Pairings in Python, Solidity, and the EVM</h1>
<p>Sometimes also called bilinear mappings, bilinear pairings allow us to take three numbers, $a$, $b$, and $c$, where $ab = c$, encrypt them to become $E(a)$, $E(b)$, $E(c)$, where $E$ is an encryption function, then send the two encrypted values to a verifier who can verify $E(a)E(b) = E(c)$ but not know the original values. We can use bilinear pairings to prove that a 3rd number is the product of the first two without knowing the first two original numbers.</p>
<p>We will explain bilinear pairings at a high level and provide some examples in Python.</p>
<h2>Prerequisites</h2>
<ul>
<li>The reader should know what <a href="https://rareskills.io/post/elliptic-curve-addition">point addition</a> and scalar multiplication are in the context of elliptic curves.</li>
<li>The reader should also know the discrete logarithm problem in this context: a scalar multiplied by a point will result in another point, and it is infeasible in general to calculate the scalar given the elliptic curve point.</li>
<li>The reader should know what a <a href="https://rareskills.io/post/finite-fields">finite field</a> and cyclic group are, and what a generator is in the context of elliptic curves. We will refer to generators with the variable G.</li>
<li>The reader should know about <a href="https://rareskills.io/post/solidity-precompiles">Ethereum Precompiles</a>.<br />
  We will use capital letters to denote EC (elliptic curve) points and lower case letters to denote elements of finite fields (“scalars”). When we say element, this could be an integer in a finite field or it could be a point on an elliptic curve. The context will make it clear.</li>
</ul>
<p>It is possible to read this tutorial without fully understanding all of the above, but it will be harder to build a good intuition about this subject.</p>
<h2>How bilinear pairings work</h2>
<p>When a scalar is multiplied by a point on an elliptic curve, another elliptic curve point is produced. That is $P = pG$ where $p$ is a scalar, and $G$ is the generator. Given $P$ and $G$ we cannot determine $p$.</p>
<p>Assume $pq = r$. What we are trying to do is take</p>
<p>$$<br />
\begin{align*}<br />
P = pG\<br />
Q = qG\<br />
R = rG\<br />
\end{align*}<br />
$$</p>
<p>and convince a verifier that the discrete logs of $P$ and $Q$ multiply to produce the discrete log of $R$.</p>
<p>If $pq = r$, and $P = pG$, $Q = qG$, and $R = rG$, then we want a function such that</p>
<p>$$f(P,Q)=R$$</p>
<p>and not equal to $R$ when $pq ≠ r$. This should be true of all possible combinations of $p$, $q$, and $r$ in the group.</p>
<p>However, this is typically not how we express $R$ when using bilinear pairings. For reasons we will discuss later, it’s usually computed as</p>
<p>$$f(P,Q) = f(R,G)$$</p>
<p>$G$ is the generator point, and can be thought of as $1$. In this context. For example, $pG$ means we did $(G + G + … + G)$ $p$ times. $G$ just means we took $G$ and didn’t add anything. So in a sense, this is the same as saying $P<br />
\times Q = R \times 1$.</p>
<p>So our bilinear pairing is a function that if you plug in two elliptic curve points you get an output that <em>corresponds</em> to the product of the discrete logs of those two points.</p>
<h3>Notation</h3>
<p>A bilinear pairing is usually written as $e(P, Q)$. Here, $e$ has nothing to do with the natural logarithm, and $P$ and $Q$ are elliptic curve points.</p>
<h3>Generalization, checking if two products are equal</h3>
<p>Suppose someone gave us four elliptic curve points $P_1$, $P_2$, $Q_1$, and $Q_2$ and claimed that the discrete logs of $P_1$ and $P_2$ have the same product as $Q_1$ and $Q_2$, i.e. $p_1p_2 = q_1q_2$. Using a bilinear pairing, we can check if this is true without knowing $p_1$, $p_2$, $q_1$, or $q_2$. We simply do:</p>
<p>$$e(P_1, P_2) \stackrel{?}{=} e(Q_1, Q_2)$$</p>
<h3>What “bilinear” means</h3>
<p>Bilinear means that if a function takes two arguments, and one of them is held constant, and the other varies, then the output linearly varies with the non-constant argument.</p>
<p>If $f(x,y)$ is bilinear, and $c$ is constant, then $z = f(x, c)$ varies linearly with $x$ and $z = f(c, y)$ varies linearly with $y$.</p>
<p>We can infer from this that an elliptic curve bilinear pairing has the following property:</p>
<p>$$<br />
f(aG, bG) = f(abG, G) = f(G, abG)<br />
$$</p>
<h3>What is $e(P, Q)$ returning?</h3>
<p>To be honest, the output is so mathematically scary that it would be counterproductive to try to really explain it. This is why so much of the book earlier spent a lot of time explaining Groups, because it’s exponentially easier to understand what a Group is than to understand what $e(P, Q)$ is returning.</p>
<p>The output of a bilinear pairing is a group element, specifically an element of a finite cyclic group.</p>
<p><strong>It’s best to treat $e(P, Q)$ as a black box</strong> similar to how most programmers treat hash functions like black boxes.</p>
<p>However, despite it being a black box, we still know a lot about the properties of the output, which we call $G_T$:</p>
<ul>
<li>$G_T$ is a cyclic group, so it has a closed binary operator.</li>
<li>The binary operator of $G_T$ is associative.</li>
<li>$G_T$ has an identity element.</li>
<li>Each element in $G_T$ has an inverse.</li>
<li>Because the group is cyclic, it has a generator.</li>
<li>Because the group is cyclic and finite, then finite cyclic groups are homomorphic to $G_T$. That is, we have some way to homomorphically map elements in a finite field to elements in $G_T$.</li>
</ul>
<p>Because the group is cyclic, we have a notion of $G_T$, $2G_T$, $3G_T$, and so forth. The binary operator of $G_T$ is roughly what we would call “multiplication” so $8G_T = 2G_T * 4G_T$.</p>
<p>If you really want to know what $G_T$ “looks like”, it’s a 12-Dimensional object. The identity element isn’t so scary looking however:</p>
<p>$$(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)$$</p>
<h2>Symmetric and Asymmetric Groups</h2>
<p>The above notation implies that we are using the same generator and elliptic curve group everywhere when we say</p>
<p>$$e(aG, bG) = e(abG, G)$$</p>
<p>In practice however, it turns out to be easier to create bilinear pairings when a different group (but same order) is different for both of the arguments.</p>
<p>Specifically, we say</p>
<p>$$e(a, b) → c, \space\space a ∈ G_1, b ∈ G_2, c ∈ G_T$$</p>
<p>None of the groups used are the same.</p>
<p>However, the property we care about still holds.</p>
<p>$$e(aG_1, bG_2) = e(abG_1, G_2) = e(G_1, abG_2)$$</p>
<p>In the above equation, the group $G_T$ is not explicitly shown but that is the codomain (output space) of $e(G_1, G_2)$.</p>
<p>One could think of $G_1$ and $G_2$ being different elliptic curve equations with different parameters (but the same number of points) and that would be valid because those are different groups.</p>
<p>In a symmetric pairing, the same elliptic curve Group is used for both the arguments of the bilinear pairing function. This means that the generator and elliptic curve group used in both arguments is the same. In this case, the pairing function is often denoted as:</p>
<p>$$e(aG_1, bG_1) = e(abG_1, G_1) = e(G_1, abG_1)$$</p>
<p>In an asymmetric pairing, the arguments use different groups. For example, the first argument can use a different generator and elliptic curve group than the second argument. The pairing function can still satisfy the desired properties</p>
<p>$$e(aG_1, bG_2) = e(abG_1, G_2) = e(G_1, abG_2)$$</p>
<p>In practice we use asymmetric groups, and the difference between the groups we use is explained in the next section.</p>
<p>$G_1$ is the same group we talked about in previous chapters, and in the context of Ethereum, it’s the same <code style='font-family: Arial'>G1</code> we import from the library:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1
</code></pre>
<p>We can also import the <code style='font-family: Arial'>G2</code> from the same library as:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, G2
</code></pre>
<p>But what is $G_2$?</p>
<h2>Field Extensions and the <code style='font-family: Arial'>G2</code> point in Python</h2>
<p>Bilinear pairings are rather agnostic to the kinds of groups you opt for, but Ethereum’s $G_2$ uses elliptic curves with field extensions. If you want to be able to read Solidity code that uses ZK-SNARKS, you’ll at least need a rough idea of what these are.</p>
<p>We usually think of EC points as two points $x$ and $y$. With <em>field extensions</em>, the $x$ and $y$ themselves become two-dimensional objects $(x, y)$ pairs. This is analogous to how complex numbers “extend” real numbers and turn them into something with 2 dimensions (a real component and an imaginary component).</p>
<p>A field extension is a very abstract concept, and frankly, the relationship between a field and its extension doesn’t matter from a purely functional concept.</p>
<p>Just think of it this way:</p>
<p><img alt="Math meme about field extensions" src="assets/935a00_19c62a78929f4cb28ee0e42a14e8ff85_mv2.png" /></p>
<p>An elliptic curve in $G_2$ is an elliptic curve where both the $x$ and $y$ element are two dimensional objects.</p>
<h3>The G2 point in Python</h3>
<p>Enough theory, let’s code this and see a $G_2$ point. Install the <code style='font-family: Arial'>py_ecc</code> library as follows.</p>
<pre style='font-family: Arial'><code class="language-solidity">python -m pip install py_ecc
</code></pre>
<p>Now let’s import the functions that we need from this</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, G2, pairing, add, multiply, eq

print(G1)
# (1, 2)
print(G2)
#((10857046999023057135944570762232829481370756359578518086990519993285655852781, 11559732032986387107991004021392285783925812861821192530917403151452391805634), (8495653923123431417604973247489272438418190587263600148770280649306958101930, 4082367875863433681332203403145435568316851327593401208105741076214120093531))
</code></pre>
<p>If you look closely, you’ll see the <code style='font-family: Arial'>G2</code>, you’ll see that <code style='font-family: Arial'>G2</code> is a pair of tuples. The first tuple is the two-dimensional $x$ point and the second tuple is the two-dimensional $y$ point.</p>
<p><code style='font-family: Arial'>G1</code> and <code style='font-family: Arial'>G2</code> are the generator points for their respective groups.</p>
<p>Both <code style='font-family: Arial'>G1</code> and <code style='font-family: Arial'>G2</code> have the same order (number of points on the curve):</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, G2, eq, curve_order, multiply, eq, curve_order

x = 10 # chosen randomly
assert eq(multiply(G2, x + curve_order), multiply(G2, x))
assert eq(multiply(G1, x + curve_order), multiply(G1, x))
</code></pre>
<p>Even though <code style='font-family: Arial'>G2</code> points might seem a little strange, their behavior is the same as other cyclic groups, especially the <code style='font-family: Arial'>G1</code> group we are familiar with. This means we can construct other points with scalar multiplication (which is really repeated addition) as expected</p>
<pre style='font-family: Arial'><code class="language-solidity">print(eq(add(G1, G1), multiply(G1, 2)))
# True
print(eq(add(G2, G2), multiply(G2, 2)))
# True
</code></pre>
<p>It should be obvious that you can only add elements from the same group.</p>
<pre style='font-family: Arial'><code class="language-solidity">add(G1, G2) # TypeError
</code></pre>
<p>By the way, this library overrides some arithmetic operators (you can do that in python), meaning you can do the following:</p>
<pre style='font-family: Arial'><code class="language-solidity">print(G1 + G1 + G1 == G1*3)
# True

# The above is the same as this:
eq(add(add(G1, G1), G1), multiply(G1, 3))
# True
</code></pre>
<h2>Bilinear Pairings in Python</h2>
<p>At the beginning of this article, we said that bilinear pairings can be used to check if the discrete logs of $P$ and $Q$ multiply to produce the discrete log of $R$, i.e. $PQ = R$.</p>
<p>Here’s how we can do that in Python:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, G2, pairing, multiply, eq

P = multiply(G1, 3)
Q = multiply(G2, 8)

R = multiply(G1, 24)

assert eq(pairing(Q, P), pairing(G2, R))
</code></pre>
<p>Rather annoyingly, the library requires that you pass the <code style='font-family: Arial'>G2</code> point as the first argument to <code style='font-family: Arial'>pairing</code>.</p>
<h3>Equality of products</h3>
<p>Also at the beginning of this article we said a pairing can check:</p>
<p>$$e(P_2, P_1) \stackrel{?}{=} e(Q_2, Q_1)$$</p>
<p>Here’s how we can do that in Python:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, G2, pairing, multiply, eq

P_1 = multiply(G1, 3)
P_2 = multiply(G2, 8)

Q_1 = multiply(G1, 6)
Q_2 = multiply(G2, 4)

assert eq(pairing(P_2, P_1), pairing(Q_2, Q_1))
</code></pre>
<h3>The binary operator of $G_T$</h3>
<p>Elements in $G_T$ are combined using “multiplication” but keep in mind that this is actually a syntactic override in Python:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, G2, pairing, multiply, eq

# 2 * 3 = 6
P_1 = multiply(G1, 2)
P_2 = multiply(G2, 3)

# 4 * 5 = 20
Q_1 = multiply(G1, 4)
Q_2 = multiply(G2, 5)

# 10 * 12 = 120 (6 * 20 = 120 also)
R_1 = multiply(G1, 10)
R_2 = multiply(G2, 12)

assert eq(pairing(P_2, P_1) * pairing(Q_2, Q_1), pairing(R_2, R_1))

# Fails!
</code></pre>
<p>But the assertion fails!</p>
<p>Elements in $G_T$ behave like “powers” of a base.</p>
<p>Recall from algebra that</p>
<p>$$b^xb^y = b^{x+y}$$</p>
<p>Suppose we generate an element in $G_T$ as $e(3G_2, 2G_1)$. We might think of the element as $6G_T$, but it would be a lot more helpful to think of it as $b^6G_T$. There’s no need to know what $b$ is in this context, merely that it exists.</p>
<p>Therefore, to make our code from above work, change $R_1$ and $R_2$ to multiply to 26.</p>
<p>Our code is effectively computing:</p>
<p>$$<br />
\begin{align*}<br />
b ^ {2 \cdot 3} * b ^ {4 \cdot 5} = b ^ {13 \cdot 2}\<br />
b ^ 6 \cdot b ^ {20} = b ^ {26}<br />
\end{align*}<br />
$$</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, G2, pairing, multiply, eq

# 2 * 3 = 6
P_1 = multiply(G1, 2)
P_2 = multiply(G2, 3)

# 4 * 5 = 20
Q_1 = multiply(G1, 4)
Q_2 = multiply(G2, 5)

# 13 * 2 = 26
R_1 = multiply(G1, 13)
R_2 = multiply(G2, 2)

# b ^ {2 * 3} * b ^ {4 * 5} = b ^ {13 * 2}
# b ^ 6 * b ^ 20 = b ^ 26

assert eq(pairing(P_2, P_1) * pairing(Q_2, Q_1), pairing(R_2, R_1))
</code></pre>
<h2>Bilinear Pairings in Ethereum</h2>
<h3>EIP 197 Specification</h3>
<p>The py_ecc library is actually maintained by the <a href="https://ethereum.org/">Ethereum Foundation</a>, and it is what powers the precompile at address 0x8 in the <a href="https://github.com/ethereum/py-evm">PyEVM implementation</a>.</p>
<p>The Ethereum precompile defined in <a href="https://eips.ethereum.org/EIPS/eip-197">EIP-197</a> works on points in <code style='font-family: Arial'>G1</code> and <code style='font-family: Arial'>G2</code>, and <em>implicitly</em> works on points in $G_T$.</p>
<p>The specification of this precompile will seem a little weird at first. It takes in a list of G1 and G2 points laid out as follows:</p>
<p><code style='font-family: Arial'>A₁B₁A₂B₂...AₙBₙ : Aᵢ ∈ G1, Bᵢ ∈ G2</code></p>
<p>These were originally created as</p>
<pre style='font-family: Arial'><code class="language-solidity">A₁ = a₁G1
B₁ = b₁G2
A₂ = a₂G1
B₂ = b₂G2
...
Aₙ = aₙG1
Bₙ = bₙG2
</code></pre>
<p>The precompile returns 1 if the following is true</p>
<pre style='font-family: Arial'><code class="language-solidity">a₁b₁ + a₂b₂ + ... + aₙbₙ = 0
</code></pre>
<p>and zero otherwise.</p>
<p>This might be a bit of a head-scratcher at first! This seems to imply that the precompile is taking the discrete log of each of the points, which is accepted to be infeasible in general. Furthermore, why doesn’t it behave like pairing from the earlier Python examples? The earlier examples returned an element in $G_T$, but this precompile returns a boolean.</p>
<h4>Justification for EIP 197 design decision</h4>
<p>The first problem is that elements in $G_T$ are large, specifically, they 12-dimensional objects.</p>
<p>This will take up a lot of space in memory leading to larger gas costs. Also, because of how most ZK verification algorithms work (this is outside the scope of this article), we generally don’t check the value of the output of a pairing, but only that it is equal to other pairings. Specifically, the final step in <a href="https://rareskills.io/post/groth16">Groth16</a> (the zero knowledge algorithm used by tornado cash) looks like the following</p>
<p>$$<br />
e(A₁, B₂) = e(α₁, β₂) + e(L₁, γ₂) + e(C₁, δ₂)<br />
$$</p>
<p>Where each variable is an elliptic curve point of either $\mathbb{G}_1$ or $\mathbb{G}_2$ based on it’s subscript notation (we would have used upper-case Greek letters to stay consistent with our notation, but those look too similar to the Latin alphabet).</p>
<p>The meanings of these variables is not important at this stage. The fact that it can be written as the sum of “products” (elliptic curve pairing) is what matters. Specifically, we can write it as</p>
<p>$$<br />
0 = e(−A₁, B₂) + e(α₁, β₂) + e(L₁, γ₂) + e(C₁, δ₂)<br />
$$</p>
<p>And now it matches the precompile specification perfectly!</p>
<p>It’s not just Groth16, most zk algorithm have verification formula that looks like that, which is why the precompile was designed to work with sums of pairings rather than return the value of a single pairing.</p>
<p>If we look at the verification code of <a href="https://rareskills.io/post/how-does-tornado-cash-work">Tornado Cash</a>, we can see it is exactly implementing this (even the Greek letters match, but don’t worry if you don’t understand it yet). The $\beta_2$ simply means it is a $\mathbb{G}_2$ point, $\alpha_1$ means $\mathbb{G}_1$ point, etc.</p>
<p><img alt="annotation of the verification code in Tornado Cash" src="assets/935a00_63f7afa2360e49a09139ed2de90189fc_mv2.png" /></p>
<p>Inside the pairing function is where the call to <code style='font-family: Arial'>address(8)</code> is done to complete the pairing calculation and determine if the proof if valid or not.</p>
<p><em>Sometimes, the group $G_T$ is referred to as $G_{12}$ in the context of EIP 197.</em></p>
<h4>Sum of discrete logarithms</h4>
<p>The key insight here is that if</p>
<p>$$ab + cd = 0$$</p>
<p>Then it must also be true that</p>
<p>$$A₁B₂ + C₁D₂ = 0₁₂ \space\space\space\space A₁,C₁ ∈ G1, B₂,D₂ ∈ G2$$</p>
<p>in the $\mathbb{G}_{12}$ group.</p>
<p>The precompile isn’t actually computing the discrete logarithm, it’s simply checking if the sum of pairings is zero.</p>
<p>The sum of pairings is zero if and only if the sum of the products of the discrete logarithms is zero.</p>
<h2>End to End Solidity Example of Bilinear Pairings</h2>
<p>Let us take these inputs of <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, <code style='font-family: Arial'>c</code> and <code style='font-family: Arial'>d</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">a = 4
b = 3
c = 6
d = 2

-ab + cd = 0
</code></pre>
<p>Putting it into the formula we can get</p>
<p>$$<br />
A₁B₂ + C₁D₂ = e(−aG1, bG2) + e(cG1, dG2) = 0<br />
$$</p>
<p>In Python this will equate to</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import neg, multiply, G1, G2
a = 4
b = 3
c = 6
d = 2
# negate G1 * a to make the equation sum up to 0

print(neg(multiply(G1, a)))
#(3010198690406615200373504922352659861758983907867017329644089018310584441462, 17861058253836152797273815394432013122766662423622084931972383889279925210507)
print(multiply(G2, b))
# ((2725019753478801796453339367788033689375851816420509565303521482350756874229, 7273165102799931111715871471550377909735733521218303035754523677688038059653), (2512659008974376214222774206987427162027254181373325676825515531566330959255, 957874124722006818841961785324909313781880061366718538693995380805373202866))
print(multiply(G1, c))
# (4503322228978077916651710446042370109107355802721800704639343137502100212473, 6132642251294427119375180147349983541569387941788025780665104001559216576968)
print(multiply(G2, d))
# ((18029695676650738226693292988307914797657423701064905010927197838374790804409, 14583779054894525174450323658765874724019480979794335525732096752006891875705), (2140229616977736810657479771656733941598412651537078903776637920509952744750, 11474861747383700316476719153975578001603231366361248090558603872215261634898))
</code></pre>
<p>Here’s the output in a structured format</p>
<pre style='font-family: Arial'><code class="language-solidity">aG1_x = 3010198690406615200373504922352659861758983907867017329644089018310584441462,
aG1_y = 17861058253836152797273815394432013122766662423622084931972383889279925210507,

bG2_x1 = 2725019753478801796453339367788033689375851816420509565303521482350756874229,
bG2_x2 = 7273165102799931111715871471550377909735733521218303035754523677688038059653,
bG2_y1 = 2512659008974376214222774206987427162027254181373325676825515531566330959255,
bG2_y2 = 957874124722006818841961785324909313781880061366718538693995380805373202866,

cG1_x = 4503322228978077916651710446042370109107355802721800704639343137502100212473,
cG1_y = 6132642251294427119375180147349983541569387941788025780665104001559216576968,

dG2_x1 = 18029695676650738226693292988307914797657423701064905010927197838374790804409,
dG2_x2 = 14583779054894525174450323658765874724019480979794335525732096752006891875705,
dG2_y1 = 2140229616977736810657479771656733941598412651537078903776637920509952744750,
dG2_y2 = 11474861747383700316476719153975578001603231366361248090558603872215261634898
</code></pre>
<p>Now we have the values encrypted into points in the $\mathbb{G}_1$ and $\mathbb{G}_2$ groups, someone else or a program can confirm that we computed $e(A_1,B_2)+e(C_1,D_2)=0$ correctly without knowing the individual values of <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, <code style='font-family: Arial'>c</code>, or <code style='font-family: Arial'>d</code>. Here’s a Solidity contract that uses the ecPairing precompile to confirm that we computed the equations with valid values.</p>
<p>We create a file Pairings.sol to <a href="https://rareskills.io/post/foundry-testing-solidity">unit test in Foundry</a> (we will supply the test file next)</p>
<pre style='font-family: Arial'><code class="language-solidity">// SPDX-License-Identifier: MIT
pragma solidity ^0.8.13;
contract Pairings {
    /** 
     *  returns true if == 0,
     *  returns false if != 0,
     *  reverts with &quot;Wrong pairing&quot; if invalid pairing
     */
     function run(uint256[12] memory input) public view returns (bool) {
        assembly {
            let success := staticcall(gas(), 0x08, input, 0x0180, input, 0x20)
            if success {
                return(input, 0x20)
            }
        }
        revert(&quot;Wrong pairing&quot;);
    }
}
</code></pre>
<p>We use this Foundry test file to deploy and call our Pairings contract to confirm our ecPairing calculation. The following file we call <code style='font-family: Arial'>TestPairings.sol</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">// SPDX-License-Identifier: MIT
pragma solidity ^0.8.13;
import &quot;forge-std/Test.sol&quot;;
import &quot;../src/Pairings.sol&quot;;

contract PairingsTest is Test {
    Pairings public pairings;

    function setUp() public {
        pairings = new Pairings();
    }

    function testPairings() public view {
        uint256 aG1_x = 3010198690406615200373504922352659861758983907867017329644089018310584441462;
        uint256 aG1_y = 17861058253836152797273815394432013122766662423622084931972383889279925210507;

        uint256 bG2_x1 = 2725019753478801796453339367788033689375851816420509565303521482350756874229;
        uint256 bG2_x2 = 7273165102799931111715871471550377909735733521218303035754523677688038059653;
        uint256 bG2_y1 = 2512659008974376214222774206987427162027254181373325676825515531566330959255;
        uint256 bG2_y2 = 957874124722006818841961785324909313781880061366718538693995380805373202866;

        uint256 cG1_x = 4503322228978077916651710446042370109107355802721800704639343137502100212473;
        uint256 cG1_y = 6132642251294427119375180147349983541569387941788025780665104001559216576968;

        uint256 dG2_x1 = 18029695676650738226693292988307914797657423701064905010927197838374790804409;
        uint256 dG2_x2 = 14583779054894525174450323658765874724019480979794335525732096752006891875705;
        uint256 dG2_y1 = 2140229616977736810657479771656733941598412651537078903776637920509952744750;
        uint256 dG2_y2 = 11474861747383700316476719153975578001603231366361248090558603872215261634898;

        uint256[12] memory points = [
            aG1_x,
            aG1_y,
            bG2_x2,
            bG2_x1,
            bG2_y2,
            bG2_y1,
            cG1_x,
            cG1_y,
            dG2_x2,
            dG2_x1,
            dG2_y2,
            dG2_y1
        ];

        bool x = pairings.run(points);
        console2.log(&quot;result:&quot;, x);
    }
}
</code></pre>
<p><strong>Note that the way G2 points are arranged is not the same way Python lays out the G2 points.</strong></p>
<p>This passes and prints out <code style='font-family: Arial'>true</code> to the console. Notice that the points have been labeled by their variable name, which group they belong to, and if they represent an <code style='font-family: Arial'>x</code> or a <code style='font-family: Arial'>y</code> of the elliptic curve point.</p>
<p>Its important to note that the ecPairing precompile does not expect or require an array and that our choice of using one with inline-assembly is simply optional. One could do the same with solidity like so:</p>
<pre style='font-family: Arial'><code class="language-solidity">function run(bytes calldata input) public view returns (bool) {
    // optional, the precompile checks this too and reverts (with no error) if false, this helps narrow down possible errors
    if (input.length % 192 != 0) revert(&quot;Points must be a multiple of 6&quot;);
    (bool success, bytes memory data) = address(0x08).staticcall(input);
    if (success) return abi.decode(data, (bool));
    revert(&quot;Wrong pairing&quot;);
}
</code></pre>
<p>And update the test file like so:</p>
<pre style='font-family: Arial'><code class="language-solidity">function testPairings() public view {
    uint256 aG1_x = 3010198690406615200373504922352659861758983907867017329644089018310584441462;
    uint256 aG1_y = 17861058253836152797273815394432013122766662423622084931972383889279925210507;

    uint256 bG2_x1 = 2725019753478801796453339367788033689375851816420509565303521482350756874229;
    uint256 bG2_x2 = 7273165102799931111715871471550377909735733521218303035754523677688038059653;
    uint256 bG2_y1 = 2512659008974376214222774206987427162027254181373325676825515531566330959255;
    uint256 bG2_y2 = 957874124722006818841961785324909313781880061366718538693995380805373202866;

    uint256 cG1_x = 4503322228978077916651710446042370109107355802721800704639343137502100212473;
    uint256 cG1_y = 6132642251294427119375180147349983541569387941788025780665104001559216576968;

    uint256 dG2_x1 = 18029695676650738226693292988307914797657423701064905010927197838374790804409;
    uint256 dG2_x2 = 14583779054894525174450323658765874724019480979794335525732096752006891875705;
    uint256 dG2_y1 = 2140229616977736810657479771656733941598412651537078903776637920509952744750;
    uint256 dG2_y2 = 11474861747383700316476719153975578001603231366361248090558603872215261634898;

    bytes memory points = abi.encode(
        aG1_x,
        aG1_y,
        bG2_x2,
        bG2_x1,
        bG2_y2,
        bG2_y1,
        cG1_x,
        cG1_y,
        dG2_x2,
        dG2_x1,
        dG2_y2,
        dG2_y1
    );

    bool x = pairings.run(points);
    console2.log(&quot;result:&quot;, x);
}
</code></pre>
<p>This will pass and return true just like the initial implementation because it sends the exact same calldata to the precompile.</p>
<p>The only difference is that in the first implementation, the test file sends an array of points to the pairing contract which uses inline-assembly to slice off the first 32 bytes (array length) and sends the rest to the precompile. And in the second implementation, the test file sends the abi encoded points to the pairing contract which forwards it as it is to the precompile.</p>
<h2>Learn More from RareSkills</h2>
<p>This material is taken from our <a href="https://rareskills.io/zk-bootcamp">Zero Knowledge Course</a>. Please see the program to learn more.</p>
<hr />
<h2>I really want to understand the math behind bilinear pairings</h2>
<p>You’ve been warned, the math is quite intense, and understanding it will not help you actually implement ZK Proofs, which is the goal of this book. You’ve probably used SHA-256 or Keccak256 productively without knowing their internals, and we <em>strongly</em> suggest you treat pairings the same way at this stage in your journey. Nevertheless, if you haven’t been deterred by our warnings, here is a good resource if you still want to dive into it: <a href="https://static1.squarespace.com/static/5fdbb09f31d71c1227082339/t/5ff394720493bd28278889c6/1609798774687/PairingsForBeginners.pdf">Pairings for Beginners</a>. <em>Here be dragons.</em></p>
<p><em>Originally Published July 18, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>Converting Algebraic Circuits to R1CS (Rank One Constraint System)</h1>
<p>Source: https://rareskills.io/post/rank-1-constraint-system</p>
<h1>Converting Algebraic Circuits to R1CS (Rank One Constraint System)</h1>
<p>This article is explains how to turn a set of arithmetic constraints into Rank One Constraint System (R1CS).</p>
<p>The focus of this resource is implementation: we cover a lot more corner cases of doing this transformation than other materials, discuss optimizations, and explain how the Circom library accomplishes it.</p>
<h2>Prerequisites</h2>
<ul>
<li>We assume the reader understands how to use <a href="https://www.rareskills.io/post/arithmetic-circuit">arithmetic circuits (zk circuits)</a> to represent the validity of a computation.</li>
<li>The reader is familiar with <a href="https://www.rareskills.io/post/finite-fields">modular arithmetic</a>. All operations here happen in a finite field, so $-5$ really means the additive inverse of $5$ modulo $p$ and $2/3$ means the multiplicative inverse of $3$ modulo $p$ times $2$.</li>
</ul>
<h2>Rank 1 Constraint System overview</h2>
<p>A Rank 1 Constraint System (R1CS) is an arithmetic circuit with the requirement that each equality constraint has one multiplication (and no restriction on the number of additions).</p>
<p>This makes the representation of the arithmetic circuit compatible with the use of bilinear pairings. The output of a pairing $G_1 \bullet G_2 \rightarrow G_T$ cannot be paired again, as an element in $G_T$ cannot be used as part of the input of another pairing. Hence, we only allow one multiplication per constraint.</p>
<h2>The witness vector</h2>
<p>In an arithmetic circuit, the witness is an assignment to all the signals that satisfies the constraints of the equation.</p>
<p>In a Rank 1 Constraint System the witness vector is a $1 \times n$ vector that contains the value of all the input variables, the output variable, and the intermediate values. It shows you have executed the circuit from start to finish, knowing both the input, output, and all the intermediate values.</p>
<p>By convention, the first element is always 1 to make some calculations easier, which we will demonstrate later.</p>
<p>For example, if we have the constraint</p>
<p>$$z = x^2y$$</p>
<p>that we claim to know the solution for, then that must mean we know $x$, $y$, and $z$. Because Rank One Constraint Systems require exactly one multiplication per constraint, the polynomial above constraints must be written as</p>
<p>$$<br />
\begin{align*}<br />
v₁ = xx \<br />
z = v₁y<br />
\end{align*}$$</p>
<p>A witness means we don’t just know $x$, $y$, and $z$, we must know every intermediate variable in this expanded form. Specifically, our witness is a vector:</p>
<p>$$[1, z, x, y, v₁]$$</p>
<p>where each term has a value that satisfies the constraints above.</p>
<p>For example,</p>
<p>$$[1, 18, 3, 2, 9]$$</p>
<p>is a valid witness because when we plug the values in,</p>
<p>$$[text{constant} = 1, z = 18, x = 3, y = 2, v₁ = 9]$$<br />
it satisfies the constraints</p>
<p>$$<br />
\begin{align*}<br />
v_1 = x*x \rightarrow 9 = 3\cdot3\<br />
z = v_1*y \rightarrow 18 = 9\cdot2<br />
\end{align*}$$</p>
<p>The extra 1 term is not used in this example and is a convenience we will explain later.</p>
<h2>Example 1: Transforming $z = x \cdot y$ into a Rank 1 Constraint System</h2>
<p>For our example, we will say we are proving $41 \times 103 = 4223$.</p>
<p>Therefore, our witness vector is $[1, 4223, 41, 103]$ as an assignment to $[1, z, x, y]$.</p>
<p>Before we can create an R1CS, our constraints need to be of the form</p>
<pre style='font-family: Arial'><code class="language-solidity">result = left_hand_side × right_hand_side
</code></pre>
<p>Luckily for us, it already is</p>
<p>$$<br />
\underbrace{z}_\text{result} = \underbrace{x}_\text{left hand side} \times \underbrace{y}_\text{right hand side}$$</p>
<p>This is obviously a trivial example, but trivial examples are usually a great place to start.</p>
<p>To create a valid R1CS, you need a list of formulas that contain exactly one multiplication.</p>
<p>We will discuss later how to handle cases that do not have exactly one multiplication like $z = x³$ or $z = x³ + y$.</p>
<p>Our goal is to create a system of equations of the form:</p>
<p>$$<br />
\mathbf{O}\mathbf{a} = \mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}$$</p>
<p>Where $O$, $L$, and $R$ are matrices of size $n$ x $m$ ($n$ rows and $m$ columns).</p>
<p>Matrix $\mathbf{L}$ encodes the the variable on the left side of the multiplication and $\mathbf{R}$ encodes the variables on the right side of the multiplication. $\mathbf{O}$ encodes the result variables. The vector $\mathbf{a}$ is the witness vector.</p>
<p>Specifically, $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ are matrices with the same number of columns as the witness vector $\mathbf{a}$, and each column represents the same variable the index is using.</p>
<p>So for our example, the witness has 4 elements $(1, z, x, y)$ so each of our matrices will have 4 columns, so $m = 4$.</p>
<p>The number of rows will correspond to the number of constraints in the circuit. In our case, we have only one constraint: $z = x * y$, so we will only have one row, so $n = 1$.</p>
<p>Let’s jump to the answer and explain how we obtained it.</p>
<p>$$\mathbf{O}\mathbf{a} = \mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}$$</p>
<p>$$<br />
\underbrace{\begin{bmatrix}<br />
0 &amp; 1 &amp; 0 &amp; 0 \<br />
\end{bmatrix}}_{\mathbf{O}}\mathbf{a} =<br />
\underbrace{\begin{bmatrix}<br />
0 &amp; 0 &amp; 1 &amp; 0 \<br />
\end{bmatrix}}_{\mathbf{L}}\mathbf{a}\circ<br />
\underbrace{\begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; 1 \<br />
\end{bmatrix}}_{\mathbf{R}}\mathbf{a}$$<br />
$$<br />
\begin{bmatrix}<br />
0 &amp; 1 &amp; 0 &amp; 0 \<br />
\end{bmatrix}<br />
\begin{bmatrix}<br />
1 \<br />
4223 \<br />
41 \<br />
103 \<br />
\end{bmatrix}=<br />
\begin{bmatrix}<br />
0 &amp; 0 &amp; 1 &amp; 0 \<br />
\end{bmatrix}<br />
\begin{bmatrix}<br />
1 \<br />
4223 \<br />
41 \<br />
103 \<br />
\end{bmatrix}\circ<br />
\begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; 1 \<br />
\end{bmatrix}<br />
\begin{bmatrix}<br />
1 \<br />
4223 \<br />
41 \<br />
103 \<br />
\end{bmatrix}$$</p>
<p>In this example, each item in the matrix serves as an indicator variable for whether or not the variable the column corresponds to is present. (Technically, it’s the coefficient of the variable, but we’ll get to that later).</p>
<p>For the left hand terms, $x$ is the only variable present on the left side of the multiplication, so if the columns represent $[1, z, x, y]$, then…</p>
<p>$\mathbf{L}$ is $[0, 0, 1, 0]$, because $x$ is present, and none of the other variables are.</p>
<p>$\mathbf{R}$ is $[0, 0, 0, 1]$ because the only variable in the right side of the multiplication is $y$, and</p>
<p>$\mathbf{O}$ is $[0, 1, 0, 0]$ because we only have the $z$ variable in the "output" of the multiplication.</p>
<p>We don’t have any constants anywhere so the 1 column is zero everywhere (We’ll discuss when it is non-zero later).</p>
<p>This equation is correct, which we can verify in Python</p>
<pre style='font-family: Arial'><code class="language-solidity">import numpy as np

# define the matrices
O = np.matrix([[0,1,0,0]])
L = np.matrix([[0,0,1,0]])
R = np.matrix([[0,0,0,1]])

# witness vector
a = np.array([1, 4223, 41, 103])

# Multiplication `*` is element-wise, not matrix multiplication.
# Result contains a bool indicating an element-wise indicator that the equality is true for that element.
result = np.matmul(O, a) == np.matmul(L, a) * np.matmul(R, a) 

# check that every element-wise equality is true
assert result.all(), &quot;result contains an inequality&quot;
</code></pre>
<p>You may be wondering what the point of this is, aren’t we just saying that $41 \times 103 = 4223$ in a much less compact way?</p>
<p>You would be correct.</p>
<p>An R1CS can be quite verbose, but they map nicely to <a href="https://www.rareskills.io/post/quadratic-arithmetic-program">Quadratic Arithmetic Programs (QAPs)</a>, which can be made succinct. But we will not concern ourselves with QAPs here.</p>
<p>But this is an important point of R1CS. A R1CS communicates exactly the same information as the original arithmetic constraints, but with only one multiplication per equality constraint. In this example, we have only one constraint, but we’ll add more in the next example.</p>
<h2>Example 2: Transforming r = x <em>y</em> z * u</h2>
<p>In this slightly more complicated example, we need to deal with intermediate variables now. Each row of our computation can only have one multiplication, so we must break up our equation as follows:</p>
<p>$$<br />
\begin{align*}<br />
v_1 &amp;= xy \<br />
v_2 &amp;= zu \<br />
r &amp;= v_1v_2<br />
\end{align*}$$</p>
<p>There is no rule to say we had to break it up like that, the following is also valid:</p>
<p>$$<br />
\begin{align*}<br />
v_1 &amp;= xy \<br />
v_2 &amp;= v_1z \<br />
r &amp;= v_2u<br />
\end{align*}$$</p>
<p>We will use the first transformation for this example.</p>
<h3>Size of $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$</h3>
<p>Because we are dealing with 7 variables $(r, x, y, z, u, v_1, v_2)$, our witness vector will have eight elements (the first being the constant 1) and our matrices will have eight columns.</p>
<p>Because we have three constraints, the matrices will have three rows.</p>
<h3>Left hand terms and right hand terms</h3>
<p>This example will strongly enforce the idea of a "left hand term" and a "right hand term." Specifically, $x$, $z$, and $v_1$ are left hand terms, and $y$, $u$, and $v_2$ are right hand terms.</p>
<p>$$<br />
\underbrace{<br />
\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}<br />
}_\text{ Output Terms }<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
\underbrace{<br />
\begin{matrix}<br />
x \<br />
z \<br />
v_1 \<br />
\end{matrix}<br />
}_\text{ Left Hand Terms }<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\underbrace{<br />
\begin{matrix}<br />
y \<br />
u \<br />
v_2 \<br />
\end{matrix}<br />
}_\text{ Right Hand Terms }$$</p>
<h3>Constructing matrix $\mathbf{L}$ from left hand terms</h3>
<p>Let’s construct the matrix A. We know it will have three rows (since there are three constraints) and eight columns (since there are eight variables).</p>
<p>$$<br />
\begin{bmatrix}<br />
&amp; l_{1,2} &amp; l_{1,3} &amp; l_{1,4} &amp; l_{1,5} &amp; l_{1,6} &amp; l_{1,7} &amp; l_{1,8} \<br />
&amp; l_{2,2} &amp; l_{2,3} &amp; l_{2,4} &amp; l_{2,5} &amp; l_{2,6} &amp; l_{2,7} &amp; l_{2,8} \<br />
&amp; l_{3,2} &amp; l_{3,3} &amp; l_{3,4} &amp; l_{3,5} &amp; l_{3,6} &amp; l_{3,7} &amp; l_{3,8} \<br />
\end{bmatrix}$$</p>
<p>Our witness vector will be multiplied by this, so let’s define our witness vector to have the following layout:</p>
<p>$$<br />
\begin{bmatrix}<br />
1 &amp; r &amp;x &amp; y &amp; z &amp; u &amp; v_1 &amp; v_2<br />
\end{bmatrix}$$</p>
<p>This informs us as to what $\mathbf{L}$’s columns represents:</p>
<p>$$<br />
\mathbf{L} =<br />
\begin{bmatrix}<br />
l_{1, 1} &amp; l_{1, r} &amp; l_{1, x} &amp; l_{1, y} &amp; l_{1, z} &amp; l_{1, u} &amp; l_{1, v_1} &amp; l_{1, v_2} \<br />
l_{2, 1} &amp; l_{2, r} &amp; l_{2, x} &amp; l_{2, y} &amp; l_{2, z} &amp; l_{2, u} &amp; l_{2, v_1} &amp; l_{2, v_2} \<br />
l_{3, 1} &amp; l_{3, r} &amp; l_{3, x} &amp; l_{3, y} &amp; l_{3, z} &amp; l_{3, u} &amp; l_{3, v_1} &amp; l_{3, v_2} \<br />
\end{bmatrix}$$</p>
<h4>First row of $\mathbf{L}$</h4>
<p>In the first row, for the first left variable, we have $v₁ = xy$:</p>
<p>$$<br />
\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
\underset{\mathbf{L}}{\boxed{<br />
\begin{matrix}<br />
\color{red}{x} \<br />
z \<br />
v_1 \<br />
\end{matrix}<br />
}}<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\begin{matrix}<br />
y \<br />
u \<br />
v_2 \<br />
\end{matrix}$$</p>
<p>This means with respect to the left hand side, the variable $x$ is present, and no other variables are present. Therefore, we transform the first row as follows:</p>
<p>$$<br />
\mathbf{L}=\begin{bmatrix}<br />
0 &amp; 0 &amp; \color{red}{1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
l_{2, 1} &amp; l_{2, r} &amp; l_{2, x} &amp; l_{2, y} &amp; l_{2, z} &amp; l_{2, u} &amp; l_{2, v_1} &amp; l_{2, v_2} \<br />
l_{3, 1} &amp; l_{3, r} &amp; l_{3, x} &amp; l_{3, y} &amp; l_{3, z} &amp; l_{3, u} &amp; l_{3, v_1} &amp; l_{3, v_2} \<br />
\end{bmatrix}$$</p>
<p>Recall that the columns of $\mathbf{L}$ are labelled as follows:</p>
<p>$$<br />
\begin{bmatrix}<br />
1 &amp; r &amp;x &amp; y &amp; z &amp; u &amp; v_1 &amp; v_2\<br />
\end{bmatrix}$$</p>
<p>and we see that the $1$ is in the $x$ column.</p>
<h4>Second row of $\mathbf{L}$</h4>
<p>Working our way down, we see that only $z$ is present for the left-hand side of our systems of equations.</p>
<p>$$<br />
\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
\underset{\mathbf{L}}{\boxed{<br />
\begin{matrix}<br />
x \<br />
\color{green}{z} \<br />
v_1 \<br />
\end{matrix}<br />
}}<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\begin{matrix}<br />
y \<br />
u \<br />
v_2 \<br />
\end{matrix}$$</p>
<p>Therefore, we set everything in that row to be zero, except the column that represents $z$.</p>
<p>$$<br />
\mathbf{L}=\begin{bmatrix}<br />
0 &amp; 0 &amp; \color{red}{1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; \color{green}{1} &amp; 0 &amp; 0 &amp; 0 \<br />
l_{3, 1} &amp; l_{3, r} &amp; l_{3, x} &amp; l_{3, y} &amp; l_{3, z} &amp; l_{3, u} &amp; l_{3, v_1} &amp; l_{3, v_2} \<br />
\end{bmatrix}$$</p>
<h4>Third row of $\mathbf{L}$</h4>
<p>Finally, we have $v₁$ as the only present variable in the left hand operators in the third row</p>
<p>$$<br />
\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
\underset{\mathbf{L}}{\boxed{<br />
\begin{matrix}<br />
x \<br />
z \<br />
\color{violet}{v_1} \<br />
\end{matrix}<br />
}}<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\begin{matrix}<br />
y \<br />
u \<br />
v_2 \<br />
\end{matrix}$$</p>
<p>This completes matrix $\mathbf{L}$</p>
<p>$$<br />
\mathbf{L}=\begin{bmatrix}<br />
0 &amp; 0 &amp; \color{red}{1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; \color{green}{1} &amp; 0 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \color{violet}{1} &amp; 0 \<br />
\end{bmatrix}$$</p>
<p>The following image should make the mapping more clear:</p>
<p>$$<br />
\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
\underset{\mathbf{L}}{\boxed{<br />
\begin{matrix}<br />
\color{red}{x} \<br />
\color{green}{z} \<br />
\color{violet}{v_1} \<br />
\end{matrix}<br />
}}<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\begin{matrix}<br />
y \<br />
u \<br />
v_2 \<br />
\end{matrix}<br />
\space\space\space\space<br />
\begin{array}{c}<br />
\begin{array}{cc}<br />
\begin{matrix}<br />
1 &amp; r &amp; x &amp; y &amp; z &amp; u &amp; v_1 &amp; v_2 \<br />
\end{matrix}<br />
\end{array} \[10pt]<br />
\begin{array}{cc}<br />
\begin{bmatrix}<br />
0 &amp; 0 &amp; \color{red}{1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; \color{green}{1} &amp; 0 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \color{violet}{1} &amp; 0 \<br />
\end{bmatrix}<br />
\end{array}<br />
\end{array}$$</p>
<h4>Alternative transformation of $\mathbf{L}$</h4>
<p>We could accomplish this same exercises by expanding the left hand values of</p>
<p>$$<br />
\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
{<br />
\begin{matrix}<br />
x \<br />
z \<br />
v_1 \<br />
\end{matrix}<br />
}<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\begin{matrix}<br />
y \<br />
u \<br />
v_2 \<br />
\end{matrix}$$</p>
<p>as</p>
<p>$$<br />
\begin{align*}<br />
v_1 &amp;= (0\cdot 1 + 0\cdot r + \boxed{1\cdot x} + 0\cdot y + 0\cdot z + 0\cdot u + 0\cdot v_1 + 0\cdot v_2) \times y\<br />
v_2 &amp;= (0\cdot 1 + 0\cdot r + 0\cdot x + 0\cdot y + \boxed{1\cdot z} + 0\cdot u + 0\cdot v_1 + 0\cdot v_2) \times u\<br />
r &amp;= (0\cdot 1 + 0\cdot r + 0\cdot x + 0\cdot y + 0\cdot z + 0\cdot u + \boxed{1\cdot v_1} + 0\cdot v_2) \times v_2 \<br />
\end{align*}$$</p>
<p>We can do that because adding zero terms doesn’t change the values. We just have to be careful to expand the zero variables to have the same “columns” as how we’ve defined the witness vector.</p>
<p>And then, if we take the coefficients (shown in boxes) out of the above expansion,</p>
<p>$$<br />
\begin{align*}<br />
v_1 &amp;= (\boxed{0}\cdot 1 + \boxed{0}\cdot r + \boxed{1}\cdot x + \boxed{0}\cdot y + \boxed{0}\cdot z + \boxed{0}\cdot u + \boxed{0}\cdot v_1 + \boxed{0}\cdot v_2) \times y\<br />
v_2 &amp;= (\boxed{0}\cdot 1 + \boxed{0}\cdot r + \boxed{0}\cdot x + \boxed{0}\cdot y + \boxed{1}\cdot z + \boxed{0}\cdot u + \boxed{0}\cdot v_1 + \boxed{0}\cdot v_2) \times u\<br />
r &amp;= (\boxed{0}\cdot 1 + \boxed{0}\cdot r + \boxed{0}\cdot x + \boxed{0}\cdot y + \boxed{0}\cdot z + \boxed{0}\cdot u + \boxed{1}\cdot v_1 + \boxed{0}\cdot v_2) \times v_2 \<br />
\end{align*}$$</p>
<p>We get the same matrix for $\mathbf{L}$ that we generated a moment ago.</p>
<p>$$<br />
\mathbf{L}=\begin{bmatrix}<br />
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \<br />
\end{bmatrix}$$</p>
<h3>Constructing matrix $\mathbf{R}$ from right hand terms</h3>
<p>$$<br />
R = \begin{bmatrix}<br />
r_{1,1} &amp; r_{1,r} &amp; r_{1,x} &amp; r_{1,y} &amp; r_{1,z} &amp; r_{1,u} &amp; r_{1,v_1} &amp; r_{1,v_2} \<br />
r_{2,1} &amp; r_{2,r} &amp; r_{2,x} &amp; r_{2,y} &amp; r_{2,z} &amp; r_{2,u} &amp; r_{2,v_1} &amp; r_{2,v_2} \<br />
r_{3,1} &amp; r_{3,r} &amp; r_{3,x} &amp; r_{3,y} &amp; r_{3,z} &amp; r_{3,u} &amp; r_{3,v_1} &amp; r_{3,v_2} \<br />
\end{bmatrix}$$</p>
<p>Matrix $\mathbf{R}$ represents the right hand terms of our equation:</p>
<p>$$<br />
\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
{<br />
\begin{matrix}<br />
x \<br />
z \<br />
v_1 \<br />
\end{matrix}<br />
}<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\underset{\mathbf{R}}{\boxed{<br />
\begin{matrix}<br />
y \<br />
u \<br />
v_2 \<br />
\end{matrix}}}$$</p>
<p>Matrix $\mathbf{R}$ must have 1s representing $y$, $u$, and $v_2$. The row in the matrix corresponds to the row of the arithmetic constraint, i.e. we can number the constraints (rows) as follows:</p>
<p>$$<br />
\begin{matrix}<br />
(1) \<br />
(2) \<br />
(3) \<br />
\end{matrix}<br />
\space\space<br />
\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
{<br />
\begin{matrix}<br />
x \<br />
z \<br />
v_1 \<br />
\end{matrix}<br />
}<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\underset{\mathbf{R}}{\boxed{<br />
\begin{matrix}<br />
y \<br />
u \<br />
v_2 \<br />
\end{matrix}}}$$</p>
<p>So the first row has 1 in the $y$ column, the second row has 1 in the $u$ column, and the third row has 1 in the $v_2$ column. Everything else is zero.</p>
<p>This results in the following for matrix $\mathbf{R}$:</p>
<p>$$<br />
\begin{array}{c}<br />
\begin{array}{cc}<br />
\begin{matrix}<br />
1 &amp; r &amp; x &amp; y &amp; z &amp; u &amp; v_1 &amp; v_2 \<br />
\end{matrix}<br />
\end{array} \[10pt]<br />
\begin{array}{cc}<br />
\begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \<br />
\end{bmatrix}<br />
\end{array}<br />
\end{array}$$</p>
<p>This diagram illustrates the transformation.</p>
<p>$$<br />
\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
\begin{matrix}<br />
x \<br />
z \<br />
v_1 \<br />
\end{matrix}<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\underset{\mathbf{R}}<br />
{\boxed{<br />
\begin{matrix}<br />
\color{red}{y} \<br />
\color{green}{u} \<br />
\color{violet}{v_2} \<br />
\end{matrix}<br />
}}<br />
\space\space\space\space<br />
\begin{array}{c}<br />
\begin{array}{cc}<br />
\begin{matrix}<br />
1 &amp; r &amp; x &amp; y &amp; z &amp; u &amp; v_1 &amp; v_2 \<br />
\end{matrix}<br />
\end{array} \[10pt]<br />
\begin{array}{cc}<br />
\begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; \color{red}{1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \color{green}{1} &amp; 0 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;\color{violet}{1} \<br />
\end{bmatrix}<br />
\end{array}<br />
\end{array}$$</p>
<h3>Constructing matrix $\mathbf{O}$</h3>
<p>It is an exercise for the reader to determine that matrix $\mathbf{O}$ is</p>
<p>$$<br />
\mathbf{O}=\begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \<br />
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
\end{bmatrix}$$</p>
<p>using the column labels consistent with earlier matrices.</p>
<p>By way of reminder, $\mathbf{C}$ is derived from the result of the multiplication</p>
<p>$$<br />
\underset{\mathbf{C}}{<br />
\boxed{\begin{matrix}<br />
v_1 \<br />
v_2 \<br />
r \<br />
\end{matrix}}}<br />
\begin{matrix}<br />
=\<br />
=\<br />
=<br />
\end{matrix}<br />
\begin{matrix}<br />
x \<br />
z \<br />
v_1 \<br />
\end{matrix}<br />
\begin{matrix}<br />
\times\<br />
\times\<br />
\times<br />
\end{matrix}<br />
\begin{matrix}<br />
y \<br />
u \<br />
v_2 \<br />
\end{matrix}$$</p>
<p>And the column labels are as follows</p>
<p>$$<br />
\begin{array}{c}<br />
\begin{array}{cc}<br />
\begin{matrix}<br />
1 &amp; r &amp; x &amp; y &amp; z &amp; u &amp; v_1 &amp; v_2 \<br />
\end{matrix}<br />
\end{array} \[10pt]<br />
\begin{array}{cc}<br />
\begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \<br />
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br />
\end{bmatrix}<br />
\end{array}<br />
\end{array}$$</p>
<p>Checking our work for $r = x \cdot y \cdot z \cdot u$</p>
<pre style='font-family: Arial'><code class="language-solidity">import numpy as np

# enter the A B and C from above
L = np.matrix([[0,0,1,0,0,0,0,0],
              [0,0,0,0,1,0,0,0],
              [0,0,0,0,0,0,1,0]])

R = np.matrix([[0,0,0,1,0,0,0,0],
              [0,0,0,0,0,1,0,0],
              [0,0,0,0,0,0,0,1]])

O = np.matrix([[0,0,0,0,0,0,1,0],
              [0,0,0,0,0,0,0,1],
              [0,1,0,0,0,0,0,0]])

# random values for x, y, z, and u
import random
x = random.randint(1,1000)
y = random.randint(1,1000)
z = random.randint(1,1000)
u = random.randint(1,1000)

# compute the algebraic circuit
r = x * y * z * u
v1 = x*y
v2 = z*u

# create the witness vector
a = np.array([1, r, x, y, z, u, v1, v2])

# element-wise multiplication, not matrix multiplication
result = np.matmul(O, a) == np.multiply(np.matmul(L, a), np.matmul(R, a))

assert result.all(), &quot;system contains an inequality&quot;
</code></pre>
<h2>Example 3: Addition with a constant</h2>
<p>What if we want to build a rank one constraint system for the following?</p>
<p>$$z = x * y + 2$$</p>
<p>This is where that 1 column comes in handy.</p>
<h3>Addition is free</h3>
<p>You’ve probably heard the statement “addition is free” in the context of ZK-SNARKs. What that means is we don’t have to create an additional constraint when we have an addition operation.</p>
<p>We could write the above formula as</p>
<p>$$\begin{align*}<br />
v_1 = xy \<br />
z = v_1 + 2 \<br />
\end{align*}$$</p>
<p>but that would make our R1CS larger than it needs to be.</p>
<p>Instead, we can write it as</p>
<p>$$-2 + z = xy$$</p>
<p>Then the variable $z$ and the constant $-2$ are automatically “combined” when we multiply $\mathbf{a}$ by $\mathbf{O}$ with our witness vector.</p>
<p>Our witness vector has the form <code style='font-family: Arial'>[1, z, x, y]</code>, so our matrix $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ are as follows:</p>
<p>$$<br />
\mathbf{L} = \begin{bmatrix}<br />
0 &amp; 0 &amp; 1 &amp; 0 \<br />
\end{bmatrix}$$<br />
$$</p>
<p>mathbf{R} = begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; 1<br />
end{bmatrix}$$</p>
<p>$$<br />
\mathbf{O} = \begin{bmatrix}<br />
-2 &amp; 1 &amp; 0 &amp; 0 \<br />
\end{bmatrix}$$<br />
Whenever there are additive constants, we simply place them in the $1$ column, which by convention is the first column.<br />
Again, let’s do some unit tests on our math:<br />
“`python<br />
import numpy as np<br />
import random</p>
<h1>Define the matrices</h1>
<p>L = np.matrix([[0,0,1,0]])<br />
R = np.matrix([[0,0,0,1]])<br />
O = np.matrix([[-2,1,0,0]])</p>
<h1>pick random values to test the equation</h1>
<p>x = random.randint(1,1000)<br />
y = random.randint(1,1000)<br />
z = x * y + 2 # witness vector<br />
a = np.array([1, z, x, y])</p>
<h1>check the equality</h1>
<p>result = O.dot(a) == np.multiply(np.matmul(L, a), R.dot(a))<br />
assert result.all(), “result contains an inequality”<br />
“`</p>
<h2>Example 4: Multiplication with a constant</h2>
<p>In all of the examples above, we never multiplied variables by constants. That’s why the entries in the R1CS was always 1. As you may have guessed from the above example, the entry in the matrices is the same value of the constant the variable is multiplied by as the following example will show.<br />
Let’s work out the solution for<br />
$$z = 2x^2 + y$$<br />
Note that when we say “one multiplication per constraint” we mean the multiplication of two variables. Muliplication with a constant is not “real” multiplication because it is really repeated addition of the same variable.<br />
The following solution is valid, but creates unnecessary rows:<br />
$$</p>
<p>begin{align<em>}<br />
v_1 &amp;= xx<br />
z &amp;= 2v_1 + y<br />
end{align</em>}</p>
<p>$$<br />
The more optimal solution is as follows:<br />
$$-y + z = 2xx$$<br />
Using the more optimal solution, our witness vector will have the form <code style='font-family: Arial'>[1, out, x, y]</code>.<br />
The matrices will be defined as follows:<br />
$$</p>
<p>begin{align<em>}<br />
mathbf{L} &amp;= begin{bmatrix}<br />
0 &amp; 0 &amp; 2 &amp; 0<br />
end{bmatrix}<br />
mathbf{R} &amp;= begin{bmatrix}<br />
0 &amp; 0 &amp; 1 &amp; 0<br />
end{bmatrix}<br />
mathbf{O} &amp;= begin{bmatrix}<br />
0 &amp; 1 &amp; 0 &amp; -1<br />
end{bmatrix}<br />
end{align</em>}</p>
<p>$$<br />
Symbolically multiplying the above by <code style='font-family: Arial'>[1, z, x, y]</code> in the r1cs form gives us our original equation back:<br />
$$</p>
<h1>begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; -1 end{bmatrix} begin{bmatrix} 1 z x y end{bmatrix}</h1>
<p>begin{bmatrix}<br />
0 &amp; 0 &amp; 2 &amp; 0<br />
end{bmatrix}<br />
begin{bmatrix}<br />
1<br />
z<br />
x<br />
y<br />
end{bmatrix}circ<br />
begin{bmatrix}<br />
0 &amp; 0 &amp; 1 &amp; 0<br />
end{bmatrix}<br />
begin{bmatrix}<br />
1<br />
z<br />
x<br />
y<br />
end{bmatrix}</p>
<p>$$$$</p>
<p>z – y = 2xx</p>
<p>$$$$</p>
<p>z = 2x^2 + y</p>
<p>$$<br />
so we know we set up $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ correctly.<br />
Here we have one row (constraints) and one<br />
“true” multiplication. As a general rule:<br />
The number of constraints in a Rank One Constraint system should be equal the number of non-constant multiplications.</p>
<h2>Example 5: A large constraint</h2>
<p>Let’s do something less trivial that incorporates everything learned above<br />
Suppose we have the following constraint:<br />
$$</p>
<p>z = 3x^2y + 5xy – x – 2y + 3</p>
<p>$$<br />
We will break it up as follows:<br />
$$</p>
<p>begin{align<em>}<br />
v_1 &amp;= 3xx<br />
v_2 &amp;= v_1y<br />
-v_2 + x + 2y – 3 + z &amp;= 5xy<br />
end{align</em>}</p>
<p>$$<br />
Note how all the addition terms have been moved to the left (this is what we did in the addition example, but it is more apparent here).<br />
Leaving the right hand side as $5xy$ in the third row is arbitrary. We could divide both sides by 5 and have the final constraint be<br />
$$</p>
<p>frac{-v_2}{5} + frac{x}{5} + frac{2y}{5} – frac{3}{5} + frac{z}{5}= xy</p>
<p>$$<br />
This doesn’t change the witness however, so both are valid. Since everything is done in a finite field, this operation is multiplying the left-hand-side and the right-hand-side by the multiplicative inverse of 5.<br />
Our witness vector will be of the form<br />
$$[1, z, x, y, v_1, v_2]$$<br />
And our matrices will have three rows, since we have three constraints:<br />
$$</p>
<p>begin{align<em>}<br />
color{red}{v_1} &amp;= color{green}{3x}color{violet}{x}<br />
color{red}{v_2} &amp;= color{green}{v_1}color{violet}{y}<br />
color{red}{-v_2 + x + 2y – 3 + z} &amp;= color{green}{5x}color{violet}{y}<br />
end{align</em>}</p>
<p>$$<br />
We’ve marked the output $\mathbf{O}$ in red, the left hand side $\mathbf{L}$ in green, and the right hand side $\mathbf{R}$ in violet. This produces the following matrices:<br />
$$</p>
<p>L = begin{bmatrix}<br />
0 &amp; 0 &amp; textcolor{green}{3} &amp; 0 &amp; 0 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; textcolor{green}{1} &amp; 0<br />
0 &amp; 0 &amp; textcolor{green}{5} &amp; 0 &amp; 0 &amp; 0<br />
end{bmatrix}</p>
<p>$$<br />
$$</p>
<p>R = begin{bmatrix}<br />
0 &amp; 0 &amp; textcolor{violet}{1} &amp; 0 &amp; 0 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; textcolor{violet}{1} &amp; 0 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; textcolor{violet}{1} &amp; 0 &amp; 0<br />
end{bmatrix}</p>
<p>$$<br />
$$</p>
<p>O = begin{bmatrix}</p>
<p>0 &amp; 0 &amp; 0 &amp; 0 &amp; textcolor{red}{1} &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; textcolor{red}{1}<br />
textcolor{red}{-3} &amp; textcolor{red}{1} &amp; textcolor{red}{1} &amp; textcolor{red}{2} &amp; 0 &amp; textcolor{red}{-1}<br />
end{bmatrix}</p>
<p>$$<br />
with column labels<br />
$$\begin{bmatrix}1 &amp; \text{out} &amp; x &amp; y &amp; v_1 &amp; v_2\end{bmatrix}$$<br />
Let’s check our work as usual.<br />
“`python<br />
import numpy as np<br />
import random</p>
<h1>Define the matrices</h1>
<p>L = np.array([[0,0,3,0,0,0],<br />
[0,0,0,0,1,0],<br />
[0,0,5,0,0,0]])<br />
R = np.array([[0,0,1,0,0,0],<br />
[0,0,0,1,0,0],<br />
[0,0,0,1,0,0]])<br />
O = np.array([[0,0,0,0,1,0],<br />
[0,0,0,0,0,1],<br />
[-3,1,1,2,0,-1]])</p>
<h1>pick random values for x and y</h1>
<p>x = random.randint(1,1000)<br />
y = random.randint(1,1000)</p>
<h1>this is our orignal formula</h1>
<p>out = 3 * x * x * y + 5 * x * y – x – 2 * y + 3 # the witness vector with the intermediate variables inside<br />
v1 = 3*x*x<br />
v2 = v1 * y<br />
w = np.array([1, out, x, y, v1, v2])<br />
result = O.dot(w) == np.multiply(L.dot(w),R.dot(w))<br />
assert result.all(), “result contains an inequality”<br />
“`</p>
<h2>Rank 1 Constraint Systems do not require starting with a single polynomial</h2>
<p>To keep things simple, we’ve been using examples of the form $z = xy + …$ but most realistic arithmetic constraints are going to be a set of arithmetic constraints, not a single one.<br />
For example, suppose we are proving that an array $[x₁, x₂, x₃, x₄]$ is binary and $v$ is less than 16. The set of constraints will be<br />
$$</p>
<p>begin{align<em>}<br />
x₁² &amp;= x₁<br />
x₂² &amp;= x₂<br />
x₃² &amp;= x₃<br />
x₄² &amp;= x₄<br />
v &amp;= 8x₄ + 4x₃ + 2x₂ + x₁<br />
end{align</em>}</p>
<p>$$<br />
To get this into a rank one constraint system, we notice that the final row doesn’t have any multiplication, so we can substitute $x_1$ into the first constraint:<br />
$$</p>
<p>begin{align<em>}<br />
x₁² &amp;= v – 8x₄ – 4x₃ – 2x₂<br />
x₂² &amp;= x₂<br />
x₃² &amp;= x₃<br />
x₄² &amp;= x₄<br />
end{align</em>}</p>
<p>$$<br />
Assuming our witness vector is $[1, v, x_1, x_2, x_3, x_4]$, we can create the R1CS as follows:<br />
$$</p>
<p>begin{align<em>}<br />
mathbf{L} &amp;= begin{bmatrix}<br />
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1<br />
end{bmatrix}<br />
mathbf{R} &amp;= begin{bmatrix}<br />
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1<br />
end{bmatrix}<br />
mathbf{O} &amp;= begin{bmatrix}<br />
0 &amp; 1 &amp; 0 &amp; -2 &amp; -4 &amp; -8<br />
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1<br />
end{bmatrix}<br />
end{align</em>}</p>
<p>$$<br />
Doing the substitution is not strictly necessary, but it saves a row in the R1CS. In a later section, we will show a valid R1CS where we do not do the substitution.</p>
<h2>Everything is done modulo prime in r1cs</h2>
<p>In the above examples, we used traditional arithmetic for the sake of simplicity, but real world implementations use modular arithmetic instead.<br />
The reason is simple: encoding numbers like 2/3 leads to ill-behaved floats which are computationally intensive and error prone.<br />
If we do all our math modulo a prime number, let’s say 23, then encoding $2/3$ is straightforward. It’s the same as $2 \cdot 3^{-1}$, and multiplying by two and raising to the power of negative 1 are straightforward in modular arithmetic</p>
<h2>Circom implementation.</h2>
<p>In Circom, a language for constructing Rank 1 Constraint Systems, the finite field uses the prime number $21888242871839275222246405745257275088548364400416034343698204186575808495617$ (this is equal to the order of the BN128 curve we discussed in <a href="https://www.rareskills.io/post/elliptic-curves-finite-fields">Elliptic Curves over Finite Fields</a>).<br />
This means $-1$ in that representation is<br />
“`python<br />
p = 21888242871839275222246405745257275088548364400416034343698204186575808495617</p>
<h1>1 – 2 = -1</h1>
<p>(1 – 2) % p</p>
<h1>21888242871839275222246405745257275088548364400416034343698204186575808495616</h1>
<p>“`</p>
<h3>Circom for out = x * y</h3>
<p>If we write <code style='font-family: Arial'>out = x \* y</code> in Circom, it would look like the following:<br />
“`javascript<br />
pragma circom 2.0.0;<br />
template Multiply2() {<br />
signal input x;<br />
signal input y;<br />
signal output out;<br />
out &lt;== x * y;<br />
}<br />
component main = Multiply2();</p>
<pre style='font-family: Arial'><code style='font-family: Arial'>Let’s turn this into an R1CS file and print the R1CS file
```bash
circom multiply2.circom --r1cs --sym
snarkjs r1cs print multiply2.r1cs
</code></pre>
<p>We get the following output:<br />
<img alt="console result of Circom compilation" src="https://rareskills.io/wp-content/uploads/post-media/rank-1-constraint-system/935a00_ce8574af090e4b4d8465fd45d7dda8ff~mv2.png" /><br />
This looks quite a bit different from our R1CS solution, but it is actually encoding the same information.<br />
Here are the differences in Circom’s implementation:<br />
- Columns with zero value are not printed<br />
- Circom writes $\mathbf{O}\mathbf{a} = \mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}$ as $\mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a} - \mathbf{O}\mathbf{a} = \mathbf{0}$<br />
What about the 21888242871839275222246405745257275088548364400416034343698204186575808495616 which is really -1?<br />
Circom's solution is<br />
$$</p>
<p>begin{align<em>}<br />
A &amp;= begin{bmatrix}<br />
0 &amp; 0 &amp; -1 &amp; 0<br />
end{bmatrix}<br />
B &amp;= begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; 1<br />
end{bmatrix}<br />
C &amp;= begin{bmatrix}<br />
0 &amp; -1 &amp; 0 &amp; 0<br />
end{bmatrix}<br />
end{align</em>}</p>
<p>$$<br />
Even though the negative ones might be unexpected, with the witness vector <code style='font-family: Arial'>[1 out x y]</code>, this is actually consistent with the form $\mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a} – \mathbf{O}\mathbf{a} = \mathbf{0}$. (We will see in a second that Circom did indeed use this column assignment).<br />
You can plug in values for $x$, $y$, and out and see that the equation $\mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a} – \mathbf{O}\mathbf{a} = \mathbf{0}$ holds.<br />
Let’s see Circom’s variable to column assignment. Let’s recompile our circuit with a wasm solver:<br />
“<code style='font-family: Arial'>bash
circom multiply2.circom –r1cs –wasm –sym
cd multiply2\_js/
“</code><br />
We create the <code style='font-family: Arial'>input.json</code> file<br />
“<code style='font-family: Arial'>bash
echo ‘{“x”: “11”, “y”: “9”}’ &gt; input.json
“</code><br />
And compute the witness<br />
“<code style='font-family: Arial'>bash
node generate\_witness.js multiply2.wasm input.json witness.wtns
snarkjs wtns export json witness.wtns witness.json
cat witness.json
“</code><br />
We get the following result:<br />
<img alt="terminal output after computing the witness" src="https://rareskills.io/wp-content/uploads/post-media/rank-1-constraint-system/935a00_6ffb172a2e7649cab8cc6db8cace8de8~mv2.png" /><br />
It is clear that Circom is using the same column layout we have been using: <code style='font-family: Arial'>[1, out, x, y]</code>, as $x$ was set to $11$ and $y$ to $9$ in our <code style='font-family: Arial'>input.json</code>.<br />
If we use Circom’s generated witness (replacing the massive number with -1 for readability), then we see Circom’s R1CS is correct<br />
$$</p>
<p>begin{align<em>}<br />
mathbf{a} &amp;= begin{bmatrix}<br />
1 &amp; 99 &amp; 11 &amp; 9<br />
end{bmatrix}<br />
mathbf{L} &amp;= begin{bmatrix}<br />
0 &amp; 0 &amp; -1 &amp; 0<br />
end{bmatrix} rightarrow mathbf{L}mathbf{a} = -11<br />
mathbf{R} &amp;= begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; 1<br />
end{bmatrix} rightarrow mathbf{R}mathbf{a} = 9<br />
mathbf{O} &amp;= begin{bmatrix}<br />
0 &amp; -1 &amp; 0 &amp; 0<br />
end{bmatrix} rightarrow mathbf{O}mathbf{a} = -99<br />
end{align</em>}</p>
<p>$$<br />
$$</p>
<p>begin{align<em>}<br />
Aw cdot Bw – Cw &amp;= 0<br />
(-11)(9) – (-99) &amp;= 0<br />
-99 + 99 &amp;= 0<br />
end{align</em>}</p>
<p>$$<br />
$\mathbf{L}$ has one coefficient of $-1$ for $x$, $\mathbf{R}$ has one coefficient of $+1$ for $y$, and $\mathbf{O}$ has $-1$ for $\text{out}$. In modular form, this is identical to what the terminal outputted above:<br />
<img alt="terminal output of the R1CS" src="https://rareskills.io/wp-content/uploads/post-media/rank-1-constraint-system/935a00_36651c70d5aa49d89059cbae553be7e9~mv2.png" /></p>
<h3>Checking the rest of our work</h3>
<p>By way of review, the formulas we explored were<br />
$$</p>
<p>begin{align}<br />
z &amp;= x  <em>y<br />
z &amp;= x</em>  y  <em>z</em>  u<br />
z &amp;= x * y + 2<br />
z &amp;= 3x^2 y + 5xy – x – 2y + 3<br />
end{align}</p>
<p>$$<br />
We just did (1) in the section above, for this section we will illustrate the principle that the number of non-constant multiplications is the number of constraints.<br />
The circuit for (2) is:<br />
“`javascript<br />
pragma circom 2.0.8;<br />
template Multiply4() {<br />
signal input x;<br />
signal input y;<br />
signal input z;<br />
signal input u;<br />
signal v1;<br />
signal v2;<br />
signal out;<br />
v1 &lt;== x * y;<br />
v2 &lt;== z * u;<br />
out &lt;== v1 * v2;<br />
}<br />
component main = Multiply4();</p>
<pre style='font-family: Arial'><code style='font-family: Arial'>With everything we’ve discussed so far, the Circom output and the annotations should be self-explanatory.
![annotation of constraint generation for Multiply4()](https://rareskills.io/wp-content/uploads/post-media/rank-1-constraint-system/935a00\_21b46f6f9ffd4a80b1a2a374be0de279~mv2.png)
With that in mind, our other formulas should have constraints as follows:
$$

begin{align}
z &amp;= x  *y &amp;&amp; text{1 constraint}
z &amp;= x*  y  *z*  u &amp;&amp; text{3 constraints}
z &amp;= x \* y + 2 &amp;&amp; text{1 constraint}
z &amp;= 3x^2 y + 5xy – x – 2y + 3 &amp;&amp; text{3 constraints}
end{align}

$$
It is an exercise for the reader to write the Circom circuits and verify the above.
### You do not need a witness to calculate the R1CS
Note that in the Circom code we never supplied the witness before calculating the R1CS. We supplied the witness earlier to make the example less abstract and to make it easy to check our work, but it isn’t necessary. This is important, because if a verifier needed a witness to construct an R1CS, then the prover would have to give the hidden solution away!
When we say “witness” we mean a vector with populated values. The verifier knows the “structure” of the witness, i.e. the variable to column assignments, but doesn’t know the values.
## An R1CS is valid even if it is not optimized
A valid transformation from a polynomial to an R1CS is not unique. You can encode the same problem with more constraints, which is less efficient. Here is an example.
In some R1CS tutorials, the constraints for a formula like
$$z = x² + y$$
is transformed to
$$

begin{align*}
v₁ &amp;= x x
z &amp;= v₁ + y
end{align*}

$$
As we’ve noted, this is not efficient. However, you create a valid R1CS for this using the methodology in this article. We simply add a dummy multiplication like so:
$$

begin{align*}
v₁ &amp;= x x
z &amp;= (v₁ + y)*  1
end{align\*}

$$
Our witness vector is of the form $[1, z, x, y, v1]$ and $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ are defined as follows:
$$

mathbf{L} = begin{bmatrix}
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0
0 &amp; 0 &amp; 0 &amp; 1 &amp; 1
end{bmatrix}

$$
$$

mathbf{R} = begin{bmatrix}
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0
end{bmatrix}

$$
$$

mathbf{O} = begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0
end{bmatrix}

$$
The second row of $\mathbf{L}$ accomplishes the addition, and the multiply by one is accomplished by using the first element of the second row of $\mathbf{R}$.
This is perfectly valid, but the solution has one more row and and one more column than it needs.
## What if there are no multiplications?
What if we want to encode the following circuit?
$$

z = x + y

$$
This is pretty useless in practice, but for the sake of completeness, there can be solved with a dummy multiplication by one.
$$

out = (x + y)\*1

$$
With our typical witness vector layout of $[1, z, x, y]$, we have the following matrices:
$$

begin{align*}
mathbf{L} &amp;= begin{bmatrix}
0 &amp; 0 &amp; 1 &amp; 1
end{bmatrix}
mathbf{R} &amp;= begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0
end{bmatrix}
mathbf{O} &amp;= begin{bmatrix}
0 &amp; 1 &amp; 0 &amp; 0
end{bmatrix}
end{align*}

## Rank One Constraint Systems are for convenience
The original [paper for Groth16](https://eprint.iacr.org/2016/260.pdf) don’t have any reference to the term Rank One Constraint System. A R1CS is handy from an implementation perspective, but from a pure math perspective, it is simply explicitly labeling and grouping the coefficients of different variables. So when you read academic papers on the subject, it is usually missing because it is an implementation detail of a more abstract concept.
## Handy Resources
– This [web tool calculates R1CS](https://asecuritysite.com/zero/go\_r1cs) for a set of constraints (but it only works with one input and output variable).
– [Vitalik’s famous example of x\*\*3 + x + 5 == 35](https://medium.com/@VitalikButerin/quadratic-arithmetic-programs-from-zero-to-hero-f6d558cea649)
– [Zero knowledge blog’s R1CS tutorial](https://www.zeroknowledgeblog.com/index.php/the-pinocchio-protocol/r1cs)
## Learn more with RareSkills
This blog post is taken from learning materials in our [zero knowledge course](https://www.rareskills.io/zk-bootcamp).



&lt;div style='page-break-after: always;'&gt;&lt;/div&gt;



# Building a Zero Knowledge Proof from an R1CS

Source: https://rareskills.io/post/r1cs-zkp



# Building a Zero Knowledge Proof from an R1CS

Given an [arithmetic circuit](https://rareskills.io/post/arithmetic-circuit) encoded as a [Rank 1 Constraint System](https://rareskills.io/post/rank-1-constraint-system), it is possible to create a ZK-proof of having a witness, albeit not a succinct one. This article describes how to accomplish that.

A zero knowledge proof for an R1CS is accomplished by converting the witness vector into [finite field elliptic curve points](https://rareskills.io/post/elliptic-curves-finite-fields) and replacing the Hadamard product with a [bilinear pairing](https://rareskills.io/post/bilinear-pairing) for each row.

Given a Rank 1 Constraint System where each matrix has $n$ rows and $m$ columns, we write it as

$$\mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}=\mathbf{O}\mathbf{a}$$

Where $\mathbf{L}$, $\mathbf{R}$, $\mathbf{O}$ are matrices with $n$ rows and $m$ columns, and $\mathbf{a}$ is the witness vector (containing a satisfying assignment to each of the signals in the arithmetic circuit). The vector $\mathbf{a}$ has 1 column and $m$ rows and $\circ$ is element-wise multiplication (Hadamard product).

In expanded form, it looks like

$$
\left[ \begin{array}{ccc}
l\_{1,1} &amp; \cdots &amp; l\_{1,m} \\
\vdots &amp; \ddots &amp; \vdots \\
l\_{n,1} &amp; \cdots &amp; l\_{n,m}
\end{array} \right]
\left[ \begin{array}{c}
a\_1 \\
\vdots \\
a\_m
\end{array} \right]
\circ
\left[ \begin{array}{ccc}
r\_{1,1} &amp; \cdots &amp; r\_{1,m} \\
\vdots &amp; \ddots &amp; \vdots \\
r\_{n,1} &amp; \cdots &amp; r\_{n,m}
\end{array} \right]
\left[ \begin{array}{c}
a\_1 \\
\vdots \\
a\_m
\end{array} \right]
=
\left[ \begin{array}{ccc}
o\_{1,1} &amp; \cdots &amp; o\_{1,m} \\
\vdots &amp; \ddots &amp; \vdots \\
o\_{n,1} &amp; \cdots &amp; o\_{n,m}
\end{array} \right]
\left[ \begin{array}{c}
a\_1 \\
\vdots \\
a\_m
\end{array} \right]
$$

$$
=
\left[ \begin{array}{ccc}
a\_1 l\_{1,1} + \cdots + a\_m l\_{1,m} \\
\vdots \\
a\_1 l\_{n,1} + \cdots + a\_m l\_{n,m}
\end{array} \right]
\circ
\left[ \begin{array}{ccc}
a\_1 r\_{1,1} + \cdots + a\_m r\_{1,m} \\
\vdots \\
a\_1 r\_{n,1} + \cdots + a\_m r\_{n,m}
\end{array} \right]
=
\left[ \begin{array}{ccc}
a\_1 o\_{1,1} + \cdots + a\_m o\_{1,m} \\
\vdots \\
a\_1 o\_{n,1} + \cdots + a\_m o\_{n,m}
\end{array} \right]
$$

$$
=
\left[ \begin{array}{ccc}
\sum\_{i=1}^m a\_i l\_{1,i} \\
\sum\_{i=1}^m a\_i l\_{2,i} \\
\vdots \\
\sum\_{i=1}^m a\_i l\_{n,i}
\end{array} \right]
\circ
\left[ \begin{array}{ccc}
\sum\_{i=1}^m a\_i r\_{1,i} \\
\sum\_{i=1}^m a\_i r\_{2,i} \\
\vdots \\
\sum\_{i=1}^m a\_i r\_{n,i}
\end{array} \right]
=
\left[ \begin{array}{ccc}
\sum\_{i=1}^m a\_i o\_{1,i} \\
\sum\_{i=1}^m a\_i o\_{2,i} \\
\vdots \\
\sum\_{i=1}^m a\_i o\_{n,i}
\end{array} \right]
$$

$$
=
\begin{array}{ccc}
\sum\_{i=1}^m a\_i l\_{1,i} \sum\_{i=1}^m a\_i r\_{1,i} = \sum\_{i=1}^m a\_i o\_{1,i} \\
\sum\_{i=1}^m a\_i l\_{2,i} \sum\_{i=1}^m a\_i r\_{2,i} = \sum\_{i=1}^m a\_i o\_{2,i} \\
\vdots \\
\sum\_{i=1}^m a\_i l\_{n,i} \sum\_{i=1}^m a\_i r\_{n,i} = \sum\_{i=1}^m a\_i o\_{n,i}
\end{array}
$$

In this setup, we can prove to a verifier that we have a witness vector $\mathbf{a}$ that satisfies the R1CS simply by giving them the vector $\mathbf{a}$, but with the obvious drawback that this is not a zero knowledge proof!

## Zero knowledge proof algorithm for an R1CS.

If we “encrypt” the witness vector by multiplying each entry with $G\_1$ or $G\_2$, the math will still work properly!

To understand this, consider that if we carry out the matrix multiplication

$$
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
\end{bmatrix}
\begin{bmatrix}
4 \\
5
\end{bmatrix}
= \begin{bmatrix}
14 \\
32
\end{bmatrix}
$$

and also

$$
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
\end{bmatrix}
\begin{bmatrix}
4G\_1 \\
5G\_1
\end{bmatrix}
= \begin{bmatrix}
14G\_1 \\
32G\_1
\end{bmatrix}
$$

The discrete logs of the two elliptic curve points in the second matrix multiplication are the same value as the elements of the first matrix multiplication.

In other words, each time we multiply the column vector by a row in the square matrix, we carry out two elliptic curve point multiplications and one elliptic curve addition.

### Notation for elliptic curves

We say $[aG\_1]\_1$ is a $\mathbb{G}\_1$ elliptic curve point created from multiplying the field element $a$ by $G\_1$. We say $[aG\_2]\_2$ is a $\mathbb{G}\_2$ elliptic curve point generated by multiplying $a$ with the generator $G\_2$. Because of the discrete log problem, we cannot extract $a$ given $[aG\_1]\_1$ or $[aG\_2]\_2$. Given a $A \in \mathbb{G}\_1$ and $B \in \mathbb{G}\_2$ point, we say the pairing of the two points is $A\bullet B$.

### Prover steps

Let’s encrypt our $\mathbf{a}$ vector by multiplying each entry with the generator point $G\_1$ to produce the elliptic curve point $[a\_iG\_1]\_1$.

For the matrix $\mathbf{L}$, we are doing the following:

$$
\left[ \begin{array}{ccc}
l\_{1,1} &amp; \cdots &amp; l\_{1,m} \\
\vdots &amp; \ddots &amp; \vdots \\
l\_{n,1} &amp; \cdots &amp; l\_{n,m}
\end{array} \right]
\left[ \begin{array}{c}
[a\_1 G\_1]\_1 \\
\vdots \\
[a\_m G\_1]\_1
\end{array} \right]
=
\left[ \begin{array}{ccc}
l\_{1,1}[a\_1 G\_1]\_1 &amp; + \cdots + &amp; l\_{1,m}[a\_m G\_1]\_1 \\
\vdots &amp; \ddots &amp; \vdots \\
l\_{n,1}[a\_1 G\_1]\_1 &amp; + \cdots + &amp; l\_{n,m}[a\_m G\_1]\_1
\end{array} \right]
=
\left[ \begin{array}{c}
\sum\_{i=1}^m l\_{1,i}[a\_i G\_1]\_1 \\
\sum\_{i=1}^m l\_{2,i}[a\_i G\_1]\_1 \\
\vdots \\
\sum\_{i=1}^m l\_{n,i}[a\_i G\_1]\_1
\end{array} \right]
$$

In anticipation of the Hadamard product becoming a list of elliptic curve pairings, we can use $G\_2$ points to also encrypt the $\mathbf{a}$ vector so the verifier can do the pairing.

$$
\left[ \begin{array}{ccc}
r\_{1,1} &amp; \cdots &amp; r\_{1,m} \\
\vdots &amp; \ddots &amp; \vdots \\
r\_{n,1} &amp; \cdots &amp; r\_{n,m}
\end{array} \right]
\left[ \begin{array}{c}
[a\_1 G\_2]\_2 \\
\vdots \\
[a\_m G\_2]\_2
\end{array} \right]
=
\left[ \begin{array}{ccc}
r\_{1,1}[a\_1 G\_2]\_2 &amp; + \cdots + &amp; r\_{1,m}[a\_m G\_2]\_2 \\
\vdots &amp; \ddots &amp; \vdots \\
r\_{n,1}[a\_1 G\_2]\_2 &amp; + \cdots + &amp; r\_{n,m}[a\_m G\_2]\_2
\end{array} \right]
=
\left[ \begin{array}{c}
\sum\_{i=1}^m r\_{1,i}[a\_i G\_2]\_2 \\
\sum\_{i=1}^m r\_{2,i}[a\_i G\_2]\_2 \\
\vdots \\
\sum\_{i=1}^m r\_{n,i}[a\_i G\_2]\_2
\end{array} \right]
$$

After this operation, we have a single column of elliptic curve points in $G\_1$ originating from the multiplication $\mathbf{L}\mathbf{a}$ and a single column of $G\_2$ points from $\mathbf{R}\mathbf{a}$.

The naive next step would be to encrypt the $\mathbf{a}$ vector with $G\_{12}$ points so that the verifier can pair the result of $\mathbf{L}\mathbf{a}$ with $\mathbf{R}\mathbf{a}$ to see if it equals $\mathbf{O}\mathbf{a}$, but $G\_{12}$ points are massive, so we would rather have the verifier pair the $\mathbf{O}\mathbf{a}$ elliptic curve points in $G\_1$ then pair each entry with a $G\_2$ point. Pairing with a $G\_2$ point is, in a sense, “multiplying by one” but turning the $G\_1$ point into a $G\_{12}$ point.

The prover then hands the $G\_1$ vector and the $G\_2$ vector to the verifier.

### Verification step

Thus, the verification step becomes

$$
\left[ \begin{array}{c}
\sum\_{i=1}^m l\_{i,1}[a\_i G\_1]\_1 \\
\sum\_{i=1}^m l\_{i,1}[a\_i G\_1]\_1 \\
\vdots \\
\sum\_{i=1}^m l\_{i,1}[a\_i G\_1]\_1
\end{array} \right]
\begin{matrix}
\bullet \\
\bullet \\
\vdots \\
\bullet
\end{matrix}
\left[ \begin{array}{c}
\sum\_{i=1}^m r\_{i,1}[a\_i G\_2]\_2 \\
\sum\_{i=1}^m r\_{i,1}[a\_i G\_2]\_2 \\
\vdots \\
\sum\_{i=1}^m r\_{i,1}[a\_i G\_2]\_2
\end{array} \right]
\stackrel{?}{=}
\left[ \begin{array}{c}
\sum\_{i=1}^m o\_{i,1}[a\_i G\_1]\_1 \\
\sum\_{i=1}^m o\_{i,1}[a\_i G\_1]\_1 \\
\vdots \\
\sum\_{i=1}^m o\_{i,1}[a\_i G\_1]\_1
\end{array} \right]
\begin{matrix}
\bullet \\
\bullet \\
\vdots \\
\bullet
\end{matrix}
\left[ \begin{array}{c}
G\_2 \\
G\_2 \\
\vdots \\
G\_2
\end{array} \right]
$$

$$
=
\begin{array}{c}
\sum\_{i=1}^m l\_{i,1}[a\_i G\_1]\_1\bullet \sum\_{i=1}^m r\_{i,1}[a\_i G\_2]\_2 \\
\sum\_{i=1}^m l\_{i,2}[a\_i G\_1]\_1\bullet \sum\_{i=1}^m r\_{i,2}[a\_i G\_2]\_2 \\
\vdots \\
\sum\_{i=1}^m l\_{i,n}[a\_i G\_1]\_1\bullet \sum\_{i=1}^m r\_{i,n}[a\_i G\_2]\_2
\end{array}
\stackrel{?}{=}
\begin{array}{c}
\sum\_{i=1}^m o\_{i,1}[a\_i G\_1]\_1\bullet G\_2 \\
\sum\_{i=1}^m o\_{i,2}[a\_i G\_1]\_1\bullet G\_2 \\
\vdots \\
\sum\_{i=1}^m o\_{i,n}[a\_i G\_1]\_1 \bullet G\_2
\end{array}
$$

The above vectors of $G\_{12}$ elements will be element-wise equal if and only if the prover has provided a valid witness.

Well, almost. We’ll get to that in a following section.

First we need to mention an important implementation detail

### Public inputs

If our knowledge claim is “I know $x$ such that $x³ + 5x + 5 = y$ where $y = 155$”, then our witness vector will probably look like the following:

$$[1, y, x, v]$$

where $v = x^2$. In this case, we need $[1, y]$ to be public. To accomplish that, we simply don’t encrypt the first two elements of the witness. The verifier will check the public outputs, then encrypt the public inputs by multiplying them with a $G\_1$ or $G\_2$ point so that the verification formula does not change.

### Dealing with a malicious prover.

Because the vectors are encrypted, the verifier cannot immediately know if the vector of $\mathbb{G}₁$ points encrypts the same values as the vector of $\mathbb{G}₂$ points.

That is, the prover is supplying $\mathbf{a}G\_1$ and $\mathbf{a}G\_2$. Since the verifier doesn’t know the discrete logs of the vector of points, how does the verifier know that the vector of $\mathbb{G}₁$ points has the same discrete logs as the vector of $\mathbb{G}₂$ points?

The verifier can check for the equality of the discrete logs (without knowing them) by pairing both vectors of points with a vector of the *opposite* generator and seeing that the resulting $\mathbb{G}\_{12}$ points are equal. Specifically,

$$
\begin{bmatrix}
a\_1G\_1 \\
a\_2G\_1 \\
\vdots \\
a\_mG\_1
\end{bmatrix}
\begin{matrix}
\bullet \\
\bullet \\
\vdots \\
\bullet
\end{matrix}
\begin{bmatrix}
G\_2 \\
G\_2 \\
\vdots \\
G\_2
\end{bmatrix}
\stackrel{?}{=}
\begin{bmatrix}
a\_1G\_2 \\
a\_2G\_2 \\
\vdots \\
a\_mG\_2
\end{bmatrix}
\begin{matrix}
\bullet \\
\bullet \\
\vdots \\
\bullet
\end{matrix}
\begin{bmatrix}
G\_1 \\
G\_1 \\
\vdots \\
G\_1
\end{bmatrix}
$$

## This algorithm is mostly academic

This algorithm is very inefficient for the verifier. If the matrices in the R1CS are large (and for interesting algorithms, they will be), then the verifier has a lot of pairings and elliptic curve additions to do. Elliptic curve addition is rather fast, but elliptic curve pairings are slow (and cost a lot of gas on Ethereum).

However, it is nice to see that at this stage, zero knowledge proofs are possible, and if you have a good grasp of elliptic curve operations (and haven’t forgotten your matrix arithmetic), they aren’t hard to understand.

### Making this technique truly zero knowledge

As it is right now, our witness vector cannot be decrypted, however it can be guessed. If an attacker (someone trying to discover the unencrypted witness) uses some auxiliary information to make an educated guess at the witness, they can check their work by multiplying their guessed witness vector by the elliptic curve point generators and seeing if the result is the same as the prover’s witness vectors.

We will learn how to defend against witness guessing in our coverage of [Groth16](https://rareskills.io/post/groth16).

Also, keep in mind nobody does the described algorithm in the real world, as it is too inefficient. However, if you implement it, it will help you practice implementing meaningful elliptic curve arithmetic and build a functional end-to-end (almost) zero knowledge proof.

You can see an example implementation of algorithm described here by Obront in [this repo](https://github.com/zobront/homerolled-zk).

## Learn more with RareSkills

This material is from our [Zero Knowledge Course](https://rareskills.io/zk-bootcamp).

*Originally Published August 26, 2023*



&lt;div style='page-break-after: always;'&gt;&lt;/div&gt;



# Lagrange Interpolation with Python

Source: https://rareskills.io/post/python-lagrange-interpolation



# Lagrange Interpolation with Python

Lagrange interpolation is a technique for computing a polynomial that passes through a set of $n$ points.

## Interpolating a vector as a polynomial

### Examples

#### A straight line through two points

Consider that if we have two points, they can be interpolated with a line. For example, given $(1, 1)$ and $(2, 2)$, we can draw a line that intersects both points, it would be a degree $1$ polynomial $y = x$.

#### A single point

Now consider that if we have one point, we can draw a line through that point with a degree 0 polynomial. For example, if the point is $(3, 5)$ we can draw a line through it $y = 5$ (which is a degree $0$ polynomial).

#### Three points and a parabola

The pattern that we can “draw a polynomial through” $n$ points with a (at most) degree $n – 1$ polynomial holds for any number of points. For example, the points $(0, 0), (1, 1), (2, 4)$ can be interpolated with $y = x^2$. If those points happened to be a straight line, e.g. $(0, 0), (1, 1), (2, 2)$, then we could draw a line through $(1, 1)$ and $(2, 2)$ with a degree 1 polynomial $y = x$, but in general, three points won’t be collinear, so we’ll need a degree 2 polynomial to cross all the points.

## Python code for Lagrange interpolation

For our purposes it isn’t important to understand how to compute this polynomial, as there are math libraries that will do it for us. The most common algorithm is *Lagrange interpolation* and we show how to do that in Python.

#### Float example

We can compute a polynomial $p(x)$ that crosses through the points $(1,4), (2,8), (3,2), (4,1)$ using Lagrange interpolation.

```solidity
from scipy.interpolate import lagrange
x_values = [1, 2, 3, 4]
y_values = [4, 8, 2, 1]

print(lagrange(x_values, y_values))
#      3      2
# 2.5 x - 20 x + 46.5 x - 25
</code></pre>
<h4>Finite field example</h4>
<p>Let’s use the same polynomial as before, but this time we’ll use a finite field $\mathbb{F}_{17}$ instead of floating point numbers.</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
import numpy as np
GF17 = galois.GF(17)

xs = GF17(np.array([1,2,3,4]))
ys = GF17(np.array([4,8,2,1]))

p = galois.lagrange_poly(xs, ys)

assert p(1) == GF17(4)
assert p(2) == GF17(8)
assert p(3) == GF17(2)
assert p(4) == GF17(1)
</code></pre>
<h3>Uniqueness of the interpolating polynomial</h3>
<p>Going back to our example of the points $(1, 1), (2, 2)$, the lowest degree polynomial that interpolates them is $y = x$. In general,</p>
<p><strong>For a set of $n$ points, there is a unique lowest-degree polynomial of at most degree $n – 1$ that interpolates them.</strong></p>
<p>The polynomial of lowest degree that interpolates the points is sometimes called the <em>Lagrange polynomial</em>.</p>
<p>The consequence of this is that</p>
<p><strong>If we use the points $(1,2,…,n)$ as the $x$ values to convert a length $n$ vector to a polynomial via Lagrange interpolation, then the resulting polynomial is unique.</strong></p>
<p>In other words, given a consistent basis of x-values to interpolate a vector over, there is a unique polynomial that interpolates a given vector. Spoken another way, every length $n$ vector has a unique polynomial representation.</p>
<p>Informally, every $n$ degree vector has a unique $n – 1$ degree polynomial that “represents” it. The degree could be less if, for example, the points are collinear, but the vector will be unique.</p>
<p>The “lowest degree” part is important. Given two points, there are an extremely large number of polynomials that cross those two points — but the lowest degree polynomial is unique.</p>
<div style='page-break-after: always;'></div>

<h1>The Schwartz-Zippel Lemma and its application to Zero Knowledge Proofs</h1>
<p>Source: https://rareskills.io/post/schwartz-zippel-lemma</p>
<h1>The Schwartz-Zippel Lemma and its application to Zero Knowledge Proofs</h1>
<p>Nearly all ZK-Proof algorithms rely on the Schwartz-Zippel Lemma to achieve succintness.</p>
<p>The Schwartz-Zippel Lemma states that if we are given two polynomials $p(x)$ and $q(x)$ with degrees $d_p$ and $d_q$ respectively, and if $p(x) \neq q(x)$, then the number of points where $p(x)$ and $q(x)$ intersect is less than or equal to $\mathsf{max}(d_p, d_q)$.</p>
<p>Let’s consider a few examples.</p>
<h2>Example polynomials and the Schwartz-Zippel Lemma</h2>
<h3>A straight line crossing a parabola</h3>
<p>Consider the polynomial $p(x) = x$ and $q(x) = x^2$. They intersect at $x = 0$ and $x = 1$.<br />
<img alt="Plot of y = x and y = x^2" src="assets/schwartz-zippel-x-x2-example.png" /></p>
<p>They intersect at two points, which is the maximum degree between the polynomials $y = x$ and $y = x^2$.</p>
<h3>A degree three polynomial and a degree one polynomial</h3>
<p>Consider the polynomials $p(x) = x^3$ and $q(x) = x$. The polynomials intersect at $x = -1$, $x = 0$, and $x = 1$ and nowhere else. The number of intersections is bounded by the maximum degree of the polynomials, which in this case is 3.</p>
<p><img alt="Plot of y = x^3 and y = x" src="assets/schwartz-zippel-x-x3-example.png" /></p>
<h2>Polynomials in finite fields and the Schwartz-Zippel Lemma</h2>
<p>The Schwartz-Zippel Lemma holds for polynomials in <a href="https://rareskills.io/post/finite-fields">finite fields</a> (i.e., all computations are done modulo a prime $p$).</p>
<h2>Polynomial equality testing</h2>
<p>We can test that two polynomials are equal by checking if all their coefficients are equal, but this takes $\mathcal{O}(d)$ time, where $d$ is the degree of the polynomial.</p>
<p>If instead we can evaluate the polynomials at a random point $u$ and compare the evaluations in $\mathcal{O}(1)$ time.</p>
<p>That is, in a finite field $\mathbb{F}_{p}$, we pick a random value $u$ from $[0,p)$. Then we evaluate $y_f=f(u)$ and $y_g=g(u)$. If $y_f = y_g$, then one of two things must be true:</p>
<ol>
<li>$f(x) = g(x)$</li>
<li>$f(x) \neq g(x)$ and we picked one of the $d$ points where they intersect where $d = \mathsf{max}(\deg(f), \deg(g))$</li>
</ol>
<p>If $d \ll p$, then situation 2 is unlikely to the point of being negligible.</p>
<p>For example, if the field $\mathbb{F}_{p}$ has $p \approx 2^{254}$ (a little smaller than a <a href="https://rareskills.io/post/uint-max-value-solidity">uint256</a>), and if the polynomials are not more than one million degree large, then the probability of picking a point where they intersect is</p>
<p>$$<br />
\frac{1\times 10^6}{2^{254}} \approx \frac{2^{20}}{2^{254}} \approx \frac{1}{2^{234}} \approx \frac{1}{10^{70}}<br />
$$</p>
<p>To put a sense of scale on that, the number of atoms in the universe is about $10^{78}$ to $10^{82}$, so it is extremely unlikely that we will pick a point where the polynomials intersect, if the polynomials are not equal.</p>
<h2>Using the Schwartz-Zippel Lemma to test if two vectors are equal</h2>
<p>We can combine Lagrange interpolation with the Schwartz-Zippel Lemma to test if two vectors are equal.</p>
<p>Normally, we would test vector equality by comparing if each of the $n$ components of the vectors are equal.</p>
<p>Instead, if we use a common set of $x$ values (say $[1,2,..,n]$) to interpolate the vectors:</p>
<ol>
<li>We can interpolate a polynomial for each vector $f(x)$ and $g(x)$</li>
<li>Pick a random point $u$</li>
<li>Evaluate the polynomials at $u$</li>
<li>Check if $f(u) = g(u)$</li>
</ol>
<p>Although computing the polynomials is more work, the final check is much cheaper.</p>
<p>Here is an example of carrying out this computation in Python:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
import numpy as np

p = 103
GF = galois.GF(p)

xs = GF(np.array([1,2,3]))

# arbitrary vectors
v1 =  GF(np.array([4,8,19]))
v2 =  GF(np.array([4,8,19]))


def L(v):
    return galois.lagrange_poly(xs, v)

p1 = L(v1)
p2 = L(v2)

import random
u = random.randint(0, p)

lhs = p1(u)
rhs = p2(u)

# only one check required
assert lhs == rhs
</code></pre>
<h2>Using the Schwartz-Zippel Lemma in ZK Proofs</h2>
<p>Our end goal is for the prover to send a small string of data to the verifier that the verifier can quickly check.</p>
<p>Most of the time, a ZK proof is essentially a polynomial evaluated at a random point.</p>
<p>The difficulty we have to solve is that we don’t know if the polynomial is evaluated <em>honestly</em> — somehow we have to trust the prover isn’t lying when they evaluate $f(u)$.</p>
<p>But before we get to that, we need to learn how to represent an entire arithmetic circuit as a small set of polynomials evaluated at a random point, which is the motivation for <a href="https://rareskills.io/post/quadratic-arithmetic-program">Quadratic Arithmetic Programs</a>.</p>
<div style='page-break-after: always;'></div>

<h1>Quadratic Arithmetic Programs</h1>
<p>Source: https://rareskills.io/post/quadratic-arithmetic-program</p>
<h1>Quadratic Arithmetic Programs</h1>
<p>A quadratic arithmetic program is an <a href="https://www.rareskills.io/post/arithmetic-circuit">arithmetic circuit</a>, specifically a <a href="https://www.rareskills.io/post/rank-1-constraint-system">Rank 1 Constraint System</a> (R1CS) represented as a set of polynomials. It is derived using Lagrange interpolation on a Rank 1 Constraint System. Unlike an R1CS, a Quadratic Arithmetic Program (QAP) can be tested for equality in $\mathcal{O}(1)$ time via the Schwartz-Zippel Lemma.</p>
<h2>Key ideas</h2>
<p>In the chapter on the Schwartz-Zippel Lemma, we saw that we can test if two vectors are equal in $\mathcal{O}(1)$ time by converting them to polynomials, then running the Schwartz-Zippel Lemma test on the polynomials. (To clarify, the <em>test</em> is $\mathcal{O}(1)$ time, converting the vectors to polynomials creates overhead).</p>
<p>Because a Rank 1 Constraint System is entirely composed of vector operations, we aim to test if</p>
<p>$$\mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}\stackrel{?}{=}\mathbf{O}\mathbf{a}$$</p>
<p>holds in $\mathcal{O}(1)$ time instead of $\mathcal{O}(n)$ time (where $n$ is the number of rows in $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$).</p>
<p>But before we do that, we need to understand some key properties of the relationship between vectors and polynomials that represent them.</p>
<p>For all math here, we assume we are working in a <a href="https://www.rareskills.io/post/finite-fields">finite field</a>, but we skip the $\mod p$ notation for succinctness.</p>
<h2>Homomorphisms between vector addition and polynomial addition</h2>
<h3>Vector addition is homomorphic to polynomial addition</h3>
<p>If we take two vectors, interpolate them with polynomials, then add the polynomials together, we get the same polynomial as if we added the vectors together and then interpolated the sum vector.</p>
<p>Spoken more mathematically, let $\mathcal{L}(\mathbf{v})$ be the polynomial resulting from Lagrange interpolation on the vector $\mathbf{v}$ using $(1, 2, …, n)$ as the $x$ values, where $n$ is the length of $\mathbf{v}$. The following is true:</p>
<p>$$\mathcal{L}(\mathbf{v} + \mathbf{w}) = \mathcal{L}(\mathbf{v}) + \mathcal{L}(\mathbf{w})$$</p>
<p>In other words, the polynomials resulting from interpolating vectors $\mathbf{v}$ and $\mathbf{w}$ are the same as the polynomials resulting from interpolating the vectors $\mathbf{v} + \mathbf{w}$.</p>
<h4>Worked example</h4>
<p>Let $f_1(x) = x^2$ and $f_2(x) = x^3$ $f_1$ interpolates $(1, 1), (2, 4), (3, 9)$ or the vector $[1,4,9]$ and $f_2$ interpolates $[1,8,27]$.</p>
<p>The sum of the vectors is $[2,12,36]$ and it is clear that $x^3 + x^2$ interpolates that. Let $f_3(x) = f_1(x) + f_2(x) = x^3 + x^2$.</p>
<p>$$<br />
\begin{align*}<br />
f_3(1) &amp;= 1 + 1 = 2\<br />
f_3(2) &amp;= 8 + 4 = 12\<br />
f_3(3) &amp;= 27 + 9 = 36<br />
\end{align*}$$</p>
<h4>Testing the math in Python</h4>
<p>Unit testing a proposed mathematical identity doesn’t make it true, but it does illustrate what is happening. The reader is encouraged to try out a few different vectors to see that the identity holds.</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
import numpy as np

p = 17
GF = galois.GF(p)

xs = GF(np.array([1,2,3]))

# two arbitrary vectors
v1 =  GF(np.array([4,8,2])) 
v2 =  GF(np.array([1,6,12]))

def L(v):
    return galois.lagrange_poly(xs, v)

assert L(v1 + v2) == L(v1) + L(v2)
</code></pre>
<h3>Scalar multiplication</h3>
<p>Let $\lambda$ be a scalar (specifically, a field element in finite field). Then</p>
<p>$$\mathcal{L}(\lambda \mathbf{v}) = \lambda \mathcal{L}(\mathbf{v})$$</p>
<h4>Worked example</h4>
<p>Suppose our 3 points are $[3, 6, 11]$. The polynomial that interpolates that is $f(x) = x^2 + 2$. If we multiply the vector by 3 we get $[9, 18, 33]$. The polynomial that interpolates that is</p>
<pre style='font-family: Arial'><code class="language-solidity">from scipy.interpolate import lagrange

x_values = [1, 2, 3]
y_values = [9, 18, 33]

print(lagrange(x_values, y_values))

#    2
# 3 x + 6
</code></pre>
<p>$3x^2 + 6$, which equals $3 \cdot (x^2 + 2)$.</p>
<h4>Worked example in code</h4>
<pre style='font-family: Arial'><code class="language-solidity">import galois
import numpy as np

p = 17
GF = galois.GF(p)

xs = GF(np.array([1,2,3]))

# arbitrary vector
v =  GF(np.array([4,8,2]))

# arbitrary constant
lambda_ =  GF(15)

def L(v):
    return galois.lagrange_poly(xs, v)

assert L(lambda_ * v) == lambda_ * L(v)
</code></pre>
<h3>Scalar multiplication is really vector addition</h3>
<p>When we say "multiply a vector by 3" we are really saying "add the vector to itself three times". Since we are only working in finite fields, we don’t concern ourselves with the interpretation of scalars such as "0.5"</p>
<p>We can think of both vectors under element-wise addition (in a finite field) and polynomials under addition (also in a finite field) as <a href="https://www.rareskills.io/post/group-theory">groups</a>.</p>
<p>The most important takeaway from this chapter is</p>
<p><strong>The group of vectors under addition in a finite field is homomorphic to the group of polynomials under addition in a finite field.</strong></p>
<p>This is critical because <strong>vector equality testing takes $\mathcal{O}(n)$ time, but polynomial equality testing takes $\mathcal{O}(1)$ time.</strong></p>
<p>Therefore, whereas testing R1CS equality took $\mathcal{O}(n)$ time, we can leverage this homomorphism to test the equality of R1CSs in $\mathcal{O}(1)$ time.</p>
<p>This is what a <em>Quadratic Arithmetic Program</em> is.</p>
<h2>A Rank 1 Constraint System in Polynomials</h2>
<p>Consider that matrix multiplication between a rectangular matrix and a vector can be written in terms of vector addition and scalar multiplication.</p>
<p>For example, if we have a $3 \times 4$ matrix $A$ and a 4 dimensional vector $v$, then we can write the matrix multiplication as</p>
<p>$$<br />
\mathbf{A} \cdot \mathbf{v} = \begin{bmatrix}<br />
a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14}\<br />
a_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24}\<br />
a_{31} &amp; a_{32} &amp; a_{33} &amp; a_{34}<br />
\end{bmatrix}<br />
\begin{bmatrix}<br />
v_1\<br />
v_2\<br />
v_3\<br />
v_4<br />
\end{bmatrix}$$</p>
<p>We typically think of the vector $v$ "flipping" and doing an inner product (generalized dot product) with each of the rows, i.e.</p>
<p>$$<br />
\mathbf{A}\cdot \mathbf{v} =<br />
\begin{bmatrix}<br />
a_{11}\cdot v_1 + a_{12}\cdot v_2 + a_{13}\cdot v_3 + a_{14}\cdot v_4\<br />
a_{21}\cdot v_1 + a_{22}\cdot v_2 + a_{23}\cdot v_3 + a_{24}\cdot v_4\<br />
a_{31}\cdot v_1 + a_{32}\cdot v_2 + a_{33}\cdot v_3 + a_{34}\cdot v_4<br />
\end{bmatrix}$$</p>
<p>However, we could instead think of splitting matrix $A$ into a bunch of vectors as follows:</p>
<p>$$<br />
\mathbf{A} = \begin{bmatrix}<br />
a_{11} \<br />
a_{21} \<br />
a_{31}<br />
\end{bmatrix}<br />
,<br />
\begin{bmatrix}<br />
a_{12} \<br />
a_{22} \<br />
a_{32}<br />
\end{bmatrix}<br />
,<br />
\begin{bmatrix}<br />
a_{13} \<br />
a_{23} \<br />
a_{33}<br />
\end{bmatrix}<br />
,<br />
\begin{bmatrix}<br />
a_{14} \<br />
a_{24} \<br />
a_{34}<br />
\end{bmatrix}$$</p>
<p>and multiplying each vector by a scalar from the vector $\mathbf{v}$:</p>
<p>$$<br />
\mathbf{A}\cdot \mathbf{v} = \begin{bmatrix}<br />
a_{11} \<br />
a_{21} \<br />
a_{31}<br />
\end{bmatrix}\cdot v_1<br />
+<br />
\begin{bmatrix}<br />
a_{12} \<br />
a_{22} \<br />
a_{32}<br />
\end{bmatrix}\cdot v_2<br />
+<br />
\begin{bmatrix}<br />
a_{13} \<br />
a_{23} \<br />
a_{33}<br />
\end{bmatrix}\cdot v_3<br />
+<br />
\begin{bmatrix}<br />
a_{14} \<br />
a_{24} \<br />
a_{34}<br />
\end{bmatrix}\cdot v_4$$</p>
<p>We have expressed matrix multiplication between $\mathbf{A}$ and $\mathbf{v}$ purely in terms of vector addition and scalar multiplication.</p>
<p>Because we established earlier that the group of vectors under addition in a finite field is homomorphic to the group of polynomials under addition in a finite field, can express the computation above in terms of polynomials that represent the vectors.</p>
<h2>Succintly testing that $\mathbf{A}\mathbf{v}_1 = \mathbf{B}\mathbf{v}_2$</h2>
<p>Suppose we have matrix $\mathbf{A}$ and $\mathbf{B}$ such that</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{A} = \begin{bmatrix}<br />
6 &amp; 3\<br />
4 &amp; 7\<br />
\end{bmatrix}\<br />
\mathbf{B} = \begin{bmatrix}<br />
3 &amp; 9 \<br />
12 &amp; 6\<br />
\end{bmatrix}<br />
\end{align*}$$</p>
<p>and vectors $\mathbf{v}_1$ and $\mathbf{v}_2$</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{v}_1 = \begin{bmatrix}<br />
2 \<br />
4 \<br />
\end{bmatrix}\<br />
\mathbf{v}_2 = \begin{bmatrix}<br />
2 \<br />
2 \<br />
\end{bmatrix}<br />
\end{align*}$$</p>
<p>We want to test if</p>
<p>$$<br />
\mathbf{A}\mathbf{v}_1 = \mathbf{B}\mathbf{v}_2$$</p>
<p>is true.</p>
<p>Obviously we can carry out the matrix arithmetic, but the final check will require $n$ comparisons, where $n$ is the number of rows in $\mathbf{A}$ and $\mathbf{B}$. We want to do it in $\mathcal{O}(1)$ time.</p>
<p>First, we convert the matrix multiplication $\mathbf{A}\mathbf{v}_1$ and $\mathbf{B}\mathbf{v}_2$ to the group of vectors under addition:</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{A} &amp;= \begin{bmatrix}<br />
6 \<br />
4 \<br />
\end{bmatrix}<br />
,<br />
\begin{bmatrix}<br />
3 \<br />
7 \<br />
\end{bmatrix}\<br />
\mathbf{B} &amp;= \begin{bmatrix}<br />
3 \<br />
12 \<br />
\end{bmatrix}<br />
,<br />
\begin{bmatrix}<br />
9 \<br />
6 \<br />
\end{bmatrix}<br />
\end{align*}$$</p>
<p>We now want to find the homomorphic equivalent of</p>
<p>$$<br />
\begin{bmatrix}<br />
6 \<br />
4 \<br />
\end{bmatrix}\cdot 2+<br />
\begin{bmatrix}<br />
3 \<br />
7 \<br />
\end{bmatrix}\cdot 4\stackrel{?}{=}<br />
\begin{bmatrix}<br />
3 \<br />
12 \<br />
\end{bmatrix}\cdot 2+<br />
\begin{bmatrix}<br />
9 \<br />
6 \<br />
\end{bmatrix}\cdot 2$$</p>
<p>in the polynomial group.</p>
<p>Let’s convert each of the vectors to polynomials over the $x$ values $[1,2]$:</p>
<p>$$<br />
\underbrace{<br />
\begin{bmatrix}<br />
6 \<br />
4 \<br />
\end{bmatrix}}_{p_1(x)}\cdot 2+<br />
\underbrace{<br />
\begin{bmatrix}<br />
3 \<br />
7 \<br />
\end{bmatrix}}_{p_2(x)}\cdot 4\stackrel{?}{=}<br />
\underbrace{<br />
\begin{bmatrix}<br />
3 \<br />
12 \<br />
\end{bmatrix}}_{q_1(x)}\cdot 2+<br />
\underbrace{<br />
\begin{bmatrix}<br />
9 \<br />
6 \<br />
\end{bmatrix}}_{q_2(x)}\cdot 2$$</p>
<p>We will invoke some Python to compute the Langrage interpolation:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
import numpy as np

p = 17
GF = galois.GF(p)

x_values = GF(np.array([1, 2]))

def L(v):
    return galois.lagrange_poly(x_values, v)

p1 = L(GF(np.array([6, 4])))
p2 = L(GF(np.array([3, 7])))
q1 = L(GF(np.array([3, 12])))
q2 = L(GF(np.array([9, 6])))

print(p1)
# 15x + 8 (mod 17)
print(p2)
# 4x + 16 (mod 17)
print(q1)
# 9x + 11 (mod 17)
print(q2)
# 14x + 12 (mod 17)
</code></pre>
<p>Finally, we can check if</p>
<p>$$p_1(x) \cdot 2+ p_2(x) \cdot 4 \stackrel{?}= q_1(x) \cdot 2 + q_2(x) \cdot 2$$</p>
<p>is true by invoking the Schwartz-Zippel Lemma:</p>
<pre style='font-family: Arial'><code class="language-solidity">import random
u = random.randint(0, p)
tau = GF(u) # a random point

left_hand_side = p1(tau) * GF(2) + p2(tau) * GF(4)
right_hand_side = q1(tau) * GF(2) + q2(tau) * GF(2)

assert left_hand_side == right_hand_side
</code></pre>
<p><strong>The final assert statement is able to test if $\mathbf{A}\mathbf{v}_1 = \mathbf{B}\mathbf{v}_2$ doing a single comparison instead of $n$.</strong></p>
<h2>R1CS to QAP: Succinctly testing that $\mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}=\mathbf{O}\mathbf{a}$</h2>
<p>Since we know how to test of $\mathbf{A}\mathbf{v}_1 = \mathbf{B}\mathbf{v}_2$ succinctly, can we also test if $\mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a}=\mathbf{O}\mathbf{a}$ succinctly?</p>
<p>The matrices have $m$ columns, so let’s break each of the matrices into $m$ column vectors and interpolate them on $(1, 2, …, n)$ to produce $m$ polynomials each.</p>
<p>Let $u_1(x), u_2(x), …, u_m(x)$ be the polynomials that interpolate the column vectors of $\mathbf{L}$.</p>
<p>Let $v_1(x), v_2(x), …, v_m(x)$ be the polynomials that interpolate the column vectors of $\mathbf{R}$.</p>
<p>Let $w_1(x), w_2(x), …, w_m(x)$ be the polynomials that interpolate the column vectors of $\mathbf{O}$.</p>
<p>Without loss of generality, let’s say we have 4 columns ($m = 4$) and three rows ($n = 3$).</p>
<p>Visually, this can be represented as</p>
<p>$$<br />
\begin{array}{c}<br />
\mathbf{L} = \begin{bmatrix}<br />
\quad l_{11} \quad&amp; l_{12} \quad&amp; l_{13} \quad&amp; l_{14} \quad\<br />
\quad l_{21} \quad&amp; l_{22} \quad&amp; l_{23} \quad&amp; l_{24} \quad\<br />
\quad l_{31} \quad&amp; l_{32} \quad&amp; l_{33} \quad&amp; l_{34} \quad<br />
\end{bmatrix} \<br />
\<br />
\qquad u_1(x) \quad u_2(x) \quad u_3(x) \quad u_4(x)<br />
\end{array}<br />
\begin{array}{c}<br />
\mathbf{R} = \begin{bmatrix}<br />
\quad r_{11} \quad&amp; r_{12} \quad&amp; r_{13} \quad&amp; r_{14} \quad\<br />
\quad r_{21} \quad&amp; r_{22} \quad&amp; r_{23} \quad&amp; r_{24} \quad\<br />
\quad r_{31} \quad&amp; r_{32} \quad&amp; r_{33} \quad&amp; r_{34} \quad<br />
\end{bmatrix} \<br />
\<br />
\qquad v_1(x) \quad v_2(x) \quad v_3(x) \quad v_4(x)<br />
\end{array}$$<br />
$$<br />
\begin{array}{c}<br />
\mathbf{O} = \begin{bmatrix}<br />
\quad o_{11} \quad&amp; o_{12} \quad&amp; o_{13} \quad&amp; o_{14} \quad\<br />
\quad o_{21} \quad&amp; o_{22} \quad&amp; o_{23} \quad&amp; o_{24} \quad\<br />
\quad o_{31} \quad&amp; o_{32} \quad&amp; o_{33} \quad&amp; o_{34} \quad<br />
\end{bmatrix} \<br />
\<br />
\qquad w_1(x) \quad w_2(x) \quad w_3(x) \quad w_4(x)<br />
\end{array}$$</p>
<p>Since multiplying a column vector by a scalar is homomorphic to multiplying a polynomial by a scalar, each the polynomials can be multiplied by the respective element in the witness.</p>
<p>For example,</p>
<p>$$<br />
\mathbf{L}\mathbf{a} = \begin{bmatrix}<br />
\quad l_{11} \quad&amp; l_{12} \quad&amp; l_{13} \quad&amp; l_{14} \quad\<br />
\quad l_{21} \quad&amp; l_{22} \quad&amp; l_{23} \quad&amp; l_{24} \quad\<br />
\quad l_{31} \quad&amp; l_{32} \quad&amp; l_{33} \quad&amp; l_{34} \quad<br />
\end{bmatrix}<br />
\begin{bmatrix}<br />
a_1 \<br />
a_2 \<br />
a_3 \<br />
a_4<br />
\end{bmatrix}$$</p>
<p>becomes</p>
<p>$$<br />
\begin{align*}<br />
&amp;=\begin{bmatrix}<br />
u_1(x) &amp; u_2(x) &amp; u_3(x) &amp; u_4(x)<br />
\end{bmatrix}<br />
\begin{bmatrix}<br />
a_1 \<br />
a_2 \<br />
a_3 \<br />
a_4<br />
\end{bmatrix}\<br />
&amp;=a_1u_1(x) + a_2u_2(x) + a_3u_3(x) + a_4u_4(x)\<br />
&amp;=\sum_{i=1}^4 a_iu_i(x)<br />
\end{align*}$$</p>
<p>Observe that the final result is a single polynomial with degree at most $n – 1$ (since there are $n$ rows in $\mathbf{L}$, $u_1(x), …, u_n(x)$ have degree at most $n – 1$).</p>
<p>In the general case, $\mathbf{L}\mathbf{a}$ can be written as</p>
<p>$$<br />
\sum_{i=1}^m a_iu_i(x)$$</p>
<p>after converting each of the $m$ columns to polynomials.</p>
<p>Using the same steps above, each matrix-witness product in the R1CS $\mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a} = \mathbf{O}\mathbf{a}$ can be transformed as</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{L}\mathbf{a} \rightarrow \sum_{i=1}^m a_iu_i(x) \<br />
\mathbf{R}\mathbf{a} \rightarrow \sum_{i=1}^m a_iv_i(x) \<br />
\mathbf{O}\mathbf{a} \rightarrow \sum_{i=1}^m a_iw_i(x)<br />
\end{align*}$$</p>
<p>Since each of the sum terms produces a single polynomial, we can write them as:</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{L}\mathbf{a} &amp;\rightarrow \sum_{i=1}^m a_iu_i(x) = u(x)\<br />
\mathbf{R}\mathbf{a} &amp;\rightarrow \sum_{i=1}^m a_iv_i(x) = v(x)\<br />
\mathbf{O}\mathbf{a} &amp;\rightarrow \sum_{i=1}^m a_iw_i(x) = w(x)<br />
\end{align*}$$</p>
<h3>Why interpolate all the columns?</h3>
<p>Because of the homomorphisms $\mathcal{L}(\mathbf{v}_1) + \mathcal{L}(\mathbf{v}_2) = \mathcal{L}(\mathbf{v}_1 + \mathbf{v}_2)$ and $\mathcal{L}(\lambda \mathbf{v}) = \lambda \mathcal{L}(\mathbf{v})$, if we compute $u(x)$ as $\mathcal{L}(\mathbf{L}\mathbf{a})$ we get the same result as applying Lagrange interpolation to the columns of $\mathbf{L}$ and then multiplying each of the polynomials by the respective element in $\mathbf{a}$ and summing the result.</p>
<p>Spoken another way,</p>
<p>$$<br />
\sum_{i=1}^m a_iu_i(x) = \mathcal{L}(\mathbf{L}\mathbf{a}) \mid u_i(x) \text{ is the Lagrange interpolation of column } i \text{ of } \mathbf{L}$$</p>
<p>So why not just compute a single Lagrange interpolation instead of $m$?</p>
<p>We need to make a distinction between <em>who</em> is using the QAP. The verifier (and the trusted setup which we will cover later) do not know the witness $\mathbf{a}$ and thus cannot compute $\mathcal{L}(\mathbf{L}\mathbf{a})$. This is an optimization the prover can make, but other parties in the ZK protocol cannot make use of that optimization.</p>
<p>All parties involved need to have a common agreement on the QAP — the polynomial interpolations of the matrices before any proofs and verifications are done.</p>
<h3>Polynomial degree imbalance</h3>
<p>However, we can’t simply express the final result as</p>
<p>$$u(x)v(x) = w(x)$$</p>
<p>because the degrees won’t match.</p>
<p>Multiplying two polynomials together results in a product polynomial whose degree is the sum of the degrees of the two polynomials being multiplied together.</p>
<p>Because each of $u(x)$, $v(x)$, and $w(x)$ will have degree $n – 1$, $u(x)v(x)$ will generally have degree $2n – 2$ and $w(x)$ will have degree $n – 1$, so they won’t be equal even though the underlying vectors they multiplied are equal.</p>
<p>This is because the homorphisms we established earlier only make claims about vector addition, not Hadamard product.</p>
<p>However, the vector that $u(x)v(x)$ interpolates, i.e.</p>
<p>$$((1, u(1)v(1)), (2, u(2)v(2)), …, (n, u(n)v(n)))$$</p>
<p>is the same as the vector that $w(x)$ interpolates, i.e.</p>
<p>$$((1, w(1)), \quad (2, w(2)), \quad …, \quad (n, w(n)))$$</p>
<p>In other words</p>
<p>$$((1, u(1)v(1)), (2, u(2)v(2)), …, (n, u(n)v(n))) = ((1, w(1)), (2, w(2)), …, (n, w(n)))$$</p>
<p>Although the "underlying" vectors are equal, the polynomials that interpolate them are not equal.</p>
<h3>Example of underlying equality</h3>
<p>Let’s say that $u(x)$ is the polynomial that interpolates</p>
<p>$$(1,\boxed{2}), (2,\boxed{4}), (3,\boxed{8})$$</p>
<p>and $v(x)$ is the polynomial that interpolates</p>
<p>$$(1,\boxed{4}), (2,\boxed{2}), (3,\boxed{8})$$</p>
<p>If we treat $u(x)$ as interpolating the vector $[2,4,8]$ and $v(x)$ as interpolating the vector $[4,2,8]$, then we can see that their product polynomial interpolates the Hadamard product of the two vectors. The Hadamard product of $[2,4,8]$ and $[4,2,8]$ is $[8,8,64]$.</p>
<p>If we multiply $u(x)$ and $v(x)$ together, we get $w(x) = 4x^4 – 18x^3 + 36x^2 – 42x + 28$.</p>
<p>We can see in the plot below that the product polynomial interpolates the Hadamard product $[8, 8, 64]$ of the two vectors.</p>
<p><img alt="3 point intersection of u, v, and w" src="assets/qap-3-point-cross.png" /></p>
<p>So how can we "make" $w(x)$ equal to $u(x)v(x)$ if they interpolate the same $y$ values over $(1,2,…,n)$?</p>
<h3>Interpolating the $\mathbf{0}$ vector</h3>
<p>If $\mathbf{v_1} \circ \mathbf{v_2} = \mathbf{v_3}$, then $\mathbf{v_1} \circ \mathbf{v_2} = \mathbf{v_3} + \mathbf{0}$.</p>
<p>Instead of interpolating $\mathbf{0}$ with Lagrange interpolation and getting $f(x) = 0$ (remember Lagrange interpolation finds the lowest degree interpolating polynomial), we can use a higher degree polynomial that will balance out the mismatch in degrees.</p>
<p>For example, the black polynomial ($b(x)$) in the image below interpolates $[(1,0), (2,0), (3,0)]$:</p>
<p><img alt="zero polynomial plot" src="assets/qap-zero-polynomial.png" /></p>
<p>Now, since $4x^4 -18x^3 + 8x^2 + 42x – 36$ is a valid interpolation of $[0,0,0]$, we can write our original</p>
<p>$u(x)v(x) = w(x) + b(x)$</p>
<p>and the equation will be balanced!</p>
<p>$b(x)$ was simply computed as $u(x)v(x) – w(x)$ (the blue polynomial minus red polynomial)</p>
<p>However, we can’t just let the prover pick <em>any</em> $b(x)$, otherwise they could pick a $b(x)$ that balances $u(x)v(x)$ and $w(x)$ even if they do not interpolate the same vector ($[8, 8, 64]$ in our example). The prover has too much flexibility in choosing $b(x)$. Specifically, we want to require $b(x)$ to have roots at $x = 1,2,\dots,n$ — that is, to interpolate the $\mathbf{0}$ vector. That way, the polynomial transformation of $\mathbf{v}_1 \circ \mathbf{v}_2 = \mathbf{v}_3 + \mathbf{0}$ still respects the underlying vectors.</p>
<p>To restrict their choice of $b(x)$, we can use the following theorem:</p>
<h4>The union of roots of the polynomial product</h4>
<p><strong>Theorem</strong>: If $h(x) = f(x)g(x)$ and $f(x)$ has set of roots $\set{r_f}$ and $g(x)$ has set of roots $\set{r_g}$, then $h(x)$ has roots $\set{r_f} \cup \set{r_g}$.</p>
<h5>Example</h5>
<p>Let $f(x) = (x – 3)(x – 4)$ and $g(x) = (x – 5)(x – 6)$. Then $h(x) = f(x)g(x)$ has roots $\set{3,4,5,6}$.</p>
<p>We can use the theorem above to enforce that $b(x)$ has roots at $x = 1,2,\dots,n$.</p>
<h4>Forcing $b(x)$ to be the zero vector</h4>
<p>We decompose $b(x)$ into $b(x) = h(x)t(x)$ where $t(x)$ is the polynomial</p>
<p>$$<br />
t(x) = (x-1)(x-2)\dots(x-n)$$</p>
<p>then any polynomial multiplied with $t(x)$ will also be the zero vector, as it must have roots at $x = 1,2,\dots,n$.</p>
<p>Therefore, we will replace $b(x)$ with $h(x)t(x)$ in our equation.</p>
<p>Thus, our equality will become</p>
<p>$$<br />
u(x)v(x) = w(x) + h(x)t(x)$$</p>
<p>We can compute $h(x)$ using basic algebra:</p>
<p>$$<br />
h(x) = \frac{u(x)v(x) – w(x)}{t(x)}$$</p>
<h2>QAP End-to-end</h2>
<p>Suppose we have an R1CS with matrices $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ and witness vector $\mathbf{a}$.</p>
<p>$$\mathbf{L}\mathbf{a}\circ\mathbf{R}\mathbf{a} = \mathbf{O}\mathbf{a}$$</p>
<p>The matrices have $n$ columns and $m$ rows where $n = 3$ and $m = 4$.</p>
<p>That is, $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ are as follows:</p>
<p>$$<br />
\begin{array}{c}<br />
\mathbf{L} = \begin{bmatrix}<br />
\quad l_{11} \quad&amp; l_{12} \quad&amp; l_{13} \quad&amp; l_{14} \quad\<br />
\quad l_{21} \quad&amp; l_{22} \quad&amp; l_{23} \quad&amp; l_{24} \quad\<br />
\quad l_{31} \quad&amp; l_{32} \quad&amp; l_{33} \quad&amp; l_{34} \quad<br />
\end{bmatrix} \<br />
\<br />
\mathbf{R} = \begin{bmatrix}<br />
\quad r_{11} \quad&amp; r_{12} \quad&amp; r_{13} \quad&amp; r_{14} \quad\<br />
\quad r_{21} \quad&amp; r_{22} \quad&amp; r_{23} \quad&amp; r_{24} \quad\<br />
\quad r_{31} \quad&amp; r_{32} \quad&amp; r_{33} \quad&amp; r_{34} \quad<br />
\end{bmatrix} \<br />
\<br />
\mathbf{O} = \begin{bmatrix}<br />
\quad o_{11} \quad&amp; o_{12} \quad&amp; o_{13} \quad&amp; o_{14} \quad\<br />
\quad o_{21} \quad&amp; o_{22} \quad&amp; o_{23} \quad&amp; o_{24} \quad\<br />
\quad o_{31} \quad&amp; o_{32} \quad&amp; o_{33} \quad&amp; o_{34} \quad<br />
\end{bmatrix}<br />
\end{array}$$</p>
<p>And witness vector $\mathbf{a}$ is</p>
<p>$$\mathbf{a} = \begin{bmatrix}<br />
a_1 \ a_2 \ a_3 \ a_4<br />
\end{bmatrix}$$</p>
<p>We split each of the matrices into $m$ column vectors and interpolate them on $(1, 2, …, n)$ to produce $m$ polynomials each.</p>
<p>$$<br />
\begin{array}{c}<br />
\mathbf{L} = \underbrace{\begin{bmatrix}<br />
l_{11} \l_{12} \ l_{13} \ l_{14} \<br />
\end{bmatrix}}_{u_1(x)}<br />
\quad<br />
\underbrace{\begin{bmatrix}<br />
l_{21} \ l_{22} \ l_{23} \ l_{24}<br />
\end{bmatrix}}_{u_2(x)}<br />
\quad<br />
\underbrace{\begin{bmatrix}<br />
l_{31} \ l_{32} \ l_{33} \ l_{34} \<br />
\end{bmatrix}}_{u_3(x)}<br />
\quad<br />
\underbrace{\begin{bmatrix}<br />
l_{41} \ l_{42} \ l_{43} \ l_{44}<br />
\end{bmatrix}}_{u_4(x)}<br />
\end{array}$$<br />
$$<br />
\begin{array}{c}<br />
\mathbf{R} = \underbrace{\begin{bmatrix}<br />
r_{11} \r_{12} \ r_{13} \ r_{14} \<br />
\end{bmatrix}}_{v_1(x)}<br />
\quad<br />
\underbrace{\begin{bmatrix}<br />
r_{21} \ r_{22} \ r_{23} \ r_{24}<br />
\end{bmatrix}}_{v_2(x)}<br />
\quad<br />
\underbrace{\begin{bmatrix}<br />
r_{31} \ r_{32} \ r_{33} \ r_{34} \<br />
\end{bmatrix}}_{v_3(x)}<br />
\quad<br />
\underbrace{\begin{bmatrix}<br />
r_{41} \ r_{42} \ r_{43} \ r_{44}<br />
\end{bmatrix}}_{v_4(x)}<br />
\end{array}$$<br />
$$<br />
\begin{array}{c}<br />
\mathbf{O} = \underbrace{\begin{bmatrix}<br />
o_{11} \o_{12} \ o_{13} \ o_{14} \<br />
\end{bmatrix}}_{w_1(x)}<br />
\quad<br />
\underbrace{\begin{bmatrix}<br />
o_{21} \ o_{22} \ o_{23} \ o_{24}<br />
\end{bmatrix}}_{w_2(x)}<br />
\quad<br />
\underbrace{\begin{bmatrix}<br />
o_{31} \ o_{32} \ o_{33} \ o_{34} \<br />
\end{bmatrix}}_{w_3(x)}<br />
\quad<br />
\underbrace{\begin{bmatrix}<br />
o_{41} \ o_{42} \ o_{43} \ o_{44}<br />
\end{bmatrix}}_{w_4(x)}<br />
\end{array}$$</p>
<p>Each of the matrix-vector products $\mathbf{L}\mathbf{a}$, $\mathbf{R}\mathbf{a}$, and $\mathbf{O}\mathbf{a}$ are homorphically equivalent the following polynomials:</p>
<p>$$<br />
\begin{align*}<br />
\sum_{i=1}^4 a_iu_i(x) &amp;= a_1u_1(x) + a_2u_2(x) + a_3u_3(x) + a_4u_4(x) = u(x) \<br />
\sum_{i=1}^m a_iv_i(x) &amp;= a_1v_1(x) + a_2v_2(x) + a_3v_3(x) + a_4v_4(x) = v(x) \<br />
\sum_{i=1}^m a_iw_i(x) &amp;= a_1w_1(x) + a_2w_2(x) + a_3w_3(x) + a_4w_4(x) = w(x) \<br />
\end{align*}$$</p>
<p>In our case, $t(x)$ will be</p>
<p>$$t(x) = (x – 1)(x – 2)(x – 3)$$</p>
<p>and $h(x)$ will be</p>
<p>$$h(x) = \frac{u(x)v(x) – w(x)}{t(x)}$$</p>
<p>The final formula for a QAP representation of the original R1CS is</p>
<p>$$\sum_{i=1}^4 a_iu_i(x)\sum_{i=1}^4 a_iv_i(x) = \sum_{i=1}^4 a_iw_i(x) + h(x)t(x)$$</p>
<h2>Final formula for a QAP</h2>
<p>A QAP is the following formula:</p>
<p>$$\sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x) = \sum_{i=1}^m a_iw_i(x) + h(x)t(x)$$</p>
<p>Where $u_i(x)$, $v_i(x)$, and $w_i(x)$ are polynomials that interpolate the columns of $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ respectively, $t(x)$ is $(x – 1)(x – 2)…(x – n)$, where $n$ is the number of rows in $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$, and $h(x)$ is</p>
<p>$$<br />
h(x) = \frac{\sum_{i=1}^ma_iu_i(x)\sum_{i=1}^ma_iv_i(x) – \sum_{i=1}^ma_iw_i(x)}{t(x)}$$</p>
<h2>Succinct zero knowledge proofs with Quadratic Arithmetic Programs</h2>
<p>Suppose we had a way for the verifier to send a random value $\tau$ to the prover and the prover would respond with</p>
<p>$$<br />
\begin{align*}<br />
A &amp;= u(\tau)\<br />
B &amp;= v(\tau)\<br />
C &amp;= w(\tau) + h(\tau)t(\tau)<br />
\end{align*}$$</p>
<p>The verifier could check that $AB = C$ and accept that the prover has a valid witness $\mathbf{a}$ that satisfies both the R1CS and the QAP.</p>
<p>However, this would require the verifier to trust that the prover is evaluating the polynomials correctly, and we don’t have a mechanism to force the prover to do so.</p>
<p>In the next chapter, we will show Python code to <a href="https://www.rareskills.io/post/r1cs-to-qap">convert an R1CS to a QAP</a> based on our discussion in this chapter.</p>
<p>Then we will discuss trusted setups to begin to tackle the problem of how to get the prover to evaluate the polynomials honestly.</p>
<p><em>Originally published August 23, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>R1CS to Quadratic Arithmetic Program over a Finite Field in Python</h1>
<p>Source: https://rareskills.io/post/r1cs-to-qap</p>
<h1>R1CS to Quadratic Arithmetic Program over a Finite Field in Python</h1>
<p>To make the transformation from R1CS to QAP less abstract, let’s use a real example.</p>
<p>Let’s say we are encoding the <a href="https://rareskills.io/post/arithmetic-circuit">arithmetic circuit</a></p>
<p>$$z = x⁴ – 5y²x²$$</p>
<p>Converted to a <a href="https://rareskills.io/post/rank-1-constraint-system">Rank 1 Constraint System</a>, this becomes</p>
<p>$$\begin{align*}<br />
v_1 &amp;= xx \<br />
v_2 &amp;= v_1 * v_1 &amp;&amp; //x^4\<br />
v_3 &amp;= -5yy \<br />
-v_2 + z &amp;= v_3 * v_1 &amp;&amp; //-5y^2 * x^2\<br />
\end{align*}$$</p>
<p>We need to pick a characteristic of the <a href="https://rareskills.io/post/finite-fields">finite field</a> we will do this over. When we later combine this with <a href="https://rareskills.io/post/elliptic-curves-finite-fields">elliptic curves</a>, the order of our prime field needs to equal the order of the elliptic curve. (Not matching the two a very common mistake).</p>
<p>But for now, we will pick a small number to make this manageable. We will pick the prime number 79.</p>
<p>First, we define our matrices $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">import numpy as np

# 1, out, x, y, v1, v2, v3
L = np.array([
    [0, 0, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, -5, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 1],
])

R = np.array([
    [0, 0, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, 1, 0, 0, 0],
    [0, 0, 0, 0, 1, 0, 0],
])

O = np.array([
    [0, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 0, 1],
    [0, 1, 0, 0, 0, -1, 0],
])
</code></pre>
<p>To verify we constructed the R1CS correctly (it’s very easy to mess up when doing manually!) we create a valid witness and do the matrix multiplication:</p>
<pre style='font-family: Arial'><code class="language-solidity">x = 4
y = -2
v1 = x * x
v2 = v1 * v1        # x^4
v3 = -5*y * y
z = v3*v1 + v2    # -5y^2 * x^2

# witness
a = np.array([1, z, x, y, v1, v2, v3])

assert all(np.equal(np.matmul(L, a) * np.matmul(R, a), np.matmul(O, a))), &quot;not equal&quot;
</code></pre>
<h2>Finite Field Arithmetic in Python</h2>
<p>The next step is to convert this to a field array. Doing modular arithmetic in Numpy will get very messy, but is straightforward with the galois library. This was introduced in our article on finite fields, but here is quick recap on how to use it:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois

GF = galois.GF(79)

a = GF(70)
b = GF(10)

print(a + b)
# prints 1
</code></pre>
<p>We cannot give it negative values such as GF(-1) or it will throw an exception. To convert negative numbers to their congruent representation in the field, we can add the curve order to them. To avoid “overflowing” positive values, we take the modulus with the curve order.</p>
<pre style='font-family: Arial'><code class="language-solidity">L = (L + 79) % 79
R = (R + 79) % 79
O = (O + 79) % 79
</code></pre>
<p>Our new matrices are</p>
<pre style='font-family: Arial'><code class="language-solidity">## New values of L, R, O
'''
L

[[ 0  0  1  0  0  0  0]
 [ 0  0  0  0  1  0  0]
 [ 0  0  0 74  0  0  0]
 [ 0  0  0  0  0  0  1]]

R

[[ 0  0  1  0  0  0  0]
 [ 0  0  0  0  1  0  0]
 [ 0  0  0  1  0  0  0]
 [ 0  0  0  0  1  0  0]]

O

[[ 0  0  0  0  1  0  0]
 [ 0  0  0  0  0  1  0]
 [ 0  0  0  0  0  0  1]
 [ 0  1  0  0  0 78  0]]
'''
</code></pre>
<p>We can convert them to field arrays simply by wrapping them with GF now. We will also need to recompute our witness, because it contains negative values.</p>
<pre style='font-family: Arial'><code class="language-solidity">L_galois = GF(L)
R_galois = GF(R)
O_galois = GF(O)

x = GF(4)
y = GF(-2 + 79) # we are using 79 as the field size, so 79 - 2 is -2
v1 = x * x
v2 = v1 * v1         # x^4
v3 = GF(-5 + 79)*y * y
out = v3*v1 + v2    # -5y^2 * x^2

witness = GF(np.array([1, out, x, y, v1, v2, v3]))

assert all(np.equal(np.matmul(L_galois, witness) * np.matmul(R_galois, witness), np.matmul(O_galois, witness))), &quot;not equal&quot;
</code></pre>
<h2>Polynomial interpolation in finite fields</h2>
<p>Now, we need to turn each of the columns of the matrices into a list of galois polynomials that interpolate the columns. The points we will interpolate are <code style='font-family: Arial'>x = [1,2,3,4]</code>, since we have 4 rows.</p>
<pre style='font-family: Arial'><code class="language-solidity">def interpolate_column(col):
    xs = GF(np.array([1,2,3,4]))
    return galois.lagrange_poly(xs, col)

# axis 0 is the columns.
# apply_along_axis is the same as doing a for loop over the columns and collecting the results in an array
U_polys = np.apply_along_axis(interpolate_column, 0, L_galois)
V_polys = np.apply_along_axis(interpolate_column, 0, R_galois)
W_polys = np.apply_along_axis(interpolate_column, 0, O_galois)
</code></pre>
<p>If we look again at the contents of our matrices, we expect the first two polynomials of <code style='font-family: Arial'>U_polys</code> and <code style='font-family: Arial'>V_polys</code> to be zero, and the first column of <code style='font-family: Arial'>W_polys</code> to be zero also.</p>
<p>We run the following sanity check:</p>
<pre style='font-family: Arial'><code class="language-solidity">print(U_polys[:2])
print(V_polys[:2])
print(W_polys[:1])

# [Poly(0, GF(79)) Poly(0, GF(79))]# [Poly(0, GF(79)) Poly(0, GF(79))]# [Poly(0, GF(79))]
</code></pre>
<p>The term <code style='font-family: Arial'>Poly(0, GF(79))</code> is simply a polynomial where all the coefficients are zero.</p>
<p>The reader is encouraged to evaluate the polynomials at the values in the R1CS to see they interpolate the matrix values correctly.</p>
<h2>Computing h(x)</h2>
<p>We already know $t(x)$ will be $(x – 1)(x – 2)(x – 3)(x – 4)$ since there are four rows.</p>
<p>By way of reminder, this is the formula for a Quadratic Arithmetic Program. The vector $\mathbf{a}$ is the witness:</p>
<p>$$<br />
\underbrace{\sum_{i=1}^{m} a_i u_i(x)}_\text{term 1} \underbrace{\sum_{i=1}^m a_i v_i(x)}_\text{term 2} = \underbrace{\sum_{i=1}^{m} a_i w_i(x)}_\text{term 3} + h(x)t(x)<br />
$$</p>
<p>Each of the terms is taking the inner product of the witness with the column-interpolating polynomials. That is, each of summation terms are effectively the inner product between $[a₁, …, aₘ]$ and $[u₁(x), …, uₘ(x)]$</p>
<pre style='font-family: Arial'><code class="language-solidity">def inner_product_polynomials_with_witness(polys, witness):
    mul_ = lambda x, y: x * y
    sum_ = lambda x, y: x + y
    return reduce(sum_, map(mul_, polys, witness))

term_1 = inner_product_polynomials_with_witness(U_polys, witness)

term_2 = inner_product_polynomials_with_witness(V_polys, witness)

term_3 = inner_product_polynomials_with_witness(W_polys, witness)
</code></pre>
<p>To compute $h(x)$, we simply solve for it. Note that we cannot compute $h(x)$ unless we have a valid witness, otherwise there will be a remainder.</p>
<pre style='font-family: Arial'><code class="language-solidity"># t = (x - 1)(x - 2)(x - 3)(x - 4)
t = galois.Poly([1, 78], field = GF) * galois.Poly([1, 77], field = GF) * galois.Poly([1, 76], field = GF) * galois.Poly([1, 75], field = GF)

h = (term_1 * term_2 - term_3) // t
</code></pre>
<p>Unlike <a href="https://numpy.org/doc/stable/reference/generated/numpy.poly1d.html">poly1d from numpy</a>, the galois library won’t indicate to us if there is a remainder, so we need to check if the QAP formula is still true.</p>
<pre style='font-family: Arial'><code class="language-solidity">assert term_1 * term_2 == term_3 + h * t, &quot;division has a remainder&quot;
</code></pre>
<p>The check executed above is very similar to what the verifier will check for.</p>
<p>The scheme above will not work when we evaluate the polynomials on a hidden point from a trusted setup. However, the computer doing the trusted setup will still have to execute many of the computations above.</p>
<h2>Summary</h2>
<p>In this article, we present the Python code for converting a R1CS to a QAP.</p>
<h2>Learn more with RareSkills</h2>
<p>This material is from our <a href="https://rareskills.io/zk-bootcamp">Zero Knowledge Course</a>.</p>
<div style='page-break-after: always;'></div>

<h1>Trusted Setup</h1>
<p>Source: https://rareskills.io/post/trusted-setup</p>
<h1>Trusted Setup</h1>
<p>A trusted setup is a mechanism ZK-SNARKs use to evaluate a polynomial at a secret value.</p>
<p>Observe that a polynomial $f(x)$ can be evaluated by computing the inner product of the coefficients with successive powers of $x$:</p>
<p>For example, if $f(x)=3x^3+2x^2+5x+10$, then the coefficients are $[3,2,5,10]$ and we can compute the polynomial as</p>
<p>$$<br />
f(x)=\langle[3,2,5,10],[x^3,x^2,x, 1]\rangle<br />
$$</p>
<p>In other words, we typically think of evaluating $f(2)$ for the polynomial above as</p>
<p>$$<br />
f(2)=3(2)^3+2(2)^2+5(2)+10<br />
$$</p>
<p>but we could also evaluate it as</p>
<p>$$<br />
f(2)=\langle[3,2,5,10],[8,4,2,1]\rangle = 3\cdot8+2\cdot4+5<br />
\cdot2+10\cdot1<br />
$$</p>
<p>Now suppose that someone picks a secret scalar $\tau$ and computes</p>
<p>$$<br />
[\tau^3,\tau^2,\tau,1]<br />
$$</p>
<p>then multiplies each of those points with the generator point of a cryptographic elliptic curve group. The result would be as follows:</p>
<p>$$<br />
[\Omega_3, \Omega_2, \Omega_1, G_1]=[\tau^3G_1,\tau^2G_1,\tau G_1,G_1]<br />
$$</p>
<p>Now anyone can take the <em>structure reference string</em> (SRS) $[\Omega_3, \Omega_2, \Omega_1, G_1]$ and evaluate a degree three polynomial (or less) on $\tau$.</p>
<p>For example, if we have a degree 2 polynomial $g(x)=4x^2+7x+8$, we can evaluate $g(\tau)$ by taking the inner product of the structured reference string with the polynomial:</p>
<p>$$<br />
\langle[0,4,7,8],[\Omega_3, \Omega_2, \Omega_1, G_1]\rangle=4\Omega_2+7\Omega_1+8G_1<br />
$$</p>
<p>We have now computed $g(\tau)$ <em>without knowing what $\tau$ is!</em></p>
<p>This is also called a <em>trusted setup</em> because although <em>we</em> don’t know what the discrete log of $g(\tau)$ is, the person who created the structured reference string does. This could lead to leaking information down the line, so we <em>trust</em> that the entity creating the trusted setup deletes $\tau$ and in no way remembers it.</p>
<h2>Example in Python</h2>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, add
from functools import reduce

def inner_product(points, coeffs):
    return reduce(add, map(multiply, points, coeffs))

## Trusted Setup
tau = 88
degree = 3

# tau^3, tau^2, tau, 1
srs = [multiply(G1, tau**i) for i in range(degree,-1,-1)]

## Evaluate
# p(x) = 4x^2 + 7x + 8
coeffs = [0, 4, 7, 8]

poly_at_tau = inner_product(srs, coeffs)
</code></pre>
<h2>Verifying a Trusted Setup was Generated Properly</h2>
<p>Given a structured reference string, how do we even know that they follow the structure $[x^d, x^{d-1},\dots,x,1]$ and weren’t chosen by the roll of the dice?</p>
<p>If the person doing the trusted setup also provides $\Theta=\tau G_2$, we can validate the structured reference string is indeed successive powers of $\tau$.</p>
<p>$$<br />
e(\Theta, \Omega_i)\stackrel{?}=e(G_2,\Omega_{i+1})<br />
$$</p>
<p>where $e$ is a <a href="https://rareskills.io/post/bilinear-pairing">bilinear pairing</a>. Intuitively, we are computing $\tau\cdot\tau^i$ on the left side and $1\cdot\tau^{i+1}$ on the right side..</p>
<p>To validate that $\Theta$ and $\Omega_1$ have the same discrete logarithms ($\Omega_1$ is supposed to be $\tau G_1$, we can check that</p>
<p>$$<br />
e(\Theta,G_1)\stackrel{?}=e(G_2,\Omega_1)<br />
$$</p>
<h2>Generating a structured reference string as part of a multiparty computation</h2>
<p>It’s not a good trust assumption the person generating the structured reference string actually deleted $\tau$.</p>
<p>We now describe the algorithm for multiple parties to collaboratively create the structured reference string, and as long as one of them is honest (i.e. deletes $\tau$), then the discrete logs of the structured reference string will be unknown.</p>
<p>Alice generates the structured reference string $([\Omega_n,…,\Omega_2,\Omega_1, G_1],\Theta)$ and passes it to Bob.</p>
<p>Bob verifies the SRS is “correct” by using the checks from the earlier section. Then Bob picks his own secret parameter $\gamma$ and computes</p>
<p>$$<br />
([\gamma^n\Omega_n,…,\gamma^2\Omega_2,\gamma\Omega_1,G_1],\gamma\Theta)<br />
$$</p>
<p>Note that the discrete logs of the srs are now</p>
<p>$$<br />
([(\tau\gamma)^n,…,(\tau\gamma)^2,(\tau\gamma),1],\tau\gamma)<br />
$$</p>
<p>If either Alice or Bob delete their $\tau$ or $\gamma$, then the discrete logs of the final srs are not recoverable.</p>
<p>Of course, we don’t need to limit the participants to two, we could have as many participants as we like.</p>
<p>This multiparty computation is often informally referred to as the <em>powers of tau ceremony</em>.</p>
<h2>The use of a trusted setup in ZK-SNARKs</h2>
<p>Evaluating a polynomial on a structured reference string doesn’t reveal information about the polynomial to the verifier, and the prover doesn’t know what point they are evaluating on. We will see later that this scheme helps prevent the prover from cheating and helps keep their witness zero knowledge.</p>
<div style='page-break-after: always;'></div>

<h1>Evaluating a Quadratic Arithmetic Program on a Trusted Setup</h1>
<p>Source: https://rareskills.io/post/elliptic-curve-qap</p>
<h1>Evaluating a Quadratic Arithmetic Program on a Trusted Setup</h1>
<p>Evaluating a <a href="https://rareskills.io/post/quadratic-arithmetic-program">Quadratic Arithmetic Program (QAP)</a> on a trusted setup enables a prover to demonstrate that a QAP is satisfied without revealing the witness while using a constant sized proof.<br />
Specifically, the QAP polynomials are evaluated at an unknown point $\tau$. The QAP equation<br />
$$\sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x) = \sum_{i=1}^m a_iw_i(x) + h(x)t(x)$$<br />
will be balanced if the vector $\mathbf{a}$ satisfies the equation, and unbalanced with overwhelming probability otherwise.<br />
The scheme shown here is not a secure ZK Proof, but it is a stepping stone towards showing how Groth16 works.</p>
<h2>A Concrete Example</h2>
<p>To make this a little less abstract, let’s say that matrices of the <a href="https://rareskills.io/post/rank-1-constraint-system">Rank 1 Constraint System (R1CS)</a> $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ have 3 rows and 4 columns.<br />
$$\mathbf{L}\mathbf{a} \circ \mathbf{R}\mathbf{a} = \mathbf{O}\mathbf{a}$$<br />
Since we have 3 rows, it means our interpolating polynomials will be of degree 2. Because we have 4 columns, each matrix will result in 4 polynomials (for a total of 12 polynomials).<br />
Our QAP will be<br />
$$\sum_{i=1}^4a_iu_i(x)\sum_{i=1}^4a_iv_i(x) = \sum_{i=1}^4a_iw_i(x) + h(x)t(x)$$</p>
<h2>Notation and Preliminaries</h2>
<p>We refer to the generators elliptic curve points in the groups $\mathbb{G}_1$ and $\mathbb{G}_2$ as $G_1$ and $G_2$ respectively. An element in $\mathbb{G}_1$ is denoted as $[X]_1$. An element in $\mathbb{G}_2$ is denoted as $[X]_2$. Where there might be ambiguity with the subscripts referring to indices in a list, we say $X \in \mathbb{G}_1$ or $X \in \mathbb{G}_2$. An <a href="https://rareskills.io/post/bilinear-pairing">elliptic curve pairing</a> between two points is denoted as $[X]_1 \bullet [Y]_2$.<br />
Let $\mathbf{L}_{(*,j)}$ be the $j$-th column of $\mathbf{L}$. In our example, the rows will be $(1,2,3)$ and the columns $(1,2,3,4)$. Let $\mathcal{L}(\mathbf{L}_{(*,j)})$ be the polynomial obtained from running Lagrange interpolation on the $j$-th column of $\mathbf{L}$ using the $x$ values $(1,2,3)$ and the $y$ values being the values of the $j$-th column.<br />
Since we have 4 columns, we obtain four polynomials from $\mathbf{L}$<br />
$$<br />
\begin{align*}<br />
u_1(x) = \mathcal{L}(\mathbf{L}_{(*,1)}) =u_{12}x^2 + u_{11}x+u_{10}\<br />
u_2(x) = \mathcal{L}(\mathbf{L}_{(*,2)}) =u_{22}x^2 + u_{21}x+u_{20}\<br />
u_3(x) = \mathcal{L}(\mathbf{L}_{(*,3)}) =u_{32}x^2 + u_{31}x+u_{30}\<br />
u_4(x) = \mathcal{L}(\mathbf{L}_{(*,4)}) =u_{42}x^2 + u_{41}x+u_{40}\<br />
\end{align*}<br />
$$<br />
four polynomials from $\mathbf{R}$<br />
$$<br />
\begin{align*}<br />
v_1(x) = \mathcal{L}(\mathbf{R}_{(*,1)}) =v_{12}x^2 + v_{11}x+v_{10}\<br />
v_2(x) = \mathcal{L}(\mathbf{R}_{(*,2)}) =v_{22}x^2 + v_{21}x+v_{20}\<br />
v_3(x) = \mathcal{L}(\mathbf{R}_{(*,3)}) =v_{32}x^2 + v_{31}x+v_{30}\<br />
v_4(x) = \mathcal{L}(\mathbf{R}_{(*,4)}) =v_{42}x^2 + v_{41}x+v_{40}\<br />
\end{align*}<br />
$$<br />
and four polynomials from $\mathbf{O}$<br />
$$<br />
\begin{align*}<br />
w_1(x) = \mathcal{L}(\mathbf{O}_{(*,1)}) =w_{12}x^2 + w_{11}x+w_{10}\<br />
w_2(x) = \mathcal{L}(\mathbf{O}_{(*,2)}) =w_{22}x^2 + w_{21}x+w_{20}\<br />
w_3(x) = \mathcal{L}(\mathbf{O}_{(*,3)}) =w_{32}x^2 + w_{31}x+w_{30}\<br />
w_4(x) = \mathcal{L}(\mathbf{O}_{(*,4)}) =w_{42}x^2 + w_{41}x+w_{40}\<br />
\end{align*}<br />
$$<br />
A polynomial $p_{ij}$ means the $i$-th polynomial of the and the $j$-th coefficient (power). For example, $j=2$ means the coefficient associated with $x^2$.<br />
The QAP for our example is<br />
$$<br />
\sum_{i=1}^4a_iu_i(x)\sum_{i=1}^4a_iv_i(x) = \sum_{i=1}^4a_iw_i(x) + h(x)t(x)<br />
$$<br />
where $t(x) = (x – 1)(x – 2)(x – 3)$ and $h(x)$ is<br />
$$<br />
h(x)=\frac{\sum_{i=1}^4a_iu_i(x)\sum_{i=1}^4a_iv_i(x) – \sum_{i=1}^4a_iw_i(x)}{t(x)}<br />
$$</p>
<h3>The degrees of the polynomials in the QAP with respect to the size of the R1CS</h3>
<p>A couple observations about the degrees of the polynomials in the general case:</p>
<ul>
<li>The degree of $u(x)$ and $v(x)$ could be as high as $n – 1$ because they interpolate $n$ points, where $n$ is the number of rows in the R1CS.</li>
<li>The degree of $w(x)$ could be as low as 0 if the sum of the polynomials $\sum_{i=0}^m a_iw_i(x)$ adds up to the zero polynomial, that is, the coefficients additively cancel each other out.</li>
<li>$t(x)$ is degree $n$ by definition.</li>
<li>Multiplying polynomials adds their degrees together, and dividing polynomials subtracts their degrees.</li>
</ul>
<p>Therefore, h(x) will be at most $n – 2$ because<br />
$$\underbrace{n – 1}_{<br />
\deg{u(x)}} + \underbrace{n – 1}_{\deg{v(x)}} – \underbrace{n}_{\deg{t(x)}} = n – 2$$</p>
<h2>Expanding the terms</h2>
<p>If we expand the sums from our previous example, we get the following<br />
$$<br />
\begin{align*}<br />
\sum_{i=1}^4 a_iu_i(x) &amp;= a_1(u_{12}x^2 + u_{11}x+u_{10}) + a_2(u_{22}x^2 + u_{21}x+u_{20}) + a_3(u_{32}x^2 + u_{31}x+u_{30}) + a_4(u_{42}x^2 + u_{41}x+u_{40})\<br />
&amp;= (a_1u_{12}+a_2u_{22}+a_3u_{32}+a_4u_{42})x^2 + (a_1u_{11}+a_2u_{21}+a_3u_{31}+a_4u_{41})x + (a_1u_{10}+a_2u_{20}+a_3u_{30}+a_4u_{40})\<br />
&amp;=u_{2a}x^2+u_{1a}x+u_{0a}\<br />
\sum_{i=1}^4 a_iv_i(x) &amp;= a_1(v_{12}x^2 + v_{11}x+v_{10}) + a_2(v_{22}x^2 + v_{21}x+v_{20}) + a_3(v_{32}x^2 + v_{31}x+v_{30}) + a_4(v_{42}x^2 + v_{41}x+v_{40})\<br />
&amp;= (a_1v_{12}+a_2v_{22}+a_3v_{32}+a_4v_{42})x^2 + (a_1v_{11}+a_2v_{21}+a_3v_{31}+a_4v_{41})x + (a_1v_{10}+a_2v_{20}+a_3v_{30}+a_4v_{40})\<br />
&amp;=v_{2a}x^2+v_{1a}x+v_{0a}\<br />
\sum_{i=1}^4 a_iw_i(x) &amp;= a_1(w_{12}x^2 + w_{11}x+w_{10}) + a_2(w_{22}x^2 + w_{21}x+w_{20}) + a_3(w_{32}x^2 + w_{31}x+w_{30}) + a_4(w_{42}x^2 + w_{41}x+w_{40})\<br />
&amp;= (a_1w_{12}+a_2w_{22}+a_3w_{32}+a_4w_{42})x^2 + (a_1w_{11}+a_2w_{21}+a_3w_{31}+a_4w_{41})x + (a_1w_{10}+a_2w_{20}+a_3w_{30}+a_4w_{40})\<br />
&amp;=w_{2a}x^2+w_{1a}x+w_{0a}\<br />
\end{align*}<br />
$$<br />
In each of the cases, since we are adding 4 degree 2 polynomials, we get a degree 2 polynomial.<br />
In general expression $\sum_{i=1}^m a_ip_i(x)$ produces a polynomial with at most the same power as $p(x)$ (it could be less, if for example $(a_1w_{12}+a_2w_{22}+a_3w_{32}+a_4w_{42})x^2$ added up to 0). For convenience, we have introduced the coefficients $p_{ia}$ where $i$ is the power of the coefficient and $_a$ means we combined the polynomials with the witness $\mathbf{a}$.<br />
Here are the polynomials after reducing the polynomials in this manner:<br />
$$<br />
\begin{align*}<br />
\sum_{i=1}^4 a_iu_i(x) &amp;= u_{2a}x^2+u_{1a}x+u_{0a}\<br />
\sum_{i=1}^4 a_iv_i(x) &amp;= v_{2a}x^2+v_{1a}x+v_{0a}\<br />
\sum_{i=1}^4 a_iw_i(x) &amp;= w_{2a}x^2+w_{1a}x+w_{0a}\<br />
\end{align*}<br />
$$</p>
<h2>Combining a trusted setup with a QAP</h2>
<p>We can now apply the structured reference string from the trusted setup to evaluate the polynomials.<br />
That is, given a structured reference string<br />
$$[\Omega_2, \Omega_1, G_1], [\Theta_2, \Theta_1, G_2], \space\Omega_i \in \mathbb{G}_1, \space\Theta_i \in \mathbb{G}_2$$<br />
which was computed in the trusted setup as<br />
$$<br />
\begin{align*}<br />
[\Omega_2, \Omega_1, G_1] &amp;= [\tau^2G_1, \tau G_1, G_1], \space\Omega_i \in \mathbb{G}_1\<br />
[\Theta_2, \Theta_1, G_2] &amp;= [\tau^2G_2, \tau G_2, G_2], \space\Theta_i \in \mathbb{G}_2<br />
\end{align*}<br />
$$<br />
We can compute<br />
$$<br />
\begin{align*}<br />
[A]_1 &amp;=\sum_{i=1}^4 a_iu_i(\tau) = \langle[u_{2a}, u_{1a}, u_{0a}],[\Omega_2, \Omega_1, G_1]\rangle\<br />
[B]_2 &amp;=\sum_{i=1}^4 a_iv_i(\tau) = \langle[v_{2a}, v_{1a}, v_{0a}],[\Theta_2, \Theta_1, G_2]\rangle\<br />
[C]_1 &amp;=\sum_{i=1}^4 a_iw_i(\tau) = \langle[v_{2a}, v_{1a}, v_{0a}],[\Omega_2, \Omega_1, G_1]\rangle \<br />
\end{align*}<br />
$$<br />
Here, $u_i(\tau), v_i(\tau), w_i(\tau)$ mean the polynomials were evaluated using the structured reference string generated from $\tau$ in the trusted setup, it does not mean “plug in $\tau$ and evaluate the polynomials”. Since $\tau$ was destroyed after the trusted setup, the value $\tau$ is unknown.<br />
We have computed most of the QAP using the srs, but we haven’t computed $h(x)t(x)$ yet:<br />
$$<br />
\underbrace{\sum_{i=1}^m a_iu_i(x)}_{[A]_1}\underbrace{\sum_{i=1}^m a_iv_i(x)}_{[B]_2} = \underbrace{\sum_{i=1}^m a_iw_i(x)}_{[C]_1} + \underbrace{h(x)t(x)}_{??}<br />
$$</p>
<h2>Computing $h(x)t(x)$</h2>
<p>Recall that the degree of $t(x)$ is 3 (generally $n$) and the degree of $h(x)$ is 1 (generally $n – 2$). If we multiply these together, we could get up to a degree 3 polynomial, which is more than the powers of tau ceremony provides. Instead, the powers of tau ceremony must be adjusted to provide a structured reference string for $h(x)t(x)$.<br />
The person doing the trusted setup knows $t(x)$, it is simply $(x – 1)(x – 2)…(x – n)$. However, $h(x)$ is a polynomial computed by the prover and changed based on the values of $\mathbf{a}$, so it cannot be known during the trusted setup.<br />
Note that we cannot evaluate $h(\tau)$ and $t(\tau)$ separately (using a structured reference string) and then pair them together. That would not result in a $\mathbb{G}_1$ element which we need.</p>
<h3>SRS for polynomial products</h3>
<p>Observe that the following computations all result in the same value:</p>
<ul>
<li>The polynomial $h(x)t(x)$ evaluated at $u$, or $(h(x)t(x))(u)$</li>
<li>$h(u)$ multiplied by $t(u)$, or $h(u)t(u)$ ($h$ evaluated at $u$ and $t$ evaluated at $u$)</li>
<li>$h(x)$ multiplied by the evaluation $t(u)$, then evaluated at $u$, i.e. $(h(x)t(u))(u)$</li>
</ul>
<p>We will use the third method to compute $h(\tau)t(\tau)$. Suppose, without loss of generality, that $h(x)$ is $3x^2 + 6x + 2$ and $t(u) = 4$. The computation would be<br />
$$h(x)t(u) = (3x^2 + 6x + 2) \cdot 4 = 12x^2 + 24x + 8$$<br />
If we plug in $u$ to the $12x^2 + 24x + 8$, that would be $h(u)t(u)$.<br />
However, evaluating this polynomial at $\tau$ would require the prover to know $\tau$. The key insight here is that the computation above can be structured as:<br />
$$h(u)t(u) = \langle[3, 6, 2], [4u^2, 4u, 4]\rangle=12u^2+24u+8$$<br />
If the trusted setup provides $[4u^2, 4u, 4]$, and the prover provides $[3, 6, 2]$, then the prover can compute $h(u)t(u)$ without knowing $u$, because anything involving $u$ is in the right vector of the inner product.</p>
<h3>Structured reference string for $h(\tau)t(\tau)$</h3>
<p>To create a structured reference string for $h(\tau)t(\tau)$, we create $n – 1$ evaluations of $t(\tau)$ multiplied by successive powers of $\tau$.<br />
$$[\Upsilon_{n-2}, \Upsilon_{n-3}, …, \Upsilon_1, \Upsilon_0] = [\tau^{n-2}t(\tau)G_1, \tau^{n-3}t(\tau)G_1, …, \tau t(\tau)G_1, t(\tau)G_1]$$<br />
(Somewhat confusingly, a polynomial of degree $k$ has $k+1$ terms, hence we generate $k – 1$ evaluations for a polynomial of degree $k – 2$. Note that Upsilon starts at ${n-1}$ and ends at 0).<br />
Here, $n$ is the number of rows in the R1CS, and we established that $h$ cannot have a degree greater than $n – 2$.<br />
To use the structured reference string to compute $h(\tau)t(\tau)$, the prover does:<br />
$$h(\tau)t(\tau) = \langle[h_{n-2}, h_{n-3}, …, h_1, h_0], [\Upsilon_{n-2}, \Upsilon_{n-3}, …, \Upsilon_1, \Upsilon_0] \rangle$$</p>
<h2>Evaluating a QAP on a trusted setup</h2>
<p>We now tie everything together. Suppose we have a R1CS with matrices of $n$ rows and $m$ columns. From this, we can apply Lagrange interpolation to convert it to a QAP<br />
$$\sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x) = \sum_{i=1}^m a_iw_i(x) + h(x)t(x)$$<br />
The sum terms will each produce a polynomial of degree $n – 1$ (a Lagrange polynomial has one less degree than the number of points it interpolates), and the $h(x)$ polynomial will have degree at most $n – 2$, and $t(x)$ will have degree $n$.<br />
A trusted setup generates a random field element $\tau$ and computes:<br />
$$<br />
\begin{align*}<br />
[\Omega_{n-1},\Omega_{n-2},…, \Omega_1, G_1] &amp;= [\tau^{n-1}G_1, \tau^{n-2}G_1,\dots, \tau G_1, G_1]\<br />
[\Theta_{n-1}, \Theta_{n-2}, …, \Theta_1, G_2] &amp;= [\tau^{n-1}G_2, \tau^{n-2}G_2,\dots, \tau G_2, G_2]\<br />
[\Upsilon_{n-2}, \Upsilon_{n-3}, …, \Upsilon_1, \Upsilon_0] &amp;= [\tau^{n-2}t(\tau)G_1, \tau^{n-3}t(\tau)G_1, …, \tau t(\tau)G_1, t(\tau)G_1]<br />
\end{align*}<br />
$$<br />
Note that the structured reference strings need to have enough terms to accomodate the polynomials in the QAP.<br />
Then the trusted setup destroys $\tau$ and publishes the structured reference strings:<br />
$$([\Omega_2, \Omega_1, G_1], [\Theta_2, \Theta_1, G_2], [\Upsilon_{n-2}, \Upsilon_{n-3}, …, \Upsilon_1, \Upsilon_0])$$<br />
The prover evaluates the components of the QAP as follows:<br />
$$\underbrace{\sum_{i=1}^m a_iu_i(x)}_{A}\underbrace{\sum_{i=1}^m a_iv_i(x)}_B = \underbrace{\sum_{i=1}^m a_iw_i(x) + h(x)t(x)}_{C}$$<br />
$$<br />
\begin{align*}<br />
[A]_1 &amp;=\sum_{i=1}^m a_iu_i(\tau) = \langle[u_{{n-1}a}, u_{{n-2}a}, \dots, u_{1a}, u_{0a}],[\Omega_{n-1}, \Omega_{n-2}, \dots, \Omega_1, G_1]\rangle\<br />
[B]_2 &amp;=\sum_{i=1}^m a_iv_i(\tau) = \langle[v_{{n-1}a}, v_{{n-2}a}, \dots, v_{1a}, v_{0a}],[\Theta_{n-1}, \Theta_{n-2}, \dots, \Theta_1, G_2]\rangle\<br />
[C]_1 &amp;=\sum_{i=0}^m a_iw_i(\tau) + h(\tau)t(\tau) = \langle[w_{{n-1}a}, w_{{n-2}a}, \dots, w_{1a}, w_{0a}],[\Omega_{n-1}, \Omega_{n-2}, \dots, \Omega_1, G_1]\rangle \<br />
&amp;+\langle[h_{n-2}, h_{n-3}, \dots, h_1, h_0], [\Upsilon_{n-2}, \Upsilon_{n-3}, \dots, \Upsilon_1, \Upsilon_0] \rangle\<br />
\end{align*}<br />
$$<br />
The prover publishes $([A]_1, [B]_2, [C]_1)$ and the verifier can check that<br />
$$[A]_1 \bullet [B]_2 \stackrel{?}= [C]_1 \bullet G_2$$<br />
If the witness $\mathbf{a}$ satisfies the QAP, then the equation above will be balanced. But the equation being balanced doesn’t ensure the prover knows a satisfying $\mathbf{a}$ because the prover can publish arbitrary elliptic curve points and the verifier doesn’t know if they are actually derived from the QAP.</p>
<h2>The proof is very small</h2>
<p>Observe that the proof only consists of three elliptic curve points. If a $\mathbb{G}_1$ element is 64 bytes large, and a $\mathbb{G}_2$ element is 128 bytes large, then the proof is only 256 bytes. This is true <em>regardless</em> of the size of the R1CS!<br />
The larger the R1CS, the more work the prover has, but the verifier’s work remains constant.<br />
The solution to this problem is described in the next chapter on the <a href="https://rareskills.io/post/groth16">Groth16 protocol</a>.<br />
The proof still remains of constant size in Groth16 as can be seen in the Tornado Cash source code on the <a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/Verifier.sol#L167-L171">struct</a><br />
called <code style='font-family: Arial'>Proof</code>.<br />
<em>Originally Published August 28, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>Groth16 Explained</h1>
<p>Source: https://rareskills.io/post/groth16</p>
<h1>Groth16 Explained</h1>
<p>The Groth16 algorithm enables a quadratic arithmetic program to be computed by a prover over elliptic curve points derived in a trusted setup, and quickly checked by a verifier. It uses auxiliary elliptic curve points from the trusted setup to prevent forged proofs.</p>
<h2>Prerequisites</h2>
<p>This article is a chapter in the <a href="https://www.rareskills.io/zk-book">RareSkills Book of Zero Knowledge Proofs</a>. It assumes you are familiar with the prior chapters.</p>
<h2>Notation</h2>
<p>We refer to an <a href="https://www.rareskills.io/post/elliptic-curves-finite-fields">elliptic curve point</a> belonging to the $\mathbb{G}_1$ elliptic curve group as $[x]_1$ and an elliptic curve point belonging to the $\mathbb{G}_2$ elliptic curve group as $[x]_2$. A <a href="https://www.rareskills.io/post/bilinear-pairing">pairing</a> between $[x]_1$ and $[x]_2$ is denoted as $[x]_1\bullet[x]_2$ and produces an element in $\mathbb{G}_{12}$. Variables in bold such as $\mathbf{a}$ are vectors, upper case bold letters such as $\mathbf{L}$ are matrices, and field elements (sometimes informally referred to as “scalars”) are lower case letters such as $d$. All arithmetic operations happen in a <a href="https://www.rareskills.io/post/finite-fields">finite field</a> with a characteristic that equals the order of the elliptic curve group.</p>
<p>Given an <a href="https://www.rareskills.io/post/arithmetic-circuit">Arithmetic Circuit (ZK Circuit)</a>, we convert it to a <a href="https://www.rareskills.io/post/rank-1-constraint-system">Rank 1 Constraint System (R1CS)</a> $\mathbf{L}\mathbf{a}\circ \mathbf{R}\mathbf{a} = \mathbf{O}\mathbf{a}$ with matrices of dimension $n$ rows and $m$ columns with a witness vector $\mathbf{a}$. Then, we can convert the R1CS to <a href="https://www.rareskills.io/post/quadratic-arithmetic-program">Quadratic Arithmetic Program (QAP)</a> by interpolating the columns of the matrices as $y$ values over the $x$ values $[1,2,…,n]$. Since $\mathbf{L}$, $\mathbf{R}$, and $\mathbf{O}$ have $m$ columns, we will end up with three sets of $m$ polynomials:</p>
<p>$$<br />
\begin{array}{}<br />
u_1(x),…,u_m(x) &amp; m \text{ polynomials interpolated on the }m \text{ columns of } \mathbf{L}\<br />
v_1(x),…,v_m(x)&amp; m \text{ polynomials interpolated on the }m \text{ columns of } \mathbf{R}\<br />
w_1(x),…,w_m(x)&amp; m \text{ polynomials interpolated on the }m \text{ columns of } \mathbf{O}\<br />
\end{array}$$</p>
<p>From this, we can construct a Quadratic Arithmetic Program (QAP):</p>
<p>$$<br />
\sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x) = \sum_{i=1}^m a_iw_i(x) + h(x)t(x)$$</p>
<p>where</p>
<p>$$<br />
t(x) = (x – 1)(x – 2)\dots(x – n)$$</p>
<p>and</p>
<p>$$<br />
h(x) = \frac{\sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x) – \sum_{i=1}^m a_iw_i(x)}{t(x)}$$</p>
<p>If a third party creates a structured reference string (srs) via a powers of tau ceremony, then the prover can evaluate sum terms (the $\sum a_if_i(x)$ terms) in the QAP at a hidden point $\tau$. Let the structured reference strings be computed as follows:</p>
<p>$$<br />
\begin{align*}<br />
[\Omega_{n-1}, \Omega_{n-2},\dots,\Omega_2,\Omega_1,G_1] &amp;= [\tau^nG_1,\tau^{n-1}G_1,\dots,\tau G_1,G_1] &amp;&amp; \text{srs for } G_1 \<br />
[\Theta_{n-1}, \Theta_{n-2},\dots,\Theta_2,\Theta_2,G_2] &amp;= [\tau^nG_2,\tau^{n-1}G_2,\dots,\tau G_2,G_2] &amp;&amp; \text{srs for } G_2\<br />
[\Upsilon_{n-2},\Upsilon_{n-3},\dots,\Upsilon_1,\Upsilon_0]&amp;=[\tau^{n-2}t(\tau)G_1,\tau^{n-3}t(\tau)G_1,\dots,\tau t(\tau)G_1,t(\tau)G_1] &amp;&amp; \text{srs for }h(\tau)t(\tau)\<br />
\end{align*}$$</p>
<p>We refer to $f(\tau)$ as a polynomial evaluated on a structured reference string $[\tau^dG_1,…,\tau^2G_1,\tau G_1,G_1]$ via the inner product:</p>
<p>$$<br />
f(\tau) = \sum_{i=1}^d f_i\Omega_i=\langle[f_d, f_{d-1},…,f_1,f_0],[\Omega_d,\Omega_{d-1},…,G_1]\rangle$$</p>
<p>or for $\mathbb{G}_2$ srs:</p>
<p>$$<br />
f(\tau) = \sum_{i=1}^d f_i\Theta_i=\langle[f_d, f_{d-1},…,f_1,f_0],[\Theta_d,\Theta_{d-1},…,G_2]\rangle$$</p>
<p>$f(\tau)$ is shorthand for the above expression, and produces an elliptic curve point. It does not mean the prover knows $\tau$.</p>
<p>The prover can evaluate their QAP on the trusted setup by computing:</p>
<p>$$<br />
\begin{align*}<br />
[A]_1 &amp;= \sum_{i=1}^m a_iu_i(\tau)\<br />
[B]_2 &amp;= \sum_{i=1}^m a_iv_i(\tau)\<br />
[C]_1 &amp;= \sum_{i=1}^m a_iw_i(\tau) + h(\tau)t(\tau)<br />
\end{align*}$$</p>
<p>The details of this computation are discussed in our tutorial <a href="https://www.rareskills.io/post/elliptic-curve-qap">Quadratic Arithmetic Programs over Elliptic Curves</a>.</p>
<p>If the QAP is balanced, then the following equation holds:</p>
<p>$$<br />
[A]_1\bullet[B]_2 \stackrel{?}= [C]_1\bullet G_2$$</p>
<h2>Motivation</h2>
<p>Simply presenting $([A]_1, [B]_2, [C]_1)$ is not a convincing argument that the prover knows $\mathbf{a}$ such that the QAP is balanced.</p>
<p>The prover can simply invent values $a$, $b$, $c$ where $ab = c$, compute</p>
<p>$$<br />
\begin{align*}<br />
[A]_1 &amp;= aG_1\<br />
[B]_2 &amp;= bG_2\<br />
[C]_1 &amp;= cG_1<br />
\end{align*}$$</p>
<p>and present those as elliptic curve points $[A]_1$, $[B]_2$, $[C]_1$ to the verifier.</p>
<p>Thus, the verifier has no idea if $([A]_1, [B]_2, [C]_1)$ were the result of a satisfied QAP or not.</p>
<p>We need to force the prover to be honest without introducing too much computational overhead. The first algorithm to accomplish this was “<a href="https://eprint.iacr.org/2013/279.pdf">Pinocchio: Nearly Practical Verifiable Computation</a>.” This was usable enough for ZCash to base the first version of their blockchain on it.</p>
<p>However, Groth16 was able to accomplish the same thing in much fewer steps, and the algorithm is still widely in use today, as no algorithm since has produced as efficient an algorithm for the verification step (though other algorithms have eliminated the trusted setup or significantly reduced the amount of work for the prover).</p>
<p><strong>Update for 2024:</strong> A paper rather triumphantly titled “<a href="https://eprint.iacr.org/2024/916">Polymath: Groth16 is not the limit</a>” published in Cryptology demonstrates an algorithm that requires fewer verifier steps than Groth16. However, there are no known implementations of the algorithm at this time of writing.</p>
<h2>Preventing forgery Part 1: Introducing $\alpha$ and $\beta$</h2>
<h3>An “unsolveable” verification formula</h3>
<p>Suppose we update our verification formula to the following:</p>
<p>$$[A]_1 \bullet [B]_2 \stackrel{?}= [D]_{12} + [C]_1\bullet G_2$$</p>
<p><em>Note that we are using additive notation for the $G_{12}$ group for convenience.</em></p>
<p>Here, $[D]_{12}$ is an element from $G_{12}$ and has an unknown discrete logarithm.</p>
<p>We now show that it is impossible for a verifier to provide a solution $([A]_1, [B]_2, [C]_1)$ to this equation, without knowing the discrete logarithm of $[D]_{12}$.</p>
<h4>Attack 1: Forging A and B and deriving C</h4>
<p>Suppose the prover randomly selects $a’$ and $b’$ to produce $[A]₁$ and $[B]₂$ and tries to derive a value $[C’]$ that is compatible with the verifier’s formula.</p>
<p>$$[A]_1 \bullet [B]_2 \stackrel{?}= [D]_{12} + [C]_1\bullet G_2$$</p>
<p>Knowing the discrete logarithms of $[A]₁$ and $[B]₂$, the malicious prover tries to solve for $[C’]$ by doing</p>
<p>$$\begin{align*}\underbrace{[A]_1\bullet [B]_2 – [D]_{12}}_{\chi_{12}}=[C’]_1\bullet G_2\<br />
[\chi]_{12}=[C’]_1\bullet G_2<br />
\end{align*}$$</p>
<p>The final line is requires the prover to solve for the discrete log of $\chi_{12}$, so they cannot compute a valid discrete log for $[C’]_1$.</p>
<h4>Attack 2: Forging C and deriving A and B</h4>
<p>Here the prover picks a random point $c’$ and computes $[C’]_1$. Because they know $c’$, they can try to discover a compatible combination of $a’$ and $b’$ such that</p>
<p>$$\begin{align*}[A]_1 \bullet [B]_2 &amp;\stackrel{?}= \underbrace{[D]_{12} + [C]_1\bullet G_2}_{[\zeta]_{12}}\<br />
[A]_1 \bullet [B]_2 &amp;\stackrel{?}=[\zeta]_{12}<br />
\end{align*}$$</p>
<p>This requires the prover, given $[\zeta]_{12}$, to come up with an $[A]₁$ and $[B]₂$ that pair to produce $[\zeta]_{12}$.</p>
<p>Similar to the discrete log problem, we rely on unproven cryptographic assumptions that this computation (decomposing an element in $\mathbb{G}_{12}$ into a $\mathbb{G}_1$ and $\mathbb{G}_2$ element) is infeasible. In this case, the assumption that we cannot decompose $[\zeta]_{12}$ into $[A]₁$ and $[B]₂$ is called the <em>Bilinear Diffie-Hellman Assumption</em>. The interested reader can see a related discussion on the <a href="https://en.wikipedia.org/wiki/Decisional_Diffie%E2%80%93Hellman_assumption">Decisional Diffie-Hellman Assumption</a>.</p>
<p>(Unproven does not mean unreliable. If you can find a way to prove or disprove this assumption, fame and fortune awaits you! In practice, there is no known way to decompose $[\zeta]_{12}$ into $[A]₁$ and $[B]₂$ and it is believed to be computationally infeasible.)</p>
<h3>How $\alpha$ and $\beta$ are used</h3>
<p>In practice, Groth16 doesn’t use a term $[D]_{12}$. Instead, the trusted setup generates two random scalars $\alpha$ and $\beta$ and publishes the elliptic curve points $([\alpha]_1,[\beta]_2)$ computed as:</p>
<p>$$<br />
\begin{align*}<br />
[α]_1 &amp;= α G_1 \<br />
[β]_2 &amp;= β G_2<br />
\end{align*}$$</p>
<p>What we referred to as $[D]_{12}$ is simply $[\alpha]_1 \bullet [\beta]_2$.</p>
<h3>Re-deriving the proving and verification formulas</h3>
<p>To make the verification formula $[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1\bullet[\beta]_2 + [C]_1\bullet G_2$ “solvable”, we need to alter our QAP formula to incorporate $\alpha$ and $\beta$.</p>
<p>$$\sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x) = \sum_{i=1}^m a_iw_i(x) + h(x)t(x)$$</p>
<p>Now consider what happens if we introduce terms $\theta$ and $\eta$ to the left hand side of the equation:</p>
<p>$$(\boxed{\theta}+\sum_{i=1}^m a_iu_i(x))(\boxed{\eta} +\sum_{i=1}^m a_iv_i(x)) =$$</p>
<p>$$=\boxed{\theta\eta} + \boxed{\theta}\sum_{i=1}^m a_iv_i(x) + \boxed{\eta}\sum_{i=1}^m a_iu_i(x) + \sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x)$$</p>
<p>We can substitute the rightmost terms using the original QAP definition:<br />
$$=\theta\eta + \theta\sum_{i=1}^m a_iv_i(x) + \eta\sum_{i=1}^m a_iu_i(x) + \boxed{\sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x)}$$</p>
<p>$$=\theta\eta + \theta\sum_{i=1}^m a_iv_i(x) + \eta\sum_{i=1}^m a_iu_i(x) + \boxed{\sum_{i=1}^m a_iw_i(x) + h(x)t(x)}$$</p>
<p>Now we can introduce an “expanded” QAP with the following definition:</p>
<p>$$(\theta+\sum_{i=1}^m a_iu_i(x))(\eta +\sum_{i=1}^m a_iv_i(x)) =\theta\eta + \theta\sum_{i=1}^m a_iv_i(x) + \eta\sum_{i=1}^m a_iu_i(x) + \sum_{i=1}^m a_iw_i(x) + h(x)t(x)$$</p>
<p>As a sneak peak to where we are going, if we replace $\theta$ with $[\alpha]_1$ and $\eta$ with $[\beta]_2$, we get updated verification formula from earlier:</p>
<p>$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + [C]_1\bullet G_2$$</p>
<p>where</p>
<p>$$\underbrace{([\alpha]_1+\sum_{i=1}^m a_iu_i(\tau))}_{[A]_1}\underbrace{([\beta]_2 +\sum_{i=1}^m a_iv_i(\tau))}_{[B]_2} =[\alpha]_1\bullet[\beta]_2 + (\underbrace{\alpha\sum_{i=1}^m a_iv_i(\tau) + \beta\sum_{i=1}^m a_iu_i(\tau) + \sum_{i=1}^m a_iw_i(\tau) + h(\tau)t(\tau)}_{[C]_1}) \bullet G_2$$</p>
<p>The prover can compute $[A]_1$ and $[B]_2$ without knowing $\tau$, $\alpha$, or $\beta$. Given the structured reference string (powers of $\tau$) and the elliptic curve points $([α]_1,[β]_2)$, the prover computes $[A]_1$ and $[B]_2$ as</p>
<p>$$<br />
\begin{align*}<br />
[A]_1 &amp;= [\alpha]_1 + \sum_{i=1}^m a_iu_i(\tau)\<br />
[B]_2 &amp;= [\beta]_2 + \sum_{i=1}^m a_iv_i(\tau)\<br />
\end{align*}$$</p>
<p>Here, $a_iu_i(\tau)$ does not mean the prover knows $\tau$. The prover is using the structure reference string $[\tau^{n-1}G_1,\tau^{n-2}G_1,\dots,\tau G_1,G_1]$ to compute $u_i(\tau)$ for $i=1,2,\dots,m$ and the $G_2$ srs for for $[B]_2$.</p>
<p>However, it isn’t currently possible to compute $[C]_1$ without knowing $\alpha$ and $\beta$. The prover cannot pair $[\alpha]_1$ with $\sum a_iu_i(\tau)$ and $[\beta]_2$ with $\sum a_iv_i(\tau)$ because that would create a $\mathbb{G}_{12}$ point, whereas the prover needs a $\mathbb{G}_1$ point for $[C]_1$.</p>
<p>Instead, the trusted setup needs to precompute $m$ polynomials for the problematic $C$ term of the expanded QAP.</p>
<p>$$\alpha\sum_{i=1}^m a_iv_i(\tau) + \beta\sum_{i=1}^m a_iu_i(\tau) + \sum_{i=1}^m a_iw_i(\tau)$$</p>
<p>With some algebraic manipulation, we combine the sum terms into a single sum:</p>
<p>$$=\sum_{i=1}^m (\alpha a_iv_i(\tau)+\beta a_iu_i(\tau) + a_iw_i(\tau))$$</p>
<p>and factor out $a_i$:</p>
<p>$$=\sum_{i=1}^m a_i\boxed{(\alpha v_i(\tau)+\beta u_i(\tau) + w_i(\tau))}$$</p>
<p>The trusted setup can create $m$ polynomials evaluated at $\tau$ from the boxed term above, and the prover can use that to compute the sum. The exact details are shown in the next section.</p>
<h3>Summary of the algorithm so far</h3>
<h4>Trusted setup steps</h4>
<p>Concretely, the trusted setup computes the following:<br />
$$\begin{align*}<br />
\alpha,\beta,\tau &amp;\leftarrow \text{random scalars}\<br />
[\tau^{n-1}G_1,\tau^{n-2}G_1,\dots,\tau G_1,G_1] &amp;\leftarrow \text{srs for } \mathbb{G}_1\<br />
[\tau^{n-1}G_2,\tau^{n-2}G_2,\dots,\tau G_2,G_2] &amp;\leftarrow \text{srs for } \mathbb{G}_2\<br />
[\tau^{n-2}t(\tau),\tau^{n-3}t(\tau),\dots,\tau t(\tau),t(\tau)] &amp;\leftarrow \text{srs for }h(\tau)t(\tau)\<br />
[\Psi_1]_1 &amp;= (\alpha v_1(\tau) + \beta u_1(\tau) + w_1(\tau))G_1\<br />
[\Psi_2]_1 &amp;= (\alpha v_2(\tau) + \beta u_2(\tau) + w_2(\tau))G_1\<br />
&amp;\vdots\<br />
[\Psi_m]_1 &amp;= (\alpha v_m(\tau) + \beta u_m(\tau) + w_m(\tau))G_1\<br />
\end{align*}$$</p>
<p>The trusted setup publishes</p>
<p>$$([\alpha]_1,[\beta]_2,\text{srs}_{G_1},\text{srs}_{G_2}[\Psi_1]_1,[\Psi_2]_1,\dots,[\Psi_m]_1)$$</p>
<h4>Prover steps</h4>
<p>The prover computes</p>
<p>$$\begin{align*}<br />
[A]_1 &amp;= [\alpha]_1 + \sum_{i=1}^m a_iu_i(\tau)\<br />
[B]_2 &amp;= [\beta]_2 + \sum_{i=1}^m a_iv_i(\tau)\<br />
[C]_1 &amp;= \sum_{i=1}^m a_i[\Psi_i]_1 + h(\tau)t(\tau)\<br />
\end{align*}$$</p>
<p>Note that we replaced the “problematic” polynomial</p>
<p>$$=\sum_{i=1}^m a_i\boxed{(\alpha v_i(\tau)+\beta u_i(\tau) + w_i(\tau))}$$</p>
<p>(the one that contained $\alpha$ and $\beta$) with</p>
<p>$$\sum_{i=1}^m a_i[\Psi_i]_1$$</p>
<h4>Verifier steps</h4>
<p>The verifier computes:</p>
<p>$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + [C]_1\bullet G_2$$</p>
<h2>Supporting public inputs</h2>
<p>The verifier formula so far does not support public inputs, i.e. making a portion of the witness public.</p>
<p>By convention, public portions of the witness are the first $\ell$ elements of the vector $\mathbf{a}$. To make those elements public, the prover simply reveals them:</p>
<p>$$[a_1, a_2, \dots, a_\ell]$$</p>
<p>For the verifier to test that those values were in fact used, verifier must carry out some of the computation that the prover was originally doing.</p>
<p>Specifically, the prover computes:</p>
<p>$$\begin{align*}<br />
[A]_1 &amp;= [\alpha]_1 + \sum_{i=1}^m a_iu_i(\tau)\<br />
[B]_2 &amp;= [\beta]_2 + \sum_{i=1}^m a_iv_i(\tau)\<br />
[C]_1 &amp;= \sum_{i=\ell+1}^m a_i[\Psi_i]_1 + h(\tau)t(\tau)\<br />
\end{align*}$$</p>
<p>Note that only the computation of $[C]_1$ changed — the prover only uses the $a_i$ and $\Psi_i$ terms $\ell + 1$ to $m$.</p>
<p>The verifier computes the first $\ell$ terms of the sum:<br />
$$[X]_1=\sum_{i=1}^\ell a_i\Psi_i$$</p>
<p>And the verification equation is:</p>
<p>$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + [X]_1\bullet G_2 + [C]_1\bullet G_2$$</p>
<h2>Part 2: Separating the public inputs from the private inputs with $\gamma$ or $\delta$</h2>
<h3>Forging proofs by misuing $\Psi_i$ for $i\leq\ell$</h3>
<p>The assumption in the equation above is that the prover is only using $\Psi_{\ell+1}$ to $\Psi_m$ to compute $[C]_1$, but nothing stops a dishonest prover from using $\Psi_1$ to $\Psi_{\ell}$ to compute $[C]_1$, leading to a forged proof.</p>
<p>For example, here is our current verification equation:</p>
<p>$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + \sum_{i=1}^\ell a_i\Psi_i + [C]_1\bullet G_2$$</p>
<p>If we expand the C term under the hood, we get the following:</p>
<p>$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + \sum_{i=1}^\ell a_i\Psi_i + \underbrace{(\sum_{i=\ell+1}^m a_i[\Psi_i]_1 + h(\tau)t(\tau))}_C \bullet G_2$$</p>
<p>Suppose for example and without loss of generality that $\mathbf{a} = [1,2,3,4,5]$ and $\ell=3$. In that case, the public part of the witness is $[1,2,3]$ and the private part is $[4,5]$.</p>
<p>The final equation would be as follows:</p>
<p>$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + (1\Psi_1+2\Psi_2+3\Psi_3)\bullet G2 + \underbrace{(4\Psi_4 + 5\Psi_5 + h(\tau)t(\tau))}_C \bullet G_2$$</p>
<p>However, nothing stops the prover from creating an valid portion of the public witness as [1,2,0] and moving the zeroed out public portion to the private part of the computation as follows:</p>
<p>$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + (1\Psi_1+2\Psi_2+\boxed{0\Psi_3})\bullet G2 + \underbrace{(\boxed{3\Psi_3}+4\Psi_4 + 5\Psi_5 + h(\tau)t(\tau))}_C \bullet G_2$$</p>
<p>The equation above is valid, but the witness does not necessarily satisfy the original constraints.</p>
<p>Therefore, we need to prevent the prover from using $\Psi_1$ to $\Psi_{\ell}$ as part of the computation of $[C]_1$.</p>
<h3>Introducing $\gamma$ and/or $\delta$</h3>
<p>To avoid the problem above, the trusted setup introduces a new scalar : $\gamma$ or $\delta$ to force $\Psi_{\ell+1}$ to $\Psi_m$ to be separate from $\Psi_1$ to $\Psi_{\ell}$. To do this, the trusted setup divides (multiplies by the modular inverse) the private terms (that constitute $[C]_1$) by $\delta$ and/or the public terms (that constitute $[X]_1$, the sum the verifier computes) by $\gamma$.</p>
<p>Since the $h(\tau)t(\tau)$ term is embedded in $[C]_1$, those terms also need to be divided by $\delta$. If either $\delta$ and $\gamma$ have an unknown discrete logarithm, then the forgery described earlier along possible other methods are avoided. This method was used in Zcash’s Sapling’s‑based <a href="https://github.com/ebfull/phase2/blob/master/src/lib.rs#L808">trusted setups</a> where $\gamma$ is simply left to $G_2$ and $\delta$ is still updated from $G_2$ to a random value at later trusted setup stages.</p>
<p>$$\begin{align*}<br />
\alpha,\beta,\tau,\gamma,\delta &amp;\leftarrow \text{random scalars}\<br />
[\tau^{n-1}G_1,\tau^{n-2}G_1,\dots,\tau G_1,G_1] &amp;\leftarrow \text{srs for } \mathbb{G}_1\<br />
[\tau^{n-1}G_2,\tau^{n-2}G_2,\dots,\tau G_2,G_2] &amp;\leftarrow \text{srs for } \mathbb{G}_2\<br />
[\frac{\tau^{n-2}t(\tau)}{\delta},\frac{\tau^{n-3}t(\tau)}{\delta},\dots,\frac{\tau t(\tau)}{\delta},<br />
\frac{t(\tau)}{\delta}] &amp;\leftarrow \text{srs for }h(\tau)t(\tau)\<br />
\<br />
&amp;\text{public portion of the witness}\<br />
[\Psi_1]_1 &amp;= \frac{\alpha v_1(\tau) + \beta u_1(\tau) + w_1(\tau)}{\gamma}G_1\<br />
[\Psi_2]_1 &amp;= \frac{\alpha v_2(\tau) + \beta u_2(\tau) + w_2(\tau)}{\gamma}G_1\<br />
&amp;\vdots\<br />
[\Psi_\ell]_1 &amp;= \frac{\alpha v_\ell(\tau) + \beta u_\ell(\tau) + w_\ell(\tau)}{\gamma}G_1\<br />
\<br />
&amp;\text{private portion of the witness}\<br />
[\Psi_{\ell+1}]_1 &amp;= \frac{\alpha v_{\ell+1}(\tau) + \beta u_{\ell+1}(\tau) + w_{\ell+1}(\tau)}{\delta}G_1\<br />
[\Psi_{\ell+2}]_1 &amp;= \frac{\alpha v_{\ell+2}(\tau) + \beta u_{\ell+2}(\tau) + w_{\ell+2}(\tau)}{\delta}G_1\<br />
&amp;\vdots\<br />
[\Psi_{m}]_1 &amp;= \frac{\alpha v_{m}(\tau) + \beta u_{m}(\tau) + w_{m}(\tau)}{\delta}G_1\<br />
\end{align*}$$</p>
<p>The trusted setup publishes<br />
$$([\alpha]_1,[\beta]_2,[\gamma]_2,[\delta]_2,\text{srs}_{G_1},\text{srs}_{G_2},[\Psi_1]_1,[\Psi_2]_1,\dots,[\Psi_m]_1)$$</p>
<p>The prover steps are the same as before:</p>
<p>$$\begin{align*}<br />
[A]_1 &amp;= [\alpha]_1 + \sum_{i=1}^m a_iu_i(\tau)\<br />
[B]_2 &amp;= [\beta]_2 + \sum_{i=1}^m a_iv_i(\tau)\<br />
[C]_1 &amp;= \sum_{i=\ell+1}^m a_i[\Psi_i]_1 + h(\tau)t(\tau)\<br />
\end{align*}$$</p>
<p>And the verifier steps now include pairing by $[\gamma]_2$ and/or $[\delta]_2$ to cancel out the denominators:</p>
<p>$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + [X]_1\bullet [\gamma]_2 + [C]_1\bullet [\delta]_2$$</p>
<p>or</p>
<p>$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + [X]_1\bullet [G]_2 + [C]_1\bullet [\delta]_2$$</p>
<h2>Part 3: Enforcing true zero knowledge: r and s</h2>
<p>Our scheme is not yet truly zero knowledge. If an attacker is able to guess our witness vector (which is possible if there is only a small range of valid inputs, e.g. secret voting from privileged addresses), then they can verify their guess is correct by comparing their constructed proof to the original proof.</p>
<p>As a trivial example, suppose our claim is $x_1$ and $x_2$ are both either $0$ or $1$. The corresponding arithmetic circuit would be</p>
<p>$$<br />
\begin{align*}<br />
x_1 (x_1 – 1) = 0\<br />
x_2 (x_2 – 1) = 0<br />
\end{align*}$$</p>
<p>An attacker only needs to guess four combinations to figure out what the witness is. That is, they guess a witness, generate a proof, and see if their answer matches the original proof.</p>
<p>To prevent guessing, the prover needs to “salt” their proof, and the verification equation needs to be modified to accommodate the salt.</p>
<p>The prover samples two random field elements $r$ and $s$ and adds them to $A$ and $B$ to make the witness unguessable — an attacker would have to guess both the witness and the salts $r$ and $s$:</p>
<p>$$<br />
\begin{align*}<br />
[A]_1 &amp;= [\alpha]_1 + \sum_{i=1}^m a_iu_i(\tau) + r[\delta]_1\<br />
[B]_2 &amp;= [\beta]_2 + \sum_{i=1}^m a_iv_i(\tau) + s[\delta]_2\<br />
[B]_1 &amp;= [\beta]_1 + \sum_{i=1}^m a_iv_i(\tau) + s[\delta]_1\<br />
[C]_1 &amp;= \sum_{i=\ell+1}^m a_i[\Psi_i]_1 + h(\tau)t(\tau) + As+Br-rs[\delta]_1\<br />
\end{align*}$$</p>
<p>To derive the final verification formula, let’s temporarily ignore that we don’t know the discrete logs of the Greek letter terms and compute the left-hand-side of the verification equation $AB$:</p>
<p>$$\underbrace{(\alpha + \sum_{i=1}^m a_iu_i(x) + r\delta)}_A \underbrace{(\beta + \sum_{i=1}^m a_iv_i(x) + s\delta)}_B$$</p>
<p>Expanding the terms we get:</p>
<p>$$<br />
\alpha\beta+\alpha\sum_{i=1}^m a_iv_i(x)+\alpha s\delta + \beta\sum_{i=1}^m a_iu_i(x) + \sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x)+\sum_{i=1}^m a_iu_i(x) s\delta + r\delta\beta + r\delta\sum_{i=1}^m a_iv_i(x) + r\delta s\delta$$</p>
<p>We can select out the original terms for $C$</p>
<p>$$<br />
\alpha\beta+\boxed{\alpha\sum_{i=1}^m a_iv_i(x)}+\alpha s\delta + \boxed{\beta\sum_{i=1}^m a_iu_i(x)} + \boxed{\sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x)}+\sum_{i=1}^m a_iu_i(x) s\delta + r\delta\beta + r\delta\sum_{i=1}^m a_iv_i(x) + r\delta s\delta$$</p>
<p>And combine them on the left, leaving the new terms on the right:</p>
<p>$$<br />
\alpha\beta + \boxed{\alpha\sum_{i=1}^m a_iv_i(x) + \beta\sum_{i=1}^m a_iu_i(x) + \sum_{i=1}^m a_iu_i(x)\sum_{i=1}^m a_iv_i(x)}+ \underline{\alpha s\delta + \sum_{i=1}^m a_iu_i(x) s\delta + r\delta\beta + r\delta\sum_{i=1}^m a_iv_i(x) + r\delta s\delta}$$</p>
<p>We further rearrange the underlined terms to write them in terms of $As\delta$ and $Br\delta$ as follows. We also split $r\delta s\delta$ into $rs\delta^2 + rs\delta^2 – rs\delta^2$:</p>
<p>$$<br />
=\alpha s\delta + \sum_{i=1}^m a_iu_i(x) s\delta + rs\delta^2 + r\delta\beta + r\delta\sum_{i=1}^m a_iv_i(x) + rs\delta^2 – rs\delta^2$$</p>
<p>Group the $s$ and $r$ terms together:</p>
<p>$$<br />
=(\alpha s\delta + \sum_{i=1}^m a_iu_i(x) s\delta + rs\delta^2) + (r\delta\beta + r\delta\sum_{i=1}^m a_iv_i(x) + rs\delta^2) – rs\delta^2$$</p>
<p>Factor out $s\delta$ and $r\delta$:</p>
<p>$$<br />
=\underbrace{(\alpha+ \sum_{i=1}^m a_iu_i(x) + r\delta)s\delta}_{As\delta} + \underbrace{(\beta + \sum_{i=1}^m a_iv_i(x) + s\delta)r\delta}_{Br\delta} – rs\delta^2$$</p>
<p>Substitute $A$ and $B$:</p>
<p>$$<br />
=As\delta + Br\delta – rs\delta^2$$</p>
<p>So our final equation is</p>
<p>$$(\alpha + \sum_{i=1}^m a_iu_i(x) + r\delta)(\beta + \sum_{i=1}^m a_iv_i(x) + s\delta)=\alpha\beta+\sum_{i=1}^m a_i(\alpha v_i(x) + \beta u_i(x)+w_i(x)) + h(x)t(x) + As\delta + Br\delta – rs\delta^2$$</p>
<p>We now break it into the public and private portions:</p>
<p>$$(\alpha + \sum_{i=1}^m a_iu_i(x) + r\delta)(\beta + \sum_{i=1}^m a_iv_i(x) + s\delta)=\alpha\beta+\underbrace{\sum_{i=1}^\ell a_i(\alpha v_i(x) + \beta u_i(x)+w_i(x))}_\text{public} + \underbrace{\sum_{i=\ell+1}^m a_i(\alpha v_i(x) + \beta u_i(x)+w_i(x)) + h(x)t(x) + As\delta + Br\delta – rs\delta^2}_\text{private}$$</p>
<p>We want the public portion and the private portion to be separated by $\gamma$ and $\delta$ respectively:</p>
<p>$$(\alpha + \sum_{i=1}^m a_iu_i(x) + r\delta)(\beta + \sum_{i=1}^m a_iv_i(x) + s\delta)=\alpha\beta+\gamma\frac{\sum_{i=1}^\ell a_i(\alpha v_i(x) + \beta u_i(x)+w_i(x))}{\gamma} + \delta\frac{\sum_{i=\ell+1}^m a_i(\alpha v_i(x) + \beta u_i(x)+w_i(x)) + h(x)t(x) + As\delta + Br\delta – rs\delta^2}{\delta}$$</p>
<p>$\delta$ cancels for some of the terms:</p>
<p>$$(\alpha + \sum_{i=1}^m a_iu_i(x) + r\delta)(\beta + \sum_{i=1}^m a_iv_i(x) + s\delta)=\alpha\beta+\gamma\frac{\sum_{i=1}^\ell a_i(\alpha v_i(x) + \beta u_i(x)+w_i(x))}{\gamma} + \delta\frac{\sum_{i=\ell+1}^m a_i(\alpha v_i(x) + \beta u_i(x)+w_i(x)) + h(x)t(x)}{\delta} + As + Br – rs\delta$$</p>
<p>We now separate this equation in to the verifier and prover portions. The boxed terms are the verifier portion, the underbrace terms are the terms that the prover provides:</p>
<p>$$\underbrace{(\alpha + \sum_{i=1}^m a_iu_i(x) + r\delta)}_{[A]_1}\underbrace{(\beta + \sum_{i=1}^m a_iv_i(x) + s\delta)}_{[B]_2}=\boxed{\alpha\beta}+\boxed{\gamma}\boxed{\frac{\sum_{i=1}^\ell a_i(\alpha v_i(x) + \beta u_i(x)+w_i(x))}{\gamma}} + \boxed{\delta}\underbrace{\frac{\sum_{i=\ell+1}^m a_i(\alpha v_i(x) + \beta u_i(x)+w_i(x)) + h(x)t(x)}{\delta} + As + Br – rs\delta}_{[C]_1}$$</p>
<h2>Groth16 Proof Algorithm</h2>
<p>We are now ready to show the Groth16 algorithm end-to-end. The trusted setup and the verification steps remain unchanged from the previous example where we incorporated $\gamma$ and $\delta$. Only the prover’s calculation changes to incorporate $r$ and $s$.</p>
<h3>Trusted Setup</h3>
<p>$$\begin{align*}<br />
\alpha,\beta,\tau,\gamma,\delta &amp;\leftarrow \text{random scalars}\<br />
[\tau^{n-1}G_1,\tau^{n-2}G_1,\dots,\tau G_1,G_1] &amp;\leftarrow \text{srs for } \mathbb{G}_1\<br />
[\tau^{n-1}G_2,\tau^{n-2}G_2,\dots,\tau G_2,G_2] &amp;\leftarrow \text{srs for } \mathbb{G}_2\<br />
[\frac{\tau^{n-2}t(\tau)}{\delta},\frac{\tau^{n-3}t(\tau)}{\delta},\dots,\frac{\tau t(\tau)}{\delta},<br />
\frac{t(\tau)}{\delta}] &amp;\leftarrow \text{srs for }h(\tau)t(\tau)\<br />
\<br />
&amp;\text{public portion of the witness}\<br />
[\Psi_1]_1 &amp;= \frac{\alpha v_1(\tau) + \beta u_1(\tau) + w_1(\tau)}{\gamma}G_1\<br />
[\Psi_2]_1 &amp;= \frac{\alpha v_2(\tau) + \beta u_2(\tau) + w_2(\tau)}{\gamma}G_1\<br />
&amp;\vdots\<br />
[\Psi_\ell]_1 &amp;= \frac{\alpha v_\ell(\tau) + \beta u_\ell(\tau) + w_\ell(\tau)}{\gamma}G_1\<br />
\<br />
&amp;\text{private portion of the witness}\<br />
[\Psi_{\ell+1}]_1 &amp;= \frac{\alpha v_{\ell+1}(\tau) + \beta u_{\ell+1}(\tau) + w_{\ell+1}(\tau)}{\delta}G_1\<br />
[\Psi_{\ell+2}]_1 &amp;= \frac{\alpha v_{\ell+2}(\tau) + \beta u_{\ell+2}(\tau) + w_{\ell+2}(\tau)}{\delta}G_1\<br />
&amp;\vdots\<br />
[\Psi_{m}]_1 &amp;= \frac{\alpha v_{m}(\tau) + \beta u_{m}(\tau) + w_{m}(\tau)}{\delta}G_1\<br />
\end{align*}$$</p>
<p>The trusted setup publishes<br />
$$([\alpha]_1,[\beta]_1[\beta]_2,[\gamma]_2,[\delta]_1[\delta]_2,\text{srs}_{G_1},\text{srs}_{G_2},[\Psi_1]_1,[\Psi_2]_1,\dots,[\Psi_m]_1)$$</p>
<h3>Prover Steps</h3>
<p>Prover has a witness $\mathbf{a}$ and generates random scalars $r$ and $s$.<br />
$$\begin{align*}<br />
[A]_1 &amp;= [\alpha]_1 + \sum_{i=1}^m a_iu_i(\tau)+r[\delta]_1\<br />
[B]_1 &amp;= [\beta]_1 + \sum_{i=1}^m a_iv_i(\tau)+s[\delta]_1\<br />
[B]_2 &amp;= [\beta]_2 + \sum_{i=1}^m a_iv_i(\tau)+s[\delta]_2\<br />
[C]_1 &amp;= \sum_{i=\ell+1}^m a_i[\Psi_i]_1 + h(\tau)t(\tau)+[A]_1s+[B]_1r-rs[\delta]_1\<br />
\end{align*}$$</p>
<p>The prover publishes $([A]_1, [B]_2, [C]_1, [a_1,…,a_\ell])$.</p>
<h3>Verifier Steps</h3>
<p>The verifier checks</p>
<p>$$<br />
\begin{align*}<br />
[X]_1&amp;=\sum_{i=1}^\ell a_i\Psi_i\<br />
[A]_1\bullet[B]_2 &amp;\stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + [X]_1\bullet [\gamma]_2 + [C]_1\bullet [\delta]_2<br />
\end{align*}$$</p>
<h2>Verifying Groth16 in Solidity</h2>
<p>At this point, you have sufficient knowledge to understand the proof verification code in Solidity. Here is <a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/Verifier.sol#L192">Tornado Cash’s proof verification code</a>. The reader is encouraged to read the source code closely. If the reader is comfortable with Solidity assembly programming, then understanding this source code will not be difficult as the variable names are consistent with the ones in this article.</p>
<p>There is also library support for <a href="https://lib.rs/crates/groth16-solana">Groth16 on Solana</a>.</p>
<h2>Security Issues to Be Aware Of</h2>
<h3>Groth16 is Malleable</h3>
<p>Groth16 proofs are malleable. Given a valid proof</p>
<p>$([A]_1, [B]_2, [C]_1)$, an attacker can compute the point negation of $[A]_1$ and $[B]_2$ and present a new proof as $([A’]_1, [B’]_2, [C]_1)$ where $[A’]_1 = \mathsf{neg}([A]_1)$ and $[B’]_2 = \mathsf{neg}([B]_2)$.</p>
<p>To see that $[A]_1\bullet[B]_2 = [A’]_1\bullet[B’]_2$, consider the following code:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, G2, multiply, neg, eq, pairing

# chosen arbitrarily
x = 10
y = 100
A = multiply(G1, x)
B = multiply(G2, y)

A_p = neg(A)
B_p = neg(B)

assert eq(pairing(B, A), pairing(B_p, A_p))
</code></pre>
<p>Intuitively, the attacker is multiplying $A$ and $B$ by $-1$, and $(-1)\times(-1)$ cancels itself out in the pairing.</p>
<p>Hence, if the verification formula accepts<br />
$$[A]_1\bullet[B]_2 \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + [X]_1\bullet [\gamma]_2 + [C]_1\bullet [\delta]_2$$</p>
<p>then it will also accept</p>
<p>$$\mathsf{neg}([A]_1)\bullet\mathsf{neg}([B]_2) \stackrel{?}= [\alpha]_1 \bullet [\beta]_2 + [X]_1\bullet [\gamma]_2 + [C]_1\bullet [\delta]_2$$</p>
<p>The defense against this attack is described in the following section.</p>
<p>You can see a proof of concept of this attack in this <a href="https://medium.com/@cryptofairy/exploring-vulnerabilities-the-zksnark-malleability-attack-on-the-groth16-protocol-8c80d13751c5">article</a>.</p>
<h3>The prover can create an unlimited number of proofs for the same witness</h3>
<p>This isn’t a “security issue” per se — it is necessary to achieve Zero Knowledge. However, the application needs a mechanism to track which facts have already been proven and cannot rely on the uniqueness of the proof to achieve that.</p>
<h2>Learn more with RareSkills</h2>
<p>Our ability to publish material like this free of charge depends on the continued support of our students. Consider signing up for our <a href="https://www.rareskills.io/zk-bootcamp">Zero Knowledge Bootcamp</a>, <a href="https://www.rareskills.io/web3-blockchain-bootcamps">Web3 Bootcamps</a>, or getting a job on <a href="https://www.raretalent.xyz">RareTalent</a>.</p>
<p><em>Originally Published August 31, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>Introduction to ZK Circuits with Circom</h1>
<p>Source: https://rareskills.io/post/circom-intro</p>
<h1>Introduction to ZK Circuits with Circom</h1>
<p>Circom is a programming language for creating Rank 1 Constraint Systems (R1CS) and populating the witness vector of the R1CS.</p>
<p>The R1CS format is of interest because of the utility of that format for constructing SNARKs, particularly <a href="https://rareskills.io/post/groth16">Groth16</a>. With SNARKs, we enable verifiable computation, allowing us to prove the correctness of a computation. When verifying, the interested party expends less computational effort to confirm the correctness than they would need to perform the computation themselves. It is also possible to generate the proof without revealing the underlying data, and in this case, we refer to it as zkSNARKs.</p>
<p>The first part of our ZK book focused on proving a witness’s validity for a given R1CS. This resource focuses on how to programmatically generate an R1CS and how to design them to model realistic algorithms such as a virtual machine or cryptographic hash functions.</p>
<h2>Prerequisites</h2>
<p>We expect the reader to already be familiar with the following chapters from our ZK Book:</p>
<ul>
<li><a href="https://rareskills.io/post/p-vs-np">https://rareskills.io/post/p-vs-np</a></li>
<li><a href="https://rareskills.io/post/arithmetic-circuit">https://rareskills.io/post/arithmetic-circuit</a></li>
<li><a href="https://rareskills.io/post/finite-fields">https://rareskills.io/post/finite-fields</a></li>
<li><a href="https://rareskills.io/post/rank-1-constraint-system">https://rareskills.io/post/rank-1-constraint-system</a></li>
</ul>
<p>We are going to assume the reader knows what an R1CS is and what it represents. This is fully explained in the four chapters above.</p>
<p>It is not necessary to fully understand the math behind ZK to use Circom, but there are some principles that must be fully grasped, or Circom will not make sense.</p>
<p>Nonetheless, if the reader is serious about having a career in ZK, learning the foundations of ZK is essential. For that, we highly recommend reading through the first two sections of the <a href="staging.rareskills.io/zk-book">ZK book</a> and building the Groth16 proof system from scratch to enforce learning.</p>
<p>However, if the reader’s objective is to quickly understand ZK applications, then we recommend reading the four chapters listed above and then using this resource.</p>
<h2>Why Circom Exists</h2>
<p>Circom was created to address two major issues in developing constraint systems for SNARKs.</p>
<ol>
<li>Manually designing constraint systems is tedious and error-prone, especially when dealing with large-scale or repetitive constraints.</li>
<li>Populating the witness is equally challenging and requires manual computation of intermediary values that could otherwise be derived programmatically.</li>
</ol>
<p>Thus, Circom 1) simplifies constraint design and 2) automates witness population.</p>
<h3>1. Designing the constraint system is tedious</h3>
<p>The tasks of manually designing a set of (correct) constraints and then translating them into R1CS are tedious and error-prone. Circom was created to make this task less challenging and tedious by programmatically generating the constraints.</p>
<p>For example, to say the value <code style='font-family: Arial'>x</code> can only have the values $\set{1,2,3}$, we can express that with the constraint</p>
<p>$$<br />
0 === (x – 1) (x – 2) (x – 3)<br />
$$</p>
<p>However, a R1CS can only have one non-constant multiplication per constraint, so we must break up the above constraint into two constraints:</p>
<p>$$<br />
\begin{align*}<br />
s &amp;=== (x – 1)(x – 2) &amp;&amp;= x * x – 3 * x + 2\<br />
0 &amp;=== s(x – 3) &amp;&amp;= x * s – 3 * s<br />
\end{align*}<br />
$$</p>
<p>For small systems, this manual translation is manageable. However, it would be extremely annoying to do it by hand if we needed to create this constraint for 100 or even 1000 variables. If we have thousands of very similar constraints, it would be preferable to create a “template” for the constraints and generate the constraints in a for loop. Circom allows us to create these constraints programmatically.</p>
<p>For example, suppose we wanted to constrain 1,000 variables to have the values $\set{0,1}$. Circom can generate these values in a loop as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Constrain1000Example() {
  signal input in[1000];

  for (var i = 0; i &lt; 1000; i++) {
    0 === in[i] * (in[i] - 1);
  }
}

component main = Constrain1000Example();
</code></pre>
<p>We will explain the syntax further in later chapters, but the core idea is that we defined a constraint <code style='font-family: Arial'>0 === in[i] * (in[i] - 1)</code> and repeated it 1000 times.</p>
<h3>2. Populating the witness is tedious</h3>
<p>The witness in the context of ZK is an assignment to the variables that satisfies all the constraints in an arithmetic circuit.</p>
<p>As we saw in the article on <a href="https://rareskills.io/post/arithmetic-circuit">arithmetic circuits</a>, proving one number is less than another number requires converting both of the numbers to binary, as “greater than” is not meaningful in a finite field since the numbers wrap around.</p>
<p>Expressing the number $x$ in binary, assuming it fits in four bits, requires $x$ to satisfy the following constraints:</p>
<p>$$<br />
\begin{align*}<br />
x&amp;===b_0+2b_1+4b_2+8b_3\<br />
0&amp;===b_0(b_0 – 1)\<br />
0&amp;===b_1(b_1 – 1)\<br />
0&amp;===b_2(b_2 – 1)\<br />
0&amp;===b_3(b_3 – 1)<br />
\end{align*}<br />
$$</p>
<p>Here, $b_0$ is the least significant bit, and $b_3$ is the most significant bit. The prover must supply $b_0, b_1, b_2, b_3$, which are the binary bits of $x$, along with $x$ itself.</p>
<p>In this case, proving that $x$ is a four-bit number has become five times more tedious because, in addition to $x$, we also have to provide the binary values of $x$, even though they can be derived deterministically and straightforwardly. Circom automates this process and allows us to write code to populate variables in the witness based on other variables. For example, to populate the binary variables, we could write the following Circom code (the following code lacks some necessary safety features — please do not copy it blindly):</p>
<pre style='font-family: Arial'><code class="language-solidity">b_0 &lt;-- x &amp; 1;        // get the first bit of x via bitmask
b_1 &lt;-- (x &gt;&gt; 1) &amp; 1; // get the second bit of x
b_2 &lt;-- (x &gt;&gt; 2) &amp; 1; // get the third bit of x
b_3 &lt;-- (x &gt;&gt; 3) &amp; 1; // get the fourth bit of x
</code></pre>
<p>The code above <em>generates</em> the witness but does not create the constraints in our formula:</p>
<p>$$<br />
\begin{align*}<br />
x&amp;===b_0+2b_1+4b_2+8b_3\<br />
0&amp;===b_0(b_0 – 1)\<br />
0&amp;===b_1(b_1 – 1)\<br />
0&amp;===b_2(b_2 – 1)\<br />
0&amp;===b_3(b_3 – 1)<br />
\end{align*}<br />
$$</p>
<p>The above circuit translated to Circom would be (the syntax will be explained more later):</p>
<pre style='font-family: Arial'><code class="language-solidity">template BinaryConstraint() {

  // assign the values to b_0,...,b_3
  x === b_0 + 2*b_1 + 4*b_2 + 8*b_3;
  0 === b_0*(b_0 - 1);
  0 === b_1*(b_1 - 1);
  0 === b_2*(b_2 - 1);
  0 === b_3*(b_3 - 1);
}
</code></pre>
<p>One major convenience of Circom is that its code resembles mathematics in arithmetic circuits, so it is easy to translate a system of equations to Circom.</p>
<p>The idea is instead of supplying $(x,b_0,b_1,b_2,b_3)$ to the circuit, we only supply $x$. Circom will compute the binary values for us and then fill out the constraints with the computed values.</p>
<p>In addition to automating constraint generation, Circom improves the process of populating the witness through its “assign and constrain” operator, <code style='font-family: Arial'>&lt;==</code>.</p>
<h2>The advantage of <code style='font-family: Arial'>&lt;==</code> assign and constrain in Circom</h2>
<p>Circom further simplifies the witness population through its “assign and constrain” operator <code style='font-family: Arial'>&lt;==</code>. Suppose we have the constraint:</p>
<pre style='font-family: Arial'><code class="language-solidity">z === x * y
</code></pre>
<p>If we supply the values for <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code>, it would be a bit annoying to also have to supply the value for <code style='font-family: Arial'>z</code> because <code style='font-family: Arial'>z</code> only has one possible solution.<br />
With Circom, we use <code style='font-family: Arial'>&lt;==</code> as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">z &lt;== x * y
</code></pre>
<p>With this, the variable <code style='font-family: Arial'>z</code> no longer needs to be provided as an input as Circom populates it for us, and its value will be locked into $x\cdot y$ for the rest of the circuit.</p>
<p>Hence, Circom saves a user from the hassle of explicitly providing a value for every element in the witness, which is a major selling point for Circom’s convenience.</p>
<h2>Circom is both a DSL and a programming language</h2>
<p>The biggest source of confusion when programming in Circom is that it is both a programming language (similar to Javascript) and a DSL that compiles to an R1CS. In that sense, it is a bit like Solidity. Solidity can affect the underlying blockchain state by transferring Ether, but it can also behave like a regular programming language. The “programming language” portion of Circom is to aid with automatic witness population as described earlier. However, to the newcomer, it is not always clear which parts of Circom affect the underlying R1CS.</p>
<p>For example, the following is a valid Circom code that computes the power of a number:</p>
<pre style='font-family: Arial'><code class="language-solidity">function power(base, exp) {
  return base ** exp;
}

template Power() {
  signal input base;
  signal input exp;
  signal output out;

  out &lt;-- power(base, exp);
}

component main = Power();

/* INPUT = {
  &quot;base&quot;: &quot;3&quot;,
  &quot;exp&quot;: &quot;2&quot;
} */
</code></pre>
<p>However, the code above does not generate any constraints (so it wouldn’t be useful for proving anything). As we will learn later, the <code style='font-family: Arial'>&lt;--</code> operator has the sole purpose of generating the witness, not generating the constraints.</p>
<h2>Why Learn Circom</h2>
<p>As one of the oldest Domain-Specific Languages (DSLs) for ZK, Circom has the most available <strong>libraries</strong> and <strong>projects</strong> that you can learn from and is <strong>battle-tested</strong>.</p>
<p>We think that learning the more modern ZK DSLs, such as <a href="https://github.com/privacy-scaling-explorations/halo2">Halo2</a> and <a href="https://github.com/Plonky3/Plonky3">Plonky3</a> will be much easier if we teach Circom first, so we’re doing that.</p>
<p>To see why, here is the code for computing the <a href="https://github.com/Divide-By-0/halo2-examples/blob/master/src/fibonacci/example1.rs">Fibonacci sequence in Halo2</a> and the code for computing <a href="https://github.com/BrianSeong99/Plonky3_Fibonacci/blob/master/src/main.rs">Fibonacci in Plonky3</a>. A cursory look at the examples should convince the reader that those DSLs might not be the best place to start for a beginner. Here is the Circom code to prove that <code style='font-family: Arial'>out</code> is the correct n-th Fibonnaci number. It is much easier to understand by comparison:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.6;

// proves `out` is the nth
// fibonnaci number
template Fibonacci(n) {
  var offset = n + 1;
  assert(n &gt; 2);

  signal fib[offset];
  signal output out;

  fib[0] &lt;== 0;
  fib[1] &lt;== 1;

  for (var i = 2; i &lt; offset; i++) {
    fib[i] &lt;== fib[i-1] + fib[i - 2];
  }

  out &lt;== fib[n];
}

// 5th fibonnaci number is 5
// 0 1 1 2 3 5
component main = Fibonacci(5);
</code></pre>
<p>In contrast, Circom has a relatively simple learning curve for beginners diving into ZK development.</p>
<h2>Don’t Noir, Cairo, and Leo abstract away the need to learn constraint writing?</h2>
<p>You can write smart contracts on ZK blockchains or layer 2s using Rust-like languages, such as Noir, Cairo, and Leo, that are designed to “hide” the constraint generation from the programmer. If your goal is simply to write applications for these blockchains, learning how ZK constraints work under the hood is not strictly necessary.</p>
<p>However, consider that every serious Solidity programmer has a decent grasp of how the Ethereum Virtual Machine (EVM) works and can write basic assembly. Knowing what is happening behind the scenes will help you write more efficient code, and this resource accomplishes that goal.</p>
<p>Additionally, there are many bugs that rise up in these execution environments due to the underlying ZK execution model. Understanding what is actually private, what limitations may exist on control flow, common errors when using fields, or gaining the ability to safely use unconstrained functions in Noir or custom constraints in <a href="https://docs.minaprotocol.com/zkapps/o1js">o1js</a> all require a low-level understanding.</p>
<h3>The goal of this series</h3>
<p>Nonetheless, high-level ZK languages do not make constraint writing obsolete — in fact — they increase the demand for experts who truly understand how they work. The purpose of this resource is to onboard more advanced developers and security auditors to be able to develop and secure the underlying blockchain, virtual machine, and compiler environments that these high-level ZK languages use.</p>
<h2>How this resource is structured</h2>
<p>This resource is divided into two main parts:</p>
<ol>
<li>The first part teaches the syntax of Circom. Specifically, we teach how to write constraints and program Circom to populate most of the witness values for us.</li>
<li>The second part of this resource teaches how to design constraints for ZK applications in general. We’ll use Circom for the examples, but the content applies to other ZK DSLs, such as Halo2 or Plonky3.</li>
</ol>
<p>We will also touch on security issues in ZK applications throughout the content.</p>
<h2>Learning comes not only with study but with practice</h2>
<p>Many of the chapters include explicit exercises or some unfinished code that is “left as an exercise for the reader”. <strong>Your learning journey will be far more effective if you solve those problems</strong>. We designed those problems to serve as a review of what you just read to enforce the learning. They do not require any special “insight” or “cleverness” to solve if you correctly understand the written resource. Our hope is that the exercises at the end will feel somewhat “obvious” after reading the material (if not, please raise an issue or open a pull request in the exercises’ repository!)</p>
<h2>Installing Circom</h2>
<p>The instructions for installing Circom are here: <a href="https://docs.circom.io/getting-started/installation/#installing-dependencies">https://docs.circom.io/getting-started/installation/#installing-dependencies</a></p>
<p>There is also an online IDE for Circom here: <a href="https://zkrepl.dev/">https://zkrepl.dev/</a></p>
<h2>Addendum: Plonk vs Groth16 for Circom</h2>
<p>For readers familiar with the Plonk proving system, it’s worth noting that we write the same circuit for both Plonk prover systems and the Groth16 prover system.</p>
<p>Groth16 allows an unlimited number of addition operations per constraint but only one non-constant multiplication (consider that a Rank 1 Constraint System has one multiplication per row). In contrast, Plonk only allows one multiplication or one addition per constraint, and not both. The one-multiplication-per-constraint limitation will become apparent as we explore Circom.</p>
<p>However, Circom circuits that are compatible with Groth16 will also work with Plonk. The snarkjs library that uses Rank 1 Constraints Systems as an input translates it to a Plonk constraint system if the developer so wishes.</p>
<p>Circom is, therefore, agnostic to whether the intended underlying proof system is Groth16 or Plonk. As long as the circuit is compatible with Groth16, it can also be compatible with Plonk with no additional changes from the developer.</p>
<h2>Authorship and Credits</h2>
<p>Calnix wrote the first part of this book and significantly influenced its overall structure. Please <a href="https://x.com/cal_nix">follow Calnix on X</a> and maybe drop a thank you.</p>
<p>We are grateful to <a href="https://veridise.com">Veridise</a>, <a href="https://pse.dev/en">Privacy Scaling Explorations</a>, Marco Besier from <a href="https://www.zksecurity.xyz">zkSecurity</a>, and <a href="https://chainlight.io">Chainlight</a> for their helpful reviews of this work.</p>
<div style='page-break-after: always;'></div>

<h1>Hello World Circom</h1>
<p>Source: https://rareskills.io/post/hello-world-circom</p>
<h1>Hello World Circom</h1>
<h2>Introduction</h2>
<p>This chapter shows the relationship between Circom code and the Rank 1 Constraint System (R1CS) it compiles to.</p>
<p>Understanding R1CSs is <strong>critical</strong> to understanding Circom, so be sure to brush up on <a href="https://rareskills.io/post/rank-1-constraint-system">Rank 1 Constraint Systems</a> if you haven’t already.</p>
<p>To explain the workings of Circom, we will start with a few examples.</p>
<h2><strong>Example 1: Simple Multiplication</strong></h2>
<p>Assume we are trying to create ZK proofs to assess if someone knows the product of two arbitrary numbers: <code style='font-family: Arial'>c = a * b</code>.</p>
<p>Put another way, for some <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code>, we are looking to <strong>verify</strong> that the user has computed the correct value for <code style='font-family: Arial'>c</code>.</p>
<p>In pseudocode, the verification would be as follows (note, this is <em>not</em> Circom code):</p>
<pre style='font-family: Arial'><code class="language-solidity">def someVerification(a, b, c):
  res = a * b 
  assert res == c, &quot;invalid calculation&quot;
</code></pre>
<p>Consequently, our R1CS would have just one constraint, namely the following:</p>
<pre style='font-family: Arial'><code class="language-solidity">assert c == a * b
</code></pre>
<p>R1CS expresses such constraints in a structured matrix format. According to what we saw in the <a href="https://rareskills.io/post/rank-1-constraint-system">chapter on R1CS</a>, the witness vector $\mathbf{w}$ should be written as <code style='font-family: Arial'>[1, a, b, c]</code>, and the corresponding R1CS can be written as:</p>
<p>$$<br />
\begin{bmatrix}<br />
0&amp;1&amp;0&amp;0\<br />
\end{bmatrix}\mathbf{w}<br />
\circ<br />
\begin{bmatrix}<br />
0&amp;0&amp;1&amp;0\<br />
\end{bmatrix}\mathbf{w}<br />
=<br />
\begin{bmatrix}<br />
0&amp;0&amp;0&amp;1\<br />
\end{bmatrix}\mathbf{w}<br />
$$</p>
<p>If a = 3, b = 4, and c = 12, the operation above would be:</p>
<p>$$<br />
\begin{bmatrix}<br />
0&amp;1&amp;0&amp;0\<br />
\end{bmatrix}<br />
\begin{bmatrix}<br />
1\<br />
3\<br />
4\<br />
12\<br />
\end{bmatrix}<br />
\circ<br />
\begin{bmatrix}<br />
0&amp;0&amp;1&amp;0\<br />
\end{bmatrix}\begin{bmatrix}<br />
1\<br />
3\<br />
4\<br />
12\<br />
\end{bmatrix}<br />
=<br />
\begin{bmatrix}<br />
0&amp;0&amp;0&amp;1\<br />
\end{bmatrix}\begin{bmatrix}<br />
1\<br />
3\<br />
4\<br />
12\<br />
\end{bmatrix}<br />
$$</p>
<p>This is how we would write the above constraint in Circom:</p>
<pre style='font-family: Arial'><code class="language-solidity">template SomeCircuit() {
  // inputs
  signal input a;
  signal input b;
  signal input c;

  // constraints 
  c === a * b;
}

component main = SomeCircuit();
</code></pre>
<ul>
<li>Given inputs <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, <code style='font-family: Arial'>c</code>, the circuit verifies that <code style='font-family: Arial'>a * b</code> is indeed equal to <code style='font-family: Arial'>c</code>.</li>
<li>The circuit serves to verify, <strong>not</strong> to compute. This is why <code style='font-family: Arial'>c</code> (the output of the calculation) is also one of the required inputs.</li>
<li>The <code style='font-family: Arial'>===</code> operator defines the constraint as previously expressed in R1CS form. <code style='font-family: Arial'>===</code> behaves like an assertion, so the circuit will not be satisfied if invalid inputs are supplied. In the code above, <code style='font-family: Arial'>c === a * b</code> constrains <code style='font-family: Arial'>c</code> to have a value equal to the product of <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code>.</li>
</ul>
<h2>zkRepl, an online IDE for Circom</h2>
<p>For quick experiments, <a href="https://zkrepl.dev">zkRepl</a> is fantastic and convenient tool.</p>
<p>We can conveniently test the code above in zkRepl by supplying the inputs as a comment:</p>
<p><img alt="zkRepl showing the Circom compiler output" src="assets/OneConstraint.png" /></p>
<p><em><strong>Note:</strong> The input is supplied as a JSON object in a comment when using zkrepl. To test if the code compiles an the input satisfies the circuit, use shift-enter.</em></p>
<p>The “non-linear constraints” equals 1 (see the red box) because the underlying R1CS has one row constraint with a multiplication between two signals. This is to be expected since we have a single <code style='font-family: Arial'>===</code>.</p>
<h3><strong><code style='font-family: Arial'>template</code> , <code style='font-family: Arial'>component</code> , <code style='font-family: Arial'>main</code></strong></h3>
<ul>
<li><strong>Templates</strong> define a blueprint for circuits, like a class defines the structure for objects in OOP (Object Oriented Programming).</li>
<li>A <strong>component</strong> is an instantiation of a template, similar to how an object is an instance of a class in Object Oriented Programming.</li>
</ul>
<pre style='font-family: Arial'><code class="language-solidity">// create template
template SomeCircuit() {
  // .... stuff
}

// instantiate template 
component main = SomeCircuit();
</code></pre>
<p><code style='font-family: Arial'>component main = SomeCircuit()</code> is needed because Circom requires a single top-level component, <code style='font-family: Arial'>main</code>, to define the circuit structure that will be compiled.</p>
<h3><code style='font-family: Arial'>signal input</code></h3>
<ul>
<li>Signal inputs are values that will be provided from outside the component. (Circom does not enforce a value is actually provided — it is up to the developer to ensure that the values are actually supplied. If they aren’t, this can lead to a security vulnerability — this will be explored in a later chapter.)</li>
<li>Input signals are immutable and cannot be altered.</li>
<li>Signals are exactly the variables in a Rank 1 Constraint System witness vector.</li>
</ul>
<h2>The Finite Field of Circom</h2>
<p>Circom performs arithmetic in a <a href="https://rareskills.io/post/finite-fields">finite field</a> with an order of <code style='font-family: Arial'>21888242871839275222246405745257275088548364400416034343698204186575808495617</code>, which we will simply call $p$. It is a <strong>254-bit number</strong>, corresponding to the curve order of the bn128 elliptic curve. This curve is widely used, in particular it’s the one made available via precompiles in the EVM. Since Circom was intended to be used for developing ZK-SNARK applications on Ethereum, it makes sense to make the field size match the curve order of the bn128 curve.</p>
<p>Circom allows the default order to be changed via command-line argument.</p>
<p>The following should be obvious to the reader:</p>
<ul>
<li><code style='font-family: Arial'>p</code> under <code style='font-family: Arial'>mod p</code> is congruent to <code style='font-family: Arial'>0</code>;</li>
<li><code style='font-family: Arial'>p-1</code> is the largest integer in the <a href="https://rareskills.io/post/finite-field">finite field</a> <code style='font-family: Arial'>mod p.</code></li>
<li>Passing values that are larger than <code style='font-family: Arial'>p-1</code> will result in overflow.</li>
</ul>
<h2>Example 2: BinaryXY</h2>
<p>Let’s look at a second example to conclude this section.</p>
<p>Consider a circuit that verifies whether the values passed to it are binary, i.e., <code style='font-family: Arial'>0</code> or <code style='font-family: Arial'>1</code>.</p>
<p>If the input variables are <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code>, the system of constraints would be:</p>
<pre style='font-family: Arial'><code class="language-solidity">(1):  x * (x - 1) === 0
(2):  y * (y - 1) === 0
</code></pre>
<p><em>Recall that, by definition, every constraint in an R1CS can have at most one multiplication between variables.</em></p>
<p>*<strong>x<em>(x-1) === 0 checks if x is a binary digit</em></strong></p>
<ul>
<li><em>There are only 2 roots for this polynomial expression.</em></li>
<li><em>I.e., x = 0 or x = 1.</em></li>
</ul>
<p><strong>Expressed in Circom</strong></p>
<pre style='font-family: Arial'><code class="language-solidity">template IsBinary() {

  signal input x;
  signal input y;

  x * (x - 1) === 0;
  y * (y - 1) === 0;
}

component main = IsBinary();
</code></pre>
<h3><strong>Alternative Expression: Using Arrays</strong></h3>
<p>In Circom, we have the option to declare our inputs as separate signals or to declare an array which contains all the inputs. It is more conventional in Circom to group all the inputs into an <strong><em>array</em></strong> of signals called <code style='font-family: Arial'>in</code> instead of providing separate inputs <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code>.</p>
<p>Following the convention, we will represent the earlier circuit as follows. Arrays are indexed starting at zero, as you would normally expect:</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsBinary() {

  // array of 2 input signals
  signal input in[2];

  in[0] * (in[0] - 1) === 0;
  in[1] * (in[1] - 1) === 0;
}

// instantiate template 
component main = IsBinary();
</code></pre>
<h2>Only witnesses that satisfy the constraints are accepted</h2>
<p>Circom can only generate a proof for an input that actually satisfies the circuit. In the following circuit (copied from the code directly above), we supply <code style='font-family: Arial'>[0, 2]</code> as an input that only accepts {0,1} for any element of the array.</p>
<p>For 0, we have <code style='font-family: Arial'>0 * (0 - 1) === 0</code>, which is ok. However, for <code style='font-family: Arial'>2 * (2-1) === 2</code>, we have a constraint violation as indicated in the red box in the figure below.</p>
<p><img alt="zkRepl showing the Circom constraints are not satisfied" src="assets/IsBinaryViolation.png" /></p>
<h1>Circom in the command line</h1>
<p>This section introduces common Circom commands. We assume the reader has already installed Circom and the required dependencies.</p>
<p>Create a new directory and add a file called <code style='font-family: Arial'>somecircuit.circom</code> inside with the following code:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.8;

template SomeCircuit() {
  // inputs
  signal input a;
  signal input b;
  signal input c;

  // constraints 
  c  === a * b;
}

component main = SomeCircuit();
</code></pre>
<h2>1. Compiling Circuits</h2>
<p>In the terminal, execute the following command to compile:</p>
<pre style='font-family: Arial'><code class="language-solidity">circom somecircuit.circom --r1cs --sym --wasm
</code></pre>
<ul>
<li>The <code style='font-family: Arial'>--r1cs</code> flag means to output an r1cs file, the <code style='font-family: Arial'>--sym</code> flag gives the variables a human-readable name (more info can be found in the <a href="https://docs.circom.io/circom-language/formats/sym/">sym docs</a>), and <code style='font-family: Arial'>--wasm</code> is for generating wasm code to populate the witness of the R1CS, given an input JSON (shown in a later section).</li>
<li>Interchange the name of the circuit <code style='font-family: Arial'>somecircuit.circom</code> to be compiled as needed.</li>
</ul>
<p>This is the expected output:</p>
<p><img alt="circom command line result" src="assets/CircomSomeCircuitCMDLine.png" /></p>
<ul>
<li>Observe that non-linear constraints are listed as 1, indicative of <code style='font-family: Arial'>a * b === c</code>.</li>
<li>Wires is the number of columns in the R1CS. In this example, we have a constant column and three signals <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, <code style='font-family: Arial'>c</code>.</li>
</ul>
<p>The compiler creates the following:</p>
<ul>
<li><code style='font-family: Arial'>somecircuit.r1cs</code> file</li>
<li><code style='font-family: Arial'>somecircuit.sym</code> file</li>
<li><code style='font-family: Arial'>somecircuit_js</code> directory</li>
</ul>
<h3><strong>.r1cs File</strong></h3>
<ul>
<li>This file contains the circuit’s R1CS system of constraints in binary format.</li>
<li>Can be used with different tool stacks to construct proving/verifying statements (e.g. snarkjs, libsnark).</li>
</ul>
<p>Note that R1CS files are sort of like binary, in the sense that running <code style='font-family: Arial'>cat &lt;file&gt;</code> will give you gibberish.</p>
<p>Running <code style='font-family: Arial'>snarkjs r1cs print somecircuit.r1cs</code>, we get the following human-readable output:</p>
<pre style='font-family: Arial'><code class="language-solidity">[INFO]  snarkJS: [ 21888242871839275222246405745257275088548364400416034343698204186575808495616main.a ] * [ main.b ] - [ 21888242871839275222246405745257275088548364400416034343698204186575808495616main.c ] = 0
</code></pre>
<p>In Circom, arithmetic operations are conducted within a finite field, so <code style='font-family: Arial'>21888242871839275222246405745257275088548364400416034343698204186575808495616</code> is actually representative of <code style='font-family: Arial'>-1</code>. In the R1CS file however, the constraint operator is <code style='font-family: Arial'>=</code> instead of <code style='font-family: Arial'>==</code> or <code style='font-family: Arial'>===</code>.</p>
<p>We can confirm this by checking <code style='font-family: Arial'>-1 mod p</code>, (in Python: <code style='font-family: Arial'>-1 % p</code>), where <code style='font-family: Arial'>p</code> is the order of Circom’s finite field. If we translate the large values that <code style='font-family: Arial'>snarkjs r1cs print somecircuit.r1cs</code> printed to negative numbers, we get:</p>
<pre style='font-family: Arial'><code class="language-solidity">[-1 * main.a] * [main.b] - [-1 * main.c] = 0
</code></pre>
<p>We will now convert the expression above into the more familiar <code style='font-family: Arial'>a * b === c</code>. The algebra is shown next:</p>
<pre style='font-family: Arial'><code class="language-solidity">[-1 * main.a] * [main.b] - [-1 * main.c] = 0

    [-main.a] * [main.b] - [-main.c] = 0 // distribute -1

     [main.a] * [main.b] + [-main.c] = 0 // multiply both sides by -1

     [main.a] * [main.b] = [main.c] // move -main.c to the other side
</code></pre>
<p>Again, observe that this matches with the constraint (<code style='font-family: Arial'>a * b === c</code>) described in <code style='font-family: Arial'>somecircuit.circom</code>.</p>
<h3>.sym File</h3>
<p>The <code style='font-family: Arial'>somecircuit.sym</code> file is a <strong>symbols file</strong> generated during compilation. This file is essential because:</p>
<ul>
<li>It maps human-readable variable names to their corresponding positions in the R1CS for debugging.</li>
<li>It helps in printing the constraint system in a more understandable format, making it easier to verify and debug your circuit.</li>
</ul>
<h3>somecircuit_js Directory</h3>
<p>The <code style='font-family: Arial'>somecircuit_js</code> directory contains artifacts for witness generation:</p>
<ul>
<li><code style='font-family: Arial'>somecircuit.wasm</code></li>
<li><code style='font-family: Arial'>generate_witness.js</code></li>
<li><code style='font-family: Arial'>witness_calculator.js</code></li>
</ul>
<p>The <code style='font-family: Arial'>generate_witness.js</code> file is what we will use in the next section, the other two files are helpers for <code style='font-family: Arial'>generate_witness.js</code>.</p>
<p>By supplying input values for the circuit, these artifacts will calculate the necessary intermediate values and create a witness that can be used to generate a ZK proof.</p>
<h2>2. Calculating the Witness</h2>
<p>To generate the witness, we must supply the public input values for the circuit. We do this by creating an <code style='font-family: Arial'>inputs.json</code> file in the <code style='font-family: Arial'>somecircuit_js</code> directory.</p>
<p>Say we want to create a witness for input values <code style='font-family: Arial'>a=1</code>, <code style='font-family: Arial'>b=2</code>, <code style='font-family: Arial'>c=2</code>. The JSON file would be like so:</p>
<pre style='font-family: Arial'><code class="language-solidity">{&quot;a&quot;: &quot;1&quot;,&quot;b&quot;: &quot;2&quot;,&quot;c&quot;: &quot;2&quot;}
</code></pre>
<p>Circom expects strings instead of numbers because JavaScript does not work accurately with integers larger than $2^{53}$ (<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/MAX_SAFE_INTEGER">source</a>).</p>
<p>Run this command in the <code style='font-family: Arial'>somecircuit_js</code> directory:</p>
<pre style='font-family: Arial'><code class="language-solidity">node generate_witness.js **somecircuit.wasm** inputs.json witness.wtns
</code></pre>
<p>The output is the computed witness as a <code style='font-family: Arial'>witness.wtns</code> file.</p>
<p>💡</p>
<p>If we had passed values that did not honor the constraint, <code style='font-family: Arial'>a*b === c</code>, e.g. <code style='font-family: Arial'>a=1</code>, <code style='font-family: Arial'>b=2</code>, <code style='font-family: Arial'>c=3</code>, <code style='font-family: Arial'>witness_calculator.js</code> would throw an error.</p>
<h3><strong>Examine The Computed Witness: witness.wtns</strong></h3>
<p>If you run <code style='font-family: Arial'>cat witness.wtns</code>, the output is gibberish.</p>
<p><img alt="witness file cat to terminal" src="assets/BinaryFile.png" /></p>
<p>This is because <code style='font-family: Arial'>witness.wtns</code> is a binary file in a format accepted by snarkjs.</p>
<p>To get the human-readable form, we export it to JSON via: <code style='font-family: Arial'>snarkjs wtns export json witness.wtns</code>. We then view the JSON using <code style='font-family: Arial'>cat witness.json</code>:</p>
<p><img alt="witness json cat to terminal" src="assets/Witness.png" /></p>
<ul>
<li>The first <code style='font-family: Arial'>1</code> is the constant portion of the witness, which is always <code style='font-family: Arial'>1</code>. We have that <code style='font-family: Arial'>a = 1</code>, <code style='font-family: Arial'>b = 2</code>, and <code style='font-family: Arial'>c = 2</code> since our input JSON was <code style='font-family: Arial'>{"a": "1","b": "2","c": "2"}</code>.</li>
<li>snarkjs ingests the <code style='font-family: Arial'>witness.wtns</code> file to output <code style='font-family: Arial'>witness.json</code>.</li>
<li>The computed witness adheres to the R1CS layout of the witness vector: <code style='font-family: Arial'>[1, a, b, c]</code> = <code style='font-family: Arial'>[1, 1, 2, 2]</code></li>
</ul>
<h2>Example: isbinary.circom</h2>
<p>Let’s run through a less trivial example: <code style='font-family: Arial'>isbinary.circom</code>. The form of the constraints should be familiar to the reader (recall example 2).</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsBinary() {

  // array of 2 input signals
  signal input in[2];

  in[0] * (in[0] - 1) === 0;
  in[1] * (in[1] - 1) === 0;
}

// instantiate template 
component main = IsBinary();
</code></pre>
<h3><strong>Compile Circuit</strong></h3>
<ul>
<li><strong><code style='font-family: Arial'>circom isbinary.circom --r1cs --sym --wasm</code></strong></li>
<li>Sanity check on terminal output: <code style='font-family: Arial'>non-linear constraints: 2</code></li>
</ul>
<p><img alt="checking the number of r1cs constraints in the terminal" src="assets/NonLinearConstraints.png" /></p>
<p>This makes sense, as our circuit contains two assertions, each involving a multiplication of signals.</p>
<p>Next we examine the R1CS file: The command <code style='font-family: Arial'>snarkjs r1cs print isbinary.r1cs</code> results in the following output:</p>
<pre style='font-family: Arial'><code class="language-solidity">[INFO]  snarkJS: [ 218882428718392752222464057452572750885483644004160343436982041865758084956161 +main.in[0] ] * [ main.in[0] ] - [  ] = 0
[INFO]  snarkJS: [ 218882428718392752222464057452572750885483644004160343436982041865758084956161 +main.in[1] ] * [ main.in[1] ] - [  ] = 0
</code></pre>
<p>Notice that this large number is slightly different from the <strong>-1 mod p</strong> coefficient highlighted earlier (I.e.:<code style='font-family: Arial'>21888242871839275222246405745257275088548364400416034343698204186575808495616</code>)</p>
<p>Observe the additional digit, <code style='font-family: Arial'>1</code>, at the end:</p>
<ul>
<li><code style='font-family: Arial'>21888242871839275222246405745257275088548364400416034343698204186575808495616</code></li>
<li><code style='font-family: Arial'>21888242871839275222246405745257275088548364400416034343698204186575808495616(1)</code></li>
</ul>
<p>*<em>The reason there is a <code style='font-family: Arial'>1</code> at the end is due to a flaw in how snarkjs formats the output. It is “trying” to say -1</em>  1 but has no space between them.**</p>
<p>We will now algebraically transform the output of snarkjs to the original constraints of:</p>
<pre style='font-family: Arial'><code class="language-solidity">(in[0] - 1) * in[0] === 0
(in[1] - 1) * in[0] === 0
</code></pre>
<p>The derivation is as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">// original circom output
[ 218882428718392752222464057452572750885483644004160343436982041865758084956161 +main.in[0] ] * [ main.in[0] ] - [  ] = 0
[ 218882428718392752222464057452572750885483644004160343436982041865758084956161 +main.in[1] ] * [ main.in[1] ] - [  ] = 0

// remove empty terms
[ (21888242871839275222246405745257275088548364400416034343698204186575808495616)1 +main.in[0] ] * [ main.in[0] ] = 0
[ (21888242871839275222246405745257275088548364400416034343698204186575808495616)1 +main.in[1] ] * [ main.in[1] ] = 0

// rewrite p - 1 as -1
[ (-1)1 +main.in[0] ] * [ main.in[0] ] = 0
[ (-1)1 +main.in[1] ] * [ main.in[1] ] = 0

// simplify
[ main.in[0] - 1] * [ main.in[0] ] = 0
[ main.in[1] - 1] * [ main.in[1] ] = 0
</code></pre>
<h3><strong>Generating The Witness</strong></h3>
<ul>
<li>Create an <code style='font-family: Arial'>inputs.json</code> file in the <code style='font-family: Arial'>./isbinary_js</code> directory.</li>
<li>We will opt to pass values <code style='font-family: Arial'>in[0] = 1</code>, <code style='font-family: Arial'>in[1] = 0</code>.</li>
<li>We will use the following for <code style='font-family: Arial'>inputs.json</code>.</li>
</ul>
<pre style='font-family: Arial'><code class="language-solidity">{&quot;in&quot;: [&quot;1&quot;,&quot;0&quot;]}
</code></pre>
<ul>
<li>Generate <code style='font-family: Arial'>witness.wtns</code>: <code style='font-family: Arial'>node generate_witness.js isbinary.wasm inputs.json witness.wtns</code> (in the <code style='font-family: Arial'>isbinary_js</code> directory)</li>
<li>Now that <code style='font-family: Arial'>witness.wtns</code> has been created, export it to JSON, so we can examine it:<br />
<code style='font-family: Arial'>snarkjs wtns export json witness.wtns</code></li>
<li>We would get the following output on executing <code style='font-family: Arial'>cat witness.json</code>:</li>
</ul>
<pre style='font-family: Arial'><code class="language-solidity">[
 &quot;1&quot;,  // 1
 &quot;1&quot;,  // in[0] 
 &quot;0&quot;   // in[1] 
]
</code></pre>
<ul>
<li>The computed signal matches the R1CS layout of the witness vector, <code style='font-family: Arial'>[1, in[0], in[1]]</code>, as do their respective values.</li>
</ul>
<h2>Generating a ZK Proof</h2>
<p>Once the R1CS has been created, the reader can follow the steps in the <a href="https://docs.circom.io/getting-started/proving-circuits/">Circom documentation to generate the ZK Proof</a> and an accompanying smart contract verifier.</p>
<h2>Practice Problems</h2>
<p>Test your understanding/learning from this chapter by solving these puzzles from our ZK Puzzles repo. Each puzzle requires you to fill in the missing logic. You can check your answers simply by running the unit tests.</p>
<ul>
<li><a href="https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/BinaryXY/BinaryXY.circom">https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/BinaryXY/BinaryXY.circom</a></li>
<li><a href="https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/MultiplyNoOut/MultiplyNoOut.circom">https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/MultiplyNoOut/MultiplyNoOut.circom</a></li>
<li><a href="https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/FourBitBinary/FourBitBinary.circom">https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/FourBitBinary/FourBitBinary.circom</a></li>
</ul>
<div style='page-break-after: always;'></div>

<h1>Circom Template Parameters, Variables, Loops, If Statements, Assert</h1>
<p>Source: https://rareskills.io/post/circom-syntax</p>
<h1>Circom Template Parameters, Variables, Loops, If Statements, Assert</h1>
<p>This chapter covers essential syntax, which you’ll see in most Circom programs. With Circom, we’re able to define a Rank 1 Constraint System (R1CS) using code instead of explicitly defining each constraint. We’ll explore those tools in this chapter.</p>
<h2><strong>Template Parameters</strong></h2>
<p>Previously, we looked at a circuit (<code style='font-family: Arial'>IsBinary</code>) that verified whether the supplied inputs were indeed binary. That circuit was hardcoded to accept only 2 inputs.</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsBinary() {

  signal input in[2];

  in[0] * (in[0] - 1) === 0;
  in[1] * (in[1] - 1) === 0;
}

component main = IsBinary();
</code></pre>
<p>While the above code works for two inputs, modifying it to support large <code style='font-family: Arial'>n</code> inputs would require manually adding constraints, which is a bad developer experience.</p>
<p>Therefore, Circom allows us to constrain an arbitrary number of signals using the following pattern to automatically generate the constraints:</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsBinary(n) {

  // array of n inputs
  signal input in[n];

  // n loops: n constraints
  for (var i = 0; i &lt; n; i++) {
    in[i] * (in[i] - 1) === 0;
  }
}

// instantiated w/ 4 inputs &amp; 4 constraints
component main = IsBinary(4);
</code></pre>
<p>Notice that the template declaration has changed to include <code style='font-family: Arial'>n</code> in the parenthesis.</p>
<ul>
<li><code style='font-family: Arial'>n</code> here is known as a template parameter</li>
<li><code style='font-family: Arial'>n</code> is used within the circuit to specify the size of the array <code style='font-family: Arial'>in</code></li>
<li>on instantiating the template, we must specify the value of <code style='font-family: Arial'>n</code></li>
</ul>
<h2>Circuits and constraints in Circom must have a fixed, known structure</h2>
<p>Although constraints can be generated programmatically, <strong>the existence and configuration of constraints cannot conditionally depend on signals.</strong></p>
<p>While templates can use parameters, the circuit must be static and clearly defined. There is no support for “dynamic-length” circuits or constraints — everything must be fixed and well-defined from the start.</p>
<p>Imagine having an R1CS system of constraints whose structure was mutable based on input signal values. Neither the prover nor the verifier could operate as the number of constraints is not set in stone.</p>
<p>The value for <code style='font-family: Arial'>n</code> must be set at compile time.</p>
<h2>For loop and Variables: <code style='font-family: Arial'>for</code>, <code style='font-family: Arial'>var</code></h2>
<p>We now explain the <code style='font-family: Arial'>for</code> loop introduced above.</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsBinary(n) {

  // array of n inputs
  signal input in[n];

  // n loops: n constraints
  for (var i = 0; i &lt; n; i++) {
    in[i] * (in[i] - 1) === 0;
  }
}

// instantiated with 4 inputs &amp; 4 constraints
component main = IsBinary(4);
</code></pre>
<ul>
<li>both inputs and loop iterations are defined by <code style='font-family: Arial'>n</code></li>
<li>for each input, a constraint is defined with the purpose of verifying that the input is either <code style='font-family: Arial'>0</code> or <code style='font-family: Arial'>1</code></li>
</ul>
<p>We have introduced two new keywords into the circuit: <code style='font-family: Arial'>for</code> and <code style='font-family: Arial'>var</code></p>
<ul>
<li><code style='font-family: Arial'>for</code> works like you’re used to.</li>
<li>The <code style='font-family: Arial'>var</code> keyword declares a <strong>variable</strong>; in this case, <code style='font-family: Arial'>i</code>, as seen in the loop definition.</li>
<li>The equal symbol <code style='font-family: Arial'>=</code> assigns the value on the right to the variable on the left.</li>
</ul>
<p>Here, the variable <code style='font-family: Arial'>i</code> is used to programmatically refer to different signals in the input array while creating constraints for them. Being able to programmatically generate constraints is extremely useful, as doing this by hand when hundreds or thousands of constraints are involved would be extremely error-prone.</p>
<h3>Variables</h3>
<p>Variables hold non-signal data and are mutable. Here is an example of a variable declaration outside of a loop:</p>
<pre style='font-family: Arial'><code class="language-solidity">template VariableExample(n) {
  var acc = 2;
  signal s;
}
</code></pre>
<ul>
<li>By default, variables are <strong>not</strong> part of the R1CS system of constraints.</li>
<li>We will shortly see that variables can be used as additive or multiplicative constants inside the R1CS.</li>
<li>Variables are used to compute values outside the R1CS to help define the R1CS.</li>
<li>When working with variables, Circom behaves like a normal programming language.</li>
<li>Math operations are done modulo <code style='font-family: Arial'>p</code>. The full list of operators is provided in the <a href="https://docs.circom.io/circom-language/basic-operators/#arithmetic-operators">Circom documentation here</a>. These will feel familiar coming from a C-like language (e.g. <code style='font-family: Arial'>++</code>, <code style='font-family: Arial'>**</code>, <code style='font-family: Arial'>&lt;=</code>, etc). However, keep in mind that <code style='font-family: Arial'>/</code> means multiplication with the multiplicative inverse, and <code style='font-family: Arial'>\</code> means integer division.</li>
<li>However, the only valid operators for signals are <code style='font-family: Arial'>+</code>, <code style='font-family: Arial'>*</code>, <code style='font-family: Arial'>===</code>, <code style='font-family: Arial'>&lt;--</code>, and <code style='font-family: Arial'>&lt;==</code>. We will discuss <code style='font-family: Arial'>&lt;--</code> and <code style='font-family: Arial'>&lt;==</code> in a later article.</li>
</ul>
<h2>If statements</h2>
<p>Circom allows us to conditionally create constraints using <code style='font-family: Arial'>if</code> statements — but these conditions must be deterministic and known at compile time. An example is shown next:</p>
<h3>Example: Enforcing Equality on the Even Indexes</h3>
<p>Suppose we have two arrays. We could use the following template to generate constraints which enforce that the items at even indices are equal (without checking the odds)</p>
<pre style='font-family: Arial'><code class="language-solidity">template EqualOnEven(n) {
  signal input in1[n];
  signal input in2[n];

  for (var i = 0; i &lt; n; i++) {
    if (i % 2 == 0) {
      in1[i] === in2[i];
    }
    // otherwise no constraint is generated
  }
}
</code></pre>
<p>Note that the variable <code style='font-family: Arial'>i</code> dictates which constraints get generated.</p>
<h3>Signals cannot be used for branching conditions in if statements or for loops</h3>
<p>The following code is not allowed because signal <code style='font-family: Arial'>a</code> is used as the conditional for the <code style='font-family: Arial'>if</code> statement:</p>
<pre style='font-family: Arial'><code class="language-solidity">template IfStatementViolation() {
  signal input a;
  signal input b;

  if (a == 2) {
    b === 3;
  }
  else {
    b === 4;
  }
}
</code></pre>
<p>In a Rank 1 Constraint System, there can only be addition and multiplication between signals. Circom is only a thin wrapper on top of a Rank 1 Constraint System. Therefore, it cannot “translate” an if statement to addition and multiplication.</p>
<p>It is still possible to do a conditional operation (if statement) based on signals in Circom — this is the subject of a later chapter. But for now, consider that there is no “direct” translation from an <code style='font-family: Arial'>if</code> statement to a single multiplication.</p>
<h3>Using Variables as Part of Constraints</h3>
<p>Variables can be used as part of constraints. In the example below, we enforce that the input array <code style='font-family: Arial'>in[n]</code> is a Fibonacci sequence. Note that a variable array syntax is <code style='font-family: Arial'>var varName[size]</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsFib(n) {
  assert(n &gt; 1);

  signal input in[n];

  // generate the Fibonacci sequence
  var correctFibo[n];
  correctFibo[0] = 0;
  correctFibo[1] = 1;

  for (var i = 2; i &lt; n; i++) {
    correctFibo[i] = correctFibo[i - 1] + correctFibo[i - 2];
  }


  // assert that the input is a Fibonacci sequence
  for (var i = 0; i &lt; n; i++) {
    in[i] === correctFibo[i];
  }
}
</code></pre>
<p>Of note:</p>
<ul>
<li>The <code style='font-family: Arial'>assert(n &gt; 1)</code> does not generate any constraints. It prevents the template from getting instantiated if the condition for the template parameter is not met.</li>
<li>We can enforce that a signal has a certain value by doing <code style='font-family: Arial'>signal === var</code>. This is the same as doing <code style='font-family: Arial'>signal === 5</code> or some other constant.</li>
</ul>
<h3>Circom Does Not Have a Constant Keyword</h3>
<p>Instead, we can use variables to assign a name to a magic number to improve readability. For example:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Equality() {
  signal input in[2];

  var left = 0;
  var right = 1;

  // require the inputs
  // to be equal
  in[left] === in[right];
}
</code></pre>
<h2>Variables Can Be Added to and Multiplied by Other Signals</h2>
<p>In Circom, variables can be added to or multiplied by signals, just like constants. In the example below, we require that <code style='font-family: Arial'>in2[]</code> is <code style='font-family: Arial'>in1[]</code> multiplied by its index.</p>
<p>For example, if <code style='font-family: Arial'>in1[] = [3,5,6]</code> then it must be the case that <code style='font-family: Arial'>in2[] = [0,5,12]</code> because <code style='font-family: Arial'>[3,5,6]</code> gets element-wise multiplied by <code style='font-family: Arial'>[0,1,2]</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsIndexMultiplied(n) {
  signal input in1[n];
  signal input in2[n];

  for (var i = 0; i &lt; n; i++) {
    in1[i] * i === in2[i];
  }
}

component main = IsIndexMultiplied(3);

/* INPUT = {&quot;in1&quot;: [0,1,2], &quot;in2&quot;: [0,1,4]} */

// accept
// in1[] = [0,1,2]
// in2[] = [0,1,4]

// reject
// in1[] = [0,1,2]
// in2[] = [0,0,2]
</code></pre>
<p>You can test the code <a href="https://zkrepl.dev">here</a>.</p>
<h2>Key Takeaways</h2>
<ul>
<li>Behind the scenes, if variables are added or multiplied with a signal, the variable gets compiled to a constant in the R1CS.</li>
<li>For signals, it is not allowed to do operations other than addition, subtraction, or multiplication because an R1CS can only have addition or multiplication with a constant. Subtraction behind the scenes is simply addition with the additive inverse.</li>
<li>If a signal is divided by a constant (or a variable holding a constant), it will multiply that signal by the multiplicative inverse of the constant unless the constant is 0, in which case the code will not compile.</li>
</ul>
<h2>Practice problems</h2>
<p>Try out the following problems from ZK Puzzles. Run the tests to check your answer.</p>
<ul>
<li><a href="https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/AllBinary/AllBinary.circom">https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/AllBinary/AllBinary.circom</a></li>
<li><a href="https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/MultiANDNoOut/MultiANDNoOut.circom">https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/MultiANDNoOut/MultiANDNoOut.circom</a></li>
<li><a href="https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/IncreasingDistance/IncreasingDistance.circom">https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/IncreasingDistance/IncreasingDistance.circom</a></li>
</ul>
<div style='page-break-after: always;'></div>

<h1>Quadratic Constraints</h1>
<p>Source: https://rareskills.io/post/quadratic-constraints</p>
<h1>Quadratic Constraints</h1>
<h2>Circom Constraints</h2>
<p>A Rank 1 Constraint System has at most one multiplication between signals per constraint. This is called a “quadratic” constraint. Any constraint containing an operation other than addition or multiplication will be rejected by Circom with the “Non quadratic constraints are not allowed” error.</p>
<p>The following two examples will not compile because they have more than one multiplication of signals per constraint.</p>
<h2>Non quadratic constraint example 1</h2>
<p>Compiling the following will result in the following <code style='font-family: Arial'>error: [T3001]: Non quadratic constraints are not allowe</code></p>
<pre style='font-family: Arial'><code class="language-solidity">template QuadraticViolation1() {
  signal input a;
  signal input b;
  signal input c;
  signal input d;

  // two multiplications per constraint
  // is not allowed
  a * b === c * d;
}
</code></pre>
<h2>Non quadratic constraint example 2</h2>
<p>Similar to the previous example, the following constraint has two multiplications between signals.</p>
<pre style='font-family: Arial'><code class="language-solidity">template QuadraticViolation2() {
  signal input a;
  signal input b;
  signal input c;
  signal input d;

  // two multiplications per constraint
  // is not allowed
  a * b * c === d;
}
</code></pre>
<h2>Constant multiplications do not count</h2>
<p>Therefore, the following examples will compile, even though there is more than one multiplication.</p>
<pre style='font-family: Arial'><code class="language-solidity">a * b === c;

2*a * 3*b === 4*c; // integer coefficients allowed

a * b + c === d; // addition and one multiplication allowed

a + b + c === d; // multiplication is optional

a * b + c === d + e + f; // no restrictions on number of additions
</code></pre>
<h2>Quadratic Form and R1CS</h2>
<p>Recall that in <a href="https://rareskills.io/post/arithmetic-circuit">arithmetization</a>, we flatten our verification program into a series of intermediate steps, where each intermediate step only contains a single multiplication between unknown variables.</p>
<p><strong>Consider the following verification example:</strong></p>
<pre style='font-family: Arial'><code class="language-solidity">def someProblem(x, y, out):
  res = y^2 + 4*(x^2)*y -2 
  assert out == res, &quot;incorrect inputs&quot;;
</code></pre>
<p><strong>Conversion to an R1CS would yield us:</strong></p>
<pre style='font-family: Arial'><code class="language-solidity">v1  === y * y
v2  === x * x 
out === v1 + (4v2 * y) - 2
</code></pre>
<ul>
<li>The R1CS format requires us to restructure the problem into intermediate steps that only have 1 multiplication operation between signals to adhere the quadratic constraint limitation.</li>
<li>This creates our system of constraints.</li>
</ul>
<p><strong>Consequently, the R1CS representation would be:</strong></p>
<pre style='font-family: Arial'><code class="language-solidity">//     Cw = Aw * Bw
       v1  = y * y
       v2  = x * x 
out -v1 +2 = (4v2 * y)
</code></pre>
<p>Since we previously ensured that there was only 1 multiplication per constraint, we are able to express the system of constraints in vector form, which is an R1CS.</p>
<h2>Examples of Non-multiplicative Operators Causing a Non quadratic Constraint</h2>
<p>If an illegal operation (not addition or multiplication) is used in a constraint, the Circom compiler will report the “Non quadratic constraints are not allowed!” error.</p>
<p>Here, we provide some examples.</p>
<h3>Example 1: Signals Cannot Be Used to Index Signal Arrays</h3>
<p>The following operation will result in a quadratic constraints violation. There is no direct translation from array indexing to addition and multiplication. The following code results in the error <code style='font-family: Arial'>Non-quadratic constraint was detected statically, using unknown index will cause the constraint to be non-quadratic</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">template KMustEqual5(n) {

  signal input in[n];
  signal input k;

  // not allowed
  in[k] === 5;
}
</code></pre>
<p>It is still technically possible to accomplish array indexing, but this requires a more complex solution we will show in a later chapter.</p>
<h3>Example 2: Signals Cannot Use Operations Such as % and &lt;&lt;</h3>
<p>The following constraints will create a “Non quadratic constraints are not allowed!” violation:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Example() {
  signal input a;
  signal input b;

  // not allowed
  a === b % 5;

  // not allowed
  a === b &lt;&lt; 2;
}
</code></pre>
<h2>How Circom handles division</h2>
<p>Somewhat subtly, Circom will allow “division” by a constant, because it can simply be replaced by multiplication by that number’s multiplicative inverse. As such, the following code is valid:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Example() {
  signal input a;
  signal input b;

  a === b / 2;
}

component main = Example();
</code></pre>
<p>However, dividing signals is not allowed because that means we computed the signal’s multiplicative inverse, which does not have a direct translation to only addition and multiplication. Computing the multiplicative inverse is usually done with the <a href="https://en.wikipedia.org/wiki/Extended_Euclidean_algorithm">extended euclidean algorithm</a>, which requires loops and conditional statements — operations that cannot be natively expressed with addition and multiplication.</p>
<pre style='font-family: Arial'><code class="language-solidity">template Example() {
  signal input a;
  signal input b;
  signal input c;

  // not allowed
  a === b / c;
}

component main = Example();
</code></pre>
<p>In contrast, subtraction of signals is allowed because it directly translates to multiplication by the constant <code style='font-family: Arial'>-1</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Example() {
  signal input a;
  signal input b;

  // allowed
  a === b - a;

  // equivalent
  a === b + -1*a
}

component main = Example();
</code></pre>
<p>Integer division, as opposed to multiplication by the modular inverse is represented by the <code style='font-family: Arial'>\</code> and it is not allowed to be applied to signals:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Example() {
  signal input a;
  signal input b;

  // can only use \ with variables
  // not signals
  a === b \ 2;
}

component main = Example();
</code></pre>
<p>For variables, you have both integer division and “normal” division (i.e., multiplication with the multiplicative inverse of the divisor).</p>
<p>For signals, on the other hand, only “normal” division (in the above sense) is allowed.</p>
<h2>Summary</h2>
<p>A constraint can only have one multiplication between signals, but there is no limit on the number of additions.</p>
<p>It might seem that this restriction makes it impossible to express any interesting computation beyond simple arithmetic, but we will see later in this tutorial series that there exist numerous clever design patterns to work around this limitation.</p>
<p>Once we understand the design patterns, we can compose them to model much more complex algorithms.</p>
<div style='page-break-after: always;'></div>

<h1>Symbolic Variables in Circom</h1>
<p>Source: https://rareskills.io/post/circom-symbolic-variable</p>
<h1>Symbolic Variables in Circom</h1>
<p>A symbolic variable in Circom is a variable that has been assigned values from a signal.</p>
<p>When a signal is assigned to a variable (thereby turning it into a symbolic variable), the variable becomes a container for that signal and for any arithmetic operations applied to it. A symbolic variable is declared using the <code style='font-family: Arial'>var</code> keyword, just like other variables.</p>
<p>For example, the following two circuits are equivalent, i.e. they produce the same underlying R1CS:</p>
<pre style='font-family: Arial'><code class="language-solidity">template ExampleA() {
    signal input a;
    signal input b;
    signal input c;

    a * b === c;
}

template ExampleB() {
    signal input a;
    signal input b;
    signal input c;

    // symbolic variable v &quot;contains&quot; a * b
    var v = a * b;

    // a * b === c under the hood
    v === c;
}
</code></pre>
<p>In <code style='font-family: Arial'>ExampleB</code>, the symbolic variable <code style='font-family: Arial'>v</code> is simply a placeholder for the expression <code style='font-family: Arial'>a * b</code>. Both <code style='font-family: Arial'>ExampleA</code> and <code style='font-family: Arial'>ExampleB</code> are compiled using the exact same R1CS, and there is zero functional difference between them.</p>
<h2>Use cases of symbolic variables</h2>
<h3>Checking That $\sum\texttt{in}[i]=\texttt{sum}$</h3>
<p>Symbolic variables are extremely handy if we want to sum up an array of signals in a loop. In fact, summing signals in a loop is their most common use case:</p>
<pre style='font-family: Arial'><code class="language-solidity">// assert sum of in === sum
template Sum(n) {
    signal input in[n];
    signal input sum;

    var accumulator;
    for (var i = 0; i &lt; n; i++) {
        accumulator += in[i];
    }

    // in[0] + in[1] + in[2] + ... + in[n - 1] === sum
    accumulator === sum;
}
</code></pre>
<h3>Checking That <code style='font-family: Arial'>in</code> Is a Valid Binary Representation of <code style='font-family: Arial'>k</code></h3>
<p>A more interesting example is proving that <code style='font-family: Arial'>in[n]</code> is the binary representation of <code style='font-family: Arial'>k</code> for a templated value of <code style='font-family: Arial'>n</code>. In the circuit below, we check that:</p>
<p>$$<br />
\texttt{in[0]}+2\cdot\texttt{in[1]}+4\cdot\texttt{in[2]} +\dots2^{n-1}\cdot\texttt{in[n-1]} == k<br />
$$</p>
<p>If all the signals in <code style='font-family: Arial'>in</code> are constrained to be $\set{0,1}$, then that implies <code style='font-family: Arial'>in[]</code> is the binary representation of <code style='font-family: Arial'>k</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsBinaryRepresentation(n) {

    signal input in[n];
    signal input k;

    // in is binary only
    for (var i = 0; i &lt; n; i++) {
        in[i] * (in[i] - 1) === 0;
    }

    // in is the binary representation of k
    var acc; // symbolic variable
    var powersOf2 = 1; // regular variable
    for (var i = 0; i &lt; n; i++) {
        acc += powersOf2 * in[i];
        powersOf2 *= 2;
    }

    acc === k;
}
</code></pre>
<h3>Why symbolic variables are helpful</h3>
<p>Consider the earlier example of proving that $\sum\texttt{in}[i]=\texttt{sum}$. Without symbolic variables, it’s very clumsy to express</p>
<pre style='font-family: Arial'><code class="language-solidity">sum === in[0] + in[1] + in[2] + ... + in[n-1];
</code></pre>
<p>if we don’t know what <code style='font-family: Arial'>n</code> is in advance. Even if <code style='font-family: Arial'>n</code> was fixed, say at 32, actually typing out 32 variables by hand would be annoying. Thus, symbolic variables enable us to incrementally construct <code style='font-family: Arial'>in[0] + in[1] + in[2] + ...</code> without explicitly writing out the signals.</p>
<h2>Non quadratic Constraints With Symbolic Variables</h2>
<p>Because symbolic variables can “contain” a multiplication between two signals, they can lead to embedding two multiplications into one constraint if we aren’t careful. Consider the following example, that will not compile:</p>
<pre style='font-family: Arial'><code class="language-solidity">template QViolation() {
    signal input a;
    signal input b;
    signal input c;
    signal input d;

    // v &quot;contains&quot; a * b
    var v = a * b;

    // error: there are two
    // multiplications
    // in this constraint
    v === c * d;
}
</code></pre>
<p>In the code above, the symbolic variable <code style='font-family: Arial'>v</code> has one multiplication in it and we declared <code style='font-family: Arial'>v == a*b</code>. So the constraint <code style='font-family: Arial'>v === c * d;</code> is equivalent to <code style='font-family: Arial'>a * b = c * d;</code>. Hence, the above code will not compile.</p>
<h2>Arbitrary operators are allowed with non-symbolic variables</h2>
<p>Doing operations like computing the modulo or bitshifting are allowed with (non-symbolic) variables. However, this means that the variable can no longer be used as part of a constraint:</p>
<pre style='font-family: Arial'><code class="language-solidity">// this has no constraints
// but it will compile
template Ok() {
    signal input a;
    signal input b;

    var v = a % b;
}
</code></pre>
<p>The above example will compile because <code style='font-family: Arial'>v</code> is not used in a constraint. However, if we use <code style='font-family: Arial'>v</code> in a constraint, the code will not compile. An example of this is shown below:</p>
<pre style='font-family: Arial'><code class="language-solidity">template NotOk() {
    signal input a;
    signal input b;
    signal input c;

    var v = a % b;

    // non-quadratic constraint
    c === v;
}
</code></pre>
<h3>Symbolic variables cannot be used to determine the boundary of a loop or the condition</h3>
<p>Similarly, only regular variables can be used to determine the boundary of a loop or the condition of an if statement. If a symbolic variable is used, then the code will not compile:</p>
<pre style='font-family: Arial'><code class="language-solidity">template NotOk() {
    signal input a;
    signal input b;
    signal input c;

    var v = a * b;

    // v is a symbolic variable
    // used in an if statement
    if (v == 0) {
        c === 0;
    } else {
        c === 1;
    }
}
</code></pre>
<h2>Summary</h2>
<p>Symbolic variables are variables that were assigned a value from a signal. They are most frequently used for adding a parameterizable number of signals together, as the sum can be accumulated in a for loop. They are effectively a “container” or “bucket” that holds either a single signal or a collection of signals that are added or multiplied together. If a variable is never assigned a value from a signal, then it is not a symbolic variable.</p>
<p>Since symbolic variables contain signals, care must be taken to avoid quadratic constraint violations when using them.</p>
<div style='page-break-after: always;'></div>

<h1>Intermediate Signals and Sub-Component</h1>
<p>Source: https://rareskills.io/post/circom-intermediate-signals</p>
<h1>Intermediate Signals and Sub-Component</h1>
<p>Circom’s primary purpose is to compile down to a Rank 1 Constraint System (R1CS), but its secondary purpose is to populate the witness.</p>
<p>For most circuits, the value of a few signals determines what the rest of the signals will be.</p>
<p>For example, it may seem a bit redundant to supply <code style='font-family: Arial'>c</code> as an input to the following template because its value is completely dependent on <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Mul() {
  signal input a;
  signal input b;
  signal input c;

  c === a * b;
}
</code></pre>
<p>A more motivating example follows next.</p>
<h2>Breaking up a non-quadratic constraint</h2>
<p>Suppose we want to create an R1CS for <code style='font-family: Arial'>a * b * c === d</code>. Since R1CS allows one multiplication per constraint, we have to create another signal <code style='font-family: Arial'>s</code> and an additional constraint to break up the multiplication:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Mul3() {
  signal input a;
  signal input b;
  signal input c;
  signal input d;

  signal input s;

  s === a * b;
  d === s * c;
}
</code></pre>
<p>It would be extremely tedious to supply another input every time we do more than one multiplication, especially in larger circuits with numerous multiplications. Furthermore, the value for <code style='font-family: Arial'>s</code> in the example above is deterministically dependent on <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code>.</p>
<h2>Intermediate signals and assignment</h2>
<p>To avoid the hassle of supplying <code style='font-family: Arial'>s</code>, Circom offers the <code style='font-family: Arial'>==&gt;</code> and <code style='font-family: Arial'>&lt;==</code> operators that assigns the value of <code style='font-family: Arial'>s</code> to be calculated by Circom (remember that part of Circom’s functionality is to generate the witness). Thus, the value of <code style='font-family: Arial'>s</code> will not need to be supplied as an input. The <code style='font-family: Arial'>==&gt;</code> and <code style='font-family: Arial'>&lt;==</code> operators (precisely) means “assign and constrain:”</p>
<pre style='font-family: Arial'><code class="language-solidity">template Mul3() {
  signal input a;
  signal input b;
  signal input c;
  signal input d;

  // no longer an input
  signal s;

  a * b ==&gt; s;
  s * c === d;
}
</code></pre>
<p>Circom is flexible on the direction of the arrow, <code style='font-family: Arial'>a * b ==&gt; s</code> means the same as <code style='font-family: Arial'>s &lt;== a * b</code>.</p>
<p>In the code above, <code style='font-family: Arial'>s</code> is called an <em>intermediate signal</em>. An intermediate signal is a signal defined as <code style='font-family: Arial'>signal</code> keyword without the <code style='font-family: Arial'>input</code> keyword. Therefore, <code style='font-family: Arial'>signal s</code> is an intermediate signal, but <code style='font-family: Arial'>signal input a</code> is not.</p>
<p><strong>The underlying R1CS is identical between the two templates above. The <code style='font-family: Arial'>==&gt;</code> simply saves us the hassle of supplying the value for <code style='font-family: Arial'>s</code> as part of the input.</strong></p>
<p>Assuming the witness vector $\mathbf{w}$ is represented as <code style='font-family: Arial'>[1, a, b, c, d, s]</code>, the underlying R1CS would be as follows:</p>
<p>$$<br />
\begin{bmatrix}<br />
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1<br />
\end{bmatrix}\mathbf{w}<br />
\circ<br />
\begin{bmatrix}<br />
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\<br />
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0<br />
\end{bmatrix}\mathbf{w}=<br />
\begin{bmatrix}<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0<br />
\end{bmatrix}\mathbf{w}<br />
$$</p>
<p>This can be thought of as passing Circom the witness <code style='font-family: Arial'>[1, a, b, c, d, _]</code> and Circom computing the full witness <code style='font-family: Arial'>[1, a, b, c, d, s]</code> based on the input.</p>
<p><strong>Assignment to <code style='font-family: Arial'>s</code> happens outside the R1CS. The R1CS only checks that a matrix equation is satisfied by the witness vector $\mathbf{w}$. The R1CS expects the witness to be provided and does not compute any of its values. This approach simplifies circuit design and reduces the manual effort while keeping the R1CS structure unchanged.</strong></p>
<h2>Signal Values Cannot Be Re-Assigned With <code style='font-family: Arial'>&lt;==</code></h2>
<p>A signal represents a concrete entry in the witness vector. Thus, it cannot change the value once it is set. As such, the following code will not compile:</p>
<pre style='font-family: Arial'><code class="language-solidity">template CannotReassign() {
  signal input a;
  signal input b;

  signal c;

  c &lt;== a * b;

  // not allowed
  // c already set
  c &lt;== a * a;
}
</code></pre>
<h2>Real Example: Checking the Product of an Array</h2>
<p>The more multiplications we have in our circuit, the more handy the <code style='font-family: Arial'>==&gt;</code> operator becomes because it saves on having to supply additional input signals.</p>
<p>Suppose we wanted to enforce that the input signal <code style='font-family: Arial'>k</code> is the result of the product of all the signals in the array <code style='font-family: Arial'>in[n]</code>. In other words, we are checking:</p>
<p>$$<br />
\prod_{i=0}^{n – 1}\texttt{in}[i]===k<br />
$$</p>
<p>This would introduce a significant amount of intermediate signals. To keep the code clean, we can have all the intermediate signals be assigned to a separate array as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">template KProd(n) {
  signal input in[n];
  signal input k;

  // intermediate signal array
  signal s[n];

  s[0] &lt;== in[0];
  for (var i = 1; i &lt; n; i++) {
    s[i] &lt;== s[i - 1] * in[i];
  }

  k === s[n - 1];
}
</code></pre>
<p>Based on the code above, <code style='font-family: Arial'>s[n - 1]</code> holds the value</p>
<p>$$<br />
\prod_{i=0}^{n – 1}\texttt{in}[i]<br />
$$</p>
<p>which we can then constrain to be equal to <code style='font-family: Arial'>k</code>.</p>
<h2>Breaking Circom Into Templates</h2>
<p>Now that we understand the <code style='font-family: Arial'>&lt;==</code> operator, we can understand how Circom uses templates to make code more modular.</p>
<p>Similar to our example <code style='font-family: Arial'>Mul3</code>, suppose we have a circuit that takes 3 inputs and enforces that their product is a 4th (here is the code reproduced):</p>
<pre style='font-family: Arial'><code class="language-solidity">template Mul3() {
  signal input a;
  signal input b;
  signal input c;
  signal input d; // d === a * b * c

  // no longer an input
  signal s;

  a * b ==&gt; s;
  s * c === d;
}
</code></pre>
<p>But suppose we had to do this twice with eight inputs. In this case, it might be tempting to copy and paste the code twice for the inputs (a,b,c,d), and (x,y,z,u), which would be ugly.</p>
<pre style='font-family: Arial'><code class="language-solidity">template Mul3x2() {
  signal input a;
  signal input b;
  signal input c;
  signal input d; // d === a * b * c

  signal input x;
  signal input y;
  signal input z;
  signal input u; // u === x * y * z

  // ugly code here
}
</code></pre>
<p>Instead, we can put <code style='font-family: Arial'>Mul3</code> as a separate template as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">// separate template
template Mul3() {
  signal input a;
  signal input b;
  signal input c;
  signal input d; // d === a * b * c

  // no longer an input
  signal s;

  a * b ==&gt; s;
  s * c === d;
}

// main component
template Mul3x2() {
  signal input a;
  signal input b;
  signal input c;
  signal input d; // d === a * b * c

  signal input x;
  signal input y;
  signal input z;
  signal input u; // u === x * y * z

  component m3_1 = Mul3();
  m3_1.a &lt;== a;
  m3_1.b &lt;== b;
  m3_1.c &lt;== c;
  m3_1.d &lt;== d;

  component m3_2 = Mul3();
  m3_2.a &lt;== x;
  m3_2.b &lt;== y;
  m3_2.c &lt;== z;
  m3_2.d &lt;== u;
}
</code></pre>
<p>Of note:</p>
<ul>
<li>We declare components with the syntax <code style='font-family: Arial'>component m3_1 = Mul3();</code>. This is the same syntax we use to declare the main component.</li>
<li>We “connect” the signals using the <code style='font-family: Arial'>&lt;==</code> operator.</li>
<li>The code above is entirely equivalent to copying and pasting the core logic of <code style='font-family: Arial'>Mul3</code> twice.</li>
</ul>
<h2>Passing Results Back From Templates</h2>
<p>It would be handy in some situations if a sub-component could “pass results back” to the component that created it.</p>
<p>For example, the following main component uses a sub-component <code style='font-family: Arial'>Square</code> to assign and constrain <code style='font-family: Arial'>out</code> to be the square of <code style='font-family: Arial'>in</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">template Square() {
  signal input in;
  signal output out;

  out &lt;== in * in;
}

template Main() {
  signal input a;
  signal input b;
  signal input sumOfSquares;

  component a2 = Square();
  component b2 = Square();

  a2.in &lt;== a;
  b2.in &lt;== b;

  // assert that a^2 + b^2 === sum of Squares
  a2.out + b2.out === sumOfSquares;
}

component main = Main();
</code></pre>
<p><strong>In the context of sub-components, an output signal is a signal that expects to be assigned a value via the <code style='font-family: Arial'>&lt;==</code> operator and can be used to pass values back to the component that created it.</strong></p>
<p>In the context of the <code style='font-family: Arial'>main</code> component — an output signal means something entirely different — we will explain that in a later chapter.</p>
<h2>Example: Binary to Number</h2>
<p>The <a href="https://github.com/iden3/circomlib">circomlib library</a> is a library of Circom templates for various common operations. One such operation is to convert a binary array to a signal. We have seen previously that this can be accomplished with $b_0+2b_1+4b_2+…+2^{n-1}b_{n-1}=v$. Here is how we can do it in a separate component. The following template can be found in the <a href="https://github.com/iden3/circomlib/blob/master/circuits/bitify.circom">bitify.circom</a> file of the Circom library:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Bits2Num(n) {
  signal input in[n];
  signal output out;

  // lc is short for &quot;linear combination&quot;
  // it serves as an accumulator variable
  var lc1=0;

  var e2 = 1;
  for (var i = 0; i&lt;n; i++) {
    lc1 += in[i] * e2;
    e2 += e2 + e2; // could also be e2 *= 2;
  }

  lc1 ==&gt; out;
}
</code></pre>
<p>We don’t need to copy and paste code from the library — it can be “included” similar to how other languages import other files:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/bitify.circom&quot;;

template Main(n) {
  signal input in[n];
  signal input v;

  // instantiate the Bits2Num component
  component b2n = Bits2Num(n);

  // loop over each binary value
  // and assign and constrain it to the
  // b2n input array
  for (var i = 0; i &lt; n; i++) {
    b2n.in[i] &lt;== in[i];
  }

  b2n.out === v;
}

component main = Main(4);

/* INPUT = {&quot;in&quot;: [1, 0, 0, 1], &quot;v&quot;: 9} */
</code></pre>
<p>The above component can be tested in zkrepl, but if running locally, the import path needs to be set according to how the directory is configured. Typically, Circomlib is <a href="https://www.npmjs.com/package/circomlib">installed</a> with yarn or npm.</p>
<h2>One line component example</h2>
<p>Rather than assign the input signals to a component separately, it is possible to provide them as an argument. This is called an “<a href="https://docs.circom.io/circom-language/anonymous-components-and-tuples/">anonymous component</a>.” Consider the following example:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Mul() {
  signal input in[2];
  signal output out;

  out &lt;== in[0] * in[1];
}

template Example() {

  signal input a;
  signal input b;

  signal output out;

  // one line instantiation
  out &lt;== Mul()([a, b]);
}

component main = Example();
</code></pre>
<h2>Output signals should not be ignored</h2>
<p>An output signal must be part of constraints in the component that instantiated it. If an output signal is left “floating” then in some circumstances, a malicious prover can assign any value to it. More on this will be covered in <a href="https://rareskills.io/post/underconstrained-circom">hacking underconstrained circuits</a>.</p>
<h2>Summary</h2>
<ul>
<li>The <code style='font-family: Arial'>&lt;==</code> and <code style='font-family: Arial'>==&gt;</code> saves us the hassle of supplying the value of a signal explicitly in the input.json.</li>
<li>We can use <code style='font-family: Arial'>&lt;==</code> or <code style='font-family: Arial'>==&gt;</code> whenever the value of one signal is directly determined by the value of another.</li>
<li><code style='font-family: Arial'>&lt;==</code> is equivalent to <code style='font-family: Arial'>==&gt;</code> . The arguments are simply reversed, but the effect is the same.</li>
<li>Components can instantiate other sub-components and send values to their input signals using <code style='font-family: Arial'>&lt;==</code> or <code style='font-family: Arial'>==&gt;</code>.</li>
<li>The <code style='font-family: Arial'>output</code> signals of a sub-component should be constrained to equal other signals in the component that instantiated it.</li>
</ul>
<div style='page-break-after: always;'></div>

<h1>Public and Private Inputs</h1>
<p>Source: https://rareskills.io/post/circom-public-private-inputs</p>
<h1>Public and Private Inputs</h1>
<p>A public input in Circom is a signal in the witness that will be revealed to the verifier.</p>
<p>For example, suppose we want to create a ZK proof that states: “we know the input to a hash that produced 0x492c…9254.” To make this claim meaningful, the value 0x492c…9254 (the target hash output) must be public. Otherwise, we are semantically claiming that “we hashed something,” which isn’t as useful.</p>
<p>The following circuit claims, “I multiplied two numbers together and got a third:”</p>
<pre style='font-family: Arial'><code class="language-solidity">template Main() {
  signal input a;
  signal input b;
  signal input c;

  a * b === c;
}

component main = Main();
</code></pre>
<p>This next circuit makes a similar claim, but with the change that the result is public “I multiplied two numbers together and got a third whose value is publicly known:”</p>
<pre style='font-family: Arial'><code class="language-solidity">template Main() {
  signal input a;
  signal input b;
  signal input c;

  a * b === c;
}

component main {public [c]} = Main();
</code></pre>
<ul>
<li>All input signals are private by default unless they are made explicitly public using the <code style='font-family: Arial'>component main {public [c]}</code> syntax. The main component is the only place where we can define which inputs are public.</li>
<li>The list <code style='font-family: Arial'>[c]</code> is a list of signals to make public. It could have contained more signals, such as <code style='font-family: Arial'>[a,c]</code>, if we wanted to make <code style='font-family: Arial'>a</code> public also.</li>
<li>Only input signals can be specified as public, intermediate signals cannot.</li>
</ul>
<p>The template above compiles to a Rank-1 Constraint System (R1CS) identical to the following, where we introduce the <code style='font-family: Arial'>output</code> keyword in the main component:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Main() {
  signal input a;
  signal input b;
  signal output c;

  a * b ==&gt; c;
}

component main = Main();
</code></pre>
<p>In the two templates above, <code style='font-family: Arial'>c</code> is public and constrained to be the product of <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code>. Therefore, the underlying R1CS is the same. However, the second version is more “convenient” since we don’t have to explicitly supply <code style='font-family: Arial'>c</code>. In the first circuit that uses <code style='font-family: Arial'>component main {public [c]}</code>, if we supply <code style='font-family: Arial'>c</code> that does not obey the constraints, the witness will not be generated. However, in the second circuit using <code style='font-family: Arial'>c</code> as an output, the witness generator automatically computes the correct value for <code style='font-family: Arial'>c</code>, eliminating manual input.</p>
<p>Since <code style='font-family: Arial'>c</code> is wholly determined by <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code>, there is no reason to explicitly provide a value for <code style='font-family: Arial'>c</code>, so the <code style='font-family: Arial'>output</code> notation is to be preferred.</p>
<p>Note that outputs are public.</p>
<p>In the case of inputs, if we want to make some public, then that means we have a signal whose value is <em>not</em> wholly determined by other signal values. In such cases, we must use the <code style='font-family: Arial'>public</code> modifier method. For example, if we claim “I multiplied <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, and <code style='font-family: Arial'>c</code> together to get <code style='font-family: Arial'>d</code>, with <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>d</code> public, but <code style='font-family: Arial'>b</code> and <code style='font-family: Arial'>c</code> private, we would structure that circuit as:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Main() {
  signal input a; // explicitly public
  signal input b;
  signal input c;
  signal output d; // implicitly public

  signal s &lt;== a * b; // intermediate signal
  d &lt;== c * s;
}

component main{public [a]} = Main();
</code></pre>
<p>Here is how to understand <code style='font-family: Arial'>output</code> signals:</p>
<ul>
<li>For a sub-component, an <code style='font-family: Arial'>output</code> is a signal that will be assigned a value from the other inputs and potentially be used later by the component that instantiates the sub-component.</li>
<li>For the main component, an <code style='font-family: Arial'>output</code> is a public signal in the witness whose value should be wholly determined by other input signals. <em>Declaring an output signal and not assigning a value to it can create a vulnerability because a prover can assign any value they want. We’ll show the mechanism of this exploit in an upcoming chapter.</em></li>
</ul>
<p>Despite the name “output”, <strong>there is no mechanism to get the “output” from the main component — Circom cannot return anything. There is no way for some other codebase to read the value of “output.”</strong></p>
<p><strong>It only generates an R1CS, helps compute the witness for the R1CS. Snarkjs then uses the Circom code to generate a ZK proof that the witness satisfies the R1CS.</strong></p>
<p>Circom isn’t being “executed”, which is why it doesn’t “return” anything. You aren’t “running” Circom, you are merely describing an abstract circuit that is being turned into two parts: R1CS and a witness generator, which are used separately.</p>
<p>An output signal in the main component can be thought of as an intermediate signal that is public to the verifier.</p>
<h2>Witness layout with public signals</h2>
<p>Circom arranges the witness vector as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">[constant, public signals, private signals]
</code></pre>
<p>Let’s use “I multiplied hidden values <code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>, with a public value <code style='font-family: Arial'>c</code> together to get a public value of <code style='font-family: Arial'>d</code>” as an example:</p>
<pre style='font-family: Arial'><code class="language-solidity">// assert that a*b === c*d
template Example() {
  signal input a;
  signal input b;
  signal input c;
  signal input d;

  signal s;

  s &lt;== a * b;
  d === s * c;
}

component main {public [c, d]} = Example();
</code></pre>
<p>Note that we could save some code by making <code style='font-family: Arial'>d</code> an output, but we don’t do that here to make the upcoming demonstration more clear.</p>
<p>To see how the witness is structured:</p>
<ol>
<li>Save the file above as <code style='font-family: Arial'>Example.circom</code></li>
<li>Compile it with <code style='font-family: Arial'>circom Example.circom --sym --r1cs --wasm</code></li>
<li>Create the <code style='font-family: Arial'>input.json</code>: <code style='font-family: Arial'>echo '{"a": "3", "b": "4", "c":"2", "d":"24"}' &gt; input.json</code></li>
<li><code style='font-family: Arial'>cd example_js</code></li>
<li>Compute the witness: <code style='font-family: Arial'>node generate_witness.js example.wasm ../input.json witness.wtns</code></li>
<li>Convert the witness to json and cat it: <code style='font-family: Arial'>snarkjs wej witness.wtns &amp;&amp; cat witness.json</code></li>
</ol>
<p>We should get the following result. Note that this matches the values we supplied for <code style='font-family: Arial'>input.json</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">[
 &quot;1&quot;, // constant
 &quot;2&quot;, // c (public signal)
 &quot;24&quot;, // d (public signal)
 &quot;3&quot;, // a
 &quot;4&quot;, // b
 &quot;12&quot; // s
]
</code></pre>
<p>Thus, we can see the witness layout is always:</p>
<ul>
<li>The constant entry in the witness (which is always 1)</li>
<li>The public signals (<code style='font-family: Arial'>c</code>, <code style='font-family: Arial'>d</code>)</li>
<li>The input signals (<code style='font-family: Arial'>a</code>, <code style='font-family: Arial'>b</code>)</li>
<li>The intermediate signals (<code style='font-family: Arial'>s</code>).</li>
</ul>
<h2>Summary</h2>
<ul>
<li>Inputs are private by default</li>
<li>We can make inputs public by using the <code style='font-family: Arial'>component main {public [in1, in2]} = Main();</code> syntax</li>
<li>Outputs are public signals</li>
<li>Outputs are signals that are computed for the user based on other inputs</li>
</ul>
<div style='page-break-after: always;'></div>

<h1>Indicate Then Constrain</h1>
<p>Source: https://rareskills.io/post/indicate-then-constrain</p>
<h1>Indicate Then Constrain</h1>
<p>If we want to say that “<code style='font-family: Arial'>x</code> can be equal to 5 or 6” we can simply use the following constraint:</p>
<pre style='font-family: Arial'><code class="language-solidity">(x - 6) * (x - 5) === 0
</code></pre>
<p>However, suppose we want to say that “<code style='font-family: Arial'>x</code> is less than 5 or <code style='font-family: Arial'>x</code> is greater than 17.” In this case, we cannot just combine both conditions directly, because if <code style='font-family: Arial'>x</code> is less than 5, it will violate the constraint that <code style='font-family: Arial'>x</code> is greater than 17 and vice versa.</p>
<p>The solution is to create <em>indicator</em> signals of the different conditions (e.g., <code style='font-family: Arial'>x</code> being less than 5, or being greater than 17), then apply constraints to the indicators.</p>
<h2>Circomlib Comparator Library</h2>
<p>The <a href="https://github.com/iden3/circomlib/blob/master/circuits/comparators.circom">Circomlib comparator library</a> contains a component <code style='font-family: Arial'>LessThan</code> that returns 0 or 1 to indicate if <code style='font-family: Arial'>in[0]</code> is less than <code style='font-family: Arial'>in[1]</code>. How this component works is described in the <a href="https://rareskills.io/post/arithmetic-circuit">Arithmetic Circuit</a> chapter. But as a summary, suppose <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> are at most 3 bits large. If we compute <code style='font-family: Arial'>z = 2^3 + (x - y)</code>, then if <code style='font-family: Arial'>x</code> is less than <code style='font-family: Arial'>y</code>, <code style='font-family: Arial'>z</code> will be less than 2^3 and vice versa (2^3 = 8). Since <code style='font-family: Arial'>z</code> is a 4-bit number, we can efficiently check if <code style='font-family: Arial'>z</code> is less than 2^3 by looking only at the most significant bit. 2^3 in binary is <code style='font-family: Arial'>1000₂</code>. Every 4-bit number greater than or equal to 2^3 has the most significant bit equal to 1, and every 4-bit number less than 2^3 has the most significant bit equal to 0.</p>
<table>
<thead>
<tr>
<th>Number</th>
<th>Binary Representation</th>
<th>Is greater or equal to 2^3</th>
</tr>
</thead>
<tbody>
<tr>
<td>15</td>
<td>1111</td>
<td>Yes</td>
</tr>
<tr>
<td>14</td>
<td>1110</td>
<td>Yes</td>
</tr>
<tr>
<td>13</td>
<td>1101</td>
<td>Yes</td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>1010</td>
<td>Yes</td>
</tr>
<tr>
<td>9</td>
<td>1001</td>
<td>Yes</td>
</tr>
<tr>
<td>8 (2^3)</td>
<td>1000</td>
<td>Yes</td>
</tr>
<tr>
<td>7</td>
<td>0111</td>
<td>No</td>
</tr>
<tr>
<td>6</td>
<td>0110</td>
<td>No</td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>0010</td>
<td>No</td>
</tr>
<tr>
<td>1</td>
<td>0001</td>
<td>No</td>
</tr>
<tr>
<td>0</td>
<td>0000</td>
<td>No</td>
</tr>
</tbody>
</table>
<p>For general n-bit numbers, we can check if <code style='font-family: Arial'>x</code> is greater than or equal to 2^n by checking if the most significant bit is set. Therefore, we can generalize that if <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> are <code style='font-family: Arial'>n-1</code> bit numbers, then we can detect if <code style='font-family: Arial'>x &lt; y</code> by checking if the most significant bit of <code style='font-family: Arial'>2^(n-1) + (x - y)</code> is set or not.</p>
<p>Here is a minimal example of using the LessThan template:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/comparators.circom&quot;;

template Example () {
  signal input a;
  signal input b;
  signal output out;

  // 252 will be explained in the next section
  out &lt;== LessThan(252)([a, b]);
}

component main = Example();

/* INPUT = {
  &quot;a&quot;: &quot;9&quot;,
  &quot;b&quot;: &quot;10&quot;
} */
</code></pre>
<h2>Where 252 comes from</h2>
<p>Numbers in a <a href="https://rareskills.io/post/finite-fields">finite field</a> (which is what Circom uses) cannot be compared to each other as “less than” or “greater” since the typical algebraic laws of inequalities do not hold.</p>
<p>For example, if $x &gt; y$, then if $c$ is positive, it should always be true that $x+c&gt;y+c$. However, this is not true in a finite field. We could pick $c$ such that it is the additive inverse of $x$, i.e. $x + c=0\mod p$. We will then end up with a nonsensical statement that 0 is greater than a non-zero number. For example, if $p = 7$ and $x=2$ and $y=1$ we have that $x&gt;y$. However, if we add $6$ to both $x$ and $y$, then we have \$0&gt;1$.</p>
<p>The 252 specifies the number of bits in the <code style='font-family: Arial'>LessThan</code> component to limit how large <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> can be, so that a meaningful comparison can be made (the section above used 4 bits as an example).</p>
<p>Circom can hold numbers up to 253 bits large in the finite field. For security reasons discussed in the <a href="https://rareskills.io/post/circom-aliascheck">Alias Check</a> chapter, we should not convert a field element to a binary representation that can encode numbers larger than the field. Therefore, Circom does not allow comparison templates to be instantiated with more than 252 bits (<a href="https://github.com/iden3/circomlib/blob/252f8130105a66c8ae8b4a23c7f5662e17458f3f/circuits/comparators.circom#L90">source code</a>).</p>
<p>However, recall that for <code style='font-family: Arial'>LessThan(n)</code> we need to compute <code style='font-family: Arial'>z = 2^n + (x - y)</code>, and <code style='font-family: Arial'>2^n</code> needs to be one bit larger than <code style='font-family: Arial'>x</code> or <code style='font-family: Arial'>y</code>. Therefore, <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> need to be at most $2^{n-1}$ bits large. Since Circom supports numbers up to 253 bits large, <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> must be at most 252 bits large.</p>
<h2>x is less than 5 or x is greater than 17</h2>
<p>Thankfully, the Circomlib library will do the bulk of the work for us. We will use the output signals of LessThan and GreaterThan components to <em>indicate</em> if x is less than 5 or greater than 17.</p>
<p>Then, we <em>constrain</em> that at least one of them is 1 by using the OR component (which is simply <code style='font-family: Arial'>out &lt;== a + b - a * b</code> under the hood).</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.6;

include &quot;circomlib/comparators.circom&quot;;
include &quot;circomlib/gates.circom&quot;;

template DisjointExample1() {
  signal input x;

  signal indicator1;
  signal indicator2;

  indicator1 &lt;== LessThan(252)([x, 5]);
  indicator2 &lt;== GreaterThan(252)([x, 17]);

  component or = OR();
  or.a &lt;== indicator1;
  or.b &lt;== indicator2;

  or.out === 1;
}

component main = DisjointExample1();

/* INPUT = {
  &quot;x&quot;: &quot;18&quot;
} */
</code></pre>
<p><strong>It is very important to include the constraint <code style='font-family: Arial'>or.out === 1;</code>, otherwise the circuit would accept the signals <code style='font-family: Arial'>indicator1</code> and <code style='font-family: Arial'>indicator2</code> both being zero. We’ll get back to this in greater detail towards the end of this chapter.</strong></p>
<h3>Simplifying the code</h3>
<p>The code above can be simplified by using the indicator signals implicitly, as demonstrated next:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.6;

include &quot;circomlib/comparators.circom&quot;;
include &quot;circomlib/gates.circom&quot;;

template DisjointExample1() {
  signal input x;

  component or = OR();
  or.a &lt;== LessThan(252)([x, 5]);
  or.b &lt;== GreaterThan(252)([x, 17]);
  or.out === 1;   
}

component main = DisjointExample1();

/* INPUT = {
  &quot;x&quot;: &quot;18&quot;
} */
</code></pre>
<h2>It is not the case that both x &lt; 100 and y &lt; 100</h2>
<p>To express the above case where both <code style='font-family: Arial'>x</code> &lt; 100 and <code style='font-family: Arial'>y</code> &lt; 100,” we can use a NAND gate. The NAND gate returns 1 for all combinations <strong>except</strong> when both inputs are 1, which has the following truth table:</p>
<table>
<thead>
<tr>
<th>a</th>
<th>b</th>
<th>out</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Therefore, we can create an indicator signal that <code style='font-family: Arial'>x</code> is less than 100 and an indicator signal that <code style='font-family: Arial'>y</code> is less than 100, and constrain a NAND relationship between them.</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.6;

include &quot;circomlib/comparators.circom&quot;;
include &quot;circomlib/gates.circom&quot;;

template DisjointExample2() {
  signal input x;
  signal input y;

  component nand = NAND();
  nand.a &lt;== LessThan(252)([x, 100]);
  nand.b &lt;== LessThan(252)([y, 100]);
  nand.out === 1;   
}

component main = DisjointExample2();

/* INPUT = {
  &quot;x&quot;: &quot;18&quot;,
  &quot;y&quot;: &quot;100&quot;
} */
</code></pre>
<h2>k is greater than at least 2 of x, y, or z</h2>
<p>In this example, we are trying to express that <code style='font-family: Arial'>k</code> is greater than <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> or <code style='font-family: Arial'>k</code> is greater than <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>z</code>, or <code style='font-family: Arial'>k</code> is greater than <code style='font-family: Arial'>y</code> and <code style='font-family: Arial'>z</code>. <code style='font-family: Arial'>k</code> could be greater than <code style='font-family: Arial'>x</code>, <code style='font-family: Arial'>y</code>, and <code style='font-family: Arial'>z</code>, but that isn’t required.</p>
<p>Since it is verbose to express such a complicated logic expression above, it’s simpler to add up the number of signals <code style='font-family: Arial'>k</code> is greater than, and then check that this number is 2 or more.</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.6;

include &quot;circomlib/comparators.circom&quot;;
include &quot;circomlib/gates.circom&quot;;

template DisjointExample3(n) {
  signal input k;
  signal input x;
  signal input y;
  signal input z;

  signal totalGreaterThan;

  signal greaterThanX;
  signal greaterThanY;
  signal greaterThanZ;

  greaterThanX &lt;== GreaterThan(252)([k, x]);
  greaterThanY &lt;== GreaterThan(252)([k, y]);
  greaterThanZ &lt;== GreaterThan(252)([k, z]);

  totalGreaterThan = greaterThanX + greaterThanY + greaterThanZ;

  signal atLeastTwo;
  atLeastTwo &lt;== GreaterEqThan(252)([totalGreaterThan, 2]);
  atLeastTwo === 1;
}

component main = DisjointExample3();

/* INPUT = {
  &quot;k&quot;: 20
  &quot;x&quot;: 18,
  &quot;y&quot;: 100,
  &quot;z&quot;: 10
} */
</code></pre>
<h2>Do not forget to constrain the outputs of components!</h2>
<p>Sometimes, developers may forget to constrain the output of components, <strong>which can lead to severe security vulnerabilities!</strong> For example, the following code might seem like it enforces that <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> both be equal <code style='font-family: Arial'>1</code>, but this is not the case. <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> could be zero (or any other arbitrary value). The <em>output</em> of the AND gate will be zero if <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> are zero, but the output is not constrained to be anything.</p>
<pre style='font-family: Arial'><code class="language-solidity">template MissingConstraint1() {
  signal input x;
  signal input y;

  component and = AND();
  and.a &lt;== x;
  and.b &lt;== y;

  // and.out is not constrained, so x and y can have any values!
}
</code></pre>
<p>Similarly, the following circuit does not force <code style='font-family: Arial'>x</code> to be less than 100. The output of LessThan is 1 if <code style='font-family: Arial'>x</code> is less than 100, but the code doesn’t constrain the output to ensure that this is indeed true.</p>
<pre style='font-family: Arial'><code class="language-solidity">template MissingConstraint2() {
  signal input x;

  component lt = LessThan(252);
  lt.in[0] &lt;== x;
  lt.in[1] &lt;== 100;

  // x could be ≥ 100 since lt.out is allowed to be 0 or any other arbitrary value
}
</code></pre>
<div style='page-break-after: always;'></div>

<h1>Compute Then Constrain</h1>
<p>Source: https://rareskills.io/post/compute-then-constrain</p>
<h1>Compute Then Constrain</h1>
<p>“Compute then constrain” is a design pattern in ZK circuits where an algorithm’s correct output is first computed without constraints. The correctness of the solution is then verified by enforcing invariants related to the algorithm.</p>
<h2>Motivation for compute then constrain</h2>
<p>If one were limited to only using add, multiply, and assign-then-constrain (<code style='font-family: Arial'>==&gt;</code>), then many calculations would be extremely challenging to model and require an extremely large number of additions and multiplications.</p>
<p>For example, computing the square root of a number requires several iterative estimates. This would make the circuit size considerably larger.</p>
<p>Therefore, it is often more practical to run the computation outside the circuit — i.e., do not generate any constraints while computing the answer — then configure constraints which are satisfied if and only if the computed answer is correct.</p>
<p>We will show a lot of examples from the <a href="https://github.com/iden3/circomlib/blob/master/circuits/bitify.circom">Bitify</a> and <a href="https://github.com/iden3/circomlib/blob/master/circuits/comparators.circom">Comparator</a> libraries in Circomlib, which use this pattern heavily.</p>
<h2>The “thin arrow” <code style='font-family: Arial'>&lt;--</code> operator in Circom and how it differs from <code style='font-family: Arial'>&lt;==</code></h2>
<p>The <code style='font-family: Arial'>&lt;==</code> operator assigns a value to a signal and creates a constraint. Because constraints must be quadratic, we cannot carry out operations such as the following:</p>
<pre style='font-family: Arial'><code class="language-solidity">template InputEqualsZero() {
  signal input in;
  signal output out;

  // out = 1 if in == 0
  out &lt;== in == 0 ? 1 : 0;
}

component main = InputEqualsZero();
</code></pre>
<p>Compiling the above circuit will result in a non-quadratic constraints error, since the ternary operator cannot be directly expressed as a single multiplication between signals. In fact, Circom directly rejects any operation on signals that is not multiplication or addition.</p>
<p>At first, it may seem that Circom cannot compute <code style='font-family: Arial'>out</code> for us and requires it to be provided as public input instead. However, this would be very inconvenient if a lot of signals were involved.</p>
<p>We need a mechanism to tell Circom “compute and assign the value for this signal as a function of other signals, but do not create a constraint.” The syntax for that operation is the <code style='font-family: Arial'>&lt;--</code> operator:</p>
<pre style='font-family: Arial'><code class="language-solidity">template InputEqualsZero() {
  signal input in;
  signal output out;

  // out = 1 if in == 0
  out &lt;-- in == 0 ? 1 : 0;
}

component main = InputEqualsZero();
</code></pre>
<p>The operation <code style='font-family: Arial'>in == 0 ? 1 : 0;</code> is sometimes called an “out-of-circuit computations” or a “hint.”</p>
<p>The code above will compile, <strong>but <code style='font-family: Arial'>out</code> and <code style='font-family: Arial'>in</code> have no constraints applied to them</strong>.</p>
<p>The <code style='font-family: Arial'>&lt;--</code> operator is very convenient because it allows us to compute values without generating constraints that eliminates the need to manually supply certain signal values. However, it has also been a source of security bugs.</p>
<p><strong>Circom does not enforce that developers create the appropriate constraints after using <code style='font-family: Arial'>&lt;--</code> and this has been a common source of critical and high vulnerabilities in Circom. Even if the developer adds no constraints, thus creating very dangerous code, the compiler will not even give a warning or a notice. Unconstrained signals can take any value, thus allowing the circuit to produce ZK proofs for nonsensical statements.</strong></p>
<p>We will teach in a later tutorial, <a href="https://rareskills.io/post/underconstrained-circom">exploiting underconstrained circuits</a>, how to exploit Circom code that misuses the <code style='font-family: Arial'>&lt;--</code> operator. For now, think of it as an operation that saves us the trouble of supplying a certain signal value ourselves while still requiring us to constrain that signal later.</p>
<p>Computing and the constraining is best understood by examples, which the rest of this chapter provides.</p>
<h2>Example 1: Modular Square Root</h2>
<p>A modular square root of a number $q$ is a number $r$ such that $r^2=q\pmod p$. However, not all field elements have a modular square root. The constraint that models the correct computation of a square root is straightforward (although computing the square root is not).</p>
<p>Consider the following code, which proves that <code style='font-family: Arial'>out</code> is the modular square root of <code style='font-family: Arial'>in</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">function sqrt(n) {
  // do some magic (see the next code block)
  return r;
}

template ValidSqrt() {
  signal input in;
  signal output out; // sqrt(in)

  out &lt;-- sqrt(in);
  out * out === in; // ensure sqrt was correct
  // the `*` is implicity done modulo p
}
</code></pre>
<p>Here, <code style='font-family: Arial'>out &lt;-- sqrt(in)</code> assigns the square root to <code style='font-family: Arial'>out</code> without adding constraints.</p>
<p>The <a href="https://github.com/iden3/circomlib/blob/master/circuits/pointbits.circom">pointbits</a> file in Circomlib provides the function for computing the modular square root. Note that functions must be declared outside of a Circom template. A “function” in Circom is simply a convenience for putting related code into a contained block.</p>
<pre style='font-family: Arial'><code class="language-solidity">function sqrt(n) {

    if (n == 0) {
        return 0;
    }

    // Test that have solution
    var res = n ** ((-1) &gt;&gt; 1);
//        if (res!=1) assert(false, &quot;SQRT does not exists&quot;);
    if (res!=1) return 0;

    var m = 28;
    var c = 19103219067921713944291392827692070036145651957329286315305642004821462161904;
    var t = n ** 81540058820840996586704275553141814055101440848469862132140264610111;
    var r = n ** ((81540058820840996586704275553141814055101440848469862132140264610111+1)&gt;&gt;1);
    var sq;
    var i;
    var b;
    var j;

    while ((r != 0)&amp;&amp;(t != 1)) {
        sq = t*t;
        i = 1;
        while (sq!=1) {
            i++;
            sq = sq*sq;
        }

        // b = c ^ m-i-1
        b = c;
        for (j=0; j&lt; m-i-1; j ++) b = b*b;

        m = i;
        c = b*b;
        t = t*c;
        r = r*b;
    }

    if (r &lt; 0 ) {
        r = -r;
    }

    return r;
}
</code></pre>
<p>Modular square roots have two solutions: the square root itself and its additive inverse. Thus, we can generate both solutions as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">template ValidSqrt() {
  signal input in;
  signal output out1; // sqrt(in)
  signal output out2; // -sqrt(in)

  out1 &lt;-- sqrt(in);
  out2 &lt;-- out1 * -1; // Computation Step (Unconstrained)
  out1 * out1 === in; // Verification Step (Constraint-Based):
  out2 * out2 === in; // Verification Step
}
</code></pre>
<p><strong>WARNING:</strong> The code presented here is hardcoded to the default field size of Circom. If you configure Circom to use some other field, it may produce the wrong answer!</p>
<p>The example above demonstrates that computing the square root is much simpler when constraints are not a concern — if we tried to compute the square root using only multiplication and addition, the circuit would be unreasonably large. The correctness of the result can then be enforced through constraints afterward.</p>
<p>This illustrates how Circom can be both a traditional programming language and also a constraint generation DSL. The function <code style='font-family: Arial'>sqrt(n)</code> portion of the code is traditional programming, but the constraint <code style='font-family: Arial'>in === out * out</code> generates the constraint.</p>
<h2>Example 2: Sudoku</h2>
<p>If a computation is too difficult or computationally expensive to model through constraints—that is, if it requires many gates and intermediate signals—one can simply provide it as an input and assume the prover obtained the answer by other means.</p>
<p>To actually solve a Sudoku puzzle, one must run a search algorithm for possible solutions, likely using depth-first search. However, we do not need to prove we ran a search algorithm directly — producing a valid solution is sufficient to prove that we ran the search algorithm. Because there are numerous <a href="https://github.com/nalinbhardwaj/snarky-sudoku/blob/main/circuits/sudoku.circom">Sudoku tutorials</a> for Circom on the internet already, we do not produce an example here.</p>
<h2>Example 3: Modular Inverse</h2>
<p>Suppose we want to compute the multiplicative inverse of signal <code style='font-family: Arial'>in</code>, i.e., find a signal <code style='font-family: Arial'>out</code> such that <code style='font-family: Arial'>out * in === 1</code>.</p>
<p>One way to compute multiplicative inverses is to use Fermat’s Little Theorem:</p>
<p>$$<br />
x^{-1}=x^{p-2}\pmod p<br />
$$</p>
<p>However, using such a large exponent (Circom’s default is $p\approx2^{254}$) will result in a lot of multiplications and a very large circuit. Instead, it would be better to compute the multiplicative inverse <em>outside</em> of the circuit and then prove we have the correct multiplicative inverse. For example:</p>
<pre style='font-family: Arial'><code class="language-solidity">template MulInv() {
  signal input in;
  signal output out;

  // Fermat's little theorem
  // compute:
  // note that -2 = p - 2 mod p
  var inv = in ** (-2);
  out &lt;-- inv;

  // then constrain
  out * in === 1;
}

component main = MulInv();
</code></pre>
<p>Here, we only have one constraint: <code style='font-family: Arial'>out * in === 1</code>, so this is very efficient.</p>
<h3>Modular division in Circom</h3>
<p>Circom interprets the <code style='font-family: Arial'>/</code> operator as modular division, so the inverse of a value <code style='font-family: Arial'>n</code> can be computed as:</p>
<pre style='font-family: Arial'><code class="language-solidity">inv &lt;-- 1 / n;
</code></pre>
<p>The template above could be written a little more cleanly as:</p>
<pre style='font-family: Arial'><code class="language-solidity">template MulInv() {
  signal input in;
  signal output out;

  // compute
  out &lt;-- 1 / in;

  // then constrain
  out * in === 1;
}

component main = MulInv();
</code></pre>
<p>Modular division is a non-quadratic operation, so it must be used only with variables or with the thin arrow assignment — i.e. it needs to be computed out-of-circuit.</p>
<h2>Example 4: IsZero</h2>
<h3>Motivation</h3>
<p>The IsZero circuit is very handy for composing into larger computations. Suppose for example that we wanted to prove that <code style='font-family: Arial'>x</code> is less than 16 or <code style='font-family: Arial'>x</code> equals 42.</p>
<p>The following set of constraints won’t work:</p>
<pre style='font-family: Arial'><code class="language-solidity">// equal 42
x === 42

// less than 16
x === b_0 + 2*b_1 + 4*b_2 + 8*b_3
0 === b_0 * (b_0 - 1)
0 === b_1 * (b_1 - 1)
0 === b_2 * (b_2 - 1)
0 === b_3 * (b_3 - 1)
</code></pre>
<p>If <code style='font-family: Arial'>x</code> is 42, it will violate the bottom constraints and if it is less than 16 it will violate <code style='font-family: Arial'>x === 42</code>.</p>
<p>Thus, we really want subcircuits to <em>indicate</em> that a certain condition holds (i.e., <code style='font-family: Arial'>x</code> equaling 42 or being less than 16) without <em>enforcing</em> that a certain condition holds. We can then place constraints on these <em>indicators</em>. For example, suppose we had the indicators <code style='font-family: Arial'>x_eq_42</code> and <code style='font-family: Arial'>x_lt_16</code>. We can constrain that at least one of them is true with the following:</p>
<pre style='font-family: Arial'><code class="language-solidity">// at least one of the two signals is not zero
x_eq_42 * x_lt_16 === 1;
</code></pre>
<p>To create an <em>indicator</em> that <code style='font-family: Arial'>x</code> equals 42, we want to know if the value <code style='font-family: Arial'>x - 42</code> is precisely zero or not.</p>
<h3>Designing a circuit to indicate a value is zero</h3>
<p>Here, we design a circuit that returns <code style='font-family: Arial'>1</code> if the input is <code style='font-family: Arial'>0</code> and <code style='font-family: Arial'>0</code> otherwise (For the curious, the name of this function is the <a href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker Delta function</a>).</p>
<p>If we wrote such a function purely using addition and multiplication, our function would be a polynomial, which is limited in how many places it can be 0. In other words, if we wanted our function to be zero <em>everywhere</em> in our finite field, then our polynomial would have a degree nearly as large as the finite field order, which is impractical.</p>
<p>Instead, we design a set of constraints where <code style='font-family: Arial'>in</code> and <code style='font-family: Arial'>out</code> have the following properties:</p>
<table>
<thead>
<tr>
<th>in</th>
<th>out</th>
<th>constraint</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>violated</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>accepted</td>
</tr>
<tr>
<td>not 0</td>
<td>0</td>
<td>accepted</td>
</tr>
<tr>
<td>not 0</td>
<td>1</td>
<td>violated</td>
</tr>
</tbody>
</table>
<p>We need a set of constraints that require <code style='font-family: Arial'>out</code> to be 1 if <code style='font-family: Arial'>in</code> is 0, and <code style='font-family: Arial'>out</code> to be 0 if <code style='font-family: Arial'>in</code> is non-zero. Another way of thinking about this relationship is “at least one of <code style='font-family: Arial'>in</code> or <code style='font-family: Arial'>out</code> must be non-zero, but they cannot both be zero or both be non-zero.”</p>
<p>Saying that at least one of <code style='font-family: Arial'>in</code> and <code style='font-family: Arial'>out</code> must be zero can be modeled with the constraint <code style='font-family: Arial'>in * out === 0</code>.</p>
<p>In the table below, we can see that <code style='font-family: Arial'>in * out === 0</code> accepts the situation “exactly one of <code style='font-family: Arial'>in</code> and <code style='font-family: Arial'>out</code> are zero,” and it correctly rejects the situation where both <code style='font-family: Arial'>in</code> and <code style='font-family: Arial'>out</code> are non-zero:</p>
<table>
<thead>
<tr>
<th>in</th>
<th>out</th>
<th>constraint</th>
<th>in * out === 0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>violated</td>
<td>accept</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>accepted</td>
<td>accept</td>
</tr>
<tr>
<td>not 0</td>
<td>0</td>
<td>accept</td>
<td>accept</td>
</tr>
<tr>
<td>not 0</td>
<td>1</td>
<td>violated</td>
<td>violate</td>
</tr>
</tbody>
</table>
<p>The issue with the constraint <code style='font-family: Arial'>in * out === 0</code> is that it does not prevent the case where <code style='font-family: Arial'>in</code> and <code style='font-family: Arial'>out</code> are both 0 (as marked as red in the table above).</p>
<p>The missing property that we are trying to capture is that <code style='font-family: Arial'>in</code> and <code style='font-family: Arial'>out</code> cannot be zero simultaneously.</p>
<p>Naively, we could accomplish this with <code style='font-family: Arial'>in + out === 1</code>. This would mean that if <code style='font-family: Arial'>in</code> is 1 then <code style='font-family: Arial'>out</code> must be 0 and vice versa. However, the specifications say that <code style='font-family: Arial'>in</code> could be any non-zero value, for example, 100, and <code style='font-family: Arial'>100 + out</code> cannot be 1.</p>
<p>However, if we can “turn the 100 into a 1” then we can make the constraint work. This can be accomplished by computing the multiplicative inverse of <code style='font-family: Arial'>in</code> outside the circuit and subsequently applying the constraint <code style='font-family: Arial'>in * inv + out === 1</code>. If <code style='font-family: Arial'>in</code> is zero, then we make <code style='font-family: Arial'>inv</code> zero because zero does not have a multiplicative inverse. We now have the following constraints:</p>
<pre style='font-family: Arial'><code class="language-solidity">in * inv + out === 1;
in * out === 0;
</code></pre>
<p>Note that <code style='font-family: Arial'>inv</code> is not itself constrained, but this is not consequential in this case.</p>
<p>The first constraint, <code style='font-family: Arial'>in * inv + out === 1;</code> only serves the purpose of disallowing both <code style='font-family: Arial'>in</code> and <code style='font-family: Arial'>out</code> to be zero. If both <code style='font-family: Arial'>in</code> and <code style='font-family: Arial'>out</code> are zero, then the constraint will be violated regardless of the value of <code style='font-family: Arial'>inv</code>.</p>
<p>To summarize the computations done outside the circuit:</p>
<ul>
<li>Whether <code style='font-family: Arial'>in</code> is zero or not.</li>
<li>The multiplicative inverse of <code style='font-family: Arial'>in</code>.</li>
</ul>
<p>The <a href="https://github.com/iden3/circomlib/blob/0a045aec50d51396fcd86a568981a5a0afb99e95/circuits/comparators.circom#L24">IsZero</a> component in Circomlib accomplishes the constraints outlined in this section:</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsZero() {
  signal input in;
  signal output out;

  signal inv;

  inv &lt;-- in!=0 ? 1/in : 0;

  out &lt;== -in*inv +1;
  in*out === 0;
}
</code></pre>
<p>It first computes <code style='font-family: Arial'>inv</code> outside the circuit, then constraints <code style='font-family: Arial'>out</code> to be 1 if <code style='font-family: Arial'>in</code> is zero and 0 to <code style='font-family: Arial'>out</code> otherwise.</p>
<h3>Non-deterministic inputs</h3>
<p>Values computed outside the circuit that enable us to use more concise constraints are called “advice inputs” or “non-deterministic inputs.” The <code style='font-family: Arial'>inv</code> signal in the circuit above is an example of an advice input, or non-deterministic input.</p>
<h2>Example 5: IsEqual</h2>
<p>The <a href="https://github.com/iden3/circomlib/blob/0a045aec50d51396fcd86a568981a5a0afb99e95/circuits/comparators.circom#L37">IsEqual</a> component in Circomlib is closely related to <code style='font-family: Arial'>IsZero</code> — it checks if the difference between the inputs is zero (if so, then they must be equal to each other):</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsEqual() {
  signal input in[2];
  signal output out;

  component isz = IsZero();

  in[1] - in[0] ==&gt; isz.in;

  isz.out ==&gt; out;
}
</code></pre>
<h2>Example 6: Num2Bits</h2>
<p>The <a href="https://github.com/iden3/circomlib/blob/252f8130105a66c8ae8b4a23c7f5662e17458f3f/circuits/bitify.circom#L25">Num2Bits</a> template in Circomlib decomposes a signal into <code style='font-family: Arial'>n</code> bits as specified by the template parameter:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Num2Bits(n) {
  signal input in; // number
  signal output out[n]; // binary output
  var lc1=0;

  var e2=1;
  for (var i = 0; i&lt;n; i++) {
    out[i] &lt;-- (in &gt;&gt; i) &amp; 1;
    out[i] * (out[i] -1 ) === 0;
    lc1 += out[i] * e2;
    e2 = e2+e2;
  }

  lc1 === in;
}
</code></pre>
<p><em>Please note that for <code style='font-family: Arial'>n</code> in the code above, if $2^n$ is larger than the finite field, we may have an <a href="https://rareskills.io/post/circom-aliascheck">alias bug</a>. This is explained further in that chapter.</em></p>
<p>Essentially, the code loops through each bit in the binary representation, starting from the least significant bit. On each iteration of the loop, we store the value <code style='font-family: Arial'>[1,2,4,8,…,2^i]</code> in a variable <code style='font-family: Arial'>e2</code>, which is the value that bit represents. If that bit is 1 (<code style='font-family: Arial'>out[i] &lt;-- (in &gt;&gt; i) &amp; 1;</code>), we add that value to the accumulator <code style='font-family: Arial'>lc1</code>. At each iteration in the loop, we constrain that the bit read is actually 0 or 1 (with <code style='font-family: Arial'>out[i] * (out[i] -1 ) === 0;</code>). In the end, we constrain that the computed binary value matches the original value (<code style='font-family: Arial'>lc1 === in;</code>).</p>
<p>The way it computes the binary array is best shown with an animation, which we show here:</p>
<p><a href="https://r2media.rareskills.io/ComputeThenConstrain/Num2Bits.mp4"></a></p>
<p>Comparable to the earlier examples, computing the binary value is done outside the circuit, but then we constrain afterwards to ensure that the binary array is correct.</p>
<p>The <code style='font-family: Arial'>Num2Bits</code> template is a core component in the template <code style='font-family: Arial'>LessThan</code> and other templates for comparing the relative value of signals.</p>
<p>Field elements (numbers in a finite field) cannot be directly compared to each other — they need to be converted to binary numbers first.</p>
<p>To understand how to efficiently compare the size of binary numbers in a circuit, please review the relevant section in our chapter on <a href="https://rareskills.io/post/arithmetic-circuit#:~:text=%F0%9D%91%A0-,Compute%20%E2%89%A5%20in%20binary,-If%20we%20are">Arithmetic Circuits</a> then compare the discussion there to the <a href="https://github.com/iden3/circomlib/blob/master/circuits/comparators.circom#L89">LessThan template in Circomlib</a>.</p>
<h2>Example 7: IsMax</h2>
<p>To prove that an item is the maximum in a list, we must show that it is 1) greater than or equal to every element and 2) that it is also present in the list. To understand the second requirement, consider that 100 is not the max of the list [4,5,6] even though 100 is greater than or equal to every item in the list.</p>
<p>The circuit below computes the maximum outside the circuit using a traditional for loop, then uses the <code style='font-family: Arial'>GreaterEqThan</code> component to ensure that <code style='font-family: Arial'>out</code> is greater than or equal to every other item in the list.</p>
<p>To ensure that <code style='font-family: Arial'>out</code> equals at least one of the items in the list, it sums up an <code style='font-family: Arial'>IsEqual</code> comparison to every other signal. If the sum is zero, then we know that <code style='font-family: Arial'>out</code> is not in the list. Therefore, we constrain that sum to not be zero:</p>
<pre style='font-family: Arial'><code class="language-solidity">template IsMax() {
  signal input in[3];
  signal output out;

  // compute the max as usual
  var maxx = in[0];
  for (var i = 1; i &lt; 3; i++) {
    if (in[i] &gt; maxx) {
      maxx = in[i];
    }
  }

  // propose the max, but do not constrained it yet
  out &lt;-- maxx;

  // max must be ≥ every other element
  signal gte0;
  signal gte1;
  signal gte2;

  // gte0 &lt;== GreaterEqThan(252)([out, in[0]]);
  // is shorthand for
  // component gte0 = GreaterEqThan(252);
  // gte0[0] &lt;== out;
  // gte0[1] &lt;== in[0];
  // 252 is to ensure we don't have enough
  // bits to encode numbers larger than what
  // fits in the default finite field, which
  // would lead to aliasing issues
  gte0 &lt;== GreaterEqThan(252)([out, in[0]]);
  gte1 &lt;== GreaterEqThan(252)([out, in[1]]);
  gte2 &lt;== GreaterEqThan(252)([out, in[2]]);
  gte0 === 1;
  gte1 === 1;
  gte2 === 1;

  // max must be equal to at least one element
  signal eq0;
  signal eq1;
  signal eq2;
  eq0 &lt;== IsEqual()([out, in[0]]);
  eq1 &lt;== IsEqual()([out, in[1]]);
  eq2 &lt;== IsEqual()([out, in[2]]);

  signal iz;
  iz &lt;== IsZero()(eq0 + eq1 + eq2);
  // if IsZero is 1, we have a violation
  iz === 0;
}
</code></pre>
<p>In its current form, our circuit is hardcoded to only support an array of length 3. However, it would be nice to be able to have a template for an arbitrary length input. This is the subject of an upcoming chapter.</p>
<h2>Practice Problems</h2>
<p>Write a Circom function that finds the root of a degree 2 polynomial using the quadratic formula. Remember, everything is done over a finite field, so you need to use the modular square root from the first example.</p>
<p>Then, write constraints that the two roots (if they exist) satisfy the polynomial. Pass in the polynomial to the Circom template as an array of three coefficients.</p>
<div style='page-break-after: always;'></div>

<h1>Circom Components in a Loop</h1>
<p>Source: https://rareskills.io/post/circom-component-loop</p>
<h1>Circom Components in a Loop</h1>
<p>Circom does not allow for components to be directly instantiated in a loop. For example, compiling the following code results in the error below.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template IsSorted(n) {
  signal input in[n];

  for (var i = 0; i &lt; n; i++) {
    component lt = LessEqThan(252); // error here
    lt.in[0] &lt;== in[0];
    lt.in[1] &lt;== in[1];
    lt.out === 1;
  }
}

component main = IsSorted(8);
</code></pre>
<pre style='font-family: Arial'><code class="language-solidity">Signal or component declaration inside While scope. Signal and component can only be defined in the initial scope or in If scopes with known condition
</code></pre>
<p>The workaround is to declare an array of the components but not specify the component type:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.8;
include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template IsSorted(n) {

  signal input in[n];

  // declare array of components
  // but do not specify the component type
  component lessThan[n];

  for (var i = 0; i &lt; n - 1; i++) {
    lessThan[i] = LessEqThan(252); // specify type in the loop
    lessThan[i].in[0] &lt;== in[i];
    lessThan[i].in[1] &lt;== in[i+1];
    lessThan[i].out === 1;
  }
}

component main = IsSorted(8);
</code></pre>
<p>When components are declared in this manner, it is not possible to do a “one-line assignment” to a signal like shown below:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.8;
include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template IsSorted() {

  signal input in[4];
  signal leq1;  
  signal leq2;  
  signal leq3;  

  // one line assignment to the signal
  leq1 &lt;== LessEqThan(252)([in[0], in[1]]);
  leq2 &lt;== LessEqThan(252)([in[1], in[2]]);
  leq3 &lt;== LessEqThan(252)([in[2], in[3]]);

  leq1 === 1;
  leq2 === 1;
  leq3 === 1;
}

component main = IsSorted();
</code></pre>
<p>Outside a loop, signals can be set on a single line. Inside a loop, however, we have to write out the assignment in more steps, like we did in <code style='font-family: Arial'>lessThan[i] = LessEqThan(252); // specify type in the loop</code>.</p>
<h2>Example 1: max of an array</h2>
<p>To illustrate a useful example of declaring components in a loop, we show how to prove <code style='font-family: Arial'>k</code> is the maximum of an array. To do this, we need to constrain that <code style='font-family: Arial'>k</code> is greater than or equal to every other element and that it is equal to at least one of the elements. To see why the equality check is necessary, consider that 18 is greater than or equal to all the elements in [7, 8, 15], but it is not the maximum of the array.</p>
<p>The following Circom code computes the maximum value of the array without generating constraints. Then, it runs <code style='font-family: Arial'>n</code> <a href="https://github.com/iden3/circomlib/blob/master/circuits/comparators.circom#L131">GreaterEqualThan</a> components to constrain that the proposed <code style='font-family: Arial'>max</code> value is indeed the maximum value, and also checks that at least one of the elements is equal to <code style='font-family: Arial'>k</code> using an array of <a href="https://github.com/iden3/circomlib/blob/master/circuits/comparators.circom#L37">IsEqual</a> components.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template Max(n) {
  signal input in[n];
  signal output out;

  // no constraints here, just a computation    
  // to find the max    

  var max = 0;    
  for (var i = 0; i &lt; n; i++) {        
    max = in[i] &gt; max ? in[i] : max;   
  }    

  signal maxSignal;    
  maxSignal &lt;-- max;

  // for each element in the array, assert that
  // max ≥ that element
  component GTE[n];
  component EQ[n];
  var acc;
  for (var i = 0; i &lt; n; i++) {
    GTE[i] = GreaterEqThan(252);
    GTE[i].in[0] &lt;== maxSignal;
    GTE[i].in[1] &lt;== in[i];
    GTE[i].out === 1;

    // this is used in the
    // next code block to ensure
    // that maxSignal equals at
    // least one of the inputs
    EQ[i] = IsEqual();
    EQ[i].in[0] &lt;== maxSignal;
    EQ[i].in[1] &lt;== in[i];

    // acc is greater than zero
    // (acc != 0) if EQ[i].out
    // equals 1 at least one time
    acc += EQ[i].out;
  }

  // assert that maxSignal is 
  // equal to at least one of the
  // inputs. if acc = 0 then
  // none of the inputs equals
  // maxSignal
  signal allZero;
  allZero &lt;== IsEqual()([0, acc]);
  allZero === 0;
  out &lt;== max;
}

component main = Max(8);
</code></pre>
<p><strong>Exercise:</strong> Create a circuit that does the same as above, but for the <code style='font-family: Arial'>min</code>.</p>
<h2>Example 2: array Is sorted</h2>
<p>We can assert that an array is sorted by checking that each element is less than or equal to the subsequent one. Unlike the previous example which needed <code style='font-family: Arial'>n</code> components, we need <code style='font-family: Arial'>n - 1</code> components since we are comparing neighboring values to each other. Since we have <code style='font-family: Arial'>n</code> elements, we are going to do <code style='font-family: Arial'>n - 1</code> comparisons.</p>
<p>Here is a template that constrains an input array <code style='font-family: Arial'>in[n]</code> to be sorted. Note that if an array only has one element, it is sorted by definition, and the circuit below is also compatible with that scenario:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.6;

include &quot;circomlib/comparators.circom&quot;;

template IsSorted(n) {
  signal input in[n];

  component lt[n - 1];

  // loop goes up to n - 1, not n
  for (var i = 0; i &lt; n - 1; i++) {
    lt[i] = LessThan(252);
    lt[i].in[0] &lt;== in[i];
    lt[i].in[1] &lt;== in[i+1];
    lt[i].out === 1;
  }
}

component main = IsSorted(3);
</code></pre>
<h2>Example 3: All items are unique</h2>
<p>To check that all items in a list are unique, the most straightforward way is to use a hashmap — but hashmaps are not available in arithmetic circuits. The second most efficient way is to sort the list, but sorting inside a circuit is quite tricky, so we avoid that for now. This leaves us with the brute force solution of comparing every element to every other element. This requires a nested for-loop.</p>
<p>The computation we are doing can be illustrated as follows:</p>
<p>$$<br />
\begin{array}{c|c|c|c|c|}<br />
&amp;a_1&amp;a_2&amp;a_3&amp;a_4\<br />
\hline<br />
a_1&amp;&amp;\neq&amp;\neq&amp;\neq\<br />
\hline<br />
a_2&amp;&amp;&amp;\neq&amp;\neq\<br />
\hline<br />
a_3&amp;&amp;&amp;&amp;\neq\<br />
\hline<br />
a_4\<br />
\hline<br />
\end{array}<br />
$$</p>
<p>In general, there will be</p>
<p>$$<br />
\frac{n(n-1)}{2}<br />
$$</p>
<p>inequality checks, so we will need that many components.</p>
<p>We show how to accomplish this below:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.8;

include &quot;./node_modules/circomlib/comparators.circom&quot;;

template ForceNotEqual() {
  signal input in[2];

  component iseq = IsEqual();
  iseq.in[0] &lt;== in[0];
  iseq.in[1] &lt;== in[1];
  iseq.out === 0;
}

template AllUnique (n) {
  signal input in[n];

  // the nested loop below will run
  // n * (n - 1) / 2 times
  component Fneq[n * (n-1)/2];

  // loop from 0 to n - 1
  var index = 0;
  for (var i = 0; i &lt; n - 1; i++) {
    // loop from i + 1 to n
    for (var j = i + 1; j &lt; n; j++) {
      Fneq[index] = ForceNotEqual();
      Fneq[index].in[0] &lt;== in[i];
      Fneq[index].in[1] &lt;== in[j];
      index++;
    }
  }
}

component main = AllUnique(5);
</code></pre>
<h2>Summary</h2>
<p>To use Circom components inside a loop, we declare an array of components outside the loop without specifying the type.</p>
<p>Then inside the loop, we declare the components and constrain the inputs and outputs of the component.</p>
<div style='page-break-after: always;'></div>

<h1>Hacking Underconstrained Circom Circuits With Fake Proofs</h1>
<p>Source: https://rareskills.io/post/underconstrained-circom</p>
<h1>Hacking Underconstrained Circom Circuits With Fake Proofs</h1>
<p>The <code style='font-family: Arial'>&lt;--</code> operator in Circom can be dangerous because it assigns values to signals but does not constrain them. But how do you actually ~~exploit~~ write a POC (proof of concept) for this vulnerability?</p>
<p>We will be hacking the following circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.8;

template Mul3() {

    signal input a;
    signal input b;
    signal input c;

    signal output out;

    signal i;

    a * b === 1;   // Force a * b === 1
    i &lt;-- a * b;   // i must be equal 1
    out &lt;== i * c; // out must equal c since i === 1
}

component main{public [a, b, c]} = Mul3();
</code></pre>
<p>Save this circuit as <code style='font-family: Arial'>mul3.circom</code> (short for multiply three variables).</p>
<p>The circuit seems to force the product of <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code> to be 1, then assigns 1 to <code style='font-family: Arial'>i</code>.</p>
<p>Finally, <code style='font-family: Arial'>out</code> is constrained to be <code style='font-family: Arial'>i * c</code>. Since <code style='font-family: Arial'>i</code> supposedly can only have the value <code style='font-family: Arial'>1</code>, then <code style='font-family: Arial'>out</code> must equal <code style='font-family: Arial'>c</code>.</p>
<p>The bug here is that the <code style='font-family: Arial'>&lt;--</code> is not creating a constraint but calculating a value and assigning it to <code style='font-family: Arial'>i</code>. In reality, <code style='font-family: Arial'>i</code> can be any value we want, it doesn’t have to be <code style='font-family: Arial'>a * b</code> or <code style='font-family: Arial'>1</code>.</p>
<p>The exploit involves assigning a value to <code style='font-family: Arial'>i</code> that is not <code style='font-family: Arial'>a * b === 1</code>, allowing us to set <code style='font-family: Arial'>out ≠ c</code>.</p>
<p>To summarize, the circuit writer <em>expects</em> <code style='font-family: Arial'>out = c</code>, but we will violate this assumption. In the current example, no harm is done, but in a real application this could be a problem if it was critical two signals had the same value.</p>
<p>But how to we actually create the exploit?</p>
<h2>Steps to exploit</h2>
<h3>Generating a valid proof</h3>
<p>To create a proof for a Circom circuit, we first create an <code style='font-family: Arial'>input.json</code> for the circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">{&quot;a&quot;: &quot;1&quot;, &quot;b&quot;: &quot;1&quot;, &quot;c&quot;: &quot;5&quot;}
</code></pre>
<p>This will satisfy the circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">a * b === 1;   // 1 * 1 === 1
i &lt;-- a * b;   // 1 &lt;-- 1 * 1
out &lt;== i * c; // 5 &lt;== 1 * 5;
// out === c as the dev expects
</code></pre>
<p>We compile the circuit to an r1cs using the following command:</p>
<pre style='font-family: Arial'><code class="language-solidity">circom mul3.circom --r1cs --wasm --sym
</code></pre>
<p>We then generate a witness with the wasm file it created, using <code style='font-family: Arial'>input.json</code> as the input:</p>
<pre style='font-family: Arial'><code class="language-solidity">cd mul3_js/
node generate_witness.js mul3.wasm ../input.json ../witness.wtns
cd ..
</code></pre>
<p>We can see the witness snarkjs computed for us with the following command:</p>
<pre style='font-family: Arial'><code class="language-solidity">snarkjs wtns export json witness.wtns witness.json
cat witness.json
</code></pre>
<p><img alt="witness.json output" src="assets/935a00_5747a762d4304c3989927f4a236adfc1_mv2.png" /></p>
<h3>Witness signal layout</h3>
<p>The first entry in the witness vector is always <code style='font-family: Arial'>1</code>. (This was explained in our <a href="https://rareskills.io/post/rank-1-constraint-system">r1cs article</a> which the reader can consult). The rest of the elements in the vector are the values in the circuit. We can see which element corresponds to which signal by viewing the <code style='font-family: Arial'>input.json</code>, <code style='font-family: Arial'>mul3.sym</code>, and the <code style='font-family: Arial'>witness.json</code> file:</p>
<pre style='font-family: Arial'><code class="language-solidity">cat input.json
cat mul3.sym
cat witness.json
</code></pre>
<p>We show the output and add the labels to the witness.json file below in yellow:</p>
<p><img alt="witness signal labels" src="assets/935a00_7d0345e1fc664c74a1d1d76f35550120_mv2.png" /></p>
<p>To exploit this circuit, we want to assign a value to <code style='font-family: Arial'>i</code> that causes <code style='font-family: Arial'>out ≠ c</code>. However, Circom does not give us a mechanism to write directly to signals that are not input signals, and <code style='font-family: Arial'>i</code> is not an input signal (maybe to make our hack a little harder?). (snarkjs does provide a fullprove api which seems to do this, but this <a href="https://github.com/iden3/snarkjs/issues/107">code has been broken since 2021</a>).</p>
<h3>Example malicious witness</h3>
<p>One such malicious witness:</p>
<pre style='font-family: Arial'><code class="language-solidity">[
    &quot;1&quot;,
    &quot;10&quot;, // out
    &quot;1&quot;,   // a
    &quot;1&quot;,   // b
    &quot;5&quot;,   // c
    &quot;2&quot;    // i
]
</code></pre>
<p>This will satisfy the constraints:</p>
<pre style='font-family: Arial'><code class="language-solidity">a * b === 1;   // 1 * 1 = 1
i &lt;-- a * b;   // 2 &lt;-- 1 * 1 is ok because &lt;-- is not a constraint!
out &lt;== i * c; // 10 = 2 * 5;
</code></pre>
<p>Right now, we have a valid witness which snarkjs will create a proof for:</p>
<pre style='font-family: Arial'><code class="language-solidity">snarkjs wtns check mul3.r1cs witness.wtns
</code></pre>
<p><img alt="snarkjs witness check" src="assets/935a00_7d37546c94a34d6d9f40c8e15fe81cdc_mv2.png" /></p>
<p>Our goal is to create a witness file that satisfies the circuit but violates the expected property that <code style='font-family: Arial'>out = c</code>.</p>
<h2>Understanding the layout of witness.wtns</h2>
<p>The <code style='font-family: Arial'>witness.wtns</code> is a binary file. Unfortunately, as noted above, Circom and snarkjs do not provide an API to take a json witness vector and output a .wtns file. The format of the .wtns file can be determined by looking at the source <a href="https://github.com/iden3/circom_runtime/blob/master/build/main.cjs#L533">code that generates it</a>. However, a quick examination of the binary file is sufficient.</p>
<p>We see in the code linked above that it writes a <code style='font-family: Arial'>Uint8Array</code> to a file. So let’s parse the file as a <code style='font-family: Arial'>Uint8Array</code> with the following code and print it out:</p>
<pre style='font-family: Arial'><code class="language-solidity">const fs = require('fs');

const filePath = 'witness.wtns';

const data = fs.readFileSync(filePath);

let data_arr = new Uint8Array(data);
console.dir(data_arr, {'maxArrayLength': null});
</code></pre>
<p><img alt="witness.wtns binary layout" src="assets/935a00_24f0ffb3fec04d978f341cfc5159923f_mv2.png" /></p>
<p>Without going into the details of how this <code style='font-family: Arial'>witness.wtns</code> is formatted, we can still see the values of our witness laid out in the same order as the <code style='font-family: Arial'>witness.json</code>!</p>
<p><img alt="witness binary values displayed" src="assets/935a00_9f8d17fdab314b9ea83ae08963617e37_mv2.jpeg" /></p>
<p>Now we are ready to create a fake witness by overwriting the binary file where the values for these signals <code style='font-family: Arial'>i</code> and <code style='font-family: Arial'>out</code> are stored:</p>
<pre style='font-family: Arial'><code class="language-solidity">const fs = require('fs');

const filePath = 'witness.wtns';

const data = fs.readFileSync(filePath);
console.log(&quot;Before&quot;);
console.dir(data, {'maxArrayLength': null});

data[108] = 10; // `out`
data[236] = 2;  // `i`

console.log(&quot;After&quot;);
console.dir(data, {'maxArrayLength': null});

fs.writeFileSync('exploit_witness.wtns', data);
</code></pre>
<p>After running our code to create the fake witness, we can see the values corresponding to <code style='font-family: Arial'>out</code> and <code style='font-family: Arial'>i</code> have been altered as planned (the changed bytes are annotated with a red box, the rest are unchanged):</p>
<p><img alt="highlighting changed bytes in witness.wtns" src="assets/935a00_06cc72c5a445454c9e71ff54bd543f68_mv2.png" /></p>
<p>The code above also writes the file <code style='font-family: Arial'>exploit_witness.wtns</code> for us, which is simply the array of bytes printed above.</p>
<p>When we verify <code style='font-family: Arial'>exploit_witness.wtns</code> against the circuit using snarkjs:</p>
<pre style='font-family: Arial'><code class="language-solidity">snarkjs wtns check mul3.r1cs exploit_witness.wtns
</code></pre>
<p><img alt="snarkjs wtns check" src="assets/935a00_81192f33ba3b4f90bf53a623fafc2cae_mv2.png" /></p>
<p>The witness satisfies the circuit!</p>
<p>From here, we can simply follow the <a href="https://docs.circom.io/getting-started/proving-circuits/#verifying-from-a-smart-contract">proving steps in the Circom documentation</a> to create a fake proof to exploit the circuit.</p>
<h2>Learn more with RareSkills</h2>
<p>Please see our <a href="https://rareskills.io/zk-bootcamp">Zero Knowledge Course</a> to learn more topics in ZK.</p>
<p><em>Originally Published Mar 18</em></p>
<div style='page-break-after: always;'></div>

<h1>AliasCheck and Num2Bits_strict in Circomlib</h1>
<p>Source: https://rareskills.io/post/circom-aliascheck</p>
<h1>AliasCheck and Num2Bits_strict in Circomlib</h1>
<p>An alias bug in Circom (or any ZK circuit language) occurs when a binary array of signals encodes a number larger than the field element can hold. We will refer to signals and field elements interchangeably in this article. We refer to the characteristic of the field as <code style='font-family: Arial'>p</code>. Loosely speaking, <code style='font-family: Arial'>p</code> is the value the signal "overflows" at. It is the value of the implicit modulo in all of the arithmetic operations.</p>
<p>By default, Circom sets p to be <code style='font-family: Arial'>21888242871839275222246405745257275088548364400416034343698204186575808495617</code>, which takes 254 bits to store. However, $2^{254} – 1 (\sim2.89\times10^{76})$ is larger than the default <code style='font-family: Arial'>p</code> ($\sim2.18\times10^{76}$). That is, 254 bits can encode numbers larger than Circom signals can store.</p>
<p>Below we plot the number line showing these values on a number line, approximately to scale:</p>
<p><img alt="Number line ranging from 0 to 2^254 -1 with a separation of p-1. The line of 0 to p-1 is colored green and p-1 to 2^254-1 is colored red." src="assets/706568_e0befb030c5449aaa5827cfba3fc2c46_mv2.png" /></p>
<p>0 to $p – 1$ (the green line segment) is the interval a Circom field element can hold, and p to $2^{254} – 1$ (the red segment) is the values a 254 bit binary value can hold, but a field element cannot.</p>
<p>The "danger zone" is 254 bit binary values larger than <code style='font-family: Arial'>p - 1</code>. These are the numbers in the interval $[p, 2^{254} − 1]$. In Circom’s (default) case, the range $[p, 2^{254} − 1]$ is</p>
<p>[21888242871839275222246405745257275088548364400416034343698204186575808495617, 28948022309329048855892746252171976963317496166410141009864396001978282409983]</p>
<p>To get a sense of scale, if we divide $p / 2^{254}$ we get 0.7561, which means a <code style='font-family: Arial'>p</code> can hold approximately 3/4ths of the numbers representable by a 254 bit number.</p>
<h2>Binary representation constraints silently fail when they overflow</h2>
<p>To constrain a binary number $b₀, b₁, …, bₙ$ to equal a field element <code style='font-family: Arial'>v</code>, we write the following <a href="https://www.rareskills.io/post/arithmetic-circuit">arithmetic circuit</a>:</p>
<p>$$<br />
b₀ + 2*b₁ + … + n*bₙ === v$$</p>
<p>and also constrain each $bᵢ$ to be $0$ or $1$.</p>
<p>However, the computation $v = b_0 + 2 * b_1 + … + n * b_n$ is done modulo <code style='font-family: Arial'>p</code>. Therefore, if the computation $b_0 + 2b_1 + … + nb_n$ overflows <code style='font-family: Arial'>p</code>, then we could present a binary number $b_0, b_1, …, b_n$ whose value is not <code style='font-family: Arial'>v</code> and create a false proof. For example, if our modulo is 11, then 2 and 13 are "equal" to each other because 13 mod 11 is 2.</p>
<h3>Small example</h3>
<p>Suppose $p = 11$ and we are using four bits to represent a field element. The bits can encode numbers as large as 15. If we encode 12 in binary as (1100) this will evaluate to 12 modulo 11 = 1. So we can claim that 1100 is the binary representation of 1.</p>
<p>Specifically:</p>
<p>$8(1) + 4(1) + 2(0) + (0) == 1 (\mod 11 )$</p>
<h2>Demonstration in Python</h2>
<p>To see the values being used by the attacker, below we recreate the constraint used by Circomlib’s <code style='font-family: Arial'>Bits2Num</code> and <code style='font-family: Arial'>Num2Bits</code> in Python so that the values can be easily printed out:</p>
<pre style='font-family: Arial'><code class="language-solidity">p = 21888242871839275222246405745257275088548364400416034343698204186575808495617

# replicates the constraints Num2Bits and Bits2Num use
def constrain_modulo_p(bits, num, p):
    multiplier = 1
    acc = 0
    for i in range(len(bits)):
        assert bits[i] == 0 or bits[i] == 1
        acc = (acc + multiplier * bits[i]) % p
        multiplier = (multiplier * 2) % p

    # binary conversion must be correct
    assert num == acc

# this cannot be done in Circom because `value` needs to be higher than p
# but less than 2^254 - 1
def malicious_witness_generator(nbits, value):
    bits = []
    for i in range(nbits):
        bit = value &gt;&gt; i &amp; 1

        bits.append(bit)

    return bits

# &quot;normal&quot; case -- constraints pass
constrain_modulo_p([1,1], 3, p)

# adversary case -- constraints pass, but the binary number is not 3
adversary_bits = malicious_witness_generator(254, 3 + p)
print(adversary_bits)

# no asserts are triggered although adversary_bits ≠ 3
constrain_modulo_p(adversary_bits, 3, p)
</code></pre>
<p>The important thing here is that <code style='font-family: Arial'>constrain_modulo_p</code> accepts two binary representations for 3: the "correct" binary representation for 3 (11), and it’s alias $3 + p$ — which is encodable with as a 254 bit number.</p>
<h2>Bug prevention with AliasCheck and Num2Bits_strict and Bits2Num_strict</h2>
<p>The "strict" versions of the bit conversion templates in <a href="https://github.com/iden3/circomlib/blob/master/circuits/bitify.circom">Circomlib’s bitify library</a> prevent the alias bug by passing the binary array to the <code style='font-family: Arial'>AliasCheck</code> template.</p>
<p><img alt="Template code comparison of Num2bits_stirct() and Bits2Num_strict. highlighting the common uses of AliasCheck template in both templates." src="assets/706568_57d292ca75b946a4aec5cfe7711b8c3e_mv2.jpeg" /></p>
<p>The <a href="https://github.com/iden3/circomlib/blob/master/circuits/aliascheck.circom">AliasCheck template</a> takes a binary array and asserts that the value encoded is less than the maximum value the field element can hold</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.0.0;

include &quot;compconstant.circom&quot;;

template AliasCheck() {

    signal input in[254];

    component compConstant = CompConstant(-1);

    for (var i=0; i&lt;254; i++) in[i] ==&gt; compConstant.in[i];
    // compConstant returns 1 if the binary
    // input is greater than the supplied constant
    compConstant.out === 0;
}
</code></pre>
<p>AliasCheck uses <code style='font-family: Arial'>-1</code> to refer to <code style='font-family: Arial'>p - 1</code>. <code style='font-family: Arial'>compConstant</code> takes a binary input (which could encode a value larger than the field element can hold) and returns 0 if it is less than or equal to a certain threshold and 1 if the binary value is greater than the threshold.</p>
<p>By constraining the output of <a href="https://github.com/iden3/circomlib/blob/master/circuits/compconstant.circom"><code style='font-family: Arial'>compConstant</code></a> to be 0, and setting the constant for comparison to be -1, <code style='font-family: Arial'>AliasCheck</code> disallows binary numbers that are larger than <code style='font-family: Arial'>p</code>.</p>
<h2>If the binary array holds fewer bits than the field can encode, there is no danger of alias bugs</h2>
<p>Applying <code style='font-family: Arial'>Num2Bits</code> to a field element also enforces a range check on that number to be less than $2^n$ where <code style='font-family: Arial'>n</code> is the number of bits. For example, if <code style='font-family: Arial'>n = 4</code> and <code style='font-family: Arial'>p</code> is the default value, and we set the input signal to be 17, the result will not silently overflow to binary 1 (0001) — the circuit will not be satisfied.</p>
<p>This is why <code style='font-family: Arial'>Num2Bits_strict</code> and <code style='font-family: Arial'>Bits2Num_strict</code> in Circomlib have the number of bits hardcoded to 254 — this is the value at which aliases can appear.</p>
<p>This is also why the <code style='font-family: Arial'>LessThan</code> template doesn’t allow the dev to build a comparator with more than 252 bits. This avoids a footgun with the alias bug.</p>
<pre style='font-family: Arial'><code class="language-solidity">template LessThan(n) {
    assert(n &lt;= 252);
    signal input in[2];
    signal output out;

    component n2b = Num2Bits(n+1);

    n2b.in &lt;== in[0] + (1&lt;&lt;n) - in[1];

    out &lt;== 1-n2b.out[n];
}
</code></pre>
<p><strong>If you change the default</strong> <strong><code style='font-family: Arial'>p</code></strong> <strong>in the Circom compiler (the</strong> <a href="https://docs.circom.io/getting-started/compiling-circuits/">-p option</a><strong>), be sure to check that</strong> <code style='font-family: Arial'>Num2Bits_strict</code><strong>,</strong> <code style='font-family: Arial'>Bits2Num_strict</code><strong>,</strong> <code style='font-family: Arial'>AliasCheck</code><strong>, and</strong> <code style='font-family: Arial'>CompConstant</code> <strong>still protect you from alias bugs because they are hardcoded to use 254 bits.</strong></p>
<h2>Spot the bug challenge</h2>
<p>Here is a <a href="https://x.com/RareSkills_io/status/1811263676747591772">CTF we posted on X</a> (formerly Twitter) that has the bug described in this article:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.8;

include &quot;.node_modules/circomlib/circuits/comparators.circom&quot;;
include &quot;.node_modules/circomlib/circuits/poseidon.circom&quot;;

template UnsafePoseidon(n) {
    signal input in;
    signal output out;

    component n2b = Num2Bits(n);
    component b2n = Bits2Num(n);
    component phash = Poseidon(1);

    n2b.in &lt;== in;
    for (var i = 0; i &lt; n; i++) {
        b2n.in[i] &lt;== n2b.out[i];
    }

    phash.inputs[0] &lt;== b2n.out;
    phash.out ==&gt; out;
}

component main = UnsafePoseidon(254);
</code></pre>
<p>The problem with the code above is that there are multiple witnesses that lead to the same hash, due to allowing 254 bits which leads to overflow.</p>
<p>Remember, <strong>the input to an arithmetic circuit is not only the signals labelled</strong> <code style='font-family: Arial'>input</code> <strong>its</strong> <strong><em>every</em></strong> <strong>signal in the circuit</strong>. Circom provides us a nice C-like programming language to "fill in" some of the signals based on the values provided in the <code style='font-family: Arial'>input</code> signals, but the code is not part of the final constraint system.</p>
<p>In the code above, the 254 bit binary array is held in the output signals of <code style='font-family: Arial'>Num2Bits</code> and the input signals of <code style='font-family: Arial'>Bits2Num</code>.</p>
<p>To inject the wrong values into the signals used to encode the binary array, we need to use the technique describe in our tutorial on <a href="https://www.rareskills.io/post/underconstrained-circom">hacking underconstrained Circom circuits</a>. To generate a proof of concept for the code above, we take the following steps.</p>
<ol>
<li>Generate a hash using a number <code style='font-family: Arial'>u</code> which is small enough to have an alias in the range, i.e. $[p, 2^{254} − 1]$.</li>
<li>Generate malicious witness using the same number applied to <code style='font-family: Arial'>input in</code> but reassign the binary array to hold the value <code style='font-family: Arial'>u + p</code>.</li>
<li>The hash generated by both of these signal assignments will be the same, but the witness is different.</li>
</ol>
<p>In summary, the code in the challenge is vulnerable to a second preimage attack via an alias bug.</p>
<p><em>Originally Published Jul 13</em></p>
<div style='page-break-after: always;'></div>

<h1>Conditional Statements in Circom</h1>
<p>Source: https://rareskills.io/post/circom-if-statement</p>
<h1>Conditional Statements in Circom</h1>
<p>Circom is very strict with the usage of if-statements. The following rules must be followed:</p>
<ul>
<li>Signals cannot be used to alter the behavior of an if-statement.</li>
<li>A signal cannot be assigned a value inside an if-statement.</li>
</ul>
<p>The example circuit below demonstrates both violations:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Foo() {

  signal input in;
  signal input cond;

  signal output out;

  // if-statements cannot depend on 
  // values not known at compile time
  if (in == 3) {
    // assigning a value inside an if-statement
    // whose value is unknown at compile time
    // is not allowed
    out &lt;== 4;
  }
}
</code></pre>
<p>If-statements are acceptable if they are not affected by any signals, and do not affect any signals.</p>
<p>Effectively, they are not part of the underlying Rank 1 Constraint system (R1CS).</p>
<p>For example, if we wanted to compute the maximum value in a list (without generating constraints), we can use the following typical solution, which Circom accepts since no signals are involved:</p>
<pre style='font-family: Arial'><code class="language-solidity">var max;
for (var i = 0; i &lt; n; i++) {
  if (arr[i] &gt; max) {
    max = arr[i];
  }
}
</code></pre>
<p>This computation creates no constraints, it is simply for convenience.</p>
<h2>Branching in Circom</h2>
<p>It might seem that Circom is incapable of conditional branching, but this is not the case. To create conditional branches in Circom, all branches of a statement must be executed, with the ‘unwanted’ branches multiplied by zero and the ‘correct’ branch multiplied by one.</p>
<h2>Example of a computation with branches</h2>
<p>Suppose we are modeling the following computation:</p>
<pre style='font-family: Arial'><code class="language-solidity">def foo(x):

  if x == 5:
    out = 14
  elif x == 9:
    out = 22
  elif x == 10:
    out = 23
  else
    out = 45

  return out
</code></pre>
<p>With no clear mathematical link between <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>out</code>, it’s best to model this conditional as directly as possible. Here is how we describe the conditional statement mathematically:</p>
<p>$$<br />
\texttt{out} = \texttt{x\_eq\_5}\cdot14+\texttt{x\_eq\_9}\cdot22+\texttt{x\_eq\_10}\cdot23+\texttt{otherwise}\cdot45\<br />
$$</p>
<ul>
<li><code style='font-family: Arial'>x_eq_5</code> equals 1 if <code style='font-family: Arial'>x</code> equals 5, and zero otherwise, which can be accomplished with <code style='font-family: Arial'>IsEqual()([x, 5])</code></li>
<li><code style='font-family: Arial'>x_eq_9</code> equals 1 when <code style='font-family: Arial'>x</code> equals 9, zero otherwise</li>
<li><code style='font-family: Arial'>x_eq_10</code> equals 1 when <code style='font-family: Arial'>x</code> equals 10, zero otherwise</li>
<li><code style='font-family: Arial'>otherwise</code> equals 1 when all of the above (<code style='font-family: Arial'>x_eq_5</code>, <code style='font-family: Arial'>x_eq_9</code>, <code style='font-family: Arial'>x_eq_10</code>) are 0.</li>
</ul>
<p>We can assign the values to the signals <code style='font-family: Arial'>x_eq_5</code>, <code style='font-family: Arial'>x_eq_9</code>, <code style='font-family: Arial'>x_eq_10</code>, and <code style='font-family: Arial'>otherwise</code> using the <code style='font-family: Arial'>IsEqual()</code> template from Circomlib — this will also enforce that they are 0 or 1. To ensure that exactly one signal is 1 and the rest are zeros, we use the following constraint:</p>
<p>$$<br />
\begin{align*}<br />
1===\texttt{x\_eq\_5}+\texttt{x\_eq\_9}+\texttt{x\_eq\_10}+\texttt{otherwise}<br />
\end{align*}<br />
$$</p>
<p>In general, we create “binary switches” that are 1 when a particular branch is active and 0 otherwise. Then, we add up the evaluation of all the branches, each multiplied by their switch.</p>
<p>Since only one branch of $\texttt{out = }…$ will be active, the rest of the evaluations are multiplied by 0 and hence don’t matter.</p>
<p>Here is the complete circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template MultiBranchConditional() {
  signal input x;

  signal output out;

  signal x_eq_5;
  signal x_eq_9;
  signal x_eq_10;
  signal otherwise;

  x_eq_5 &lt;== IsEqual()([x, 5]);
  x_eq_9 &lt;== IsEqual()([x, 9]);
  x_eq_10 &lt;== IsEqual()([x, 10]);
  otherwise &lt;== IsZero()(x_eq_5 + x_eq_9 + x_eq_10);

  signal branches_5_9;
  signal branches_10_otherwise;

  branches_5_9 &lt;== x_eq_5 * 14 + x_eq_9 * 22;
  branches_10_otherwise &lt;== x_eq_10 * 23 + otherwise * 45;

  out &lt;== branches_5_9 + branches_10_otherwise;
}

component main = MultiBranchConditional();
</code></pre>
<p>To make our code cleaner, it would be better to put the four-way branch as a separate component — that way, we can re-use the branching template.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template Branch4(cond1, cond2, cond3, branch1, branch2, branch3, branch4) {
  signal input x;
  signal output out;

  signal switch1;
  signal switch2;
  signal switch3;
  signal otherwise;

  switch1 &lt;== IsEqual()([x, cond1]);
  switch2 &lt;== IsEqual()([x, cond2]);
  switch3 &lt;== IsEqual()([x, cond3]);
  otherwise &lt;== IsZero()(switch1 + switch2 + switch3);

  signal branches_1_2 &lt;== switch1 * branch1 + switch2 * branch2;
  signal branches_3_4 &lt;== switch3 * branch3 + otherwise * branch4;

  out &lt;== branches_1_2 + branches_3_4;
}

template MultiBranchConditional() {
  signal input x;

  signal output out;

  component branch4 = Branch4(5,9,10,14,22,23,45);

  branch4.x &lt;== x;
  branch4.out ==&gt; out; // same as out &lt;== branch4.out
}

component main = MultiBranchConditional();
</code></pre>
<h2>Code when many branches are involved</h2>
<p>In the code above, we had to explicitly write <code style='font-family: Arial'>switch1</code>, <code style='font-family: Arial'>switch2</code>,…, <code style='font-family: Arial'>otherwise</code>, which could be very tedious if the code has a lot of branches.</p>
<p>Instead, we could think of our computation as the inner product (generalized dot product) of the switches and the branches:</p>
<p>$$<br />
\begin{align*}<br />
\text{out}&amp;===\langle[\text{switch}_1, \text{switch}_2,…,\text{switch}_n],[\text{branch}_1, \text{branch}_2,…,\text{branch}_n]\rangle\<br />
&amp;=\text{switch}_1\cdot\text{branch}_1+\text{switch}_2\cdot\text{branch}_2+\dots+\text{switch}_n\cdot\text{branch}_n\<br />
1&amp;===\text{switch}_1+\text{switch}_2+…+\text{switch}_n\<br />
0&amp;===\text{switch}_i*(\text{switch}_i-1),\text{i = 1…n}<br />
\end{align*}<br />
$$</p>
<p>This above formulation ensures that precisely one switch is active (equal to 1), while all others are 0, making the corresponding branch the output.</p>
<p>To implement this efficiently in Circom, we use the <code style='font-family: Arial'>EscalarProduct</code> template from <a href="https://github.com/iden3/circomlib/blob/master/circuits/multiplexer.circom">multiplexer.circom</a> . This template takes two vectors of length n, multiplies them element-wise, and sums the result. In the following code block, we use <code style='font-family: Arial'>EscalarProduct</code> to multiply each switch by each branch. Note that the final switch and branch are handled slightly differently because the final condition is a “catch-all” else statement.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;
include &quot;./node_modules/circomlib/circuits/multiplexer.circom&quot;;

template BranchN(n) {
  assert(n &gt; 1); // too small

  signal input x;

  // conds n - 1 is otherwise
  signal input conds[n - 1];

  // branch n - 1 is the otherwise branch
  signal input branches[n];
  signal output out;

  signal switches[n];

  component EqualityChecks[n - 1];

  // only compute IsEqual up to the second-to-last switch
  for (var i = 0; i &lt; n - 1; i++) {
    EqualityChecks[i] = IsEqual();

    EqualityChecks[i].in[0] &lt;== x;
    EqualityChecks[i].in[1] &lt;== conds[i];
    switches[i] &lt;== EqualityChecks[i].out;
  }

  // check the last condition
  var total = 0;
  for (var i = 0; i &lt; n - 1; i++) {
    total += switches[i];
  }

  // if none of the first n - 1 switches
  // are active, then `otherwise` must be 1
  switches[n - 1] &lt;== IsZero()(total);

  component InnerProduct = EscalarProduct(n);
  for (var i = 0; i &lt; n; i++) {
    InnerProduct.in1[i] &lt;== switches[i];
    InnerProduct.in2[i] &lt;== branches[i];
  }

  out &lt;== InnerProduct.out;
}

template MultiBranchConditional() {
    signal input x;

    signal output out;

    component branchn = BranchN(4);

  var conds[3] = [5, 9, 10];
  var branches[4] = [14, 22, 23, 45];
  for (var i = 0; i &lt; 4; i++) {
    if (i &lt; 3) {
        branchn.conds[i] &lt;== conds[i];
    }

    branchn.branches[i] &lt;== branches[i];
  }

  branchn.x &lt;== x;
  branchn.out ==&gt; out; // same as out &lt;== branch4.out
}

component main = MultiBranchConditional();
</code></pre>
<h2>When is it okay to use if-statements?</h2>
<p>Suppose we wanted to create a template that returns a completely different circuit depending on the circuit parameter. For example, if we are creating a <code style='font-family: Arial'>Max</code> component that takes an array <code style='font-family: Arial'>in[n]</code> and returns the max, it would be more efficient to simply return the 0th item in the index if <code style='font-family: Arial'>n</code> is equal to 1.</p>
<p>Below, we show an example of a valid use of the if-statement when used with defining constraints. Here, the if-statement is executed at compile time, so the template will produce a well-defined circuit:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template Max(n) {
  signal input in[n];
  signal output out;

  assert(n &gt; 0);

  if (n == 1) {
    out &lt;== in[0];
  }

  // it is okay to declare signals inside
  // the if-statement because the evaluation
  // of the if-statement is known at compile time
  else if (n == 2) {
    signal zeroGtOne;
    signal branch0;
    signal branch1;

    zeroGtOne &lt;== GreaterThan(252)([in[0], in[1]]);
    branch0 &lt;== zeroGtOne * in[0];
    branch1 &lt;== (1 - zeroGtOne) * in[1];

    out &lt;== branch0 + branch1;
  }
  else {
    // case for n &gt; 2
  }
}

component main = Max(2);
</code></pre>
<h2>Conditional statements are not zk-friendly</h2>
<p>A key design implication is that each condition in a Circom circuit doubles its size since branches can’t be “short-circuited.” Unlike traditional programming, all branches are computed.</p>
<p>When using ZK to prove a computation, we want to optimize for</p>
<ol>
<li>Having as few branches as possible, as each branch increases the work of the prover.</li>
<li>Having the total computational cost across all branches be minimized, not just the expected computation based on the probability of a branch.</li>
<li>Avoiding conditional statements where possible.</li>
</ol>
<div style='page-break-after: always;'></div>

<h1>Quin Selector</h1>
<p>Source: https://rareskills.io/post/quin-selector</p>
<h1>Quin Selector</h1>
<p>The Quin Selector is a design pattern that allows us to use a signal as an index for an array of signals.</p>
<p>As a prerequisite, we assume the reader has read the chapter on Conditional Statements in Circom.</p>
<p>The following code does not compile, but it illustrates what we are trying to accomplish:</p>
<pre style='font-family: Arial'><code class="language-solidity">template ArraySelect(n) {

  signal input in[n];
  signal input index;

  signal output out;

  // won't compile -- non-quadratic constraints
  out &lt;== in[index];
}
</code></pre>
<p>To express something conditional in Circom, we multiply the desired branch by one and the others by zero, then sum all the branches. The zeroed branches won’t have any effect on the sum. The Quin selector follows the same logic: we multiply the desired index by 1 and the rest by zero, then sum the result.</p>
<p>As an example, suppose our input array is <code style='font-family: Arial'>in = [5,9,14,20]</code>. Selecting the item at index 2 means we compute:</p>
<p>$$<br />
5\cdot0+9\cdot0+\boxed{14\cdot1}+20\cdot0=14<br />
$$</p>
<p>In other words, we do an inner product computation between <code style='font-family: Arial'>[5,9,14,20]</code> and <code style='font-family: Arial'>[0,0,1,0]</code>, which results in 14.</p>
<p>Each “switch” becomes 0 or 1 if <code style='font-family: Arial'>index</code> equals the desired index.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;./node_modules/circomlib/comparators.circom&quot;;

template ArraySelect(n) {

  signal input in[n];
  signal input index;
  signal output out;

  component eqs[n];

  // prod keeps a running product
  signal prod[n];

  // prod = 1 * in[i] if i == index else 0
  for (var i = 0; i &lt; n; i++) {
    eqs[i] = IsEqual();
    eqs[i].in[0] &lt;== i;
    eqs[i].in[1] &lt;== index;

    prod[i] &lt;== eqs[i].out * in[i];
  }

  // sum the result
  var sum;
  for (var i = 0; i &lt; n; i++) {
    sum += prod[i];
  }

  out &lt;== sum;
}
</code></pre>
<p>The code above does not constrain the index to be less than the size of the array. If the index is out of bounds, then the code will return 0 as the result. The <a href="https://github.com/darkforest-eth/circuits/blob/master/perlin/QuinSelector.circom">Quin Selector implementation in DarkForest</a> includes a range check on <code style='font-family: Arial'>index</code>, so we refer the reader to that template, upon which the above examples were based:</p>
<pre style='font-family: Arial'><code class="language-solidity">// out is the sum of in
template CalculateTotal(n) {
  signal input in[n];
  signal output out;

  signal sums[n];

  sums[0] &lt;== in[0];

  for (var i = 1; i &lt; n; i++) {
      sums[i] &lt;== sums[i-1] + in[i];
  }

  out &lt;== sums[n-1];
}

template QuinSelector(choices) {
  signal input in[choices];
  signal input index;
  signal output out;

  // Ensure that index &lt; choices
  component lessThan = LessThan(252);
  lessThan.in[0] &lt;== index;
  lessThan.in[1] &lt;== choices;
  lessThan.out === 1;

  component calcTotal = CalculateTotal(choices);
  component eqs[choices];

  // For each item, check whether its index equals the input index.
  for (var i = 0; i &lt; choices; i ++) {
    eqs[i] = IsEqual();
    eqs[i].in[0] &lt;== i;
    eqs[i].in[1] &lt;== index;

    // eqs[i].out is 1 if the index matches. As such, at most one input to
    // calcTotal is not 0.
    calcTotal.in[i] &lt;== eqs[i].out * in[i];
  }

  // Returns 0 + 0 + 0 + item
  out &lt;== calcTotal.out;
}
</code></pre>
<p>As an optimization, then step <code style='font-family: Arial'>component lessThan = LessThan(252);</code> doesn’t need 252 bits to ensure the <code style='font-family: Arial'>index</code> is less than <code style='font-family: Arial'>choices</code>. Depending on our application, we could use a much smaller number of bits to make the comparison and save on the number of constraints generated under the hood.</p>
<h2>Circomlib Implementation of Quin Selector</h2>
<p>The <a href="https://github.com/iden3/circomlib/blob/master/circuits/multiplexer.circom">multiplexer</a> in the Circomlib library accomplishes the same thing as Quin Selector. However, it indexes a 2-dimensional array and returns a 1-dimensional array. For example, given the array <code style='font-family: Arial'>in = [[5,5],[6,6],[7,7]]</code> and <code style='font-family: Arial'>index = 1</code>, it would return <code style='font-family: Arial'>[6, 6]</code>.</p>
<p>The component has the following inputs and outputs:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Multiplexer(wIn, nIn) {
  signal input inp[nIn][wIn];
  signal input sel;
  signal output out[wIn];

  // ...
}
</code></pre>
<p>Using the example <code style='font-family: Arial'>in = [[5,5],[6,6],[7,7]]</code>, <code style='font-family: Arial'>wIn</code> would be 2 and <code style='font-family: Arial'>nIn</code> would be 3. The signal <code style='font-family: Arial'>sel</code> is the index to pick; for example if <code style='font-family: Arial'>sel = 1</code> then <code style='font-family: Arial'>out = [6,6]</code>.</p>
<p>Instead of looping through the array and checking if the index <code style='font-family: Arial'>IsEqual</code> to the <code style='font-family: Arial'>sel</code> value, the Multiplexer generates a “mask” of all zeros with a 1 at the desired index and multiples that mask with the input. For example, if <code style='font-family: Arial'>sel = 1</code> it generates the mask <code style='font-family: Arial'>[0,1,0]</code> and multiplies the input by that mask.</p>
<p>Here is an example of using Circomlib’s multiplexer:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/multiplexer.circom&quot;;

template MultiplexerExample(n) {
  signal input in[n];
  signal input k;
  signal output out;

  component mux = Multiplexer(1, n);

  for (var i = 0; i &lt; n; i++) {
    mux.inp[i][0] &lt;== in[i];
  }
  mux.sel &lt;== k;

  out &lt;== mux.out[0];
}

component main = MultiplexerExample(4);

/* INPUT = {
  &quot;in&quot;: [3, 7, 9, 11],
  &quot;k&quot;: &quot;1&quot;
} */
</code></pre>
<h2>Historical Note</h2>
<p>This algorithm was referred to as “Linear Scan” in the <a href="https://akosba.github.io/papers/xjsnark.pdf">xjsnark paper</a>, which predates the eth Dark Forest implementation. Credit to <a href="https://x.com/0xerhant/status/1873831895055950247">0xerhant</a> for pointing this out.</p>
<div style='page-break-after: always;'></div>

<h1>Introduction to Stateful Computations in ZK</h1>
<p>Source: https://rareskills.io/post/stateful-zk</p>
<h1>Introduction to Stateful Computations in ZK</h1>
<p>When carrying out iterative computations such as powers, factorials, or computing the Fibonacci sequence, we need to “stop the computation” after a certain point.</p>
<p>For example, if we are computing $x^7$, we would multiply $x$ by itself seven times. However, conditional stopping is not possible in an arithmetic circuit. Since circuits are of a fixed size (the underlying R1CS has a fixed number of rows and you can’t change it), they must be large enough to account for every exponent we would care to compute.</p>
<p>Therefore, the solution is to compute every possible value up to some limit greater than what we expect to compute in practice. Then we use a Quin selector to pick the desired value.</p>
<p>This chapter shows an example of doing this with the factorial, Fibonacci sequence, and leaves computing the power as an exercise.</p>
<p>We can think of each of these computations as a state machine that goes through a fixed state-transition a certain number of times (where the number of iterations is determined at proving time and not baked into the circuit).</p>
<p>These sequences only have one possible computation at each step (e.g. add the previous two states or multiply the previous state by some number). However, if we add conditional branching at each state, then we have all the components necessary for stateful computation.</p>
<p>In this chapter we will only show examples where there is only one possible state-change and the number of state changes is variable. In the upcoming chapters we will show how to make the state transition itself be conditional, i.e. have multiple possible state transitions.</p>
<h2>Factorial Example</h2>
<p>We now show how to write a circuit that proves we correctly computed</p>
<p>$$<br />
y =x!\pmod p<br />
$$</p>
<p>where $!$ is the factorial and $p$ is the default field modulus.</p>
<p>To compute a factorial in a traditional programming language, such as Python, the code would be as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">def factorial_mod_p(x, p):
  if x == 0:
    return 1

  acc = 1
  for i in range(1, x+1):
    acc = (acc * i) % p

  return acc
</code></pre>
<p>However, the code above will have a few issues if directly translated to Circom:</p>
<ul>
<li>Circom does not support if statements, so the <code style='font-family: Arial'>if x == 0: return 0</code> line will not compile.</li>
<li>Circom does not support loops of an unknown number of iterations. Since <code style='font-family: Arial'>x</code> determines the value of the loop, this also won’t compile. Circom compiles to an R1CS under the hood, and the underlying R1CS needs to have a fixed size and can’t change size based on the value of the inputs.</li>
</ul>
<p>To make the code compatible with an arithmetization representation like R1CS, we need to compute the factorial from zero up to some upper bound that we intend to support.</p>
<p>For example, if we know we will never need to compute more than 99 factorial, then we must compute every factorial from 0 to 99 inclusive. If we want to create a proof for 80 factorial, we still need to compute the factorials from 0 to 99, but we use a Quin selector to return the result for 80.</p>
<p>Here is a Python example that has no if-statements and a fixed-length loop:</p>
<pre style='font-family: Arial'><code class="language-solidity">def factorial_mod_p(x, p):

  assert x &lt; 100
  # allocate the array
  ans = [0] * 100
  ans[0] = 1 # 0! = 1

  for i in range(1, 100):
      ans[i] = (ans[i-1] * i) % p

  return ans[x]
</code></pre>
<p>In a sense, we are creating an array of length 100 and populating the values with the factorial of that index. We will then “select” the factorial we care about using the Quin Selector.</p>
<p>The translation to Circom is straightforward:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;./node_modules/circomlib/circuits/multiplexer.circom&quot;;
include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template factorial(n) {
  signal input in;
  signal output out;

  // precompute factorials from 0 to n
  signal factorials[n+1];

  // compute the factorials
  factorials[0] &lt;== 1;
  for (var i = 1; i &lt;= n; i++) {
    factorials[i] &lt;== factorials[i - 1] * i;
  }

  // ensure that in &lt; n
  signal inLTn;
  inLTn &lt;== LessThan(252)([in, n]);
  inLTn === 1;

  // select the factorial of interest
  component mux = Multiplexer(1, n);
  mux.sel &lt;== in;

  // assign factorials into the multiplexer
  for (var i = 0; i &lt; n; i++) {
    mux.inp[i][0] &lt;== factorials[i];
  }

  out &lt;== mux.out[0];
}

component main = factorial(100);

/*
  INPUT = { &quot;in&quot;: &quot;3&quot; }
*/
</code></pre>
<h3>An insecure implementation</h3>
<p>Many engineers new to Circom often use an “intuitive” solution that avoids any issues with quadratic constraints and produces code such as the following:</p>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.8;

include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template factorial(n) {
  signal input in;
  signal output out;

  signal factorials[n + 1];

  // compute the factorials
  var acc = 1;
  for (var i = 1; i &lt; n; i++) {
    acc = acc * i;
  }

  out &lt;-- acc;
}

component main = factorial(100);
</code></pre>
<p>Although <code style='font-family: Arial'>out</code> will have the correct answer, the total absence of <code style='font-family: Arial'>&lt;==</code> or <code style='font-family: Arial'>===</code> operators means the circuit has no constraints.</p>
<p><strong>In the code above, the programmer has produced code to correctly compute the factorial, but not to constrain it.</strong></p>
<h2>Fibonacci Modulo p Example</h2>
<p>In the factorial example, we had to “hardcode” the 0th entry of <code style='font-family: Arial'>factorials</code> to be 1, since 0! = 1. In the Fibonacci sequence, the first two numbers are 1, and everything after that is the sum of the previous two numbers in the sequence. Therefore, for the Fibonacci code, we hardcode the first two values and then compute the rest.</p>
<p>The circuit below computes the Fibonacci sequence up to the nth number modulo p, then outputs the “in” Fibonacci number of interest.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;./node_modules/circomlib/circuits/multiplexer.circom&quot;;
include &quot;./node_modules/circomlib/circuits/comparators.circom&quot;;

template Fibonacci(n) {
  assert(n &gt;= 2); // so we don't break the hardcoding

  signal input in; // compute the kth Fibonacci number
  signal output out;

  // precompute Fibonacci sequence from
  // 0 to n
  signal fib[n + 1];

  // compute the factorials
  fib[0] &lt;== 1;
  fib[1] &lt;== 1;

  for (var i = 2; i &lt; n; i++) {
    fib[i] &lt;== fib[i - 1] + fib[i - 2];
  }

  // ensure that in &lt; n
  signal inLTn;
  inLTn &lt;== LessThan(252)([in, n]);
  inLTn === 1;

  // select the fibonacci number 
  // of interest
  component mux = Multiplexer(1, n);
  mux.sel &lt;== in;

  // assign Fibonacci into
  // the Quin Selector
  for (var i = 0; i &lt; n; i++) {
    mux.inp[i][0] &lt;== fib[i];
  }

  out &lt;== mux.out[0];
}

component main = Fibonacci(99);

/*
  INPUT = {&quot;in&quot;: 5}
*/
</code></pre>
<p>As usual, it is important to explicitly constrain each update of the Fibonacci sequence, and not simply compute the result in an unconstrained loop.</p>
<h3>Exercise:</h3>
<p>Complete the <a href="https://github.com/RareSkills/zero-knowledge-puzzles/blob/main/Power/pow.circom">pow exercise</a> in Circom Puzzles.</p>
<div style='page-break-after: always;'></div>

<h1>Swapping Two Items in an Array in Circom</h1>
<p>Source: https://rareskills.io/post/circom-array-swap</p>
<h1>Swapping Two Items in an Array in Circom</h1>
<p>This chapter shows how to swap two signals in a list of signals. This is an important subroutine for a sorting algorithm. More generally, lists are a fundamental building block for more interesting functions like hash functions or modeling memory in a CPU, so we must learn how to update their values.</p>
<p>Swapping two items in a list is one of the first things programmers learn, a typical solution looks like the following:</p>
<pre style='font-family: Arial'><code class="language-solidity"># s and t are indexes of array arr
def swap(arr, s, t):
  temp = arr[s];
  arr[s] = arr[t];
  arr[t] = temp;
  return arr
</code></pre>
<p>However, in a ZK circuit, this operation can be surprisingly tricky.</p>
<ul>
<li>First, we cannot directly index an array of signals. For that, we need to use a Quin selector.</li>
<li>Second, we cannot “write to” a signal in an array of signals because signals are immutable.</li>
</ul>
<p>Instead, we need to create a new array and copy the old values to the new array, subject to the following conditions:</p>
<ul>
<li>If we are at index <code style='font-family: Arial'>s</code>, write the value at <code style='font-family: Arial'>arr[t]</code></li>
<li>If we are at index <code style='font-family: Arial'>t</code>, write the value at <code style='font-family: Arial'>arr[s]</code></li>
<li>Otherwise, write the original value</li>
</ul>
<p>Every write we make to the new array is a conditional operation.</p>
<p>The Quin selector was explained in a prior chapter — we won’t replicate the code here to save space.</p>
<h2>Swap in Circom</h2>
<p>The component below will swap the item at index <code style='font-family: Arial'>s</code> with the item at index <code style='font-family: Arial'>t</code> and return a new array. (The following code has a bug, try to find it! The answer is given later.)</p>
<pre style='font-family: Arial'><code class="language-solidity">template Swap(n) {
  signal input in[n];
  signal input s;
  signal input t;
  signal output out[n];

  // we do not check that
  // s &lt; n or t &lt; n
  // because the Quin selector
  // does that

  // get the value at s
  component qss = QuinSelector(n);
  qss.idx &lt;== s;
  for (var i = 0; i &lt; n; i++) {
    qss.in[i] &lt;== in[i];
  }

  // get the value at t
  component qst = QuinSelector(n);
  qst.idx &lt;== t;
  for (var i = 0; i &lt; n; i++) {
    qst.in[i] &lt;== in[i];
  }

  // qss.out holds in[s]
  // qst.out holds in[t]

  component IdxEqS[n];
  component IdxEqT[n];
  component IdxNorST[n];
  signal branchS[n];
  signal branchT[n];
  signal branchNorST[n];
  for (var i = 0; i &lt; n; i++) {
    IdxEqS[i] = IsEqual();
    IdxEqS[i].in[0] &lt;== i;
    IdxEqS[i].in[1] &lt;== s;

    IdxEqT[i] = IsEqual();
    IdxEqT[i].in[0] &lt;== i;
    IdxEqT[i].in[1] &lt;== t;

    // if IdxEqS[i].out + IdxEqT[i].out
    // equals 0, then it is not i ≠ s and i ≠ t
    IdxNorST[i] = IsZero();
    IdxNorST[i].in &lt;== IdxEqS[i].out + IdxEqT[i].out;

    // if we are at index s,
    // write in[t]
    // if we are at index t,
    // write in[s]
    // else write in[i]
    branchS[i] &lt;== IdxEqS[i].out * qst.out;
    branchT[i] &lt;== IdxEqT[i].out * qss.out;
    branchNorST[i] &lt;== IdxNorST[i].out * in[i];
    out[i] &lt;==  branchS[i] + branchT[i] + branchNorST[i];
  }
}
</code></pre>
<p>Note that the final conditional statement</p>
<pre style='font-family: Arial'><code class="language-solidity">branchS[i] &lt;== IdxEqS[i].out * qst.out;
branchT[i] &lt;== IdxEqT[i].out * qss.out;
branchNorST[i] &lt;== IdxNorST[i].out * in[i];
out[i] &lt;==  branchS[i] + branchT[i] + branchNorST[i];
</code></pre>
<p>cannot be written as</p>
<pre style='font-family: Arial'><code class="language-solidity">out[i] &lt;==  IdxEqS[i].out * qst.out + IdxEqT[i].out * qss.out + IdxNorST[i].out * in[i]
</code></pre>
<p>because that would produce a non-quadratic constraints error (there is more than one multiplication in the constraint).</p>
<h2>Catch the bug</h2>
<p>There is a bug in the code above — can you catch it before scrolling down?</p>
<h2>The bug in the code</h2>
<p>The problem with the code above is that it doesn’t account for the fact that the value at <code style='font-family: Arial'>s</code> might equal the value at <code style='font-family: Arial'>t</code> (<code style='font-family: Arial'>s == t</code>). In that circumstance, the value written to the index will be the value at that index added to itself.</p>
<h2>Fixing the problem</h2>
<p>To prevent this, we need to explicitly detect if <code style='font-family: Arial'>s == t</code> and multiply one of either <code style='font-family: Arial'>branchS</code> or <code style='font-family: Arial'>branchT</code> by zero to avoid doubling the value. In other words, if the switches for <code style='font-family: Arial'>s</code> and <code style='font-family: Arial'>t</code> are both active, then the resulting value would be <code style='font-family: Arial'>s + t</code>. But we don’t want that, we want the value to effectively remain unchanged by selecting <code style='font-family: Arial'>branchS</code> or <code style='font-family: Arial'>branchT</code> arbitrarily (they will have the same value):</p>
<pre style='font-family: Arial'><code class="language-solidity">template Swap(n) {
  signal input in[n];
  signal input s;
  signal input t;
  signal output out[n];

  // NEW CODE to detect if s == t
  signal sEqT;
  sEqT &lt;== IsEqual()([s, t]);

  // get the value at s
  component qss = QuinSelector(n);
  qss.idx &lt;== s;
  for (var i = 0; i &lt; n; i++) {
    qss.in[i] &lt;== in[i];
  }

  // get the value at t
  component qst = QuinSelector(n);
  qst.idx &lt;== t;
  for (var i = 0; i &lt; n; i++) {
    qst.in[i] &lt;== in[i];
  }

  component IdxEqS[n];
  component IdxEqT[n];
  component IdxNorST[n];
  signal branchS[n];
  signal branchT[n];
  signal branchNorST[n];
  for (var i = 0; i &lt; n; i++) {
    IdxEqS[i] = IsEqual();
    IdxEqS[i].in[0] &lt;== i;
    IdxEqS[i].in[1] &lt;== s;

    IdxEqT[i] = IsEqual();
    IdxEqT[i].in[0] &lt;== i;
    IdxEqT[i].in[1] &lt;== t;

    // if IdxEqS[i].out + IdxEqT[i].out
    // equals 0, then it is not i ≠ s and i ≠ t
    IdxNorST[i] = IsZero();
    IdxNorST[i].in &lt;== IdxEqS[i].out + IdxEqT[i].out;

    // if we are at index s, write in[t]
    // if we are at index t, write in[s]
    // else write in[i]
    branchS[i] &lt;== IdxEqS[i].out * qst.out;
    branchT[i] &lt;== IdxEqT[i].out * qss.out;
    branchNorST[i] &lt;== IdxNorST[i].out * in[i];

    // multiply branchS by zero if s equals T
    out[i] &lt;==  (1-sEqT) * (branchS[i]) + branchT[i] + branchNorST[i];
  }
}
</code></pre>
<h2>Conclusion</h2>
<p>Any array manipulation in Circom requires creating a new array and copying the old values to the new one, except where the update happens.</p>
<p>By using this pattern in a loop, we can do things like sort a list, model data structures like stacks and queues, and even change the state of a CPU or VM. We will see examples of those in the following chapters.</p>
<div style='page-break-after: always;'></div>

<h1>ZK Proof of Selection Sort</h1>
<p>Source: https://rareskills.io/post/sort-circuit</p>
<h1>ZK Proof of Selection Sort</h1>
<p>Most computations of interest are generally “stateful” — that is, they need to go through a series of steps to produce the final result.</p>
<p>Sometimes, we do not need to show we executed the computation but only show the result. For example, if A is a list, we can prove B is the sorted version of list A by showing B is a permutation of A, and all the elements of B are in order. There is no need to show that we executed each step of the sorting algorithm correctly. We’ve already shown how to prove the elements of a list are in order, but efficiently proving that one list is a permutation of the other is surprisingly tricky, so we will introduce that technique later.</p>
<p>In general, there are many realistic computations that do not allow for simply proving that the result is correct. Notably, proving that we correctly executed <code style='font-family: Arial'>sha256("RareSkills")</code> requires actually executing every step of the hash function correctly.</p>
<p>Since hash functions are a bit intimidating, we introduce the concept of stateful computation by showing how to prove we carried out Selection Sort on a list correctly. As noted above, this approach is “overkill” because it is simpler to prove the output list is a sorted permutation of the input — it does not matter what algorithm we used to sort the list.</p>
<p>However, we still show the Selection Sort algorithm as we consider it a gentle introduction to stateful computation.</p>
<p>Selection Sort works by</p>
<ul>
<li>iterating through the list</li>
<li>at each index <code style='font-family: Arial'>i</code>, comparing the value at <code style='font-family: Arial'>i</code> to the sublist containing <code style='font-family: Arial'>i</code> and every item in front (<code style='font-family: Arial'>i..n-1</code> inclusive)</li>
<li>swapping the item at <code style='font-family: Arial'>i</code> with the minimum of the sublist <code style='font-family: Arial'>i..n-1</code></li>
</ul>
<p>Selection Sort is illustrated in the animation below:</p>
<p><a href="https://r2media.rareskills.io/SortCircuit/SelectionSortC.mp4"></a></p>
<p>Since signals are immutable in ZK circuits, every time we swap, we need to create a new list. For example, if we sorted [5,2,3,4], the sequence of state transitions would be:</p>
<ol>
<li>i = 0, [5,2,3,4] —&gt; swap —&gt; [2,5,3,4]</li>
<li>i = 1, [2,5,3,4] —&gt; swap —&gt; [2,3,5,4]</li>
<li>i = 2, [2,3,5,4] —&gt; swap —&gt; [2,3,4,5]</li>
</ol>
<p>To prove we executed Selection Sort properly, we need to prove that at iteration <code style='font-family: Arial'>i</code>, we swapped the item at <code style='font-family: Arial'>i</code> with the minimum of the sublist <code style='font-family: Arial'>i…n - 1</code>. We’ve already built up most of the requisite components for this in the previous chapters:</p>
<ul>
<li>We can prove that a certain item is a minimum of a list, and that it is at a certain index.</li>
<li>We can prove that we swapped two items in a list.</li>
</ul>
<p>In this chapter, we simply combine these components together. To start, we build a template that proves we correctly identified the index of the minimum value in a sublist:</p>
<pre style='font-family: Arial'><code class="language-solidity">template GetMinAtIdx(n) {
  signal input in[n];

  // compute and constrain min and idx
  // to be the min value in the list
  // and the index of the minimum value
  signal output min;
  signal output idx;

  // compute the minimum and its index
  // outside of the constraints
  var minv = in[0];
  var idxv = 0;
  for (var i = 1; i &lt; n; i++) {
    if (in[i] &lt; minv) {
      minv = in[i];
      idxv = i;
    }
  }
  min &lt;-- minv;
  idx &lt;-- idxv;

  // constrain that min is ≤ all others
  component lte[n];
  for (var i = 0 ; i &lt; n; i++) {
    lte[i] = LessEqThan(252);
    lte[i].in[0] &lt;== min;
    lte[i].in[1] &lt;== in[i];
    lte[i].out === 1;
  }

  // assert min is really at in[idx]
  component qs = QuinSelector(n);
  qs.index &lt;== idx;
  for (var i = 0; i &lt; n; i++) {
    qs.in[i] &lt;== in[i];
  }
  qs.out === min;
}
</code></pre>
<h2>One iteration of the sorting algorithm</h2>
<p>The first step in Selection Sort is to swap the item at index 0 with the minimum item in the entire list (which could be the item at index 0). Below is the code for swapping the number at a particular index with the minimum item in front of it.</p>
<pre style='font-family: Arial'><code class="language-solidity">template Swap(n) {
  signal input in[n];
  signal input s;
  signal input t;
  signal output out[n];

  // NEW CODE to detect if s == t
  signal sEqT;
  sEqT &lt;== IsEqual()([s, t]);

  // get the value at s
  component qss = QuinSelector(n);
  qss.index &lt;== s;
  for (var i = 0; i &lt; n; i++) {
    qss.in[i] &lt;== in[i];
  }

  // get the value at t
  component qst = QuinSelector(n);
  qst.index &lt;== t;
  for (var i = 0; i &lt; n; i++) {
    qst.in[i] &lt;== in[i];
  }

  component IdxEqS[n];
  component IdxEqT[n];
  component IdxNorST[n];
  signal branchS[n];
  signal branchT[n];
  signal branchNorST[n];
  for (var i = 0; i &lt; n; i++) {
    dxEqS[i] = IsEqual();
    dxEqS[i].in[0] &lt;== i;
    dxEqS[i].in[1] &lt;== s;

    dxEqT[i] = IsEqual();
    dxEqT[i].in[0] &lt;== i;
    dxEqT[i].in[1] &lt;== t;

    / if IdxEqS[i].out + IdxEqT[i].out
    / equals 0, then it is not i ≠ s and i ≠ t
    dxNorST[i] = IsZero();
    dxNorST[i].in &lt;== IdxEqS[i].out + IdxEqT[i].out;

    / if we are at index s, write in[t]
    / if we are at index t, write in[s]
    / else write in[i]
    ranchS[i] &lt;== IdxEqS[i].out * qst.out;
    ranchT[i] &lt;== IdxEqT[i].out * qss.out;
    ranchNorST[i] &lt;== IdxNorST[i].out * in[i];

    / multiply branchS by zero if s equals t
    ut[i] &lt;==  (1-sEqT) * (branchS[i]) + branchT[i] + branchNorST[i];
  }
}

template Select(n, start) {
  // unsorted list
  signal input in[n];

  // index start swapped with the min
  signal output out[n];

  // we will define GetMinAtIdxStartingAt in the next codeblock
  component minIdx0 = GetMinAtIdxStartingAt(n, start);
  for (var i = 0; i &lt; n; i++) {
      minIdx0.in[i] &lt;== in[i];
  }

  component Swap0 = Swap(n);
  Swap0.s &lt;== start; // swap 0 with the min
  Swap0.t &lt;== minIdx0.idx; // with the min (could be idx 0)
  for (var i = 0; i &lt; n; i++) {
      Swap0.in[i] &lt;== in[i];
  }

  // copy to out
  for (var i = 0; i &lt; n; i++) {
      out[i] &lt;== Swap0.out[i];
  }
}
</code></pre>
<p>Of course, we ought to parameterize this because we are going to repeat this process for indexes <code style='font-family: Arial'>0…n - 2</code>. To do this, we will modify <code style='font-family: Arial'>GetMinAtIdx</code> to only consider values after a <code style='font-family: Arial'>start</code> index:</p>
<pre style='font-family: Arial'><code class="language-solidity">// formerly GetMinAtIdx
template GetMinAtIdxStartingAt(n, start) {
  signal input in[n];
  signal output min;
  signal output idx;

  // only look for values start and later
  var minv = in[start];
  var idxv = start;
  for (var i = start + 1; i &lt; n; i++) {
    if (in[i] &lt; minv) {
      minv = in[i];
      idxv = i;
    }
  }
  min &lt;-- minv;
  idx &lt;-- idxv;

  // only compare to values start and later
  component lt[n];

  // CHANGES HERE: LOOP FROM START TO N-1
  for (var i = start ; i &lt; n; i++) {
    lt[i] = LessEqThan(252);
    lt[i].in[0] &lt;== min;
    lt[i].in[1] &lt;== in[i];
    lt[i].out === 1;
  }

  // Quin Selector -- ensure that
  // assert min is really at in[idx]
  component qs = QuinSelector(n);
  qs.index &lt;== idx;
  for (var i = 0; i &lt; n; i++) {
    qs.in[i] &lt;== in[i];
  }
  qs.out === min;
}
</code></pre>
<h2>Final Algorithm</h2>
<p>To prove we carried out Selection Sort properly, we simply repeat the template above <code style='font-family: Arial'>n - 2</code> times.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/comparators.circom&quot;;

// ----QUIN SELECTOR ----
template CalculateTotal(n) {
  signal input in[n];
  signal output out;

  signal sums[n];

  sums[0] &lt;== in[0];

  for (var i = 1; i &lt; n; i++) {
    sums[i] &lt;== sums[i-1] + in[i];
  }

  out &lt;== sums[n-1];
}

// from https://github.com/darkforest-eth/circuits/blob/master/perlin/QuinSelector.circom
template QuinSelector(choices) {
  signal input in[choices];
  signal input index;
  signal output out;

  // Ensure that index &lt; choices
  component lessThan = LessThan(4);
  lessThan.in[0] &lt;== index;
  lessThan.in[1] &lt;== choices;
  lessThan.out === 1;

  component calcTotal = CalculateTotal(choices);
  component eqs[choices];

  // For each item, check whether its index equals the input index.
  for (var i = 0; i &lt; choices; i ++) {
    eqs[i] = IsEqual();
    eqs[i].in[0] &lt;== i;
    eqs[i].in[1] &lt;== index;

    // eqs[i].out is 1 if the index matches. As such, at most one input to
    // calcTotal is not 0.
    calcTotal.in[i] &lt;== eqs[i].out * in[i];
  }

  // Returns 0 + 0 + 0 + item
  out &lt;== calcTotal.out;
}

// Given array in[n]
// swap the items at index
// s and t
template Swap(n) {
  signal input in[n];
  signal input s;
  signal input t;
  signal output out[n];

  // NEW CODE to detect if s == t
  signal sEqT;
  sEqT &lt;== IsEqual()([s, t]);

  // get the value at s
  component qss = QuinSelector(n);
  qss.index &lt;== s;
  for (var i = 0; i &lt; n; i++) {
    qss.in[i] &lt;== in[i];
  }

  // get the value at t
  component qst = QuinSelector(n);
  qst.index &lt;== t;
  for (var i = 0; i &lt; n; i++) {
    qst.in[i] &lt;== in[i];
  }

  component IdxEqS[n];
  component IdxEqT[n];
  component IdxNorST[n];
  signal branchS[n];
  signal branchT[n];
  signal branchNorST[n];
  for (var i = 0; i &lt; n; i++) {
    IdxEqS[i] = IsEqual();
    IdxEqS[i].in[0] &lt;== i;
    IdxEqS[i].in[1] &lt;== s;

    IdxEqT[i] = IsEqual();
    IdxEqT[i].in[0] &lt;== i;
    IdxEqT[i].in[1] &lt;== t;

    // if IdxEqS[i].out + IdxEqT[i].out
    // equals 0, then it is not i ≠ s and i ≠ t
    IdxNorST[i] = IsZero();
    IdxNorST[i].in &lt;== IdxEqS[i].out + IdxEqT[i].out;

    // if we are at index s, write in[t]
    // if we are at index t, write in[s]
    // else write in[i]
    branchS[i] &lt;== IdxEqS[i].out * qst.out;
    branchT[i] &lt;== IdxEqT[i].out * qss.out;
    branchNorST[i] &lt;== IdxNorST[i].out * in[i];

    // multiply branchS by zero if s equals t
    out[i] &lt;==  (1-sEqT) * (branchS[i]) + branchT[i] + branchNorST[i];
  }
}

// Find the smallest element starting
// at index start
template GetMinAtIdxStartingAt(n, start) {
  signal input in[n];
  signal output min;
  signal output idx;

  // only look for values start and later
  var minv = in[start];
  var idxv = start;
  for (var i = start + 1; i &lt; n; i++) {
    if (in[i] &lt; minv) {
      minv = in[i];
      idxv = i;
    }
  }
  min &lt;-- minv;
  idx &lt;-- idxv;

  // only compare to values start and later
  component lt[n];

  // CHANGES HERE: LOOP FROM START TO N-1
  for (var i = start ; i &lt; n; i++) {
    lt[i] = LessEqThan(252);
    lt[i].in[0] &lt;== min;
    lt[i].in[1] &lt;== in[i];
    lt[i].out === 1;
  }

  // Quin Selector -- ensure that
  // assert min is really at in[idx]
  component qs = QuinSelector(n);
  qs.index &lt;== idx;
  for (var i = 0; i &lt; n; i++) {
    qs.in[i] &lt;== in[i];
  }
  qs.out === min;
}

// Given an array in, swap
// start with the smallest element
// in front of it
template Select(n, start) {
  // unsorted list
  signal input in[n];

  // index 0 swapped with the min
  signal output out[n];

  component minIdx0 = GetMinAtIdxStartingAt(n, start);
  for (var i = 0; i &lt; n; i++) {
    minIdx0.in[i] &lt;== in[i];
  }

  component Swap0 = Swap(n);
  Swap0.s &lt;== start; // swap 0 with the min
  Swap0.t &lt;== minIdx0.idx; // with the min (could be idx 0)
  for (var i = 0; i &lt; n; i++) {
    Swap0.in[i] &lt;== in[i];
  }

  // copy to out
  for (var i = 0; i &lt; n; i++) {
    out[i] &lt;== Swap0.out[i];
  }
}

// ---- CORE ALGORITHM ----
template SelectionSort(n) {
  assert(n &gt; 0);

  signal input in[n];
  signal output out[n];

  signal intermediateStates[n][n];

  component SSort[n - 1];
  for (var i = 0; i &lt; n; i++) {
    // copy the input to the first row of
    // intermediateStates. Note that we can do
    // if(i == 0) because i is not a signal
    // and i is known at compile time
    if (i == 0) {
      for (var j = 0; j &lt; n; j++) {
        intermediateStates[0][j] &lt;== in[j];
      }
    }

    else {
      // select sort n items starting at i - 1
      // for i = 1, we compare item at 0 to
      // the rest of the list
      SSort[i - 1] = Select(n, i - 1);

      // load in the intermediate state i -1
      for (var j = 0; j &lt; n; j++) {
        SSort[i - 1].in[j] &lt;== intermediateStates[i - 1][j];
      }
      // write the sorted result to row i
      for (var j = 0; j &lt; n; j++) {
        SSort[i - 1].out[j] ==&gt; intermediateStates[i][j];
      }
    }
  }

  // write the final state to the ouput
  for (var i = 0; i &lt; n; i++) {
    intermediateStates[n-1][i] ==&gt; out[i];
  }
}

component main = SelectionSort(9);

/* INPUT = {&quot;in&quot;: [3,1,8,2,4,0,1,2,4]} */
</code></pre>
<h1>Conclusion</h1>
<p>The concept of an “intermediate state” and proving that we moved between intermediate states correctly is core to the verification of most ZK algorithms proved in practice, notably hash functions and ZK Virtual Machines. The Selection Sort algorithm presented in this chapter provides a gentle introduction to stateful computation.</p>
<div style='page-break-after: always;'></div>

<h1>Modeling the Stack Data Structure in ZK</h1>
<p>Source: https://rareskills.io/post/zk-stack</p>
<h1>Modeling the Stack Data Structure in ZK</h1>
<p>This tutorial shows how to create a stack in Circom.</p>
<p>Be warned — this chapter is long.</p>
<p>However, the strategy for creating ZK proofs about stacks will be essential when we create a simple ZK Virtual Machine (ZKVM) in the next chapter. Most of the work for understanding how a ZKVM works has been front-loaded to this chapter.</p>
<p>The stack is able to <strong>push</strong> a number to the top of the stack, <strong>pop</strong> the top of the stack, or make <strong>no changes</strong>.</p>
<p>Stacks are inherently mutable, but in Circom, we are only allowed to use an immutable array. Therefore, a stack must be modeled with an immutable array. Each time the stack changes (via pop or push), we create a new array that represents the new stack state.</p>
<p>This might seem inefficient, but there is no way around this as signals in ZK are immutable (more advanced ZK techniques have ways of optimizing this, but that discussion is outside the scope of this article).</p>
<p>The first requirement is that the stack has a fixed “maximum height” — which is how long the immutable array is. To track how much of the stack is “used,” we employ a separate variable called a “stack pointer,” which is a zero-based index and tells us where to push the next item. <code style='font-family: Arial'>sp</code> points to an unused placed in the array where a new value should be pushed, which is one index above the top of the stack. The diagram below illustrates a stack with three items, 10, 16, and 20:</p>
<p>$$<br />
\begin{align*}\texttt{sp}=3\<br />
\begin{array}{|c|c|c|c|c|c|c|}<br />
\hline<br />
\text{array}&amp;10 &amp; 16 &amp; 20 &amp; \circ &amp; \space &amp; \space \ \hline<br />
\text{index}&amp;0 &amp; 1 &amp;2 &amp;3 &amp;4 &amp;5\<br />
\hline<br />
\end{array}<br />
\end{align*}<br />
$$</p>
<p>The stack pointer <code style='font-family: Arial'>sp</code> is pointing to index 3, which is empty and denoted with $\circ$. The stack ignores any value at index 3 (the current <code style='font-family: Arial'>sp</code>) or greater. They could have non-zero values, but the circuit would not consider them.</p>
<p>Suppose we push the item <code style='font-family: Arial'>21</code> onto the stack. This means we increment the stack pointer and copy all the previous values. The updated stack now has <code style='font-family: Arial'>sp</code> = 4.</p>
<p>$$<br />
\begin{align*}\texttt{sp}=4\<br />
\begin{array}{|c|c|c|c|c|c|}<br />
\hline<br />
10&amp; 16 &amp; 20 &amp; \circ &amp; \space \ \hline<br />
10 &amp; 16 &amp; 20 &amp; 21&amp; \circ &amp; \space \ \hline<br />
\end{array}<br />
\end{align*}<br />
$$</p>
<p>If we pop from the stack, the <code style='font-family: Arial'>sp</code> is decremented and we do not copy the value <code style='font-family: Arial'>21</code> into the next stack:</p>
<p>$$<br />
\begin{align*}\texttt{sp}=3\<br />
\begin{array}{|c|c|c|c|c|c|}<br />
\hline<br />
10 &amp; 16 &amp; 20 &amp; \circ &amp; \space \ \hline<br />
10 &amp; 16 &amp; 20 &amp; 21&amp; \circ &amp; \space \ \hline<br />
10 &amp; 16 &amp; 20 &amp; \circ&amp; \space &amp; \space \ \hline<br />
\end{array}<br />
\end{align*}<br />
$$</p>
<p>If no changes happen to the stack, we still go through some kind of computation step, where we simply copy all the previous values to the next stack.</p>
<p>$$<br />
\begin{align*}\texttt{sp}=3\<br />
\begin{array}{|c|c|c|c|c|c|}<br />
\hline<br />
10 &amp; 16 &amp; 20 &amp; \circ &amp; \space \ \hline<br />
10 &amp; 16 &amp; 20 &amp; 21&amp; \circ &amp; \space \ \hline<br />
10 &amp; 16 &amp; 20 &amp; \circ&amp; \space &amp; \space \ \hline<br />
10 &amp; 16 &amp; 20 &amp; \circ&amp; \space &amp; \space \ \hline<br />
\end{array}<br />
\end{align*}<br />
$$</p>
<p>Since <code style='font-family: Arial'>sp</code> changes with each iteration, we need to store the new value in a new signal each time because the value of a signal can’t be updated once assigned. Therefore, we use an array to track the value of <code style='font-family: Arial'>sp</code> on each iteration:</p>
<p>$$<br />
\begin{array}{|c||c|c|c|c|c|c|}<br />
\hline<br />
\texttt{sp}\downarrow &amp; \<br />
\hline<br />
3 &amp; 10 &amp; 16 &amp; 20 &amp; \circ &amp; \space \ \hline<br />
4 &amp; 10 &amp; 16 &amp; 20 &amp; 21&amp; \circ &amp; \space \ \hline<br />
3 &amp;10 &amp; 16 &amp; 20 &amp; \circ&amp; \space &amp; \space \ \hline<br />
3 &amp;10 &amp; 16 &amp; 20 &amp; \circ&amp; \space &amp; \space \ \hline<br />
\end{array}<br />
$$</p>
<p>Just as there is a maximum height of the stack, there is also a maximum number of times we can update it, as the table modeling the stack (a 2D Circom array under the hood) must be of fixed size.</p>
<p>The maximum size depends on our application. In a blockchain setting, it is very unlikely 1 million instructions can be executed, but when creating circuits for more intensive applications we may need to allocate a larger circuit to accommodate potential stack sizes.</p>
<h3>Constraints for the stack</h3>
<p>Regardless of whether we push, pop, or do nothing, the items from 0 to <code style='font-family: Arial'>sp - 2</code> inclusive need to be copied (and constrained to be equal) to the next stack. In the example below, the stack pointer was at 4, then we executed a pop operation, and the stack pointer became 3. The items at stack pointer index 0, 1, 2 (in orange) were copied.</p>
<p>$$<br />
\begin{array}{|c||c|c|c|c|c|c|}<br />
\hline<br />
\texttt{sp} &amp; 0 &amp; 1 &amp; 2 &amp;3 &amp; 4 &amp; 5\<br />
\hline<br />
4 &amp; \color{orange}{10} &amp; \color{orange}{16} &amp; \color{orange}{20} &amp; 21&amp; \circ &amp; \space \ \hline<br />
3 &amp;\color{orange}{10} &amp; \color{orange}{16} &amp; \color{orange}{20} &amp; \circ&amp; \space &amp; \space \space \ \hline<br />
\end{array}<br />
$$</p>
<p>If the instruction is pop, then we also require that the new stack pointer is 1 less than the previous.</p>
<h3>Constraints for push</h3>
<p>During a push, all the values up to <code style='font-family: Arial'>sp - 1</code> must be copied into the new stack (note that the stack pointer points to an empty region, so there is no need to copy it). The value at <code style='font-family: Arial'>sp - 1</code> must be constrained to be the value pushed.</p>
<p>$$<br />
\begin{array}{|c||c|c|c|c|c|c|}<br />
\hline<br />
\texttt{sp} &amp; 0 &amp; 1 &amp; 2 &amp;3 &amp; 4 &amp; 5\<br />
\hline<br />
3 &amp;\color{orange}{10} &amp; \color{orange}{16} &amp; \color{orange}{20} &amp; \circ&amp; \space &amp; \space<br />
\space \ \hline<br />
4 &amp; \color{orange}{10} &amp; \color{orange}{16} &amp; \color{orange}{20} &amp; 24&amp; \circ &amp; \space \ \hline<br />
\end{array}<br />
$$</p>
<p>In the example above, the stack pointer was 3, and we copied the items in 0, 1, 2. We also constrain that the stack pointer increments by 1. We must also constrain the stack pointer to be one larger than it was previously.</p>
<h3>Constraints for pop</h3>
<p>During a pop, all values up to <code style='font-family: Arial'>sp - 2</code> must be copied into the new stack. We constrain the stack pointer to decrement by one.</p>
<p>$$<br />
\begin{array}{|c||c|c|c|c|c|c|}<br />
\hline<br />
\texttt{sp} &amp; 0 &amp; 1 &amp; 2 &amp;3 &amp; 4 &amp; 5\<br />
\hline<br />
3 &amp;\color{orange}{10} &amp; \color{orange}{16} &amp; \color{orange}{20} &amp; \circ&amp; \space &amp; \space<br />
\space \ \hline<br />
2 &amp; \color{orange}{10} &amp; \color{orange}{16} &amp; \circ &amp; &amp; &amp; \space \ \hline<br />
\end{array}<br />
$$</p>
<h3>Constraints for nop (do nothing)</h3>
<p>All values up to <code style='font-family: Arial'>sp - 1</code> must be constrained to be the same. The <code style='font-family: Arial'>sp</code> must be constrained to equal the previous iteration’s value.</p>
<p>$$<br />
\begin{array}{|c||c|c|c|c|c|c|}<br />
\hline<br />
\texttt{sp} &amp; 0 &amp; 1 &amp; 2 &amp;3 &amp; 4 &amp; 5\<br />
\hline<br />
3 &amp;\color{orange}{10} &amp; \color{orange}{16} &amp; \color{orange}{20} &amp; \circ&amp; \space &amp; \space<br />
\space \ \hline<br />
3 &amp; \color{orange}{10} &amp; \color{orange}{16} &amp; \color{orange}{20} &amp; \circ&amp; &amp; \space \ \hline<br />
\end{array}<br />
$$</p>
<h2>Updating a stack per a set of instructions</h2>
<p>At each iteration of the instructions, we need to know if we are going to push a number, pop the top, or do nothing, which we will denote with “opcodes” or “instructions” PUSH, POP, or NOP.</p>
<p>For example, suppose we are given the instructions <code style='font-family: Arial'>PUSH 10 POP PUSH 16 PUSH 15 PUSH 4 NOP POP</code>. We could represent this set of instructions as an array:</p>
<p>[PUSH, 10, POP, 0, PUSH, 16, PUSH, 15, PUSH, 4, NOP, 0, POP, 0]</p>
<p>Without loss of generality, we could refer to PUSH as the number 1, POP as 2, and NOP as 0, so the instructions could be encoded as follows.</p>
<p>$$<br />
\begin{matrix}<br />
1 &amp; 10 &amp; 2 &amp; 0 &amp; 1 &amp; 16 &amp; 1 &amp; 15 &amp; 1 &amp; 4 &amp; 0 &amp; 0 &amp; 2&amp; 0\<br />
\texttt{PUSH}&amp;&amp;\texttt{POP}&amp;&amp;\texttt{PUSH}&amp;&amp;\texttt{PUSH}&amp;&amp;\texttt{PUSH}&amp;&amp;\texttt{NOP}&amp;&amp;\texttt{POP}<br />
\end{matrix}<br />
$$</p>
<p>Note that each instruction always has a constant after it. For PUSH, this is the value to push, but for POP and NOP, the constant afterward is ignored. Putting the instruction at indexes 0,2,4,… etc., allows us to iterate through the instructions in steps of two. In other words, if the instructions could have an argument or not, then we would need to change the step size depending on whether there is an argument. This conditional step size increases the complexity of our example, so we put the opcodes at even intervals so we can always make a step of two.</p>
<p>Now, let’s generate a “metaTable” which tells us which operation will happen at each row of the execution. If we add columns <code style='font-family: Arial'>is_push</code>, <code style='font-family: Arial'>is_pop</code>, or <code style='font-family: Arial'>is_nop</code> which indicate which instruction is active, then we get the following table.</p>
<p><a href="https://r2media.rareskills.io/ZkStack/stack.mp4"></a></p>
<p>The final result would look like the following, but we will reconstruct this table step-by-step in the upcoming section:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; 10 &amp; – &amp; – &amp; – \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; – &amp; – &amp; – &amp; – \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; 16 &amp; – &amp; – &amp; – \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; 16 &amp; 15 &amp; – &amp; – \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; 16 &amp; 15 &amp; 4 &amp; – \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; 16 &amp; 15 &amp; 4 &amp; – \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 3 &amp; 16 &amp; 15 &amp; – &amp; – \\hline\end{array}<br />
$$</p>
<p>The interpretation of <code style='font-family: Arial'>sp</code> is where the next value should be written if the current instruction is push. If the instruction is <code style='font-family: Arial'>pop</code>, then <code style='font-family: Arial'>sp - 1</code> is the item that should not be copied.</p>
<h2>Populating the table</h2>
<p>To populate the table from <code style='font-family: Arial'>is_push</code> to <code style='font-family: Arial'>arg</code>, we can simply copy the instructions in a loop and set <code style='font-family: Arial'>is_push</code> to 1 if the current instruction is PUSH, and so forth for the other instructions. We would get the following result:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>The stack pointer must always start at zero, so we can simply hardcode that:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_apop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; \boxed{0}&amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>We can populate the remainder of the stack pointer column by incrementing if the instruction is PUSH, decrementing for POP, and keeping it the same for NOP. Note that we update the <em>next</em> for <code style='font-family: Arial'>sp</code> based on the current row. Therefore, on iteration 0, we update row 1 as follows:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_apop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; \boxed{0}&amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; \boxed{1}&amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>That is, because the current <code style='font-family: Arial'>is_push</code> is 1, and the current <code style='font-family: Arial'>sp</code> is 0, we write 0 + 1 to the next cell. We then fill up the rest of the <code style='font-family: Arial'>sp</code> column as follows:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; &amp; &amp; &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 3 &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>We then write the push values to the appropriate cell simply by using the <code style='font-family: Arial'>sp</code> and <code style='font-family: Arial'>arg</code> columns conditioned on if <code style='font-family: Arial'>is_push</code> is 1. Note that only rows where <code style='font-family: Arial'>is_push == 1</code> are populated in this step:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; 10 &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; 16 &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; &amp; 15 &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; &amp; &amp; 4 &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 3 &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>Note that the previous values are not copied over — we will fix that in a later section.</p>
<h3>Handling the zeroth row</h3>
<p>The zeroth row is unique because it doesn’t copy and values from the row above it. To avoid having to branch on the basis of whether we are in the zeroth row or not, we make the table one row larger than it needs to be, and then we start building our constraints at row 1. That way, each row always copies the row above it.</p>
<p>In the upcoming code demo, we hardwire the constraints in the 0th row because later, it will be inconvenient to treat the 0th row as a corner case for whether or not we copy the previous row’s values.</p>
<h2>Copying the previous row</h2>
<p>Now, we need to ensure that values from one row to the next are copied properly. A cell copies the value above it up to the next row’s stack pointer minus 1. Remember, <code style='font-family: Arial'>sp</code> points to the empty space above the stack, so <code style='font-family: Arial'>sp - 1</code> points to the top of the stack.</p>
<h3>Column and Stack Terminology</h3>
<p>Because we are storing the stack “sideways” in a table, we will refer to the bottom of the stack as column 0, the item on top of that column (if it exists) column 1, and so forth. When we say “column” we mean the “index” of the stack if the bottom one is zero.</p>
<h3>Constraints for copying</h3>
<ol>
<li>if <code style='font-family: Arial'>is_push = 1</code>, then all the items of the stack <code style='font-family: Arial'>0..sp - 1</code> inclusive must be copied. The cell at <code style='font-family: Arial'>sp</code> will contain the new incremented value. This copies the entire stack.</li>
<li>if <code style='font-family: Arial'>is_nop = 1</code>, then all the items of the stack <code style='font-family: Arial'>0..sp - 1</code> inclusive must be copied. Nothing is written to the cell at <code style='font-family: Arial'>sp</code>. This copies the entire stack.</li>
<li>if <code style='font-family: Arial'>is_pop = 1</code>, then all the items of the stack <code style='font-family: Arial'>0..sp - 2</code> inclusive must be copied. Remember, <code style='font-family: Arial'>sp</code> points to an empty cell above the stack, so <code style='font-family: Arial'>sp - 1</code> is the value that will be popped. Everything <code style='font-family: Arial'>sp - 2</code> and below must be copied. This copies everything except the top of the stack.</li>
</ol>
<p>The conditions at 2 and 3 have corner cases if the stack pointer is 0 or 1, respectively, because an underflow would happen. Therefore, we need special columns that indicate if <code style='font-family: Arial'>sp</code> is less than 2 or 1, and we need to handle the copying differently. Specifically:</p>
<ul>
<li>if <code style='font-family: Arial'>sp = 0</code>, nothing will be copied</li>
<li>if <code style='font-family: Arial'>sp = 1</code>, we will copy the cell at column 0 only if the instruction is NOP or PUSH</li>
</ul>
<p>We create additional columns (<code style='font-family: Arial'>copy0</code>, <code style='font-family: Arial'>copy1</code>, <code style='font-family: Arial'>copy2</code>, <code style='font-family: Arial'>copy3</code>) that serve as flags to indicate if the value for (<code style='font-family: Arial'>column0</code>, <code style='font-family: Arial'>column1</code>, <code style='font-family: Arial'>column2</code>, <code style='font-family: Arial'>column3</code>) respectively will be copied to the next row.</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} &amp; \texttt{copy0} &amp; \texttt{copy1} &amp; \texttt{copy2} &amp; \texttt{copy3} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; &amp; &amp; &amp; &amp; 10 &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; &amp; &amp; &amp; &amp; 16 &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; 15 &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; &amp; &amp; &amp; &amp; &amp; &amp; 4 &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<h3>Handling initial conditions</h3>
<p>We have another corner case when creating constraints for the 0th row — if we try to copy the values from the previous row, we will underflow the array. Therefore, we need to treat this row differently — we populate the value of the first row in a more “hardcoded” manner and then iteratively create the constraints starting at row 1.</p>
<h3>Determining if <code style='font-family: Arial'>copy</code> should be zero or one</h3>
<p>Recall the constraints from earlier:</p>
<ol>
<li>if <code style='font-family: Arial'>is_push = 1</code>, then all the values <code style='font-family: Arial'>0..sp - 1</code> inclusive must be copied. The cell at <code style='font-family: Arial'>sp</code> will contain the new value.</li>
<li>if <code style='font-family: Arial'>is_nop = 1</code>, then all the values <code style='font-family: Arial'>0..sp - 1</code> inclusive must be copied. Nothing is written to the cell at <code style='font-family: Arial'>sp</code>.</li>
<li>if <code style='font-family: Arial'>is_pop = 1</code>, then all the values <code style='font-family: Arial'>0..sp - 2</code> inclusive must be copied. Remember, <code style='font-family: Arial'>sp</code> points to an empty cell above the stack, so <code style='font-family: Arial'>sp - 1</code> is the value that will be popped. Everything <code style='font-family: Arial'>sp - 2</code> and below must be copied.</li>
</ol>
<p>We can summarize those into two conditions A and B:</p>
<p>A. if the <code style='font-family: Arial'>sp</code> is 1 or greater, and our column is 1 index below <code style='font-family: Arial'>sp</code>, and the current instruction is PUSH or NOP, we should copy</p>
<p>B. if the <code style='font-family: Arial'>sp</code> is 2 or greater, and our column is 2 indexes below <code style='font-family: Arial'>sp</code>, and the current instruction is POP, we should copy</p>
<p>If our column does not meet any of the above conditions, we do not copy. These include:</p>
<ul>
<li>the current column is at or above the stack pointer</li>
<li>the current column is 1 below the stack pointer, and the current instruction is pop</li>
<li>stack pointer = 0</li>
</ul>
<p>Using the rules above, let’s populate the table. At the 0th row, <code style='font-family: Arial'>sp = 0</code>, so all the copy columns will be 0:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} &amp; \texttt{copy0} &amp; \texttt{copy1} &amp; \texttt{copy2} &amp; \texttt{copy3} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 10 &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; &amp; &amp; &amp; &amp; 16 &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; 15 &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; &amp; &amp; &amp; &amp; &amp; &amp; 4 &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>At the 1st indexed row, <code style='font-family: Arial'>sp</code> is 1 or greater, but the instruction is POP, but neither condition A or B are met for any column, so we do not copy:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} &amp; \texttt{copy0} &amp; \texttt{copy1} &amp; \texttt{copy2} &amp; \texttt{copy3} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 10 &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; &amp; &amp; &amp; &amp; 16 &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; 15 &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; &amp; &amp; &amp; &amp; &amp; &amp; 4 &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>On the 2nd row, <code style='font-family: Arial'>sp</code> is 0, so nothing gets copied:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} &amp; \texttt{copy0} &amp; \texttt{copy1} &amp; \texttt{copy2} &amp; \texttt{copy3} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 10 &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 16 &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; 15 &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; &amp; &amp; &amp; &amp; &amp; &amp; 4 &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>On the 3rd row, <code style='font-family: Arial'>sp</code> is 1 and the instruction is PUSH, so column0 meets condition A:</p>
<p>“if the <code style='font-family: Arial'>sp</code> is 1 or greater, and our column is 1 indexes below <code style='font-family: Arial'>sp</code>, and the current instruction is PUSH or NOP, we should copy”</p>
<p>and <code style='font-family: Arial'>copy0</code> is set to 1:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} &amp; \texttt{copy0} &amp; \texttt{copy1} &amp; \texttt{copy2} &amp; \texttt{copy3} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 10 &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 16 &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; \boxed{1} &amp; 0 &amp; 0 &amp; 0 &amp; 16 &amp; 15 &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; &amp; &amp; &amp; &amp; &amp; &amp; 4 &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>On the 4th row, <code style='font-family: Arial'>sp</code> is 2 and the instruction is PUSH, so column 0 and column 1 meet condition A:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} &amp; \texttt{copy0} &amp; \texttt{copy1} &amp; \texttt{copy2} &amp; \texttt{copy3} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 10 &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 16 &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 16 &amp; 15 &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; \boxed{1} &amp; \boxed{1} &amp; 0 &amp; 0 &amp; 16 &amp; 15 &amp; 4 &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>On the 5th row, <code style='font-family: Arial'>sp</code> is 3 and the instruction is NOP, so column 0, 1, and 2 meet condition A, which is ”if the <code style='font-family: Arial'>sp</code> is 1 or greater, and our column is 1 index below <code style='font-family: Arial'>sp</code>, and the current instruction is PUSH or NOP, we should copy”:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} &amp; \texttt{copy0} &amp; \texttt{copy1} &amp; \texttt{copy2} &amp; \texttt{copy3} &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp;0 &amp; 0&amp; 0&amp;0 &amp; 10 &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0&amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; \texttt{16} &amp; 0 &amp; 0 &amp; 0&amp; 0&amp; 0&amp; 16 &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; \texttt{15} &amp; 1 &amp; 1&amp; 0&amp; 0&amp; 0&amp; 16&amp; 15 &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; \texttt{4} &amp; 2 &amp; 1&amp; 1&amp; 0&amp; 0&amp;16 &amp;15 &amp; 4 &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; \texttt{-} &amp; 3 &amp;\boxed{1} &amp;\boxed{1} &amp; \boxed{1}&amp;0 &amp; 16&amp; 15&amp; 4&amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; \texttt{-} &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<p>On the 6th row, <code style='font-family: Arial'>sp</code> is 1 and the instruction is POP, so we use condition B:</p>
<p>“if the <code style='font-family: Arial'>sp</code> is 2 or greater, and our column is 2 indexes below <code style='font-family: Arial'>sp</code>, and the current instruction is POP, we should copy”</p>
<p>This means that columns 0 and 1 will be copied:</p>
<p>$$<br />
\begin{array}{|c|c|c|c|c|c|c|c|c||c|c|c|c|}\hline\texttt{is\_push} &amp; \texttt{is\_pop} &amp; \texttt{is\_nop} &amp; \texttt{arg} &amp; \texttt{sp} &amp; \texttt{copy0} &amp; \texttt{copy1} &amp; \texttt{copy2} &amp; \texttt{copy3} \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 10 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 10 &amp; &amp; &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 16 &amp; &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 15 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 16 &amp; 15 &amp; &amp; \\hline\color{orange}{1} &amp; 0 &amp; 0 &amp; 4 &amp; 2 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 16 &amp; 15 &amp; 4 &amp; \\hline0 &amp; 0 &amp; \color{orange}{1} &amp; – &amp; 3 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 16 &amp; 15 &amp; 4 &amp; \\hline0 &amp; \color{orange}{1} &amp; 0 &amp; – &amp; 1 &amp; \boxed{1}&amp;\boxed{1} &amp; 0&amp; 0&amp; &amp; &amp; &amp; \\hline\end{array}<br />
$$</p>
<h3>Circom implementation of copy conditions</h3>
<p>We can create a specialized component in Circom to determine if a value should be copied from above.</p>
<ul>
<li>A. if the <code style='font-family: Arial'>sp</code> is 1 or greater, and our column is 1 indexes below <code style='font-family: Arial'>sp</code>, and the current instruction is PUSH or NOP, we should copy</li>
<li>B. if the <code style='font-family: Arial'>sp</code> is 2 or greater, and our column is 2 indexes below <code style='font-family: Arial'>sp</code>, and the current instruction is POP, we should copy</li>
</ul>
<p>This component will be used in a loop to determine if a particular column <code style='font-family: Arial'>j</code> should be copied. It sets <code style='font-family: Arial'>out = 1</code> if a particular column should be copied. This component is applied to each column for each row.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/comparators.circom&quot;;
include &quot;circomlib/gates.circom&quot;;

// RETURNS 1 IF ALL THE INPUTS ARE 1
template AND3() {
  signal input in[3];
  signal output out;

  signal temp;
  temp &lt;== in[0] * in[1];
  out &lt;== temp * in[2];
}

// j is the column number
// bits is how many bits we need
// for the LessEqThan component
template ShouldCopy(j, bits) {
  signal input sp;
  signal input is_pop;
  signal input is_push;
  signal input is_nop;

  // out = 1 if should copy
  signal output out;

  // sanity checks
  is_pop + is_push + is_nop === 1;
  is_nop * (1 - is_nop) === 0;
  is_push * (1 - is_push) === 0;
  is_pop * (1 - is_pop) === 0;

  // it's cheaper to compute ≠ 0 than &gt; 0 to avoid
  // converting the number to binary
  signal spEqZero;
  signal spGteOne;
  spEqZero &lt;== IsZero()(sp);
  spGteOne &lt;== 1 - spEqZero;

  // it's cheaper to compute ≠ 0 and ≠ 1 than ≥ 2
  signal spEqOne;
  signal spGteTwo;
  spEqOne &lt;== IsEqual()([sp, 1]);
  spGteTwo &lt;== 1 - spEqOne * spEqZero;

  // the current column is 1 or more 
  // below the stack pointer
  signal oneBelowSp &lt;== LessEqThan(bits)([j, sp - 1]);

  // the current column is 2 or more
  // below the stack pointer
  signal twoBelowSP &lt;== LessEqThan(bits)([j, sp - 2]);

  // condition A
  component a3A = AND3();
  a3A.in[0] &lt;== spGteOne;
  a3A.in[1] &lt;== oneBelowSp;
  a3A.in[2] &lt;== is_push + is_nop;

  // condition B
  component a3B = AND3();
  a3B.in[0] &lt;== spGteTwo;
  a3B.in[1] &lt;== twoBelowSP;
  a3B.in[2] &lt;== is_pop;

  component or = OR();
  or.a &lt;== a3A.out;
  or.b &lt;== a3B.out;  
  out &lt;== or.out;
}
</code></pre>
<p>We can use the above component in a loop to determine which parts of the previous stack should be copied to the new one. The following template returns an array of 0 or 1 to determine which columns should be copied. For example, if there are 4 columns and the first 2 columns should be copied, then it returns <code style='font-family: Arial'>[1, 1, 0, 0]</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">template CopyStack(m) {
  var nBits = 4;
    signal output out[m];
    signal input sp;
    signal input is_pop;
    signal input is_push;
    signal input is_nop;

    component ShouldCopys[m];

    // loop over the columns
    for (var j = 0; j &lt; m; j++) {
        ShouldCopys[j] = ShouldCopy(j, nBits);
        ShouldCopys[j].sp &lt;== sp;
        ShouldCopys[j].is_pop &lt;== is_pop;
        ShouldCopys[j].is_push &lt;== is_push;
        ShouldCopys[j].is_nop &lt;== is_nop;
        out[j] &lt;== ShouldCopys[j].out;
    }
}
</code></pre>
<h2>Final Stack</h2>
<p>The following code is the final implementation of our stack, which combines all the components together. Since we have already shown the components for <code style='font-family: Arial'>ShouldCopy</code> and <code style='font-family: Arial'>CopyStack</code>, the reader can jump down to the final component <code style='font-family: Arial'>StackBuilder</code>. The previous components are from the earlier sections. We put it into a single file so the reader can conveniently copy and paste this into <a href="https://zkrepl.dev">zkrepl</a> to test it:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/comparators.circom&quot;;
include &quot;circomlib/gates.circom&quot;;

template AND3() {
  signal input in[3];
  signal output out;

  signal temp;
  temp &lt;== in[0] * in[1];
  out &lt;== temp * in[2];
}

// j is the column number
// bits is how many bits we need
// for the LessEqThan component
template ShouldCopy(j, bits) {
  signal input sp;
  signal input is_pop;
  signal input is_push;
  signal input is_nop;

  // out = 1 if should copy
  signal output out;

  // sanity checks
  is_pop + is_push + is_nop === 1;
  is_nop * (1 - is_nop) === 0;
  is_push * (1 - is_push) === 0;
  is_pop * (1 - is_pop) === 0;

  // it's cheaper to compute ≠ 0 than &gt; 0 to avoid
  // converting the number to binary
  signal spEqZero;
  signal spGteOne;
  spEqZero &lt;== IsZero()(sp);
  spGteOne &lt;== 1 - spEqZero;

  // it's cheaper to compute ≠ 0 and ≠ 1 than ≥ 2
  signal spEqOne;
  signal spGteTwo;
  spEqOne &lt;== IsEqual()([sp, 1]);
  spGteTwo &lt;== 1 - spEqOne * spEqZero;

  // the current column is 1 or more 
  // below the stack pointer
  signal oneBelowSp &lt;== LessEqThan(bits)([j, sp - 1]);

  // the current column is 2 or more
  // below the stack pointer
  signal twoBelowSP &lt;== LessEqThan(bits)([j, sp - 2]);

  // condition A
  component a3A = AND3();
  a3A.in[0] &lt;== spGteOne;
  a3A.in[1] &lt;== oneBelowSp;
  a3A.in[2] &lt;== is_push + is_nop;

  // condition B
  component a3B = AND3();
  a3B.in[0] &lt;== spGteTwo;
  a3B.in[1] &lt;== twoBelowSP;
  a3B.in[2] &lt;== is_pop;

  component or = OR();
  or.a &lt;== a3A.out;
  or.b &lt;== a3B.out;  
  out &lt;== or.out;
}

template CopyStack(m) {
  var nBits = 4;
    signal output out[m];
    signal input sp;
    signal input is_pop;
    signal input is_push;
    signal input is_nop;

    component ShouldCopys[m];
    signal copy[m];

    // loop over the columns
  for (var j = 0; j &lt; m; j++) {
    ShouldCopys[j] = ShouldCopy(j, nBits);
    ShouldCopys[j].sp &lt;== sp;
    ShouldCopys[j].is_pop &lt;== is_pop;
    ShouldCopys[j].is_push &lt;== is_push;
    ShouldCopys[j].is_nop &lt;== is_nop;
    out[j] &lt;== ShouldCopys[j].out;
  }
}

// n is how many instructions we can handle
// since all the instructions might be push,
// our stack needs capacity of up to n
template StackBuilder(n) {
  var NOP = 0;
  var PUSH = 1;
  var POP = 2;

  signal input instr[2 * n];

  // we add one extra row for sp because
  // our algorithm always writes to the
  // next row and we don't want to conditionally
  // check for an array-out-of-bounds
  signal output sp[n + 1];

  signal output stack[n][n];

  var IS_NOP = 0;
  var IS_PUSH = 1;
  var IS_POP = 2;
  var ARG = 3;

  // metaTable is the columns IS_NOP, IS_PUSH, IS_POP, ARG
  signal metaTable[n][4];

  // first instruction must be PUSH or NOP
  (instr[0] - PUSH) * (instr[0] - NOP) === 0;

  signal first_op_is_push;
  first_op_is_push &lt;== IsEqual()([instr[0], PUSH]);

  // if the first op is NOP, we are forcing the first
  // value to be zero, but this is where the stack
  // pointer is, so it doesn't matter
  stack[0][0] &lt;== first_op_is_push * instr[1];

  // initialize the rest of the first stack to be zero
  for (var i = 1; i &lt; n; i++) {
      stack[0][i] &lt;== 0;
  }

  // we fill out the 0th elements to avoid
  // uninitialzed signals. For a particular
  // execution, we only want one possible witness
  // to correspond to a particular execution
  sp[0] &lt;== 0;
  sp[1] &lt;== first_op_is_push;
  metaTable[0][IS_PUSH] &lt;== first_op_is_push;
  metaTable[0][IS_POP] &lt;== 0;
  metaTable[0][IS_NOP] &lt;== 1 - first_op_is_push;
  metaTable[0][ARG] &lt;== instr[1];

  // spBranch is what we add to the previous
  // stack pointer based on the opcode.
  // Could be 1, 0, or -1 depending on the
  // opcode. Since the first opcode
  // cannot be POP, -1 is not an option here.
  var SAME = 0;
  var INC = 1;
  var DEC = 2;
  signal spBranch[n][3];
  spBranch[0][INC] &lt;== first_op_is_push * 1;
  spBranch[0][SAME] &lt;== (1 - first_op_is_push) * 0;
  spBranch[0][DEC] &lt;== 0;

  // populate the first row of the metaTable
  // and the stack pointer
  component EqPush[n];
  component EqNop[n];
  component EqPop[n];

  component eqSP[n][n];
  signal eqSPAndIsPush[n][n];
  for (var i = 0; i &lt; n; i++) {
      eqSPAndIsPush[0][i] &lt;== 0;
  }

  // signals and components for copying
  component CopyStack[n];
  signal previousCellIfShouldCopy[n][n];
  for (var i = 0; i &lt; n; i++) {
    previousCellIfShouldCopy[0][i] &lt;== 0;
  }
  for (var i = 1; i &lt; n; i++) {
    // check which opcode we are executing
    EqPush[i] = IsEqual();
    EqPush[i].in[0] &lt;== instr[2 * i];
    EqPush[i].in[1] &lt;== PUSH;
    metaTable[i][IS_PUSH] &lt;== EqPush[i].out;

    EqNop[i] = IsEqual();
    EqNop[i].in[0] &lt;== instr[2 * i];
    EqNop[i].in[1] &lt;== NOP;
    metaTable[i][IS_NOP] &lt;== EqNop[i].out;

    EqPop[i] = IsEqual();
    EqPop[i].in[0] &lt;== instr[2 * i];
    EqPop[i].in[1] &lt;== POP;
    metaTable[i][IS_POP] &lt;== EqPop[i].out;

    // get the instruction argument
    metaTable[i][ARG] &lt;== instr[2 * i + 1];

    // if it is a push, write to the stack
    // if it is a copy, write to the stack
    CopyStack[i] = CopyStack(n);
    CopyStack[i].sp &lt;== sp[i];
    CopyStack[i].is_push &lt;== metaTable[i][IS_PUSH];
    CopyStack[i].is_nop &lt;== metaTable[i][IS_NOP];
    CopyStack[i].is_pop &lt;== metaTable[i][IS_POP];
    for (var j = 0; j &lt; n; j++) {
      previousCellIfShouldCopy[i][j] &lt;== CopyStack[i].out[j] * stack[i - 1][j];

      eqSP[i][j] = IsEqual();
      eqSP[i][j].in[0] &lt;== j;
      eqSP[i][j].in[1] &lt;== sp[i];
      eqSPAndIsPush[i][j] &lt;== eqSP[i][j].out * metaTable[i][IS_PUSH];

      // we will either PUSH or COPY or implicilty assign 0
      stack[i][j] &lt;== eqSPAndIsPush[i][j] * metaTable[i][ARG] + previousCellIfShouldCopy[i][j];
    }

    // write to the next row's stack pointer
    spBranch[i][INC] &lt;== metaTable[i][IS_PUSH] * (sp[i] + 1);
    spBranch[i][SAME] &lt;== metaTable[i][IS_NOP] * (sp[i]);
    spBranch[i][DEC] &lt;== metaTable[i][IS_POP] * (sp[i] - 1);
    sp[i + 1] &lt;== spBranch[i][INC] + spBranch[i][SAME] + spBranch[i][DEC];
  }
}

component main = StackBuilder(3);

/* INPUT = {
  &quot;instr&quot;: [1, 16, 1, 20, 1, 22]
} */
</code></pre>
<h2>Summary</h2>
<p>To model a data structure that changes over time, we write constraints for all possible state transitions, then activate those state transitions based on flags. The flags are constrained to match the instruction for that particular state transition.</p>
<p>Although understanding the arithmetization of the stack data structure may be intimidating, we now know nearly everything we need to know to understand how to build a stack-based ZKVM, which we cover in the next chapter. To create a stack-based ZKVM, we simply modify the instructions and their respective constraints introduced in this chapter to match the opcode for the ZKVM.</p>
<p>In general, most meaningful computations can be modeled as an initial state that gets updated incrementally until a final result is reach. The stack we showed in this chapter is just a special case of this.</p>
<div style='page-break-after: always;'></div>

<h1>How a ZKVM Works</h1>
<p>Source: https://rareskills.io/post/zkvm</p>
<h1>How a ZKVM Works</h1>
<p>A <strong>Zero-Knowledge Virtual Machine</strong> (ZKVM) is a virtual machine that can create a ZK-proof that verifies it executed a set of machine instructions correctly. This allows us to take a program (a set of opcodes), a virtual machine specification (how the virtual machine behaves, what opcodes it uses, etc), and prove that the output generated is correct. A verifier does not have to re-run the program, but only check the generated ZK proof — this allows verification to be succinct. Such succinctness is the basis of what makes ZK layer 2 blockchains scalable. It also enables someone to check that a machine learning algorithm ran as claimed without re-running the entire algorithm.</p>
<p>Contrary to the name, ZKVMs are rarely “zero knowledge” in the sense that they keep the computation private. Rather, they use ZK algorithms to produce a succinct proof that the program executed correctly on a certain input so that a verifier can double-check the computation with exponentially less work. Even though revealing the program input is optional, avoiding accidental data leaks and having multiple parties agree upon private state are very challenging engineering problems that still have unsolved challenges and scaling limitations.</p>
<p>A ZKVM “computes” each step in the opcode, then constrains that the opcode was executed correctly. The constraints must be designed in such a way that we can work with an arbitrary but valid opcode sequence.</p>
<p>We can think of a ZKVM as a series of “state” transitions. The “state transition function” takes the previous state and the current opcode to be executed, an generates a new state. A ZKVM implements the “state transition function” and the circuit constraints that model its behavior. Note that “state” can include things like the “<a href="https://en.wikipedia.org/wiki/Program_counter">program counter</a>” or other bookkeeping necessary for the VM to run properly.</p>
<p>This tutorial will build an extremely simple ZKVM that only supports basic arithmetic but is extensible for other opcodes. The VM presented here only has a stack and no memory or storage. We provide suggestions for further study of ZKVMs at the end of the article.</p>
<h2>Prerequisites</h2>
<p>This tutorial assumes basic knowledge of how the EVM works (or some other stack-based architecture, such as the Java Virtual Machine). We build heavily off of the stack example from the previous chapter, so we assume knowledge of that.</p>
<h2>Simple stack-based ZKVM</h2>
<p>We will build a simple stack-based ZKVM with one special-purpose signal that contains the result of the computation. The VM is fed a series of opcodes and numbers, then outputs the final result to a special signal we call <code style='font-family: Arial'>out</code>.</p>
<p>Our ZKVM only has the following opcodes:</p>
<ul>
<li>PUSH (pushes the first argument onto the stack)</li>
<li>ADD (pops the top two items from the stack and pushes their sum)</li>
<li>MUL (pops the top two items from the stack and pushes their product)</li>
<li>NOP (no operation, don’t do anything)</li>
</ul>
<p>For simplicity, all opcodes take a single argument, but only PUSH makes use of the argument. The rest of the instructions ignore the argument. The reason we supply arguments to opcodes that do not use them is so that we do not have to conditionally check if the argument is present or not, based on the opcode.</p>
<p>There is no STOP or RETURN opcode (the substitute is explained shortly). The VM takes a <code style='font-family: Arial'>steps</code> argument and returns whatever value is at the bottom of the stack after it executes <code style='font-family: Arial'>steps</code> many instructions.</p>
<p>The following animation gives a simple example of adding two numbers together in this architecture:</p>
<p><a href="https://r2media.rareskills.io/ZKVM/zkvm.mp4"></a></p>
<p>In Circom, loops cannot be of variable-length, they must always be executed for a fixed number of iterations, as the underlying Rank 1 Constraint System (R1CS) itself must be of fixed size. Similarly, programs cannot be of variable size. However, they must have the same number of opcodes regardless of the program run.</p>
<p>To run a program with fewer opcodes than the fixed number, we simply pad it with NOP until the program is of the maximum size. To know when to “stop executing,” the user must supply the above-mentioned <code style='font-family: Arial'>steps</code> argument to determine when the value at the bottom of the stack will be returned.</p>
<p>A couple of notes about our architecture:</p>
<ul>
<li>The VM is stack-based, like the EVM, Java Virtual Machine, or (for those who know) a <a href="https://en.wikipedia.org/wiki/Reverse_Polish_notation">Reverse Polish Notation calculator</a>.</li>
<li>There are no jump instructions, so the program counter only increments.</li>
<li>All opcodes take a single argument, but ADD, MUL, and NOP ignore the argument passed to them. That allows us to always increment the program counter by the same amount each time — we don’t have to update the program counter by 2 for a PUSH and 1 for an ADD and so on. We always move the counter up by 2.</li>
<li>To read the argument of PUSH, we simply “look ahead” one index from the program counter.</li>
<li>Addition and multiplication are done using modular arithmetic (the default in Circom). We use the order of Circom’s default field as our “word size” — we don’t try to emulate VMs which have traditional word sizes like 64 bits or 256 bits. Emulating computation with bits of a fixed size is the subject of the following chapter.</li>
</ul>
<h2>Updating Our Stack Code</h2>
<p>We can make a few key changes to our stack code from the previous chapter to build a ZKVM that meets the spec described above.</p>
<ul>
<li>We removed the POP opcode since it is no longer needed.</li>
<li>We add an ADD and MUL opcode.</li>
</ul>
<p>Recall that the previous rules for copying the previous stack were:</p>
<ul>
<li>A. If the <code style='font-family: Arial'>sp</code> is 1 or greater, and column <code style='font-family: Arial'>j</code> is 1 index below <code style='font-family: Arial'>sp</code>, and the current instruction is PUSH or NOP, we should copy column <code style='font-family: Arial'>j</code></li>
<li>B. If the <code style='font-family: Arial'>sp</code> is 2 or greater, and column <code style='font-family: Arial'>j</code> is 2 indexes below <code style='font-family: Arial'>sp</code>, and the current instruction is POP, we should copy column <code style='font-family: Arial'>j</code></li>
</ul>
<p>Rule A remains unchanged, but B needs to be updated to the following:</p>
<ul>
<li>B. if the <code style='font-family: Arial'>sp</code> is 2 or greater, and column <code style='font-family: Arial'>j</code> is <strong>3</strong> indexes below <code style='font-family: Arial'>sp</code>, and the current instruction is ADD or MUL, we should copy column <code style='font-family: Arial'>j</code></li>
</ul>
<p>The reason for this change is that the previous POP instruction didn’t alter the second-to-top item on the stack, it only removed the top item. However, ADD effectively pops the stack twice and pushes the sum. Likewise, MUL pops the stack twice and pushes the product.</p>
<p>The previous stack implementation only wrote new values to the stack pointer. However, the new implementation can write the sum or product two indexes below the stack pointer. For example, the 12 in the stack below will become 15 after addition, and that location is two indexes below the stack pointer:</p>
<p>Before addition:</p>
<p>[12 , 3, sp] (sp = 3)</p>
<p>After addition:</p>
<p>[15, sp] (sp = 2)</p>
<p>Here, we have 12 as the bottom of the stack and <code style='font-family: Arial'>sp</code> pointing to the empty space above the stack.</p>
<p>Therefore, we need a signal to indicate that a particular column is two elements below the stack pointer.</p>
<p>The code below is heavily derived from the stack of the previous chapter, but it implements the updates described in this chapter. Specifically:</p>
<ul>
<li>We have replaced NOP, PUSH, and POP with NOP, PUSH, ADD, and MUL. ADD and MUL decrement the stack pointer by one, NOP keeps the stack pointer the same, and PUSH increases the stack pointer by one and copies its argument to the top of the stack.</li>
</ul>
<pre style='font-family: Arial'><code class="language-solidity">pragma circom 2.1.6;

include &quot;circomlib/comparators.circom&quot;;
include &quot;circomlib/gates.circom&quot;;

template AND3() {
  signal input in[3];
  signal output out;

  signal temp;
  temp &lt;== in[0] * in[1];
  out &lt;== temp * in[2];
}

// i is the column number
// bits is how many bits we need
// for the LessEqThan component
template ShouldCopy(i, bits) {
  signal input sp;
  signal input is_push;
  signal input is_nop;
  signal input is_add;
  signal input is_mul;

  // out = 1 if should copy
  signal output out;

  // sanity checks
  is_add + is_mul + is_push + is_nop === 1;
  is_nop * (1 - is_nop) === 0;
  is_push * (1 - is_push) === 0;
  is_add * (1 - is_add) === 0;
  is_mul * (1 - is_mul) === 0;

  // it's cheaper to compute ≠ 0 than &gt; 0 to avoid
  // converting the number to binary
  signal spEqZero;
  signal spGteOne;
  spEqZero &lt;== IsZero()(sp);
  spGteOne &lt;== 1 - spEqZero;

  // it's cheaper to compute ≠ 0 and ≠ 1 than ≥ 2
  signal spEqOne;
  signal spGteTwo;
  spEqOne &lt;== IsEqual()([sp, 1]);
  spGteTwo &lt;== 1 - spEqOne * spEqZero;

  // the current column is 1 or more 
  // below the stack pointer
  signal oneBelowSp &lt;== LessEqThan(bits)([i, sp - 1]);

  // the current column is 3 or more
  // below the stack pointer
  signal threeBelowSP &lt;== LessEqThan(bits)([i, sp - 3]);

  // condition A
  component a3A = AND3();
  a3A.in[0] &lt;== spGteOne;
  a3A.in[1] &lt;== oneBelowSp;
  a3A.in[2] &lt;== is_push + is_nop;

  // condition B
  component a3B = AND3();
  a3B.in[0] &lt;== spGteTwo;
  a3B.in[1] &lt;== threeBelowSP;
  a3B.in[2] &lt;== is_add + is_mul;

  component or = OR();
  or.a &lt;== a3A.out;
  or.b &lt;== a3B.out;  
  out &lt;== or.out;
}

template CopyStack(m) {
  var nBits = 4;
    signal output out[m];
    signal input sp;
    signal input is_add;
    signal input is_mul;
    signal input is_push;
    signal input is_nop;

    component ShouldCopys[m];
    signal copy[m];

    // loop over the columns
    for (var i = 0; i &lt; m; i++) {
      ShouldCopys[i] = ShouldCopy(i, nBits);
      ShouldCopys[i].sp &lt;== sp;
      ShouldCopys[i].is_add &lt;== is_add;
      ShouldCopys[i].is_mul &lt;== is_mul;
      ShouldCopys[i].is_push &lt;== is_push;
      ShouldCopys[i].is_nop &lt;== is_nop;
      out[i] &lt;== ShouldCopys[i].out;
    }
}

// n is how many instructions we can handle
// since all the instructions might be push,
// our stack needs capacity of up to n
template ZKVM(n) {
  var NOP = 0;
  var PUSH = 1;
  var ADD = 2;
  var MUL = 3;

  signal input instr[2 * n];

  // we add one extra row for sp because
  // our algorithm always writes to the
  // next row and we don't want to conditionally
  // check for an array-out-of-bounds
  signal output sp[n + 1];

  signal output stack[n][n];

  var IS_NOP = 0;
  var IS_PUSH = 1;
  var IS_ADD = 2;
  var IS_MUL = 3;
  var ARG = 4;
  signal metaTable[n][5];

  // first instruction must be PUSH or NOP
  (instr[0] - PUSH) * (instr[0] - NOP) === 0;

  signal first_op_is_push;
  first_op_is_push &lt;== IsEqual()([instr[0], PUSH]);

  // if the first op is NOP, we are forcing the first
  // value to be zero, but this is where the stack
  // pointer is, so it doesn't matter
  stack[0][0] &lt;== first_op_is_push * instr[1];

  // initialize the rest of the first stack to be zero
  for (var i = 1; i &lt; n; i++) {
    stack[0][i] &lt;== 0;
  }

  // we fill out the 0th elements to avoid
  // uninitialzed signals
  sp[0] &lt;== 0;
  sp[1] &lt;== first_op_is_push;
  metaTable[0][IS_PUSH] &lt;== first_op_is_push;
  metaTable[0][IS_NOP] &lt;== 1 - first_op_is_push;
  metaTable[0][IS_ADD] &lt;== 0;
  metaTable[0][IS_MUL] &lt;== 0;
  metaTable[0][ARG] &lt;== instr[1];

  // spBranch is what we add to the previous stack pointer
  // based on the opcode. Could be 1, 0, or -1 depending on the
  // opcode. Since the first opcode cannot be POP, -1 is not
  // an option here.
  var SAME = 0;
  var INC = 1;
  var DEC = 2;
  signal spBranch[n][3];
  spBranch[0][INC] &lt;== first_op_is_push * 1;
  spBranch[0][SAME] &lt;== (1 - first_op_is_push) * 0;
  spBranch[0][DEC] &lt;== 0;

  // populate the metaTable and the stack pointer
  component EqPush[n];
  component EqNop[n];
  component EqAdd[n];
  component EqMul[n];

  component eqSP[n][n];
  signal eqSPAndIsPush[n][n];
  for (var i = 0; i &lt; n; i++) {
    eqSPAndIsPush[0][i] &lt;== 0;
  }

  // signals and components for copying
  component CopyStack[n];
  signal previousCellIfShouldCopy[n][n];
  for (var i = 0; i &lt; n; i++) {
    previousCellIfShouldCopy[0][i] &lt;== 0;
  }

  component eqSPMinus2[n][n];
  signal eqSPMinus2AndIsAdd[n][n];
  signal eqSPMinus2AndIsMul[n][n];
  for (var i = 0; i &lt; n; i++) {
    eqSPMinus2AndIsAdd[0][i] &lt;== 0;
    eqSPMinus2AndIsMul[0][i] &lt;== 0;
  }

  // (the current column = sp - 2 and is_add) * sum
  signal eqSPMinus2AndIsAddWithValue[n][n];
  signal eqSPMinus2AndIsMulWithValue[n][n];

  signal sum_result[n][n];
  signal mul_result[n][n];
  for (var i = 0; i &lt; n; i++) {
    eqSPMinus2AndIsAddWithValue[0][i] &lt;== 0;
    eqSPMinus2AndIsMulWithValue[0][i] &lt;== 0;
    sum_result[0][i] &lt;== 0;
    mul_result[0][i] &lt;== 0; 
  }

  for (var i = 1; i &lt; n; i++) {
    // check which opcode we are executing
    EqPush[i] = IsEqual();
    EqPush[i].in[0] &lt;== instr[2 * i];
    EqPush[i].in[1] &lt;== PUSH;
    metaTable[i][IS_PUSH] &lt;== EqPush[i].out;

    EqNop[i] = IsEqual();
    EqNop[i].in[0] &lt;== instr[2 * i];
    EqNop[i].in[1] &lt;== NOP;
    metaTable[i][IS_NOP] &lt;== EqNop[i].out;

    EqAdd[i] = IsEqual();
    EqAdd[i].in[0] &lt;== instr[2 * i];
    EqAdd[i].in[1] &lt;== ADD;
    metaTable[i][IS_ADD] &lt;== EqAdd[i].out;

    EqMul[i] = IsEqual();
    EqMul[i].in[0] &lt;== instr[2 * i];
    EqMul[i].in[1] &lt;== MUL;
    metaTable[i][IS_MUL] &lt;== EqMul[i].out;

    // carry out the sums and muls
    for (var j = 0; j &lt; n - 1; j++) {
      sum_result[i][j] &lt;== stack[i - 1][j] + stack[i - 1][j + 1];
      mul_result[i][j] &lt;== stack[i - 1][j] * stack[i - 1][j + 1];
    }

    // these values cannot be used in practice because
    // the stack doesn't go that high.
    // However, we still need to initialize
    // them because every column checks
    // if it is sp - 1, even the last 2
    for (var j = n - 1; j &lt; n; j++) {
      sum_result[i][j] &lt;== 0;
      mul_result[i][j] &lt;== 0;
    }

    // get the instruction argument
    metaTable[i][ARG] &lt;== instr[2 * i + 1];

    // if it is a push, write to the stack
    // if it is a copy, write to the stack
    CopyStack[i] = CopyStack(n);
    CopyStack[i].sp &lt;== sp[i];
    CopyStack[i].is_push &lt;== metaTable[i][IS_PUSH];
    CopyStack[i].is_nop &lt;== metaTable[i][IS_NOP];
    CopyStack[i].is_add &lt;== metaTable[i][IS_ADD];
    CopyStack[i].is_mul &lt;== metaTable[i][IS_MUL];
    for (var j = 0; j &lt; n; j++) {
      previousCellIfShouldCopy[i][j] &lt;== CopyStack[i].out[j] * stack[i - 1][j];

      eqSP[i][j] = IsEqual();
      eqSP[i][j].in[0] &lt;== j;
      eqSP[i][j].in[1] &lt;== sp[i];
      eqSPAndIsPush[i][j] &lt;== eqSP[i][j].out * metaTable[i][IS_PUSH];

      // check if the column is two less
      // than the stack pointer
      // if so, we prepare to write the sum or
      // product here
      // if the current instruction is add or mul
      eqSPMinus2[i][j] = IsEqual();
      eqSPMinus2[i][j].in[0] &lt;== j;
      eqSPMinus2[i][j].in[1] &lt;== sp[i] - 2; // underflow doesn't matter

      eqSPMinus2AndIsAdd[i][j] &lt;== eqSPMinus2[i][j].out * metaTable[i][IS_ADD];
      eqSPMinus2AndIsMul[i][j] &lt;== eqSPMinus2[i][j].out * metaTable[i][IS_MUL];

      eqSPMinus2AndIsAddWithValue[i][j] &lt;== eqSPMinus2AndIsAdd[i][j] * sum_result[i][j];
      eqSPMinus2AndIsMulWithValue[i][j] &lt;== eqSPMinus2AndIsMul[i][j] * mul_result[i][j];
      // we will either
      // - PUSH 
      // - COPY or implicilty assign 0
      // - ADD
      // - MUL
      stack[i][j] &lt;== eqSPAndIsPush[i][j] * metaTable[i][ARG] + previousCellIfShouldCopy[i][j] + eqSPMinus2AndIsAddWithValue[i][j] + eqSPMinus2AndIsMulWithValue[i][j];
    }

    // write to the next row's stack pointer
    spBranch[i][INC] &lt;== metaTable[i][IS_PUSH] * (sp[i] + 1);
    spBranch[i][SAME] &lt;== metaTable[i][IS_NOP] * (sp[i]);
    spBranch[i][DEC] &lt;== (metaTable[i][IS_ADD] + metaTable[i][IS_MUL]) * (sp[i] - 1);
    sp[i + 1] &lt;== spBranch[i][INC] + spBranch[i][SAME] + spBranch[i][DEC];
  }
}

component main = ZKVM(5);

/* INPUT = {
    &quot;instr&quot;: [1,3,1,6,1,2,3,0,3,0]
} */
</code></pre>
<h2>Isn’t it inefficient to have a constraint for every opcode if we only use one?</h2>
<p>In our ZKVM, we conduct an addition and multiplication for every element in the stack, even though we actually use only one of them. This is not consequential for a very light operation like addition or multiplication. However, if we had an opcode for a heavy operation like hashing, this would generate significantly more constraints; we would have to populate a circuit for hashing for each item in the stack, even though only the top of the stack needs to be hashed. All this will result in unnecessary computations and high computational overhead.</p>
<p>We can improve the efficiency by using a Quin selector (or two) to determine which items on the stack will be inputs to the opcode, but this still means each iteration of the stack needs constraints for a hash even if it doesn’t use them.</p>
<p><em>We leave it as an exercise for the reader to implement two Quin selectors to only add or multiply the top two items on the stack instead of the entire stack.</em></p>
<p>This inefficiency of unnecessarily repeating unused constraints is a serious drawback of the vanilla R1CS, which does not allow for conditional use of constraints.</p>
<h2>Solutions to Improve Efficiency</h2>
<p>Two modern approaches for dramatically improving the efficiency are constraints based on lookup tables and recursive proofs.</p>
<ul>
<li>A lookup table is an arithmetization scheme where only the constraints that are actually used are part of a table, and then the ZK proof proves that it used the correct entry from the table on each instruction.</li>
<li>A recursive proof creates a separate ZK proof for each instruction and then combines the proof with another ZK proof that verifies that the input proofs are valid. (Consider that the verification algorithm in ZK can itself be modeled with an arithmetic circuit).</li>
</ul>
<p>These improvements are the subject of later parts of the ZK book. However, the thought process behind modeling what a valid state transition looks like in a VM generalizes to modern day VMs.</p>
<h2>Learn more about about ZKVMs</h2>
<p>The first proposal for a ZKVM was TinyRAM, proposed in a paper <a href="https://eprint.iacr.org/2013/507.pdf">Snarks for C: verifying Program Executions Succinctly and in Zero Knowledge</a>. The authors created, in their words “a minimalistic <a href="https://en.wikipedia.org/wiki/Reduced_instruction_set_computer">RISC</a> random-access machine with a <a href="https://en.wikipedia.org/wiki/Harvard_architecture">Harvard architecture</a> and word-addressable random-access memory.” The <a href="https://www.scipr-lab.org/doc/TinyRAM-spec-2.000.pdf">TinyRAM specification</a> requires only 29 opcodes.</p>
<p>Since this paper spawned the research into ZKVMs, it is a high-impact paper worth understanding.</p>
<p>Most modern ZKVMs are based in the RISC-V architecture, but MIPS architecture ZKVMs exist also. ZK layer 2 blockchains often use their own custom architecture.</p>
<p>Ye Zhang’s <a href="https://www.youtube.com/watch?v=vuQGdbpDWcs">video</a> on how Scroll created a ZKEVM is also fairly approachable for a high-level understanding.</p>
<div style='page-break-after: always;'></div>

<h1>32-Bit Emulation in ZK</h1>
<p>Source: https://rareskills.io/post/emulate-32-bits</p>
<h1>32-Bit Emulation in ZK</h1>
<p>The default datatype in ZK is the field element, where all arithmetic is done modulo a large prime number. However, most “real” computation is done using 32, 64, or 256-bit numbers depending on the virtual machine or execution environment.</p>
<h2>Why Do We Need 32-Bit Emulation?</h2>
<p>Many cryptographic hash functions operate on 32-bit words since, historically, 32 bits was the default word size of many CPUs. This later increased to 64 bits. The EVM uses 256 bits so that it can easily accommodate the keccak256 hash.</p>
<p>If we want to use ZK to prove the correct execution of a traditional hash function or some virtual machine that does not use finite fields (most do not), then we need to “model” traditional datatypes with a field element. Therefore, we use a field element (signal) in Circom to hold a number that cannot exceed what a 32-bit number can hold, even though the signal can hold values much larger than 32 bits.</p>
<h3>32-bit words vs finite field elements</h3>
<p>The key difference between a 32-bit word and a finite field element is the point at which they overflow. In Circom, or any language using the bn128 curve, the overflow happens at $p$ where $p$ = <code style='font-family: Arial'>21888242871839275222246405745257275088548364400416034343698204186575808495617</code>. In a 32-bit machine, the overflow happens at <code style='font-family: Arial'>4294967296</code> or, in general, at $2^n$ where $n$ is the number of bits in the virtual machine.</p>
<p>One can think of a 32-bit virtual machine doing all arithmetic modulo $2^{32}$. A normal virtual machine overflows at that number by default. When doing modular arithmetic in a finite field however, computing modulo $2^{32}$ would add quite a few constraints (as we will see later), but luckily, there is a useful math trick to do it efficiently.</p>
<p>The following two functions are equivalent at computing modulo $2^{32}$:</p>
<pre style='font-family: Arial'><code class="language-solidity">contract DemoMod32 {
  function mod32(uint256 x) public pure returns (uint256) {
    return x % (2**32);
  }

  function mod32e(uint256 x) public pure returns (uint256) {
    // only keep the 32 least significant bits
    return uint256(uint32(x)); 
  }
}
</code></pre>
<p>We can compute$\mod 2^{32}$ simply by keeping only the 32 least significant bits. A formal verification of this is in the appendix. Before we do any arithmetic on a signal containing a 32-bit number, we first need to be completely sure that the number held by the signal does, in fact, fit into 32 bits.</p>
<h2>The 32-bit range check</h2>
<p>If we are creating a ZK circuit that simulates a computation using 32-bit words, then we need to ensure that none of the signals ever hold a value greater than or equal to $2^{32}$. One intuitive thing to do is to use the <code style='font-family: Arial'>LessThan</code> template as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">signal safe;
safe &lt;== LessThan(252)([x, 2**32]);
safe === 1;
</code></pre>
<p>However, this circuit creates more constraints than necessary.</p>
<p>A more efficient approach would be to take advantage of binary representation. The key idea is to encode a number with 32 bits, and if itfits into 32 bits, the circuit executes normally. In contrast, if the number doesn’t fit into 32 bits, then the constraints cannot be satisfied. Hence, the circuit below ensures that <code style='font-family: Arial'>in</code> is $2^{32}-1$ or less.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/comparators.circom&quot;;

// 8 bit range check
template RangeCheck() {
  signal input in;
  component n2b = Num2Bits(32);
  n2b.in &lt;== in;
}

component main = RangeCheck();

// if in = 2**32 - 1, it will accept
// if in = 2**32 it will reject
</code></pre>
<p>It is not necessary to constrain the outputs of <code style='font-family: Arial'>Num2Bits</code> like with <code style='font-family: Arial'>LessThan</code>, because internally, it already constraints <code style='font-family: Arial'>out</code> to be zero or one, and also constrains the binary representation to equal the input (via <code style='font-family: Arial'>lc1 === in</code>) as can be seen in the <code style='font-family: Arial'>Num2Bits</code> template below:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Num2Bits(n) {
    signal input in;
    signal output out[n];
    var lc1=0;

    var e2=1;
    for (var i = 0; i&lt;n; i++) {
        out[i] &lt;-- (in &gt;&gt; i) &amp; 1;
        out[i] * (out[i] -1 ) === 0; // CONSTRAINT HAPPENS HERE
        lc1 += out[i] * e2;
        e2 = e2+e2;
    }

    lc1 === in;
}
</code></pre>
<h2>32-Bit Addition</h2>
<p>Suppose we want to add two field elements <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> together, which represent 32-bit numbers.</p>
<p>The naïve implementation of 32-bit addition is to turn the field element into 32-bits, then build an “addition circuit” that adds each bit and carries the overflow. However, this creates a larger circuit than necessary.</p>
<p>Instead, we can do the following:</p>
<ol>
<li>Range check <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> using the strategy outlined above</li>
<li>Add <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> together as field elements, i.e., <code style='font-family: Arial'>z &lt;== x + y</code></li>
<li>Convert <code style='font-family: Arial'>z</code> to a 33-bit number</li>
<li>Convert the least significant 32 bits of the 33-bit number to a field element.</li>
</ol>
<p>This can be visualized as follows:</p>
<p><img alt="circuit diagram showing 32 bit addition" src="assets/circuit-diagram.png" /></p>
<p>The most that <code style='font-family: Arial'>x + y</code> can overflow to is a 33-bit number. Consider that the maximum value <code style='font-family: Arial'>x</code> and <code style='font-family: Arial'>y</code> can hold is $2^{32}-1$. If we add that value to itself, we get</p>
<p>$$<br />
\begin{align*}<br />
&amp;(2^{32}-1)+(2^{32}-1)\<br />
&amp;=2\cdot(2^{32}-1)\<br />
&amp;=2^{33}-2<br />
\end{align*}<br />
$$</p>
<p>The final number needs 33 bits to hold. (Recall that the maximum number 33 bits can hold is $2^{33} – 1$. Hence, the above number is the second largest number that 33 bits can hold.) Thus, we only need 33 bits to hold the sum before we remove the 33rd bit.</p>
<p>Below is the code for emulating and verifying 32-bit addition using Circom:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/comparators.circom&quot;;
include &quot;circomlib/bitify.circom&quot;;

template Add32(n) {
  signal input x;
  signal input y;
  signal output out;

  // range check x and y
  component rCheckX = Num2Bits(32);
  component rCheckY = Num2Bits(32);
  rCheckX.in &lt;== x;
  rCheckY.in &lt;== y;

  // convert the sum to 33 bits
  component n2b33 = Num2Bits(33);
  n2b33.in &lt;== x + y;

  // convert the least significant 32 bits
  // to the final result
  component b2n = Bits2Num(32);
  for (var i = 0; i &lt; 32; i++) {
    b2n.in[i] &lt;== n2b33.out[i];
  }

  b2n.out ==&gt; out;
}
</code></pre>
<h2>32-Bit Multiplication</h2>
<p>The logic for 32-bit multiplication is extremely similar to 32-bit addition, except that we need to allow for the 32-bit multiplication to temporarily require up to 64 bits before only saving the final 32 bits:</p>
<p>$$<br />
\begin{align*}<br />
&amp;=(2^{32}-1)(2^{32}-1)\<br />
&amp;=2^{64}-2^{32}-2^{32}+2\<br />
&amp;=2^{64}-2^{33}+2<br />
\end{align*}<br />
$$</p>
<p>The final number needs 64 bits to hold.</p>
<p><em>The implementation of this circuit is left as an exercise for the reader.</em></p>
<h2>32-Bit Division and Modulo</h2>
<p>Integer division is one of the most problematic bugs in ZK, as properly constraining it is much harder than the examples of addition and multiplication. Here are some examples of underconstrained division in the wild:</p>
<ul>
<li><a href="https://code4rena.com/reports/2023-10-zksync#h-01-missing-range-constraint-on-remainder-check-in-div-opcode-implementation">https://code4rena.com/reports/2023-10-zksync#h-01-missing-range-constraint-on-remainder-check-in-div-opcode-implementation</a></li>
<li><a href="https://github.com/succinctlabs/sp1/issues/746">https://github.com/succinctlabs/sp1/issues/746</a></li>
</ul>
<p>In integer division, the relationship between the numerator, denominator, quotient, and remainder is:</p>
<p>$$<br />
\text{numerator}=\text{denominator}\times\text{quotient}+\text{remainder}<br />
$$</p>
<p>However, that constraint alone is not sufficient to ensure that the division was conducted properly.</p>
<p>For example, suppose we are trying to prove that we computed 12 / 7 = 1. Our circuit would have the values</p>
<p>$$<br />
12 = 7 \times 1 +5<br />
$$</p>
<p>However, the following witness also satisfies the constraint:</p>
<p>$$<br />
12 = 7 \times 0 + 12<br />
$$</p>
<p>We can guard against this by adding a constraint that the remainder is strictly less than the denominator.</p>
<p>Furthermore, we should be aware of the following potential bugs:</p>
<ul>
<li>This is not a concern for 32 bits in a 254-bit field (which is the default Circom uses), but we want to be sure the calculation $\text{denominator}\times\text{quotient}$ cannot overflow the underlying finite field.</li>
<li>More generally, we do not want the computation $\text{denominator}\times\text{quotient}+\text{remainder}$ to overflow. If <code style='font-family: Arial'>denominator</code> and <code style='font-family: Arial'>quotient</code> are range-checked to 32 bits, then the most amount of bits the product $\text{denominator}\times\text{quotient}$ can hold is 64 bits, and if <code style='font-family: Arial'>remainder</code> is range-checked to 32 bits, the most amount of bits $\text{denominator}\times\text{quotient}+\text{remainder}$ can require is 65 bits. Therfore, working with a VM bit size of 32 bits is not a concern for the default field of Circom, but for other VM bit sizes, such as 128 bits, an overflow is possible.</li>
<li>Division by zero can have unexpected behavior depending on which ZKVM you’re considering. The EVM for example does not panic when division by zero happens for example, but returns zero instead. In the RISC-V architecture, division by zero returns a word with all the bits set to 1.</li>
</ul>
<p>It is impractical to directly compute integer division using only addition and multiplication (efficient algorithms like <a href="https://en.wikipedia.org/wiki/Karatsuba_algorithm">Karatsuba’s method</a> for multiplication or <a href="https://en.wikipedia.org/wiki/Division_algorithm#Integer_division_(unsigned)_with_remainder">efficient integer division</a> use for-loops or recursion, which don’t map nicely to addition and multiplication), so it is better to compute the integer division result outside of the constraints.</p>
<p>In Circom, the <code style='font-family: Arial'>/</code> operator refers to modular division (multiplication by the multiplicative inverse) and the <code style='font-family: Arial'>\</code> operator refers to integer division. The following code shows how to prove we calculated the quotient and remainder correctly. We include the computation of the remainder since we get it for free when proving we computed integer division properly.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/comparators.circom&quot;;
include &quot;circomlib/bitify.circom&quot;;

template DivMod(wordSize) {
  // a wordSize over this could overflow 252
  assert(wordSize &lt; 125);

  signal input numerator;
  signal input denominator;

  signal output quotient;
  signal output remainder;

  quotient &lt;-- numerator \ denominator;
  remainder &lt;-- numerator % denominator;

  // quotient and remainder still need
  // to be range checked because the
  // prover can supply any value

  // range check all the signals
  component n2bN = Num2Bits(wordSize);
  component n2bD = Num2Bits(wordSize);
  component n2bQ = Num2Bits(wordSize);
  component n2bR = Num2Bits(wordSize);
  n2bN.in &lt;== numerator;
  n2bD.in &lt;== denominator;
  n2bQ.in &lt;== quotient;
  n2bR.in &lt;== remainder;

  // core constraint
  numerator === quotient * denominator  + remainder;

  // remainder must be less than the denominator
  signal remLtDen;

  // depending on the application, we might be able
  // to use fewer than 252 bits
  remLtDen &lt;== LessThan(wordSize)([remainder, denominator]);
  remLtDen === 1;

  // denominator is not zero
  signal isZero;
  isZero &lt;== IsZero()(denominator);
  isZero === 0;
}

component main = DivMod(32);
</code></pre>
<h2>32-Bit Bitshift</h2>
<p>Suppose we wish to emulate the following code:</p>
<pre style='font-family: Arial'><code class="language-solidity">uint32 x;
uint32 s;
x &lt;&lt; s;
</code></pre>
<p>A left-shift by <code style='font-family: Arial'>s</code> is equivalent to multiplying by $2^s$ where $s$ is the size of the shift, and a right-shift by s is equivalent to division by $2^s$. As seen in the previous chapter, the computation of powers can create a fairly large set of constraints. Therefore, it is generally more efficient to precompute every power of 2 up to the word size minus 1. Thus, for a left shift of a 32-bit number, we precompute every power of 2 up to 31 (word size (32) – 1): 1, 2, 4, 8, …, $2^{31}$ and multiply <code style='font-family: Arial'>x</code> by the appropriate selection using the conditional selection techniques discussed earlier. If the shift amount is 32 or greater, we multiply by zero.</p>
<p><em>The implementation is left as an exercise for the reader.</em></p>
<h2>32-Bit AND, NOT, OR, XOR, and NOT</h2>
<p>The <a href="https://github.com/iden3/circomlib/blob/master/circuits/gates.circom">Circomlib gates library</a> has implementations for each of these circuits and they are self-explanatory, so we encourage the reader to simply read the code there. We show templates below on how to emulate the operation for the following:</p>
<h3>Bitwise AND</h3>
<pre style='font-family: Arial'><code class="language-solidity">uint32 a;
uint32 b;
a &amp; b;
</code></pre>
<h3>Bitwise NOT</h3>
<pre style='font-family: Arial'><code class="language-solidity">uint32 x;
~x; // flip all the bits
</code></pre>
<p>Below is the code for computing and constraining the bitwise AND of <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/gates.circom&quot;;
include &quot;circomlib/bitify.circom&quot;;

template BitwiseAnd32() {
  signal input a;
  signal input b;
  signal output out;

  // range check
  component n2ba = Num2Bits(32);
  component n2bb = Num2Bits(32);
  n2ba.in &lt;== a;
  n2bb.in &lt;== b;

  component b2n = Bits2Num(32);
  component Ands[32];
  for (var i = 0; i &lt; 32; i++) {
    Ands[i] = AND();
    Ands[i].a &lt;== n2ba.out[i];
    Ands[i].b &lt;== n2bb.out[i];
    Ands[i].out ==&gt; b2n.in[i];
  }

  b2n.out ==&gt; out;
}

component main = BitwiseAnd32();
</code></pre>
<p><em>The remaining operations for NOT, OR, and XOR are left as an exercise for the reader.</em></p>
<h2>How ZK EVMs handle 256-bit numbers</h2>
<p>The default Circom field cannot hold 256-bit numbers. Instead, each word in the EVM must be modeled with a list of smaller word sizes, similar to how a 64-bit computer can emulate the EVM.</p>
<p>For example, a 256-bit number can be modeled with four 64-bit words. When adding, we carry the overflow from the less significant words to the next significant word. If the most significant word overflows, we simply discard the overflow.</p>
<h2>Appendix A: Proofs of Equivalence</h2>
<p>We used the Certora prover to demonstrate equivalence of the following functions:</p>
<pre style='font-family: Arial'><code class="language-solidity">contract DemoMod32 {
    function mod32(uint256 x) public pure returns (uint256) {
        return x % (2**32);
    }

    function mod32e(uint256 x) public pure returns (uint256) {
        // only keep the 32 least significant bits
        return uint256(uint32(x)); 
    }
}
</code></pre>
<p>Here is the Certora verification language rule:</p>
<pre style='font-family: Arial'><code class="language-solidity">methods {
  function mod32(uint256) external returns (uint256) envfree;
  function mod32e(uint256) external returns (uint256) envfree;
}

rule test_Mod32AndMod32E_Equivalence() {
  uint256 x;
  assert mod32(x) == mod32e(x);
}
</code></pre>
<p>Here is the Certora report:</p>
<p><a href="https://prover.certora.com/output/541734/6cd3303cb5f5441e8773adb5c79787d7?anonymousKey=0b945ee1440cd67efed3efba3162b0f924e8cf8f">https://prover.certora.com/output/541734/6cd3303cb5f5441e8773adb5c79787d7?anonymousKey=0b945ee1440cd67efed3efba3162b0f924e8cf8f</a></p>
<div style='page-break-after: always;'></div>

<h1>MD5 Hash In Circom</h1>
<p>Source: https://rareskills.io/post/md5-circom</p>
<h1>MD5 Hash In Circom</h1>
<p>In this tutorial, we will implement the MD5 hash in Circom both to compute the hash and to constrain in Circom that it was computed correctly.</p>
<p>Although the MD5 hash function is not cryptographically secure (since collisions have been found), the mechanics are similar to cryptographically secure hash functions.</p>
<p>Importantly, the MD5 hash function can be learned quickly. The following 14-minute video explains how the MD5 hash works. We recommend watching it first:</p>
<p><a href="https://www.youtube.com/watch?v=5MiMK45gkTY">https://www.youtube.com/watch?v=5MiMK45gkTY</a></p>
<p>To create a proof that we know the preimage of the MD5 hash without revealing it, we need to prove we executed every step of the hash correctly and produced a certain result. This tutorial shows how to design the constraints for each state transition.</p>
<p>Specifically, the MD5 hash has the following subroutines:</p>
<ul>
<li>Bitwise AND, OR, NOT, and XOR</li>
<li>LeftRotate</li>
<li>Add 32-bit numbers and overflow at $2^{32}$</li>
<li>The function <code style='font-family: Arial'>Func</code>, which combines registers <code style='font-family: Arial'>B</code>, <code style='font-family: Arial'>C</code>, and <code style='font-family: Arial'>D</code> together using bitwise operators</li>
<li>The padding step at the beginning, which adds a 1 bit after the input and puts the length (in bits) of the input</li>
</ul>
<p>Additionally, the output of MD5 is usually written as a 128-bit number in big-endian form. Suppose we have a 128-bit value <code style='font-family: Arial'>0x1234567890ABCDEF1122334455667788</code></p>
<p>In big-endian, it would be written as:</p>
<pre style='font-family: Arial'><code class="language-solidity">0x12   0x34   0x56   0x78   0x90   0xAB   0xCD   0xEF   0x11   0x22   0x33   0x44   0x55   0x66   0x77   0x88
</code></pre>
<p>In little-endian, it would be:</p>
<pre style='font-family: Arial'><code class="language-solidity">0x88   0x77   0x66   0x55   0x44   0x33   0x22   0x11   0xEF   0xCD   0xAB   0x90   0x78   0x56   0x34   0x12
</code></pre>
<p>We will need a separate routine to reverse the order of the bytes from little endian to big endian. Most hash implementations output big endian, so to easily compare our result to established libraries, we want our implementation to output in big endian format. We will later create a <code style='font-family: Arial'>ToBytes</code> component for this.</p>
<p>Although there is a significant amount of array indexing, the index we use is deterministic based on the iteration of the hash, so there is no need for a Quin selector anywhere in the hash constraints — we can hardcode the array indexing.</p>
<h2>Building an MD5 prototype in Python</h2>
<p>When building something as complex as a hash function, it is a good idea to build a reference implementation in a more familiar and easier-to-debug language such as Python, then translate the Python code to Circom.</p>
<p>Here is the Python implementation of MD5 (which only supports 448 bits of input for simplicity). This is heavily inspired by this <a href="https://github.com/Utkarsh87/md5-hashing">other implementation by Utkarsh87</a>. We have tried to make the functions behave “component-like,” so the translation to Circom is more straightforward.</p>
<p>Some implementation notes:</p>
<ul>
<li>Addition mod $2^{32}$ is done by adding the numbers and then calling the function <code style='font-family: Arial'>Overflow32()</code>.</li>
<li>We accept the inputs as an array of bytes, not as an array of bits.</li>
<li>The byte <code style='font-family: Arial'>0x80</code> looks like <code style='font-family: Arial'>10000000</code> in binary. This allows us to pad the input with a single bit at the end.</li>
<li>The output is in big-endian format.</li>
</ul>
<pre style='font-family: Arial'><code class="language-solidity">s = [7, 12, 17, 22,  7, 12, 17, 22,  7, 12, 17, 22,  7, 12, 17, 22,
     5,  9, 14, 20,  5,  9, 14, 20,  5,  9, 14, 20,  5,  9, 14, 20,
     4, 11, 16, 23,  4, 11, 16, 23,  4, 11, 16, 23,  4, 11, 16, 23,
     6, 10, 15, 21,  6, 10, 15, 21,  6, 10, 15, 21,  6, 10, 15, 21]

K = [0xd76aa478, 0xe8c7b756, 0x242070db, 0xc1bdceee,
     0xf57c0faf, 0x4787c62a, 0xa8304613, 0xfd469501,
     0x698098d8, 0x8b44f7af, 0xffff5bb1, 0x895cd7be,
     0x6b901122, 0xfd987193, 0xa679438e, 0x49b40821,
     0xf61e2562, 0xc040b340, 0x265e5a51, 0xe9b6c7aa,
     0xd62f105d, 0x02441453, 0xd8a1e681, 0xe7d3fbc8,
     0x21e1cde6, 0xc33707d6, 0xf4d50d87, 0x455a14ed,
     0xa9e3e905, 0xfcefa3f8, 0x676f02d9, 0x8d2a4c8a,
     0xfffa3942, 0x8771f681, 0x6d9d6122, 0xfde5380c,
     0xa4beea44, 0x4bdecfa9, 0xf6bb4b60, 0xbebfbc70,
     0x289b7ec6, 0xeaa127fa, 0xd4ef3085, 0x04881d05,
     0xd9d4d039, 0xe6db99e5, 0x1fa27cf8, 0xc4ac5665,
     0xf4292244, 0x432aff97, 0xab9423a7, 0xfc93a039,
     0x655b59c3, 0x8f0ccc92, 0xffeff47d, 0x85845dd1,
     0x6fa87e4f, 0xfe2ce6e0, 0xa3014314, 0x4e0811a1,
     0xf7537e82, 0xbd3af235, 0x2ad7d2bb, 0xeb86d391]

iter_to_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 6, 11, 0, 5, 10, 15, 4, 9, 14, 3, 8, 13, 2, 7, 12, 5, 8, 11, 14, 1, 4, 7, 10, 13, 0, 3, 6, 9, 12, 15, 2, 0, 7, 14, 5, 12, 3, 10, 1, 8, 15, 6, 13, 4, 11, 2, 9]

def Overflow32(x):
    return x &amp; 0xFFFFFFFF

def leftRotate(x, amount):
    #x &amp;= 0xFFFFFFFF
    xo = Overflow32(x)
    return Overflow32((xo &lt;&lt; amount | xo &gt;&gt; (32 -amount)))

def func(B, C, D, i):
    out = None

    # note that i will be 1..64 inclusive
    if i &lt;= 16:
        out = (B &amp; C) | (~B &amp; D)

    elif i &gt; 16 and i &lt;= 32:
        out = (D &amp; B) | (~D &amp; C)

    elif i &gt; 32 and i &lt;= 48:
        out = B ^ C ^ D

    elif i &gt; 48 and i &lt;= 64:
        out = C ^ (B | ~D)

    else:
        assert False, &quot;1) What&quot;

    return out

# concatenates four bytes to become 32 bits
def To32BitWord(byte1, byte2, byte3, byte4):
    return byte1 + byte2 * 2**8 + byte3 * 2**16 + byte4 * 2**24

# length is the byte where the data stops
# so if we have zero bytes, we write 0x80
# to byte 0
def md5(bytes, length):
    data = bytearray(64)
    msg = bytearray(bytes, 'ascii')

    # 56 bytes, 64 is the max
    assert length &lt; 56, &quot;too long&quot;

    if length &lt; 56:
        data[length] = 0x80
        data[56] = (length * 8).to_bytes(1, byteorder='little')[0]
        for i in range(57,64):
            data[i] = 0x00

    for i in range(0, length):
        data[i] = msg[i]

    # data is a len 64 array of bytes. However, it will be much easier to work
    # on if we turn it into a len 16 array of 32 bit words
    data_32 = [0] * 16
    for i in range(0, 16):
        data_32[i] = To32BitWord(data[4*i], data[4*i + 1], data[4*i + 2], data[4*i + 3])

    # algo runs for 64 iterations with 4 registers, each using 32 bits
    # we allocate 65, because the 0th will be the default starting value
    buffer = [[0]*4 for _ in range(65)]

    buffer[0][0] = 0x67452301
    buffer[0][1] = 0xefcdab89
    buffer[0][2] = 0x98badcfe
    buffer[0][3] = 0x10325476

    A = 0
    B = 1
    C = 2
    D = 3
    for i in range(1, 65):

        F = func(buffer[i - 1][B], buffer[i - 1][C], buffer[i - 1][D], i)
        G = iter_to_index[i - 1]
        to_rotate = buffer[i-1][A] + F + K[i - 1] + data_32[iter_to_index[i - 1]]
        rotated = leftRotate(to_rotate, s[i - 1])
        new_B = Overflow32(buffer[i-1][B] + rotated)

        buffer[i][A] = buffer[i - 1][D]
        buffer[i][B] = new_B 
        buffer[i][C] = buffer[i - 1][B]
        buffer[i][D] = buffer[i - 1][C]

    final = [0,0,0,0]
    for i, b in enumerate(buffer[64]):
        final[i] = Overflow32((b + buffer[0][i]))

    digest = final[0] + final[1] * 2**32 + final[2] * 2**64 + final[3] * 2**96

    raw = digest.to_bytes(16, byteorder='little')
    return int.from_bytes(raw, byteorder='big')


print(hex(md5(&quot;RareSkills&quot;, 10)))
</code></pre>
<h2>Prerequisite Components</h2>
<h3>Overflow32</h3>
<p>Overflow32 emulates a 32-bit overflow in a VM that happens at $2^{32}$:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Overflow32() {
    signal input in;
    signal output out;

    component n2b = Num2Bits(252);
    component b2n = Bits2Num(32);

    n2b.in &lt;== in;
    for (var i = 0; i &lt; 32; i++) {
        n2b.out[i] ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}
</code></pre>
<h2>LeftRotate</h2>
<p>LeftRotate rotates the bits as if they are in a circular buffer:</p>
<pre style='font-family: Arial'><code class="language-solidity">template LeftRotate(s) {
    signal input in;
    signal output out;

    component n2b = Num2Bits(32);
    component b2n = Bits2Num(32);

    n2b.in &lt;== in;

    for (var i = 0; i &lt; 32; i++) {
        b2n.in[(i + s) % 32] &lt;== n2b.out[i];
    }

    out &lt;== b2n.out;
}
</code></pre>
<h2>Bitwise AND, OR, XOR, and NOT</h2>
<p>The following templates were built in our tutorial on 32-bit emulation:</p>
<pre style='font-family: Arial'><code class="language-solidity">template BitwiseAnd32() {
    signal input in[2];
    signal output out;

    // range check
    component n2ba = Num2Bits(32);
    component n2bb = Num2Bits(32);
    n2ba.in &lt;== in[0];
    n2bb.in &lt;== in[1];

    component b2n = Bits2Num(32);
    component Ands[32];
    for (var i = 0; i &lt; 32; i++) {
        Ands[i] = AND();
        Ands[i].a &lt;== n2ba.out[i];
        Ands[i].b &lt;== n2bb.out[i];
        Ands[i].out ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}

template BitwiseOr32() {
    signal input in[2];
    signal output out;

    // range check
    component n2ba = Num2Bits(32);
    component n2bb = Num2Bits(32);
    n2ba.in &lt;== in[0];
    n2bb.in &lt;== in[1];

    component b2n = Bits2Num(32);
    component Ors[32];
    for (var i = 0; i &lt; 32; i++) {
        Ors[i] = OR();
        Ors[i].a &lt;== n2ba.out[i];
        Ors[i].b &lt;== n2bb.out[i];
        Ors[i].out ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}

template BitwiseXor32() {
    signal input in[2];
    signal output out;

    // range check
    component n2ba = Num2Bits(32);
    component n2bb = Num2Bits(32);
    n2ba.in &lt;== in[0];
    n2bb.in &lt;== in[1];

    component b2n = Bits2Num(32);
    component Xors[32];
    for (var i = 0; i &lt; 32; i++) {
        Xors[i] = XOR();
        Xors[i].a &lt;== n2ba.out[i];
        Xors[i].b &lt;== n2bb.out[i];
        Xors[i].out ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}

template BitwiseNot32() {
    signal input in;
    signal output out;

    // range check
    component n2ba = Num2Bits(32);
    n2ba.in &lt;== in;

    component b2n = Bits2Num(32);
    component Nots[32];
    for (var i = 0; i &lt; 32; i++) {
        Nots[i] = NOT();
        Nots[i].in &lt;== n2ba.out[i];
        Nots[i].out ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}
</code></pre>
<h3>Func</h3>
<p>Func takes registers <code style='font-family: Arial'>B</code>, <code style='font-family: Arial'>C</code> and <code style='font-family: Arial'>D</code> and combines them into a single output — and that combination depends on which iteration we are on:</p>
<pre style='font-family: Arial'><code class="language-solidity">template Func(i) {
    assert(i &lt;= 64);
    signal input b;
    signal input c;
    signal input d;

    signal output out;

    if (i &lt;= 16) {
        signal bAndc;
        component a1 = BitwiseAnd32();
        a1.in[0] &lt;== b;
        a1.in[1] &lt;== c;

        component a2 = BitwiseAnd32();
        component n1 = BitwiseNot32();
        n1.in &lt;== b;
        a2.in[0] &lt;== n1.out;
        a2.in[1] &lt;== d;

        component o1 = BitwiseOr32();
        o1.in[0] &lt;== a1.out;
        o1.in[1] &lt;== a2.out;

        out &lt;== o1.out;
    }
    else if (i &gt; 16 &amp;&amp; i &lt;= 32) {
        component a1 = BitwiseAnd32();
        a1.in[0] &lt;== d;
        a1.in[1] &lt;== b;

        component n1 = BitwiseNot32();
        n1.in &lt;== d;
        component a2 = BitwiseAnd32();
        a2.in[0] &lt;== n1.out;
        a2.in[1] &lt;== c;

        component o1 = BitwiseOr32();
        o1.in[0] &lt;== a1.out;
        o1.in[1] &lt;== a2.out;

        out &lt;== o1.out;
    }
    else if (i &gt; 32 &amp;&amp; i &lt;= 48) {
        component x1 = BitwiseXor32();
        component x2 = BitwiseXor32();

        x1.in[0] &lt;== b;
        x1.in[1] &lt;== c;
        x2.in[0] &lt;== x1.out;
        x2.in[1] &lt;== d;

        out &lt;== x2.out;
    }
    // i must be &lt;= 64 by the assert
    // statement above
    else {
        component o1 = BitwiseOr32();
        component n1 = BitwiseNot32();
        n1.in &lt;== d;
        o1.in[0] &lt;== n1.out;
        o1.in[1] &lt;== b;

        component x1 = BitwiseXor32();
        x1.in[0] &lt;== o1.out;
        x1.in[1] &lt;==c;

        out &lt;== x1.out;
    }
</code></pre>
<h3>Input padding</h3>
<p>To simplify, our hash function accepts an array of bytes as input, not an array of bits. Furthermore, we limit the input length to 56 bytes so that we can hardcode inserting the length at byte index 56 of the 64 bytes (512 bit) input the hash uses.</p>
<p>Since the input will be at most 56 bytes large, the number we must use for the length won’t be more than 448 bits, which requires at most 2 bytes to store.</p>
<pre style='font-family: Arial'><code class="language-solidity">// n is the number of bytes
template Padding(n) {
    // 56 bytes = 448 bits
    assert(n &lt; 56);

    signal input in[n];

    // 64 bytes = 512 bits
    signal output out[64];

    for (var i = 0; i &lt; n; i++) {
        out[i] &lt;== in[i];
    }

    // add 128 = 0x80 to pad the 1 bit (0x80 = 10000000b)
    out[n] &lt;== 128;

    // pad the rest with zeros
    for (var i = n + 1; i &lt; 56; i++) {
        out[i] &lt;== 0;
    }

    var lenBits = n * 8;
    if (lenBits &lt; 256) {
        out[56] &lt;== lenBits;
    }
    else {
        var lowOrderBytes = lenBits % 256;
        var highOrderBytes = lenBits \ 256;
        out[56] &lt;== lowOrderBytes;
        out[57] &lt;== highOrderBytes;
    }
}
</code></pre>
<h2>Num2Bytes</h2>
<p>To change the endianness we need to turn a signal <code style='font-family: Arial'>in</code> into an array of bytes <code style='font-family: Arial'>out</code>:</p>
<pre style='font-family: Arial'><code class="language-solidity">// n is the number of bytes
template ToBytes(n) {
    signal input in;
    signal output out[n];

    component n2b = Num2Bits(n * 8);
    n2b.in &lt;== in;

    component b2ns[n];
    for (var i = 0; i &lt; n; i++) {
        b2ns[i] = Bits2Num(8);
        for (var j = 0; j &lt; 8; j++) {
            b2ns[i].in[j] &lt;== n2b.out[8*i + j];
        }
        out[i] &lt;== b2ns[i].out;
    }
}
</code></pre>
<h2>Final Solution</h2>
<p>The code below combines all the components together to perform the MD5 hash. It also converts the result to big-endian form. The reader can test the code in <a href="https://zkrepl.dev">zkrepl</a>.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/bitify.circom&quot;;
include &quot;circomlib/gates.circom&quot;;

template BitwiseAnd32() {
    signal input in[2];
    signal output out;

    // range check
    component n2ba = Num2Bits(32);
    component n2bb = Num2Bits(32);
    n2ba.in &lt;== in[0];
    n2bb.in &lt;== in[1];

    component b2n = Bits2Num(32);
    component Ands[32];
    for (var i = 0; i &lt; 32; i++) {
        Ands[i] = AND();
        Ands[i].a &lt;== n2ba.out[i];
        Ands[i].b &lt;== n2bb.out[i];
        Ands[i].out ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}

template BitwiseOr32() {
    signal input in[2];
    signal output out;

    // range check
    component n2ba = Num2Bits(32);
    component n2bb = Num2Bits(32);
    n2ba.in &lt;== in[0];
    n2bb.in &lt;== in[1];

    component b2n = Bits2Num(32);
    component Ors[32];
    for (var i = 0; i &lt; 32; i++) {
        Ors[i] = OR();
        Ors[i].a &lt;== n2ba.out[i];
        Ors[i].b &lt;== n2bb.out[i];
        Ors[i].out ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}

template BitwiseXor32() {
    signal input in[2];
    signal output out;

    // range check
    component n2ba = Num2Bits(32);
    component n2bb = Num2Bits(32);
    n2ba.in &lt;== in[0];
    n2bb.in &lt;== in[1];

    component b2n = Bits2Num(32);
    component Xors[32];
    for (var i = 0; i &lt; 32; i++) {
        Xors[i] = XOR();
        Xors[i].a &lt;== n2ba.out[i];
        Xors[i].b &lt;== n2bb.out[i];
        Xors[i].out ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}

template BitwiseNot32() {
    signal input in;
    signal output out;

    // range check
    component n2ba = Num2Bits(32);
    n2ba.in &lt;== in;

    component b2n = Bits2Num(32);
    component Nots[32];
    for (var i = 0; i &lt; 32; i++) {
        Nots[i] = NOT();
        Nots[i].in &lt;== n2ba.out[i];
        Nots[i].out ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}

// n is the number of bytes
template ToBytes(n) {
    signal input in;
    signal output out[n];

    component n2b = Num2Bits(n * 8);
    n2b.in &lt;== in;

    component b2ns[n];
    for (var i = 0; i &lt; n; i++) {
        b2ns[i] = Bits2Num(8);
        for (var j = 0; j &lt; 8; j++) {
            b2ns[i].in[j] &lt;== n2b.out[8*i + j];
        }
        out[i] &lt;== b2ns[i].out;
    }
}

// n is the number of bytes
template Padding(n) {
    // 56 bytes = 448 bits
    assert(n &lt; 56);

    signal input in[n];

    // 64 bytes = 512 bits
    signal output out[64];

    for (var i = 0; i &lt; n; i++) {
        out[i] &lt;== in[i];
    }

    // add 128 = 0x80 to pad the 1 bit (0x80 = 10000000b)
    out[n] &lt;== 128;

    // pad the rest with zeros
    for (var i = n + 1; i &lt; 56; i++) {
        out[i] &lt;== 0;
    }

    var lenBits = n * 8;
    if (lenBits &lt; 256) {
        out[56] &lt;== lenBits;
    }
    else {
        var lowOrderBytes = lenBits % 256;
        var highOrderBytes = lenBits \ 256;
        out[56] &lt;== lowOrderBytes;
        out[57] &lt;== highOrderBytes;
    }
}
template Overflow32() {
    signal input in;
    signal output out;

    component n2b = Num2Bits(252);
    component b2n = Bits2Num(32);

    n2b.in &lt;== in;
    for (var i = 0; i &lt; 32; i++) {
        n2b.out[i] ==&gt; b2n.in[i];
    }

    b2n.out ==&gt; out;
}

template LeftRotate(s) {
    signal input in;
    signal output out;

    component n2b = Num2Bits(32);
    component b2n = Bits2Num(32);

    n2b.in &lt;== in;

    for (var i = 0; i &lt; 32; i++) {
        b2n.in[(i + s) % 32] &lt;== n2b.out[i];
    }

    out &lt;== b2n.out;
}

template Func(i) {
    assert(i &lt;= 64);
    signal input b;
    signal input c;
    signal input d;

    signal output out;

    if (i &lt; 16) {
        component a1 = BitwiseAnd32();
        a1.in[0] &lt;== b;
        a1.in[1] &lt;== c;

        component a2 = BitwiseAnd32();
        component n1 = BitwiseNot32();
        n1.in &lt;== b;
        a2.in[0] &lt;== n1.out;
        a2.in[1] &lt;== d;

        component o1 = BitwiseOr32();
        o1.in[0] &lt;== a1.out;
        o1.in[1] &lt;== a2.out;

        out &lt;== o1.out;
    }
    else if (i &gt;= 16 &amp;&amp; i &lt; 32) {
        // (D &amp; B) | (~D &amp; C)
        component a1 = BitwiseAnd32();
        a1.in[0] &lt;== d;
        a1.in[1] &lt;== b;

        component n1 = BitwiseNot32();
        n1.in &lt;== d;
        component a2 = BitwiseAnd32();
        a2.in[0] &lt;== n1.out;
        a2.in[1] &lt;== c;

        component o1 = BitwiseOr32();
        o1.in[0] &lt;== a1.out;
        o1.in[1] &lt;== a2.out;

        out &lt;== o1.out;
    }
    else if (i &gt;= 32 &amp;&amp; i &lt; 48) {
        component x1 = BitwiseXor32();
        component x2 = BitwiseXor32();

        x1.in[0] &lt;== b;
        x1.in[1] &lt;== c;
        x2.in[0] &lt;== x1.out;
        x2.in[1] &lt;== d;

        out &lt;== x2.out;
    }
    // i must be &lt; 64 by the assert statement above
    else {
        component o1 = BitwiseOr32();
        component n1 = BitwiseNot32();
        n1.in &lt;== d;
        o1.in[0] &lt;== n1.out;
        o1.in[1] &lt;== b;

        component x1 = BitwiseXor32();
        x1.in[0] &lt;== o1.out;
        x1.in[1] &lt;==c;

        out &lt;== x1.out;
    }
}

// n is the number of bytes
template MD5(n) {

    var s[64] = [7, 12, 17, 22,  7, 12, 17, 22,  7, 12, 17, 22,  7, 12, 17, 22,
     5,  9, 14, 20,  5,  9, 14, 20,  5,  9, 14, 20,  5,  9, 14, 20,
     4, 11, 16, 23,  4, 11, 16, 23,  4, 11, 16, 23,  4, 11, 16, 23,
    6, 10, 15, 21,  6, 10, 15, 21,  6, 10, 15, 21,  6, 10, 15, 21];

    var K[64] = [0xd76aa478, 0xe8c7b756, 0x242070db, 0xc1bdceee,
     0xf57c0faf, 0x4787c62a, 0xa8304613, 0xfd469501,
     0x698098d8, 0x8b44f7af, 0xffff5bb1, 0x895cd7be,
     0x6b901122, 0xfd987193, 0xa679438e, 0x49b40821,
     0xf61e2562, 0xc040b340, 0x265e5a51, 0xe9b6c7aa,
     0xd62f105d, 0x02441453, 0xd8a1e681, 0xe7d3fbc8,
     0x21e1cde6, 0xc33707d6, 0xf4d50d87, 0x455a14ed,
     0xa9e3e905, 0xfcefa3f8, 0x676f02d9, 0x8d2a4c8a,
     0xfffa3942, 0x8771f681, 0x6d9d6122, 0xfde5380c,
     0xa4beea44, 0x4bdecfa9, 0xf6bb4b60, 0xbebfbc70,
     0x289b7ec6, 0xeaa127fa, 0xd4ef3085, 0x04881d05,
     0xd9d4d039, 0xe6db99e5, 0x1fa27cf8, 0xc4ac5665,
     0xf4292244, 0x432aff97, 0xab9423a7, 0xfc93a039,
     0x655b59c3, 0x8f0ccc92, 0xffeff47d, 0x85845dd1,
     0x6fa87e4f, 0xfe2ce6e0, 0xa3014314, 0x4e0811a1,
     0xf7537e82, 0xbd3af235, 0x2ad7d2bb, 0xeb86d391];

    var iter_to_index[64] = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
     1, 6, 11, 0, 5, 10, 15, 4, 9, 14, 3, 8, 13, 2, 7, 12,
     5, 8, 11, 14, 1, 4, 7, 10, 13, 0, 3, 6, 9, 12, 15, 2,
    0, 7, 14, 5, 12, 3, 10, 1, 8, 15, 6, 13, 4, 11, 2, 9];

    signal input in[n];

    signal inp[64];
    component Pad = Padding(n);

    for (var i = 0; i &lt; n; i++) {
        Pad.in[i] &lt;== in[i];
    }
    for (var i = 0; i &lt; 64; i++) {
        Pad.out[i] ==&gt; inp[i];
    }

    signal data32[16];
    for (var i = 0; i &lt; 16; i++) {
        data32[i] &lt;== inp[4 * i] + inp[4 * i + 1] * 2**8 + inp[4 * i + 2] * 2**16 + inp[4 * i + 3] * 2**24;
    }

    var A = 0;
    var B = 1;
    var C = 2;
    var D = 3;
    signal buffer[65][4];
    buffer[0][A] &lt;== 1732584193;
    buffer[0][B] &lt;== 4023233417;
    buffer[0][C] &lt;== 2562383102;
    buffer[0][D] &lt;== 271733878;

    component Funcs[64];
    signal toRotates[64];
    component SelectInputWords[64];
    component LeftRotates[64];
    component Overflow32s[64];
    component Overflow32s2[64];
    for (var i = 0; i &lt; 64; i++) {
        Funcs[i] = Func(i);
        Funcs[i].b &lt;== buffer[i][B];
        Funcs[i].c &lt;== buffer[i][C];
        Funcs[i].d &lt;== buffer[i][D];

        Overflow32s[i] = Overflow32();
        Overflow32s[i].in &lt;== buffer[i][A] + Funcs[i].out + K[i] + data32[iter_to_index[i]];

        // rotated = rotate(to_rotate, s[i])
        toRotates[i] &lt;== Overflow32s[i].out;
        LeftRotates[i] = LeftRotate(s[i]);
        LeftRotates[i].in &lt;== toRotates[i];

        // new_B = rotated + B
        Overflow32s2[i] = Overflow32();
        Overflow32s2[i].in &lt;== LeftRotates[i].out + buffer[i][B];

        // store into the next state
        buffer[i + 1][A] &lt;== buffer[i][D];
        buffer[i + 1][B] &lt;== Overflow32s2[i].out;
        buffer[i + 1][C] &lt;== buffer[i][B];
        buffer[i + 1][D] &lt;== buffer[i][C];
    }

    component addA = Overflow32();
    component addB = Overflow32();
    component addC = Overflow32();
    component addD = Overflow32();

    // we hardcode initial state because we only
    // process one 512 bit block
    addA.in &lt;== 1732584193 + buffer[64][A];
    addB.in &lt;== 4023233417 + buffer[64][B];
    addC.in &lt;== 2562383102 + buffer[64][C];
    addD.in &lt;== 271733878 + buffer[64][D];

    signal littleEndianMd5;
    littleEndianMd5 &lt;== addA.out + addB.out * 2**32 + addC.out * 2**64 + addD.out * 2**96;

    // convert the answer to bytes and reverse
    // the bytes order to make it big endian
    component Tb = ToBytes(16);
    Tb.in &lt;== littleEndianMd5;

    // sum the bytes in reverse
    var acc;
    for (var i = 0; i &lt; 16; i++) {
        acc += Tb.out[15 - i] * 2**(i * 8);
    }
    signal output out;
    out &lt;== acc;
}

component main = MD5(10);

// The result out = 
// &quot;RareSkills&quot; in ascii to decimal
/* INPUT = {&quot;in&quot;: [82, 97, 114, 101, 83, 107, 105, 108, 108, 115]} */

// The result is 246193259845151292174181299259247598493

// The MD5 hash of &quot;RareSkills&quot; is 0xb93718dd21d2f5081239d7a16cf69b9d when converted to decimal is 246193259845151292174181299259247598493
</code></pre>
<h2>Motivation for ZK-friendly hashes</h2>
<p>The <a href="https://rareskills.io/post/rank-1-constraint-system">R1CS</a> produced by the code above is over fifty-two thousand rows long, as highlighted in the figure below. There are a lot of opportunities to reduce the size of the circuit, especially by not converting the field elements to 32-bit arrays every time we use them.</p>
<p><img alt="Screenshot showing how many constraints are created" src="assets/md5-constraints.png" /></p>
<p>However, each word in an MD5 (and other modern hashes) is 32 bits, so it will take 32 times as many signals to represent compared to regular code.</p>
<p>In the following chapter, we will learn about hashes that operate on the native finite field rather than on 32-bit words and avoid costly operations like XOR, which require decomposing signals into bits.</p>
<div style='page-break-after: always;'></div>

<h1>ZK Friendly Hash Functions</h1>
<p>Source: https://rareskills.io/post/zk-friendly-hash</p>
<h1>ZK Friendly Hash Functions</h1>
<p>ZK-friendly hash functions are hash functions that require much fewer constraints to prove and verify than traditional cryptographic hash functions.</p>
<p>Hash functions such as SHA256 or keccak256 make heavy use of bitwise operators such as XOR or bit rotation. Proving the correct execution of XOR or bit rotation requires representing the number as 32 bits, which requires 32 separate signals. Since the default word size of traditional hash functions is 32 bits, operations on this data type require 32 signals.</p>
<p>A ZK-friendly hash function uses the native field element as the default data type and avoids operations that decompose the field element into bits. The native operations on field elements are only addition and multiplication, so ZK-friendly hash function operations must only use modular addition and multiplication.</p>
<p>The properties of a hash function that we care about are:</p>
<ol>
<li><strong>Preimage resistance</strong> — given the output of a hash, computing the input should be infeasible.</li>
<li><strong>Collision resistance</strong> — given an input-output pair, it should be computationally infeasible to find another input that results in the same output.</li>
<li><strong>Pseudorandomness</strong> — the output should appear to be random — there should be no statistical relationship between the input and output.</li>
</ol>
<p>We will describe at a high level how two of the most popular ZK-friendly hash functions, Minimal Multiplicative Complexity (MiMC) and Poseidon, work. However, an analysis of why they are secure is outside the scope of this article. In fact, the security of these hash functions — despite being the most battle-tested — is still somewhat of an open question.</p>
<h2>MiMC</h2>
<p>The input to this hash function is a single field element and the output is a single field element.</p>
<p>MiMC initializes 91 random constants and stores them in an array C. These could be computed in a deterministic and transparent manner, such as taking the string “MiMC” and hashing it 91 times with SHA256, and each hash serves as the random number. These constants are fixed, public, and known to all parties. Conventionally, <code style='font-family: Arial'>C[0] = 0</code>. Then, MiMC takes a field element $t_0$ as its input and iteratively computes:</p>
<p>$$<br />
\begin{align*}<br />
\texttt{let } u &amp;= k+t_i+C_i\<br />
t_{i+1} &amp;= i\space&lt;90\space\space?\space\space u^e : \space u^e + k<br />
\end{align*}<br />
$$</p>
<p>where $e$ is some fixed exponent, often 3 or 7. $k$ is a constant that is set to 0 (why we provide an input $k$ only to set it to zero will be discussed later).</p>
<p>For MiMC to be secure, it must be the case that <code style='font-family: Arial'>gcd(e, p - 1) == 1</code>, where <code style='font-family: Arial'>gcd</code> is the greatest common divisor. For Circom’s default field size, <code style='font-family: Arial'>gcd(3, p - 1) ≠ 1</code> but <code style='font-family: Arial'>gcd(7, p - 1) = 1</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">from math import gcd
p = 21888242871839275222246405745257275088548364400416034343698204186575808495617
gcd(3, p - 1)
# 3
gcd(7, p - 1)
# 1
</code></pre>
<p>Hence, Circomlib provides MiMC7 as a hash (where 7 is the exponent). Having said that, libraries using other field sizes might use <code style='font-family: Arial'>e = 3</code> (to understand why this is the case, please see the resource linked at the end of the article).</p>
<p>Below is a minimal example of using MiMC7 with a single input:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/mimc.circom&quot;;

template ExampleMiMC() {
  signal input a;
  signal output out;

  component hash = MiMC7(91);

  hash.x_in &lt;== a;
  hash.k &lt;== 0;
  out &lt;== hash.out;
}
</code></pre>
<p>If we wish to pass multiple field elements to the hash and output a single field element, we use the following approach:</p>
<ol>
<li>We hash the first input element.</li>
<li>The output of that hash becomes the value for <code style='font-family: Arial'>k</code> of the next hash</li>
<li>The input of the next hash is the next part of the input.</li>
</ol>
<p>This can be visualized as follows:</p>
<p><img alt="multi mimc diagram" src="assets/multi-mimc.png" /></p>
<p>The MultiMiMC7 template accomplishes this for us:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/mimc.circom&quot;;

template ExampleMultiMiMC(n) {
  signal input in[n];
  signal output out;

  component hash = MultiMiMC7(n, 91);

  for (var i = 0; i &lt; n; i++) {
    hash.in[i] &lt;== in[i];
  }
  hash.k &lt;== 0;

  out &lt;== hash.out;
}
</code></pre>
<h2>Poseidon</h2>
<p>Poseidon is similar to MiMC except that it adds a step of matrix multiplication. That is, if the input is a single element, it is expanded to <code style='font-family: Arial'>[0, input]</code>, and this vector is multiplied by a series of carefully tuned 2 x 2 matrices. “Careful tuning” here means it has a certain cryptographic property that we will not get into here.</p>
<p>Therefore, instead of a single element going through a series of additions and exponentiations (as in MiMC), a vector goes through a series of element-wise additions, matrix multiplications (which produces a vector of the same dimension) and exponentiations in Poseidon.</p>
<p>Although matrix multiplication adds more constraints, it creates more “dispersion” in the hash, so Poseidon does not need as many rounds as MiMC does.</p>
<p>Below is a minimal example of using Poseidon with a single input:</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/poseidon.circom&quot;;

template Example(n) {
  signal input in[n];
  signal output out;

  component hash = Poseidon(n);

  for (var i = 0; i &lt; n; i++) {
    hash.inputs[0] &lt;== in[i];
  }
  out &lt;== hash.out;
}

component main = Example(1);

/* INPUT = {
  &quot;in&quot;: [5]
} */
</code></pre>
<p>To use more than one input signal, we change the template argument <code style='font-family: Arial'>n</code> for Poseidon to the desired value and provide the correct sized array.</p>
<h2>Poseidon vs MiMC Performance</h2>
<p>For hashes that take a single input, the underlying R1CS of MiMC has 364 constraints:</p>
<p><img alt="mimc constraints" src="assets/mimc-constraints.png" /></p>
<p>while Poseidon has 213:</p>
<p><img alt="Poseidon constraints" src="assets/Poseidon-single.png" /></p>
<p>Now, let’s compare the number of constraints generated when we have two inputs.</p>
<p>For MiMC7, the number of constraints doubles with two inputs:</p>
<p><img alt="multi mimc constraints" src="assets/multi-mimc.png" /></p>
<p>But for Poseidon, the number of constraints barely increases:</p>
<p><img alt="multi poseidon constraints" src="assets/multi-poseidon.png" /></p>
<p>The primary reason for this superior performance is that, unlike MiMC, Poseidon doesn’t redo the hash for each field element in the input. Instead, to hash a larger input, it uses a larger matrix to multiply the input vector with. Circomlib’s Poseidon doesn’t support inputs larger than 17 field elements. If we need to hash a large dataset, this can be problematic. However, if we are building a Merkle tree, we only need to hash two inputs.</p>
<h2>Seeking even more security</h2>
<p>As mentioned previously, the security properties of Poseidon and MiMC are not as well understood as highly battle-hardened hash functions such as SHA-256.</p>
<p>There exists a ZK-friendly hash function with even stronger security assumptions than Poseidon and MiMC, which is based on elliptic curves. Computing the discrete log of elliptic curve points is widely believed to be infeasible without a quantum computer. The Pedersen hash is a ZK-friendly hash function that uses elliptic curve operations as the core subroutine for computing the hash. Doing elliptic curve arithmetic in a circuit will not be as efficient as Poseidon or MiMC, but it is more efficient than traditional cryptographic hashes.</p>
<h2>Bounties on MiMC and Poseidon Bugs</h2>
<p>Currently, the Ethereum Foundation has a <a href="https://crypto.ethereum.org/bounties/mimc-hash-challenge">\$20,000 bounty</a> to find a hash collision on a variant of the MiMC hash described in this article.</p>
<p>The Ethereum Foundation is also currently financially supporting research into the security of Poseidon through the <a href="https://www.poseidon-initiative.info">Poseidon Cryptanalysis Initiative</a>. Ethereum’s founder has <a href="https://x.com/VitalikButerin/status/1894681713613164888">indicated</a> that Ethereum may switch to using Poseidon as the hash function of choice for Ethereum to make the network state easier to prove using ZK.</p>
<h2>Acknowledgement</h2>
<p>The following resource from <a href="https://risczero.com">Risc Zero</a> was consulted while creating this article:</p>
<p><a href="https://www.youtube.com/watch?v=_MIxjDs70W8">https://www.youtube.com/watch?v=_MIxjDs70W8</a></p>
<div style='page-break-after: always;'></div>

<h1>The Permutation Argument</h1>
<p>Source: https://rareskills.io/post/permutation-argument</p>
<h1>The Permutation Argument</h1>
<p>A permutation argument is a proof that two lists hold the same elements, but possibly in a different order. For example, <code style='font-family: Arial'>[2,3,1]</code> is a permutation of <code style='font-family: Arial'>[1,2,3]</code> and vice-versa.</p>
<p>The permutation argument is useful for proving one list is a sorted version of another. That is, if list B has the same elements of list A and the elements of B are sorted, then we know the prover correctly sorted A.</p>
<p>To determine if two lists are the same, we typically sort them and compare them element-wise.</p>
<p>However, to know that a list is sorted, we need to check that 1) the elements are in order and 2) the output of the sorting algorithm contains the same elements as the inputs.</p>
<p>This creates a circular dependency — to know that one list is a permutation of the other, we have to know their sorted versions are identical. But to know that the sorting algorithms were executed properly, we have to know the output of the sort is a permutation of the input.</p>
<p>This isn’t a problem in regular code, but in ZK we must constrain every step of the computation.</p>
<p>We’ve already shown how to prove a list is sorted. This chapter focuses on proving two lists are permutations of each other.</p>
<h2>Option 1: Writing constraints for a sorting algorithm</h2>
<p>In an earlier chapter, we showed how to write constraints for the Selection Sort algorithm. The Selection Sort algorithm runs in $\mathcal{O}(n^2)$ time, so it will have $\mathcal{O}(n^2)$ constraints. We could use a more efficient algorithm like merge sort to accomplish the same thing in $\mathcal{O}(n \log n)$ constraints, but better solutions exist as we will soon see.</p>
<h2>Option 2: Attempt to constrain a 1-to-1 mapping directly</h2>
<p>One can try to directly write a circuit that is satisfied if and only if one list is a permutation of the other. In other words, each element in one list has a matching element in the other list and vice-versa.</p>
<p>For example, to prove that <code style='font-family: Arial'>[a1, a2, a3]</code> is a permutation of <code style='font-family: Arial'>[b1, b2, b3]</code>, we need to show a mapping between the two, it is sufficient to prove that each <code style='font-family: Arial'>a_i</code> maps to <em>some</em> element in the <code style='font-family: Arial'>b</code> list and each <code style='font-family: Arial'>b_i</code> maps to <em>some</em> element in the <code style='font-family: Arial'>a</code> list.</p>
<p>To create a circuit to prove the dual mapping, we create a matrix of <code style='font-family: Arial'>s</code> signals defined as follows:</p>
<pre style='font-family: Arial'><code class="language-solidity">               b1             b2             b3
    --------------------------------------------
a1 | s11 = (a1-b1)  s12 = (a1-b2)  s12 = (a1-b3)
a2 | s21 = (a2-b1)  s22 = (a2-b2)  s23 = (a2-b3)
a3 | s31 = (a3-b1)  s32 = (a3-b2)  s33 = (a3-b3)
</code></pre>
<p>Note that if an <code style='font-family: Arial'>s</code> signal is 0, then the corresponding <code style='font-family: Arial'>a</code> element and <code style='font-family: Arial'>b</code> element are equal. For example, if <code style='font-family: Arial'>a3 == b1</code>, then <code style='font-family: Arial'>s31</code> will be <code style='font-family: Arial'>0</code>.</p>
<p>If we then multiply the <code style='font-family: Arial'>s</code> signals row-wise and column-wise and constrain their products to be <code style='font-family: Arial'>o</code> signals as shown below, then the <code style='font-family: Arial'>o</code> signals will be zero if there is at least one matching element.</p>
<pre style='font-family: Arial'><code class="language-solidity">     b1      b2      b3
a1  s11  ×  s12  ×  s13   =  o_row1
     ×       ×       ×
a2  s21  ×  s22  ×  s23   =  o_row2
     ×       ×       ×
a3  s31  ×  s32  ×  s33   =  o_row3
    ||      ||       ||
    o_col1  o_col2   o_col3
</code></pre>
<p>If we constrain each of the <code style='font-family: Arial'>o</code> signals to be zero, then that also constrains that each element in a has at least one matching element in b and vice-versa. Consider the interpretation of the output signals:</p>
<ul>
<li><code style='font-family: Arial'>o_row1</code> is zero if-and-only-if <code style='font-family: Arial'>a1</code> matches an element in <code style='font-family: Arial'>b</code>.</li>
<li><code style='font-family: Arial'>o_row2</code> is zero if-and-only-if <code style='font-family: Arial'>a2</code> matches an element in <code style='font-family: Arial'>b</code>.</li>
<li><code style='font-family: Arial'>o_row3</code> is zero if-and-only-if <code style='font-family: Arial'>a3</code> matches an element in <code style='font-family: Arial'>b</code>.</li>
<li><code style='font-family: Arial'>o_col1</code> is zero if-and-only-if <code style='font-family: Arial'>b1</code> matches an element in <code style='font-family: Arial'>a</code>.</li>
<li><code style='font-family: Arial'>o_col2</code> is zero if-and-only-if <code style='font-family: Arial'>b2</code> matches an element in <code style='font-family: Arial'>a</code>.</li>
<li><code style='font-family: Arial'>o_col3</code> is zero if-and-only-if <code style='font-family: Arial'>b3</code> matches an element in <code style='font-family: Arial'>a</code>.</li>
</ul>
<p>Therefore, if all of the <code style='font-family: Arial'>o</code> signals are zero, then each element of each list has a matching element in the other list.</p>
<p>The drawback of this approach is that the number of constraints grows quadratically with the length of the list.</p>
<p>Instead, we show a method to prove one list is a permutation of the other in time linear to the length of the list.</p>
<p><strong>Credit:</strong> The rest of this article is heavily based on this <a href="https://triton-vm.org/spec/permutation-argument.html">documentation of the Triton VM</a>, we simply show a Circom implementation and add some more beginner-friendly explanations.</p>
<h2>How the permutation argument works</h2>
<p>Consider a polynomial written in the form:</p>
<p>$$<br />
(x-a)(x-b)(x-c)<br />
$$</p>
<p>Its value does not change if the order of the multiplication changes:</p>
<p>$$<br />
(x-b)(x-a)(x-c)<br />
$$</p>
<p>In other words, permuting the multiplication of the <a href="https://www.notion.so/The-Permutation-Argument-14709cb3e96280489504de20d5f93804?pvs=21"><em>linear factors</em></a> of the polynomial does not change the value of the polynomial. (A linear factor is a term of the form $(x – a)$).</p>
<p>We do not check if the polynomials are equivalent by algebraically multiplying the terms. Rather, we can use a much more efficient polynomial equality test called the <a href="https://rareskills.io/post/schwartz-zippel-lemma">Schwartz-Zippel lemma</a>.</p>
<p>This test samples a random point for $x$ and plugs it into the two polynomials. If they have the same evaluation, then with extremely high probability, they are the same polynomial (to understand why this test is secure, please see the linked article above).</p>
<p>This technique can be used to prove that the arrays $[a,b,c]$ and $[b,c,a]$ are permutations of each other. We create a circuit that takes two arrays $c_1,c_2,c_3,…,c_n$ and $d_1,d_2,d_3,…,d_n$ as input and then constructs polynomials:</p>
<p>$$<br />
\begin{align*}<br />
(x – c_1)(x – c_2)…(x – c_n)\<br />
(x – d_1)(x – d_2)…(x – d_n)<br />
\end{align*}<br />
$$</p>
<p>Finally, it picks a random point for $x$, evaluates the two polynomials, and constrains the products to be the same.</p>
<p>To generate the random point, let’s call it $r$, we use the hash of the inputs, i.e., hashing the concatenation of the arrays:</p>
<p>$$<br />
r=\mathsf{hash}([a,b,c,b,c,a])<br />
$$</p>
<p>This way, the prover cannot try to “cheat” by picking a value for $r$ where the polynomials intersect. Once the prover has provided the polynomials, they cannot control the value $r$ at which these polynomials are evaluated.</p>
<p>If the prover changes the polynomials, the value $r$ they are tested at will also change.</p>
<p>The circuit below takes two lists and checks if they are permutations of each other. The array signal <code style='font-family: Arial'>prodA</code> holds the terms:</p>
<ul>
<li>$\texttt{prodA[0] = } r – a_0$</li>
<li>$\texttt{prodA[1] = prodA[0]} \cdot(r – a_1)$</li>
<li>$\texttt{prodA[2] = prodA[1]} \cdot(r – a_2)$</li>
<li>$\texttt{prodA[n – 1] = prodA[n – 2]}\cdot(r -a_{n-1})$</li>
</ul>
<p>Thus, the final entry <code style='font-family: Arial'>prodA[n - 1]</code> holds the evaluation of the polynomial at <code style='font-family: Arial'>r</code>. Here, <code style='font-family: Arial'>r</code> is <code style='font-family: Arial'>hash.out</code>, which is the Poseidon hash of all the entries of arrays <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>b</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">include &quot;circomlib/poseidon.circom&quot;;

template IsPermutation(n) {
  signal input a[n];
  signal input b[n];

  // the random point will be the hash
  // of the concatenation of the arrays
  component hash = Poseidon(2 * n);
  for (var i = 0; i &lt; n; i++) {
    hash.inputs[i] &lt;== a[i];
    hash.inputs[i + n] &lt;== b[i];
  }

  signal prodA[n];
  signal prodB[n];

  prodA[0] &lt;== hash.out - a[0];
  prodB[0] &lt;== hash.out - b[0];

  for (var i = 1; i &lt; n; i++) {
    prodA[i] &lt;== (hash.out - a[i]) * prodA[i - 1];
    prodB[i] &lt;== (hash.out - b[i]) * prodB[i - 1];
  }

  // the evaluation of the polynomials at r = hash.out
  prodA[n - 1] === prodB[n - 1];
}

component main = IsPermutation(3);

/* INPUT = {
  &quot;a&quot;: [1,2,3,4,5,6],
  &quot;b&quot;: [1,2,3,4,6,5]
}
*/
</code></pre>
<h2>Vulnerability: Not Hashing All Elements</h2>
<p>When generating random points in this manner, the hash must depend on all the inputs to the computation. Otherwise, a malicious prover can keep the hash value fixed, and tweak the values of the output array until they find an intersection point of the polynomial.</p>
<h2>A note about safety</h2>
<p>This algorithm relies on non-equal polynomials only intersecting in at most $d$ points where $d$ is the maximum degree of the two polynomials. If the size of the finite field is much greater than $d$, then we can assume that in practice, $p(r)\neq q(r)$ if $p$ and $q$ are non-equal polynomials and $r$ is a random point. Circom uses a finite field size that is slightly larger than $2^{253}$. If the polynomials have degree one million, then the probability that $r$ is an intersection point is approximately $2^{20}/2^{253}$ or $1/2^{233}$ or $1$ out of $10^{70}$. This is nearly on the same order of magnitude of the number of atoms in the universe.</p>
<p>If we used a very small finite field, however, say on the order of 31 bits, then the probability of $p(r)=q(r), q \ne p$ for a random $r$ is not negligible.</p>
<div style='page-break-after: always;'></div>

<h1>How Tornado Cash Works (Line by Line for Devs)</h1>
<p>Source: https://rareskills.io/post/how-does-tornado-cash-work</p>
<h1>How Tornado Cash Works (Line by Line for Devs)</h1>
<p><img alt="tornado cash tutorial by rareskills" src="assets/935a00_8fb7ab406b7c430dafcdff2f004b56c7_mv2.png" /></p>
<h2>Introduction to Tornado Cash</h2>
<p>Tornado cash is a cryptocurrency smart contract mixer that enables users to deposit crypto with one address and withdraw with another wallet without creating a traceable link between those two addresses.</p>
<p>Tornado cash is probably the most iconic zero knowledge smart contract application, so we will explain how it works at a sufficiently low level that a programmer can reproduce the application.</p>
<p>It is assumed that the reader knows how Merkle Trees work and that inverting a cryptographic hash is infeasible. The reader is also assumed to be at least a strong intermediate at Solidity (since we will be reading snippets of the source code).</p>
<p>Tornado Cash is a fairly advanced smart contract, so check out our <a href="https://rareskills.io/learn-solidity">Solidity Tutorial</a> first if you are still new to the language.</p>
<h2>A quick warning about Tornado Cash</h2>
<h3>Legal issues</h3>
<p>Tornado Cash is currently sanctioned by the United States government, interacting with it may “taint” your wallet and cause transactions from it to be flagged later when interacting with centralized exchanges.</p>
<p><strong>Update for 2024</strong>: As of November 28, 2024 the saction has been lifted by the US Appeals Court.</p>
<h3>Tornado cash hack</h3>
<p>On May 27, the <a href="https://github.com/tornadocash/tornado-governance">Tornado Cash governance smart contracts</a> (not the smart contracts we will review here) were hacked when an attacker passed a malicious proposal that gave them the majority of the ERC20 voting tokens for governance <a href="https://rareskills.io/post/erc20-votes-erc5805-and-erc6372">ERC20 voting tokens for governance</a>. They have since <a href="https://www.ccn.com/tornado-cash-hack-saga-comes-to-an-end-did-the-hacker-win-this-battle/">handed back control</a>, keeping a relatively small amount for themselves.</p>
<h2>How zero knowledge works (for programmers who hate math)</h2>
<p>You can understand how Tornado Cash works without knowing zero knowledge proofs algorithms, but you must know how zero knowledge proofs work. <strong>Contrary to their name and many popular examples, Zero Knowledge Proofs prove the validity of a computation, not knowledge of a certain fact. They do not carry out the computation. They take an agreed-upon computation, a proof the computation was carried out, and the result of the computation, then determine if the prover really ran the computation and produced the output. The zero knowledge part comes from the optional feature that the proof-of-computation can be represented in a way that gives no information about the inputs.</strong></p>
<p>For example, you can demonstrate you know the prime factors of a number without revealing them using the <a href="https://rareskills.io/post/solidity-rsa-signatures-for-aidrops-and-presales-beating-ecdsa-and-merkle-trees-in-gas-efficiency">RSA algorithm</a>. RSA doesn’t claim “I know two secret numbers” in isolation; it proves that you multiplied two concealed numbers together and generated the public number. RSA Encryption is really just a special case of zero knowledge proofs. In RSA, you demonstrate you know two secret primes that multiply together to produce your public key. In Zero Knowledge, you generalize that “magic multiplication” to arbitrary arithmetic and boolean operations. Once we have zero knowledge operations for elementary operations, we can build up zero knowledge proofs for more exotic things like proving we know the preimage of hash functions, Merkle roots, or even a fully functional virtual machine.</p>
<p>Another point: <strong>A zero knowledge proof verification does not carry out the computation, it only verifies someone carried out a computation and produced a claimed output.</strong></p>
<p>Yet another useful corollary: <strong>To generate, but not verify, a zero knowledge proof of a computation, you must actually carry out the computation.</strong></p>
<p>This makes sense right? How could you prove you know the preimage of a hash without actually hashing the preimage? Hence, the prover actually carries out the computation and creates some auxiliary data known as the proof to prove that they did the computation correctly.</p>
<p>When you validate an RSA signature, you don’t multiply someone else’s private key factors to produce their public key, that would defeat the purpose. You just verify the signature and the message checks out using a separate algorithm. Therefore, a computation consists of the following</p>
<pre style='font-family: Arial'><code class="language-solidity">verifier_algorithm(proof, computation, public_output) == true
</code></pre>
<p>In the context of RSA, you can think of the public key being the result of the computation, and the signature and message being the zero knowledge proof.</p>
<p>The computation and output are public. We agree that we are using a certain hash function and got a certain output. The proof “hides” the input that was used and only proves the hash function was executed and produced the output.</p>
<p><img alt="zero knowledge proof flowchart" src="assets/935a00_84da717f7427457d8018eed3bfc6a9ee_mv2.png" /></p>
<p>Given only the proof a verifier can <strong>not</strong> calculate the <code style='font-family: Arial'>public_output</code>. The verification step does not conduct the calculation, it only verifies that the claimed calculation that produces the public output based on the proof.</p>
<p>We aren’t going to teach zero knowledge algorithms in this article, but as long as you can accept that we can prove a computation took place without conducting the computation ourselves, we are good to go.</p>
<h2>How anonymous cryptocurrency transfers work: mixing</h2>
<p>Fundamentally, Tornado Cash’s strategy for anonymization is mixing, similar to other anonymous cryptocurrencies like Monero. Multiple users submit their cryptocurrency to one address, “mixing” their deposits together. They then withdraw in such a way that the depositor and the withdrawer cannot be tied together.</p>
<p>Imagine 100 people put one dollar bills into a pile. Then 100 different people show up a day later and each withdraw a one dollar bill. In this scheme, we have no idea who the original depositors were trying to send the money to.</p>
<p><img alt="cryptocurrency mixer" src="assets/935a00_9d9def80d2ed462ead091b08af0b9996_mv2.png" /></p>
<p>There is an obvious problem that if <em>anyone</em> could take the cash out of the pile, it will quickly get stolen. But if we try to leave some metadata behind about who is allowed to withdraw, then that will give away who the depositor is trying to send money to.</p>
<h3>Mixing can never be totally private</h3>
<p>When you send Ether to Tornado Cash, this is fully public. When you withdraw from Tornado Cash, this is fully public too. What isn’t public is that the two addresses involved are associate with each other (assuming there are enough other depositors and withdrawers).</p>
<p>All people can tell about an address is “this address got its Ether from Tornado Cash” or “this other address deposited to Tornado Cash.” When an address withdraws from Tornado Cash, people can’t tell which depositor the crypto came from.</p>
<h2>Tornado Cash without zero knowledge: lots of logical ORs of hash preimage proofs</h2>
<p>Let’s try to solve this problem without worrying about privacy.</p>
<p>The depositor creates two secret numbers, concatenates them, and puts the hash of it on chain when depositing the Eth (we will discuss later why we generated two secret numbers rather than one). When several people deposit, there are several public hashes sitting in a smart contract that we don’t know the preimage of.</p>
<p>The withdrawer shows up, the withdrawer reveals the preimage (the two secret numbers) of one of the hashes and takes their deposit out.</p>
<p>This clearly fails because it shows the depositor communicated to withdrawer the secret numbers off-chain.</p>
<p>However, if the withdrawer can demonstrate they know the preimage of <em>one of the hashes</em> without revealing <em>which hash it is</em> and <em>without revealing the preimage of the hash</em>, then we have a functional cryptocurrency mixer!</p>
<p>The naïve solution to this is to create a computation where we go over the hashes in a loop:</p>
<pre style='font-family: Arial'><code class="language-solidity">zkproof_preimage_is_valid(proof, hash_{1}) OR 
zkproof_preimage_is_valid(proof, hash_{2}) OR
zkproof_preimage_is_valid(proof, hash_{3}) OR
...
zkproof_preimage_is_valid(proof, hash_{n-1}) OR
zkproof_preimage_is_valid(proof, hash_{n})
</code></pre>
<p>Remember, the verifier isn’t actually carrying out the above computation, so we don’t know which hash is the valid one. The verifier (tornado cash), is simply verifying the prover carried out the above computation and it returned true. How it returned true is irrelevant, only that it did; and it can only return true if the prover knows one of the preimages.</p>
<p>It is important to note at this point: <strong>all deposit hashes are publicly known</strong>. When a user deposits, they submit the hash of the two secret numbers, and that hash is public. What we are trying to conceal is which hash the withdrawer knows the preimage of.</p>
<p>But this is a very big computation. Big for-loops with a lot of deposits are expensive. [1]</p>
<p>We need a data structure that can compactly hold a lot of hashes in it, and thankfully we do: Merkle Trees.</p>
<h2>Using Merkle trees to store a bunch of hashes</h2>
<p>Rather than looping over all the hashes, we can instead say “I know the preimage of one of the hashes” and “the hash is inside the Merkle Tree.” That accomplishes the same thing as pointing to a very long array of hashes and saying “I know the preimage of one of those hashes,” except it’s a lot more efficient.</p>
<p>Merkle Proofs are logarithmic in the size of the tree, so it doesn’t require too much extra work (compared to our massive for-loop from earlier).</p>
<p>When cryptocurrency is deposited, the user generates two secret numbers, concatenates them, hashes them, and puts the hash into the Merkle tree.</p>
<p>At withdraw time, the withdrawer produces a leaf hash preimage, then proves that leaf hash is in the tree via Merkle proof.</p>
<p>This of course ties the depositor to the withdrawer, but <strong>if we do both the Merkle proof and the leaf preimage verification in a zero knowledge manner, then the link is broken!</strong></p>
<p>Zero knowledge proofs let us prove <strong>that</strong> we generated a valid Merkle proof against the public Merkle root as well as the preimage of the leaf – without showing <strong>how</strong> we conducted that computation.</p>
<p>It is not secure enough to simply provide an zero knowledge proof of having a Merkle proof and producing the root, the withdrawer must also prove they know the preimage of the leaf.</p>
<p>The Merkle Tree’s leaves are all public. Every time someone deposits, they supply a hash which is stored publicly. Since the Merkle tree is fully public, anyone can compute a Merkle proof for any of the leaves.</p>
<p>Therefore, proving a leaf is in the tree is not sufficient to prevent theft by proof forgery.</p>
<p>The withdrawer must also prove knowledge of the preimage of the leaf in question, without revealing the leaf. Remember, the leaf itself is the hash of two numbers.</p>
<p>You can see that the <code style='font-family: Arial'>_commitment</code> argument in the <a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/Tornado.sol#L55">function for deposit</a> is public. The <code style='font-family: Arial'>_commitment</code> variable is the leaf being added to the tree, which is the hash of the two secret numbers, which the depositor does not publish.</p>
<pre style='font-family: Arial'><code class="language-solidity">/**
  @dev Deposit funds into the contract. The caller must send (for ETH) or approve (for ERC20) value equal to or `denomination` of this instance.
  @param _commitment the note commitment, which is   PedersenHash(nullifier + secret)
**/
function deposit(bytes32 _commitment) external payable nonReentrant {
    require(!commitments[_commitment], &quot;The commitment has been submitted&quot;);

    uint32 insertedIndex = _insert(_commitment);
    commitments[_commitment] = true;
    _processDeposit();

    emit Deposit(_commitment, insertedIndex, block.timestamp);
}
</code></pre>
<p>Effectively, the proof for withdrawal consists of proving the following computation has been carried out:</p>
<pre style='font-family: Arial'><code class="language-solidity">processMerkleProof(merkleProof, hash(concat(secret1, secret2))) == root
</code></pre>
<p>where <code style='font-family: Arial'>processMerkleProof</code> takes the Merkle proof and leaf as an argument, and <code style='font-family: Arial'>hash(concat(secret1, secret2))</code> produces the leaf.</p>
<p>In the context of Tornado Cash, the verifier is the Tornado Cash smart contract that will release funds to whoever supplies a valid proof.</p>
<p>The prover is the withdrawer who can prove they carried out a hash computation to produce one of the leaves. Generally, the only person who can withdraw is the same person who deposited, as they would be the only party that can prove they know the hash preimages. Of course, this user must use a different and completely unassociated address for withdrawal!</p>
<p>The withdrawer actually caries out the above computation (Merkle proof and leaf hash generation), produces the zk-proof that they carried it out properly, then supplies this proof to the smart contract.</p>
<p>The <code style='font-family: Arial'>merkleProof</code> and <code style='font-family: Arial'>{secret1, secret2}</code> are hidden in the proof, but with the proof of computation, a verifier can validate the withdrawer actually ran the computation to correctly produce the leaf and Merkle root.</p>
<p>So let’s summarize:</p>
<ul>
<li>The Depositor:</li>
<li>Generates two secret numbers and creates a commitment hash from their concatenation</li>
<li>Submits a commitment hash</li>
<li>Transfer crypto to Tornado Cash</li>
<li>Tornado Cash:</li>
<li>During the deposit stage:<ul>
<li>adds the commitment hash to the Merkle Tree</li>
</ul>
</li>
<li>The Withdrawer:</li>
<li>Generates a valid Merkle proof for the Merkle root</li>
<li>Generates the commitment hash from the two secret numbers</li>
<li>Generates a zk-proof of the above computations</li>
<li>Submits the proof to Tornado Cash</li>
<li>Tornado Cash</li>
<li>During the withdraw stage:<ul>
<li>verifies the proofs against the Merkle root</li>
<li>transfers cryptocurrency to the withdrawer</li>
</ul>
</li>
</ul>
<h2>Preventing multiple withdrawals</h2>
<p>The scheme above has an issue: what prevents us from withdrawing multiple times? Presumably, we’d have to “remove” the leaf from the Merkle Tree to account for the withdrawn deposit, but that would reveal which deposit is ours!</p>
<p>Tornado cash handles this by never removing leaves from the Merkle Tree. Once a leaf is added to the Merkle Tree, it stays there forever.</p>
<p>To prevent multiple withdrawals, the smart contract uses what is called a “nullifier scheme,” which is quite common in zero-knowledge applications and protocols.</p>
<h3>Nullifier Scheme</h3>
<p>A nullifier scheme in zero knowledge behaves like an exotic nonce that provides a layer of anonymity.</p>
<p>It will be clear why there are two secret numbers that make up a leaf, rather than one.</p>
<p>The two numbers that go into the deposit hash are the nullifier and a secret and the leaf is the hash of the <code style='font-family: Arial'>concat(nullifier, secret)</code> in that order.</p>
<p>During withdrawal, the user must submit the hash of the nullifier, i.e. the <code style='font-family: Arial'>nullifierHash</code> and proof they concatenated the nullifier and secret and hashed that to produce one of the leaves. The smart contract can then verify (using a zero knowledge algorithm) the sender really knows the preimage proof of the nullifier hash.</p>
<p>The nullifier hash is added to a mapping to ensure it is never reused.</p>
<p>This is why we need two secret numbers. If we revealed both numbers, then we would know which leaf the withdrawer was targeting! By revealing only one of the constituent numbers, it’s impossible to determine which leaf it is associated with.</p>
<p>Remember, zero knowledge proof verification cannot compute the nullifier given the secret input, it can only verify the computation, output, and proof are congruent. That is why the user must submit a public nullifierHash and proof they computed it from the hidden nullifier.</p>
<p>You can see this logic in the Tornado Cash <a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/Tornado.sol#L79">withdraw function</a> screenshotted below.</p>
<p><img alt="tornado cash withdraw function source code" src="assets/935a00_036bdf19c4094319aac7a3d0b0b5a2f3_mv2.png" /></p>
<p>Let’s summarize. The user must prove</p>
<ul>
<li>they know the preimage of the leaf</li>
<li>the nullifier has not been used before (this is a simple solidity mapping, not a zk verification step)</li>
<li>they can produce the nullifier hash and the preimage of the nullifier</li>
</ul>
<p>Here are the possible outcomes:</p>
<ul>
<li>the user provides the wrong nullifier: the zk proof for checking the nullifier and nullifier preimage will not pass</li>
<li>the user provides the wrong secret: the zk proof for the leaf preimage will not pass</li>
<li>the user provides the wrong nullifier hash (to bypass the check on line 86): the zk proof for the nullifier and nullifier primage will not pass</li>
</ul>
<h2>The Incremental Merkle Tree is a gas efficient Merkle Tree</h2>
<p>You may have noticed, we pulled a fast one in the above explanations. How can you update a Merkle Tree on chain without running out of gas? There are presumably a lot of deposits, and recomputing the whole thing would be prohibitive.</p>
<p>The incremental Merkle Tree gets around these restrictions with some clever optimizations. But before we get to the optimizations, we need to understand the restrictions.</p>
<p>An incremental Merkle Tree is a Merkle Tree of fixed depth where each leaf start as a zero value, and non-zero values are added by replacing the zero leaves starting from the left-most leaf to the right-most leaf one-by-one.</p>
<p>The animation below demonstrates an incremental Merkle Tree of depth 3, which can hold up to 8 leaves. In keeping with our domain terminology for Tornado Cash, we call these “commitments” labeled with the variable $C_i$.</p>
<p><img alt="incremental merkle tree animation" src="assets/935a00_b974bc021cd64fb3860ef2d886ea72b8_mv2.gif" /></p>
<p>Here are some important features of an incremental Merkle tree</p>
<ul>
<li>The Merkle Tree has a fixed depth of 32. This means it cannot handle more than <code style='font-family: Arial'>2^32 - 1</code> deposits. (This is an arbitrary depth chosen by Tornado Cash, but it needs to be constant).</li>
<li>The Merkle Tree starts off as a tree where all the leaves are <code style='font-family: Arial'>hash(bytes32(0))</code>.</li>
<li>As deposits are made, the left-most unused leaf is overwritten with the commitment hash. Deposits are added to the leaves in a “left to right” manner.</li>
<li>Once a deposit is made into the Merkle Tree, it cannot be removed.</li>
<li>With every new deposit, a new root is stored. Tornado cash calls this a “Merkle Tree with history.” So Tornado cash really stores an array of Merkle roots, not a single one. Obviously, the Merkle root changes as members are added.</li>
</ul>
<p>Now we have a problem: building a Merkle Tree with <code style='font-family: Arial'>2^32 - 1</code> leaves on-chain is going to run out of gas. Just computing the first level will require over 4 million iterations, which obviously won’t work.</p>
<p>But restrictions of the incremental Merkle Trees enable two key invariants that allow the smart contract to take a big computational shortcut: every thing to the right of the current node is a Merkle subtrees of predictable height where all the roots are zero, and everything to the left of the current node can be cached instead of recomputed.</p>
<h3>Clever shortcut 1: all subtrees to the right of the newest member consist of merkle subtrees with all zero leaves</h3>
<h4>Merkle subtrees with all zero leaves have predictable roots which can be precomputed.</h4>
<p>Since all of the leaves start off as zero, a significant amount of the work going into building the Merkle Tree will include computing Merkle Trees where all the leaves are zero.</p>
<p>See the image below and note how much computation is repeated when all the leaves are zero:</p>
<p><img alt="numbered leaves for an incremental merkle tree" src="assets/935a00_671a141c39f345e3931bd5b36e3945e9_mv2.png" /></p>
<p>Most of the leaf-pairs are going to be the concatenation of <code style='font-family: Arial'>bytes32(0)</code> and <code style='font-family: Arial'>bytes32(0)</code>. Then that hash is going to be concantenated with an identical hash from the sister subtree, and so forth and so on.</p>
<p>Tornado Cash pre-computes the hash of a depth-zero tree (just the hash of a <code style='font-family: Arial'>bytes32(0) leaf)</code>, the root of a sub-tree with two leaves of zero, the root of a sub-tree with four leaves of zero, the root of a sub-tree with eight leaves of zero, and so forth.</p>
<p>This means we can precompute the Merkle root for a Merkle Tree (with all zero leaves) of height 0, 1, 2, etc until 31 (remember, the height of the Merkle Tree is fixed).</p>
<p>For each possible height of a Merkle subtree where the leaves are all zero, Tornado Cash precomputes it. Here is the precomputed list in <a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/MerkleTreeWithHistory.sol#L125">Tornado Cash’s Merkle Tree With History</a>:</p>
<pre style='font-family: Arial'><code class="language-solidity">  /// @dev provides Zero (Empty) elements for a MiMC MerkleTree. Up to 32 levels
  function zeros(uint256 i) public pure returns (bytes32) {
    if (i == 0) return bytes32(0x2fe54c60d3acabf3343a35b6eba15db4821b340f76e741e2249685ed4899af6c);
    else if (i == 1) return bytes32(0x256a6135777eee2fd26f54b8b7037a25439d5235caee224154186d2b8a52e31d);
    else if (i == 2) return bytes32(0x1151949895e82ab19924de92c40a3d6f7bcb60d92b00504b8199613683f0c200);
    else if (i == 3) return bytes32(0x20121ee811489ff8d61f09fb89e313f14959a0f28bb428a20dba6b0b068b3bdb);
    else if (i == 4) return bytes32(0x0a89ca6ffa14cc462cfedb842c30ed221a50a3d6bf022a6a57dc82ab24c157c9);
    else if (i == 5) return bytes32(0x24ca05c2b5cd42e890d6be94c68d0689f4f21c9cec9c0f13fe41d566dfb54959);
    else if (i == 6) return bytes32(0x1ccb97c932565a92c60156bdba2d08f3bf1377464e025cee765679e604a7315c);
    else if (i == 7) return bytes32(0x19156fbd7d1a8bf5cba8909367de1b624534ebab4f0f79e003bccdd1b182bdb4);
    else if (i == 8) return bytes32(0x261af8c1f0912e465744641409f622d466c3920ac6e5ff37e36604cb11dfff80);
    else if (i == 9) return bytes32(0x0058459724ff6ca5a1652fcbc3e82b93895cf08e975b19beab3f54c217d1c007);
    else if (i == 10) return bytes32(0x1f04ef20dee48d39984d8eabe768a70eafa6310ad20849d4573c3c40c2ad1e30);
    else if (i == 11) return bytes32(0x1bea3dec5dab51567ce7e200a30f7ba6d4276aeaa53e2686f962a46c66d511e5);
    else if (i == 12) return bytes32(0x0ee0f941e2da4b9e31c3ca97a40d8fa9ce68d97c084177071b3cb46cd3372f0f);
    else if (i == 13) return bytes32(0x1ca9503e8935884501bbaf20be14eb4c46b89772c97b96e3b2ebf3a36a948bbd);
    else if (i == 14) return bytes32(0x133a80e30697cd55d8f7d4b0965b7be24057ba5dc3da898ee2187232446cb108);
    else if (i == 15) return bytes32(0x13e6d8fc88839ed76e182c2a779af5b2c0da9dd18c90427a644f7e148a6253b6);
    else if (i == 16) return bytes32(0x1eb16b057a477f4bc8f572ea6bee39561098f78f15bfb3699dcbb7bd8db61854);
    else if (i == 17) return bytes32(0x0da2cb16a1ceaabf1c16b838f7a9e3f2a3a3088d9e0a6debaa748114620696ea);
    else if (i == 18) return bytes32(0x24a3b3d822420b14b5d8cb6c28a574f01e98ea9e940551d2ebd75cee12649f9d);
    else if (i == 19) return bytes32(0x198622acbd783d1b0d9064105b1fc8e4d8889de95c4c519b3f635809fe6afc05);
    else if (i == 20) return bytes32(0x29d7ed391256ccc3ea596c86e933b89ff339d25ea8ddced975ae2fe30b5296d4);
    else if (i == 21) return bytes32(0x19be59f2f0413ce78c0c3703a3a5451b1d7f39629fa33abd11548a76065b2967);
    else if (i == 22) return bytes32(0x1ff3f61797e538b70e619310d33f2a063e7eb59104e112e95738da1254dc3453);
    else if (i == 23) return bytes32(0x10c16ae9959cf8358980d9dd9616e48228737310a10e2b6b731c1a548f036c48);
    else if (i == 24) return bytes32(0x0ba433a63174a90ac20992e75e3095496812b652685b5e1a2eae0b1bf4e8fcd1);
    else if (i == 25) return bytes32(0x019ddb9df2bc98d987d0dfeca9d2b643deafab8f7036562e627c3667266a044c);
    else if (i == 26) return bytes32(0x2d3c88b23175c5a5565db928414c66d1912b11acf974b2e644caaac04739ce99);
    else if (i == 27) return bytes32(0x2eab55f6ae4e66e32c5189eed5c470840863445760f5ed7e7b69b2a62600f354);
    else if (i == 28) return bytes32(0x002df37a2642621802383cf952bf4dd1f32e05433beeb1fd41031fb7eace979d);
    else if (i == 29) return bytes32(0x104aeb41435db66c3e62feccc1d6f5d98d0a0ed75d1374db457cf462e3a1f427);
    else if (i == 30) return bytes32(0x1f3c6fd858e9a7d4b0d1f38e256a09d81d5a5e3c963987e2d4b814cfab7c6ebb);
    else if (i == 31) return bytes32(0x2c7a07d20dff79d01fecedc1134284a8d08436606c93693b67e333f671bf69cc);
    else revert(&quot;Index out of bounds&quot;);
}
</code></pre>
<p>When computing the Merkle root, we always know the “z-level” we are at and can simply get the precomputed Merkle subtree root where all the leaves are zero.</p>
<h4>A technicality about “zero roots”</h4>
<p>Tornado cash doesn’t actually use the <code style='font-family: Arial'>hash(bytes32(0))</code> as the empty value, it uses <code style='font-family: Arial'>hash(“tornado”)</code>. This doesn’t affect the algorithm, as it is just a constant. However, it’s easier to discuss Incremental Merkle Trees using a notion of zeroness being zero rather than a funny constant.</p>
<h3>Clever shortcut 2: all subtrees to the left of the newest member consist of subtrees whose roots can be cached rather than recalculated</h3>
<p>Consider the case where we add the second deposit. We’ve already computed the hash of the first deposit. That hash gets cached in a mapping Tornado Cash calls <code style='font-family: Arial'>filledSubtrees</code>. A <code style='font-family: Arial'>filledSubtree</code> is simply a subtree in the Merkle Tree where all of the leaves are non-zero. We call that fs in the animation below:</p>
<p><img alt="incremental merkle tree animation for filled subtrees" src="assets/935a00_f3462b2b97724b6194ec9375062306eb_mv2.gif" /></p>
<p>The point is, any time you need an intermediate hash on the left, it’s already been computed for you.</p>
<p>This nice feature is a byproduct of the restriction that leaves cannot be changed or removed. Once a subtree is full of commitments instead of zeros, it never needs to be recomputed.</p>
<p>Now let’s generalize this. Instead of seeing the node to our left as the “first deposit”, imagine that itself is the root of a subtree.</p>
<p>In the most extreme case, consider when we put in the very last leaf. To our immediate left will be a “tree” that consists of the second-to-last leaf (depth 0), to that left will be a subtree of depth 1, to that left will be a subtree of depth 2 (with 4 leaves), to that left will be a subtree of depth 3 (with 8 leaves), etc. There won’t be more than 32 such trees in the most extreme case.</p>
<h3>Combining the shortcuts</h3>
<p>Everything to our left is a filled subtree (even if it is just a leaf) and everything to our right is always a zero leaf or a subtree that consists of all zero leaves. <strong>Since the left root is cached, and the right root is precomputed from a subtree of height $n$ of all zeros, we can compute any intermediate hash concatenation at any level efficiently, and generate a depth-32 Merkle Tree on chain with only 32 iterations.</strong> This isn’t cheap, but it’s feasible. It sure is a lot better than 4 million computations though!</p>
<h3>Hash left or hash right?</h3>
<p>But as we “hash our way to the root”, how do we know which order to concatenate the hashes of the subtrees?</p>
<p>For example, we take a new commitment hash and add it as a leaf. In the node above us, do we concatenate it as <code style='font-family: Arial'>new_commitment | other_value</code> or <code style='font-family: Arial'>other_value | new_commitment</code>?</p>
<p>Here is the trick: every even indexed node is a left child, and every odd indexed node is a right child. This is true for the leaf nodes and for every level of the tree. You can see this pattern in the diagram below.</p>
<p><img alt="numbered leaves for an incremental merkle tree" src="assets/935a00_ecefba6546ec40e1ab09591d881ec173_mv2.png" /></p>
<p>Let’s get an intuition for the pattern. If the zeroth leaf is being inserted, then we are only going to hash right on the way to the root. <code style='font-family: Arial'>0 ÷ 2</code> will remain zero, and zero is an even number using the definition above. Because zero is even, we will always hash right on our way to the root.</p>
<p>Now let’s look at the other extreme case. When the last leaf is inserted, one must always hash left on the way to the root. Every node on the way to the top is odd. This pattern generalizes to every node in the middle; starting with the index of the leaf and repeatedly dividing by two as we go up the tree will tell us if we are on a left child or on a right child. The following animation illustrates that we hash left when we are on an odd node and hash right when we are on an even node:</p>
<p><img alt="incremental merkle tree algorithm to hash left or right" src="assets/935a00_f3462b2b97724b6194ec9375062306eb_mv2.gif" /></p>
<p>So at any level, we know where are hash goes in relation to the sibling.</p>
<p>Hence, there are two pieces of information we need:</p>
<ul>
<li>the index of the node we are inserting</li>
<li>whether the current index is even or odd</li>
</ul>
<p>Here is a screenshot of Tornado Cash’s source code using that information. The for loop loops over the levels to regenerate the Merkle root based on the new leaf that was just added.</p>
<p><img alt="screenshot of Tornado Cash _insert() function" src="assets/935a00_a1e81eadb6474efd9473cae6228bafdd_mv2.png" /></p>
<p>In summary, to update the Merkle Root on-chain, we</p>
<ul>
<li>add a leaf at a new index, setting that to <code style='font-family: Arial'>currentIndex</code></li>
<li>move up a level and set <code style='font-family: Arial'>currentIndex</code> to be <code style='font-family: Arial'>currentIndex</code> divided by <code style='font-family: Arial'>two</code>. Then,</li>
<li>hash left with a <code style='font-family: Arial'>filledSubtree</code> if the <code style='font-family: Arial'>currentIndex</code> is <code style='font-family: Arial'>odd</code></li>
<li>hash right with a <code style='font-family: Arial'>precomputed</code> zero-tree if the <code style='font-family: Arial'>currentIndex</code> is <code style='font-family: Arial'>even</code>.</li>
</ul>
<p>It’s quite cool that such a non-trivial algorithm can be compressed to a small Solidity representation.</p>
<h3>Tornado Cash stores the last 30 roots, since the root keeps changing with each deposit</h3>
<p>Whenever an item is inserted, the Merkle root must necessarily change. This could create a problem if a withdrawer creates a Merkle proof for the latest root (remember, the leaves are all publicly available) but a deposit transaction comes in first and changes the root; then the Merkle proof would no longer be valid. The zk verifier algorithm ensures the proof of the Merkle proof is valid for the root, so if the root changes, the proof won’t check out.</p>
<p>To give the withdrawer time to withdraw their transaction, they can refer to up to the last 30 roots.</p>
<p>The variable roots is a mapping from <code style='font-family: Arial'>uint256</code> to <code style='font-family: Arial'>bytes32</code>. When the Merkle proof has reached the root (the loop completes) it is stored in roots. The <code style='font-family: Arial'>currentRootIndex</code> is increased up to the <code style='font-family: Arial'>ROOT_HISTORY_SIZE</code>, but once it reaches the maximum value (30) it overwrites the root in index zero. Thus, it behaves like a fixed size queue. Below is a snippet from the <code style='font-family: Arial'>_insert</code> function of Tornado Cash’s Merkle Tree code. After the root is recomputed, it is stored in the manner described above.</p>
<p><img alt="merkle tree with history lookback" src="assets/935a00_cfacd20521564251b174d35fa15b04dd_mv2.png" /></p>
<h3>Storage variables for the Incremental Merkle Tree</h3>
<p>Here are the storage variables needed for the Merkle Tree with history to work.</p>
<pre style='font-family: Arial'><code class="language-solidity">mapping(uint256 =&gt; bytes32) public filledSubtrees;
mapping(uint256 =&gt; bytes32) public roots;
uint32 public constant ROOT_HISTORY_SIZE = 30;
uint32 public currentRootIndex = 0;
uint32 public nextIndex = 0;
</code></pre>
<ul>
<li><code style='font-family: Arial'>filledSubtrees</code> is the subtrees we have already computed (i.e. all the leaves are non-zero)</li>
<li>roots is the last 30 roots</li>
<li><code style='font-family: Arial'>currentRootIndex</code> is a number from 0 to 29 to index into roots</li>
<li><code style='font-family: Arial'>nextIndex</code> is the current leaf that will be filled if the user calls Deposit.</li>
</ul>
<h3>How the public deposit() function updates the Incremental Merkle Tree</h3>
<p>When a user calls <a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/Tornado.sol#L55">deposit</a> to tornado cash, <code style='font-family: Arial'>_insert()</code> is called to update the Merkle Tree, and <code style='font-family: Arial'>_processDeposit()</code> is called afterwards.</p>
<pre style='font-family: Arial'><code class="language-solidity">function deposit(bytes32 _commitment) external payable nonReentrant {
    require(!commitments[_commitment], &quot;The commitment has been submitted&quot;);

    uint32 insertedIndex = _insert(_commitment);
    commitments[_commitment] = true;
    _processDeposit();

    emit Deposit(_commitment, insertedIndex, block.timestamp);
}
</code></pre>
<p><code style='font-family: Arial'>_processDeposit()</code> just makes sure the denomination is accurate (you can only deposit 0.1, 1, or 10 Ether depending on which Tornado Cash instance you interact with). The code for that very simple operation is below.</p>
<pre style='font-family: Arial'><code class="language-solidity">function _processDeposit() internal override {
    require(msg.value == denomination, &quot;Please send `mixDenomination` ETH along with transaction&quot;);
}
</code></pre>
<h2>Hyperoptimized MiMC Hash</h2>
<p>To compute the Merkle root on-chain, one must use a hash algorithm (obviously) but Tornado cash isn’t using the traditional <code style='font-family: Arial'>keccak256</code>; it uses MiMC instead.</p>
<p>Why this is so is a bit out of scope, but the reason is that some hashes are more computationally cheap for zero knowledge proof generation than others. MiMC was designed to be “zk friendly” but <code style='font-family: Arial'>keccak256</code> was not.<br />
“Zk friendly” means the algorithm maps naturally to how zero knowledge proof algorithms represents computations.</p>
<p>But this creates a funny conundrum that the MiMC must be computed on-chain to recompute the root when a new node is added, and Ethereum does not have precompiled contracts for zk-friendly hashes. (Maybe you could author an EIP for this?)</p>
<p>Therefore, the Tornado Cash team wrote it themselves in raw bytecode. If you look at the Etherscan contract verification for Tornado Cash, you will see a warning:</p>
<p><img alt="etherscan screenshot contains unverified code" src="assets/935a00_28335d1524064fa6affdc8efc409ef2b_mv2.png" /></p>
<p>Etherscan cannot verify raw bytecode to Solidity, since the MiMC hash was not written in Solidity.</p>
<p>The Tornado Cash team deployed the MiMC Hasher as a separate <a href="https://etherscan.io/address/0x83584f83f26af4edda9cbe8c730bc87c364b28fe">smart contract</a>. To use the MiMC hash, the Merkle Tree Code makes a cross contract call to that contract. As you can see in the code below, these are <a href="https://rareskills.io/post/solidity-staticcall">static calls</a>, since the interface defines it as pure, hence Etherscan shows it as having no transaction history.</p>
<pre style='font-family: Arial'><code class="language-solidity">interface IHasher {
    function MiMCSponge(uint256 in_xL, uint256 in_xR) external pure returns (uint256 xL, uint256 xR);
}
</code></pre>
<p>We know it’s “interface” based on the code in Tornado cash referenced above. (<a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/MerkleTreeWithHistory.sol#L15">github link</a>).</p>
<p>On a circom library github <a href="https://github.com/iden3/circomlib/issues/32">issue</a>, you can see the justification for why the code does not have a solidity version, even with assembly blocks: direct stack manipulation isn’t possible.</p>
<p>(Sidenote: very low level cryptography algorithms are a fantastic usecase for the Huff Language, which you can learn with our <a href="https://github.com/RareSkills/huff-puzzles">Huff Language Puzzles</a>).</p>
<h3>Deploying your own hash function as raw bytecode</h3>
<p>The circomlib js repository contains javascript tooling for creating raw bytecode hashes. Here is the code for generating <a href="https://github.com/iden3/circomlibjs/blob/main/src/mimcsponge_gencontract.js">MiMC</a> and the <a href="https://github.com/iden3/circomlibjs/blob/main/src/poseidon_gencontract.js">Poseidon Hash</a>.</p>
<h2>Withdrawing from Tornado Cash</h2>
<p>To start, the user must reconstruct the Merkle Tree locally using the <a href="https://github.com/tornadocash/tornado-classic-ui/blob/master/scripts/updateTree.js">updateTree script</a>. This script will download all the relevant <a href="https://rareskills.io/post/ethereum-events">solidity events</a> and reconstruct the Merkle Tree. Then, the user will generate a zero knowledge proof of the Merkle proof and leaf commitment preimages. As discussed earlier, Tornado Cash stores the last 30 Merkle roots, so this should be plenty of time for the user to submit their proof. If a user generates a proof and waits too long, they will have to regenerate the proof.</p>
<p>The tornado cash contract will check</p>
<ol>
<li>The submitted <code style='font-family: Arial'>nullifierHash</code> hasn’t been used before</li>
<li>The root is in the root history (last 30 roots)</li>
<li>The zero knowledge proof checks out:<br />
   a. The concealed hash preimage generates the leaf<br />
   b. The user actually knows the <code style='font-family: Arial'>nullifierHash</code> preimage<br />
   c. The user created a Merkle proof using that leaf which results in the proposed root.<br />
   d. The proposed root is one of the last 30 roots (this is checked publicly in the Solidity code)</li>
</ol>
<p>Here is a visualization of the above steps:</p>
<p><img alt="tornado cash withdraw workflow diagram" src="assets/935a00_e392067a72614272907a69b012da4898_mv2.png" /></p>
<p>With that understanding, the code from Tornado Cash’s withdraw function below should be self explanatory.</p>
<pre style='font-family: Arial'><code class="language-solidity">function withdraw(bytes calldata _proof,
    bytes32 _root,
    bytes32 _nullifierHash,
    address payable _recipient,
    address payable _relayer,
    uint256 _fee,
    uint256 _refund
  ) external payable nonReentrant {
    require(_fee &lt;= denomination, &quot;Fee exceeds transfer value&quot;);
    require(!nullifierHashes[_nullifierHash], &quot;The note has been already spent&quot;);
    require(isKnownRoot(_root), &quot;Cannot find your merkle root&quot;); // Make sure to use a recent onerequire(
      verifier.verifyProof(
        _proof,
        [uint256(_root), uint256(_nullifierHash), uint256(_recipient), uint256(_relayer), _fee, _refund]
      ),
      &quot;Invalid withdraw proof&quot;
    );

    nullifierHashes[_nullifierHash] = true;
    _processWithdraw(_recipient, _relayer, _fee, _refund);
    emit Withdrawal(_recipient, _nullifierHash, _relayer, _fee);
  }
</code></pre>
<p>The <code style='font-family: Arial'>_relayer</code>, <code style='font-family: Arial'>_fee</code>, and <code style='font-family: Arial'>_refund</code> above has to do with paying fees to optional transaction relayers, which we will explain shortly.</p>
<h3>The function isKnownRoot(root) validates the proposed root is one of the last 30</h3>
<p>This is a simple do-while loop that loops backwards from the current index (the last active leaf) to see if the root submitted to the withdraw function is in the history of roots. (<a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/MerkleTreeWithHistory.sol#LL96C1-L115C4">github link</a>)</p>
<p>Because there is a lookback of only 30, we don’t have to worry about unbounded loops using up too much gas.</p>
<pre style='font-family: Arial'><code class="language-solidity">/**
@dev Whether the root is present in the root history
*/function isKnownRoot(bytes32 _root) public view returns (bool) {
    if (_root == 0) {
      return false;
    }
    uint32 _currentRootIndex = currentRootIndex;
    uint32 i = _currentRootIndex;
    do {
      if (_root == roots[i]) {
        return true;
      }
      if (i == 0) {
        i = ROOT_HISTORY_SIZE; // 30
      }
      i--;
    } while (i != _currentRootIndex);
    return false;
}
</code></pre>
<h3>Circom code to define the zero knowledge proof computation</h3>
<p>This section describes the circom code used to generate the circuits that verify the ZK proof. If you are unfamiliar with Circom and want to learn it, please check out our <a href="https://github.com/RareSkills/zero-knowledge-puzzles">zero knowledge puzzles</a> in Circom.</p>
<p>Nevertheless, we will still try to explain the Circom code at a high level.</p>
<p>Circom is not executed on the blockchain, it is transpiled into terrifying Solidity code which you can see in Tornado Cash’s <a href="https://github.com/tornadocash/tornado-core/blob/master/contracts/Verifier.sol">Verifier.sol</a> . The reason it looks so scary is that it is actually executing the math for the zk-proof verification. Thankfully, Circom is a lot more readable.</p>
<p><img alt="tornado cash verifier circom" src="assets/935a00_41e1a664adda4ff6948cc479b3219ce9_mv2.png" /></p>
<p>Here we have three components: <code style='font-family: Arial'>HashLeftRight</code> which combines hashes, <code style='font-family: Arial'>DualMux</code> (which is just a utility for <code style='font-family: Arial'>MerkleTreeChecker</code>) and <code style='font-family: Arial'>MerkleTreeChecker</code>. The <code style='font-family: Arial'>MerkleTreeChecker</code> takes a <code style='font-family: Arial'>leaf</code>, a <code style='font-family: Arial'>root</code>, and a <code style='font-family: Arial'>proof</code>. The <code style='font-family: Arial'>proof</code> has two parts: the <code style='font-family: Arial'>pathElements</code> (the Merkle root of the sister subtree) and <code style='font-family: Arial'>pathIndices</code> (and indicator for the circuit to know which order to concatenate the hashes for that level).</p>
<p>The final line, <code style='font-family: Arial'>root === hashers[levels - 1].hash</code> is where the root is finally determined to match the <code style='font-family: Arial'>leaf</code> and <code style='font-family: Arial'>proof</code>.</p>
<p>Recall that the <code style='font-family: Arial'>nullifierHash</code> is the hash of the <code style='font-family: Arial'>nullifier</code> and the <code style='font-family: Arial'>commitment</code> is the hash of the <code style='font-family: Arial'>nullifier</code> and the <code style='font-family: Arial'>secret</code>. The Circom representation of this calculation is below. Although it may be hard to read, it should be clear that the inputs are the <code style='font-family: Arial'>nullifier</code> and <code style='font-family: Arial'>secret</code>, and the outputs are the <code style='font-family: Arial'>commitment</code> and the <code style='font-family: Arial'>nullifierHash</code>.</p>
<p><img alt="tornado cash commitment hasher circom" src="assets/935a00_6d3f3a1eaf144be69cbb8e58852cd576_mv2.png" /></p>
<p>Now we can get to the core of the zero knowledge algorithm</p>
<p><img alt="Tornado Cash withdraw zero knowledge circuit" src="assets/935a00_cdec5c6971c8401c8fc5172766c4d42b_mv2.png" /></p>
<p>A private signal means it is “hidden in the proof” using the nomenclature from earlier.</p>
<p>In the code above, the code verifies the <code style='font-family: Arial'>nullifier</code> hash computation was conducted in a valid manner. Then the commitment preimage and Merkle proof are provided to the MerkleTree Verifier from earlier.</p>
<p>If everything checks out, the <code style='font-family: Arial'>verifier</code> will return true, denoting the proof is valid, and the anonymous address can withdraw the deposit.</p>
<h3>Preventing frontrunning during withdrawal</h3>
<p>Security researchers may have noticed the Solidity code has no defense against frontrunning. When someone submits a valid zero knowledge proof to the mempool, what prevents someone from copying the proof and substituting the withdraw address to be their own?</p>
<p>This is something we glossed over for simplicity, but the <code style='font-family: Arial'>Withdraw.circom</code> file includes dummy signals that square the recipient (and the other parameters needed for relayers). This means the zk-proof must also demonstrate that the withdrawer squared the recipient’s address and got the square of their address (remember, addresses are just 20 byte numbers). Computing the square of the addresses and the hash of the <code style='font-family: Arial'>nullifier</code> and <code style='font-family: Arial'>secret</code> is one computation, so getting any parts of this wrong invalidates the whole proof.</p>
<p><img alt="Tornado cash withdraw circom" src="assets/935a00_68d3199ac59a4773becba63c5d3ce79d_mv2.png" /></p>
<h3>What is a relayer and fee?</h3>
<p>A relayer is an offchain bot that pays the gas for other users in exchange for some kind of payment. Tornado Cash withdrawers generally want to use completely new addresses to enhance privacy, but completely new addresses don’t have any gas to pay for the withdrawal.</p>
<p>To solve this, a withdrawer can ask a relayer to conduct the transaction in exchange for receiving a portion of the Tornado Cash withdrawal.</p>
<p>The relayer’s withdraw address must also use the same frontrunning protection described above, and that can be seen in the code screenshot above.</p>
<h2>Summary of a deposit() and withdrawal()</h2>
<p>When deposit is called:</p>
<ul>
<li>The user submits <code style='font-family: Arial'>hash(concat(nullifier, secret))</code>, along with the cryptocurrency they are depositing.</li>
<li>Tornado cash validates the amount deposited is the denomination it accepts.</li>
<li>Tornado cash adds the commitment to the next leaf. Leaves are never removed.</li>
</ul>
<p>When withdraw is called:</p>
<ul>
<li>The user reconstructs the Merkle Tree based on the events emitted by Tornado Cash</li>
<li>The user must supply the hash of the <code style='font-family: Arial'>nullifier</code> (publicly), the Merkle root they are verifying against, and a zk proof that they know the <code style='font-family: Arial'>nullifier</code>, <code style='font-family: Arial'>secret</code>, and Merkle proof</li>
<li>Tornado cash verifies the <code style='font-family: Arial'>nullifier</code> hasn’t been used before</li>
<li>Tornado cash verifies the proposed root is one of the last 30 roots</li>
<li>Tornado cash verifies the zero knowledge proof</li>
</ul>
<p>There is nothing stopping a user from submitting a nonsense leaf with no known preimage. In that case, the submitted cryptocurrency will remain stuck in the contract forever.</p>
<h2>A quick overview of the smart contract architecture</h2>
<p>The following are the smart contracts that make up Tornado Cash</p>
<p><img alt="tornado cash github source code screenshot" src="assets/935a00_fabb9fadd6c846cb8f0cb75fef6a4da5_mv2.png" /></p>
<p><code style='font-family: Arial'>Tornado.sol</code> is an abstract contract that is actually implemented by <code style='font-family: Arial'>ERC20Tornado.sol</code> or <code style='font-family: Arial'>ETHTornado.sol</code> depending on if the deployment is meant to mix ERC20s or a certain denomination of ETH. Different ETH denominations and ERC20 tokens have their own tornado cash instance.</p>
<p><code style='font-family: Arial'>MerkleTreeWithHistory.sol</code> contains the <code style='font-family: Arial'>_insert()</code> and <code style='font-family: Arial'>isKnownRoot()</code> functionality we discussed at length earlier.</p>
<p><code style='font-family: Arial'>Verifier.sol</code> is the Solidity transpilation output of the Circom circuits.</p>
<p><code style='font-family: Arial'>cTornado.sol</code> is the ERC20 token for governance, and isn’t part of the core protocol.</p>
<h2>Where Tornado Cash could improve gas efficiency</h2>
<p>Tornado Cash overall is very well architected, but there are a few opportunities for <a href="https://rareskills.io/post/gas-optimization">gas optimization</a>.</p>
<ul>
<li>Tornado Cash does a linear lookup for the precomputed Merkle subtrees with all zero leaves, but this could be done in fewer operations with a hard-coded binary search</li>
<li>Tornado Cash often uses uint32 for stack variables; uint256 would be a better choice to avoid the implicit casting the EVM does.</li>
<li>Tornado Cash has some constants which are unnecessarily modified to be public. Public constants aren’t necessary unless a smart contract is going to read them, and they increase the size of the smart contract.</li>
<li>Prefix operators (++i) are more gas efficient than postfix operators (i++), and Tornado Cash could change this operation without affecting the logic.</li>
<li><code style='font-family: Arial'>nullifierHashes</code> is a public mapping, but it also wrapped with a public view function <code style='font-family: Arial'>isSpent()</code>. This is redundant.</li>
</ul>
<h2>Conclusion</h2>
<p>And there you have it; we have surveyed the <em>entire</em> codebase of Tornado Cash and developed a good understanding of what each variable and function does. Tornado Cash packs an impressive amount of complexity into a relatively small codebase. We’ve looked at several nontrivial techniques here including:</p>
<ul>
<li>Using zero knowledge proofs to demonstrate knowledge of a hash preimage without giving away the preimage</li>
<li>How to use an incremental Merkle Tree on-chain</li>
<li>How to use a custom hash function coded from raw bytecode</li>
<li>How nullifier schemes work</li>
<li>How to withdraw anonymously</li>
<li>How to prevent zero knowledge proofs dapps from being frontrun</li>
</ul>
<h2>RareSkills</h2>
<p>This material is part of our <a href="https://rareskills.io/zk-bootcamp">Zero Knowledge Course</a>. For general smart contract development, see our <a href="https://rareskills.io/solidity-bootcamp">Solidity Bootcamp</a> for professional Solidity developers. We offer the most rigorous and up-to-date training program for smart contracts available anywhere.</p>
<h2>Clarifying Notes</h2>
<p>[1] Anyone familiar with ZK circuits knows we cannot create for loops of arbitrary length. The circuit must be built to accommodate very large arrays when it is created. However, it is still accurate to say that “looping” over that many hashes is computationally expensive, as it is equivalent to creating an infeasibly massive circuit.</p>
<p><em>Originally Published June 27, 2023</em></p>
<div style='page-break-after: always;'></div>

<h1>Introduction to ZK Bulletproofs</h1>
<p>Source: https://rareskills.io/post/bulletproofs-zk</p>
<h1>Introduction to ZK Bulletproofs</h1>
<p>Bulletproofs are a zero knowledge inner product argument, which enable a prover to convince a verifier that they correctly computed an inner product. That is, the prover has two vectors $\mathbf{a} = [a_1, a_2, \dots, a_n]$ and $\mathbf{b} = [b_1, b_2, \dots, b_n]$ and they computed $v = \langle\mathbf{a},\mathbf{b}\rangle=a_1b_1+a_2b_2 + \dots +a_nb_n$. The prover can optionally hide or reveal the vectors or the inner product result, but still convince the verifier they did the calculation honestly.</p>
<p>The verifier doesn’t receive vectors $\mathbf{a}$, $\mathbf{b}$, and scalar $v$ but rather <em>commitments</em> to these values. Very roughly, one could think of the verifier receiving a “hash” $h$ where $h = \mathsf{hash}([a_1, a_2, \dots, a_n],[b_1, b_2, \dots, b_n],v)$, then receiving a proof (which we call $\pi$) that the hash actually contains two vectors and their inner product. In other words, the verifier receives $(h, \pi)$ and is convinced that the prover carried out an inner product operation correctly on the hashed values — but doesn’t learn anything about the “contents” of the hash.</p>
<p>Over the course of executing the verification portion of the Bulletproof, the verifier will reconstruct the hash and thus be convinced the prover evaluated the inner product as claimed.</p>
<p>Of course, it is not possible to reconstruct a traditional hash without knowing the input, so Bulletproofs use a special kind of hash called a “Pedersen Commitment,” which is the subject of the first chapter in this tutorial. Pedersen Commitments are a special kind of “hash” based on elliptic curves; Pedersen Commitments can be reconstructed without knowing the original input.</p>
<p><em>Some engineers recognize the operation of combining vectors in the manner of $a_1b_1+a_2b_2 + \dots +a_nb_n$ as a “dot product.” Technically, a dot product refers to the operation on vectors in a Cartesian plane, but the inner product refers to the operation for arbitrary vectors. Therefore, we will refer to this operation as the inner product. We use bold letters such as $\mathbf{a}$ to denote a vector, lower case letters such as $v$ to denote a <a href="https://rareskills.io/post/finite-fields">finite field</a> element (roughly, a “scalar” in modular arithmetic), and angled brackets $\langle\mathbf{a},\mathbf{b}\rangle$ to denote an inner product of two vectors, which always results in a finite field element.</em></p>
<p>To prove zero knowledge in general, the prover must show that they know an assignment to an <a href="https://rareskills.io/post/arithmetic-circuit">arithmetic circuit</a> (a witness) that satisfies the circuit’s constraints. However, SNARKs, particularly <a href="https://rareskills.io/post/groth16">Groth16</a> do not accept arbitrary circuits — the circuits must be in a particular format, the <a href="https://rareskills.io/post/rank-1-constraint-system">Rank One Constraint system</a>. From there, the R1CS is converted to a <a href="https://rareskills.io/post/quadratic-arithmetic-program">Quadratic Arithmetic Program</a> (QAP) using <a href="https://rareskills.io/post/python-lagrange-interpolation">Lagrange interpolation</a> and polynomial division.</p>
<p>The additional overhead of interpolating polynomials and polynomial division increases the work significantly for the prover.</p>
<p>Bulletproofs on the other hand, allow us to create a proof for the R1CS directly, without a QAP. Consider that the following matrix operation on the left and inner product computations on the right are equivalent:</p>
<p>$$<br />
\begin{bmatrix}<br />
l_{11}, l_{12}, l_{13}\<br />
l_{21}, l_{22}, l_{23}\<br />
l_{31}, l_{32}, l_{33}\<br />
l_{41}, l_{42}, l_{43}\<br />
\end{bmatrix}<br />
\begin{bmatrix}<br />
a_1\a_2\a_3<br />
\end{bmatrix}=<br />
\begin{matrix}<br />
\langle [l_{11}, l_{12}, l_{13}],[a_1,a_2,a_3]\rangle\<br />
\langle [l_{21}, l_{22}, l_{23}],[a_1,a_2,a_3]\rangle\<br />
\langle [l_{31}, l_{32}, l_{33}],[a_1,a_2,a_3]\rangle\<br />
\langle [l_{41}, l_{42}, l_{43}],[a_1,a_2,a_3]\rangle\<br />
\end{matrix}<br />
$$</p>
<p>In other words, multiplying an $n \times m$ matrix by an $m$ dimensional vector is the same as computing $n$ inner products. So if we can directly prove an inner product was computed correctly, then we don’t need the additional steps of creating a Quadratic Arithmetic Program.</p>
<p>Furthermore, Bulletproofs do not use <a href="https://rareskills.io/post/bilinear-pairing">pairings</a> (only basic <a href="https://rareskills.io/post/elliptic-curve-addition">elliptic curve addition</a>) and do not require a <a href="https://rareskills.io/post/trusted-setup">trusted setup</a>.</p>
<h2>Drawbacks of Bulletproofs</h2>
<p>The size of the proof is logarithmic in the number of multiplications, unlike the ZK-SNARK proof which is constant size.</p>
<p>The primary drawback of Bulletproofs is that the runtime of the verifier is linear in the size of the circuit. This is because the work that would have been accomplished by the trusted setup now has to be done by the verifier.</p>
<h2>ZK Without Arithmetic Circuits</h2>
<p>One major advantage of inner products is that they can model some problems “directly” — i.e. — they don’t need an arithmetic circuit.</p>
<p>For example, proving that a number $v$ is less than $2^n$ can be done by showing that $v$ has a binary representation of $\mathbf{b}$, and the inner product of $\mathbf{b}$ and the vector $[1,2,4,8,…,2^{n-1}]$ is $v$. This directly implies that $v &lt; 2^n$. For example, if the vector of powers of 2 is $[1,2,4,8,16,32,64,128]$, then we know that $v$ must be less than 256, for the same reason that a <code style='font-family: Arial'>uint8</code> cannot hold values larger than 255. But since $\mathbf{b}$ is hidden, we don’t know the actual value of $v$. This is called a <em>range proof</em> because we know $v$ is in the range $[0,255]$ without knowing the actual value.</p>
<p>If we were instead to create an arithmetic circuit for the range proof, this would introduce substantial computational overhead.</p>
<p><em>For readers familiar with NP-Completeness, the Subset Sum problem can also be modeled directly with an inner product argument. Any problem in NP can be reduced to a Subset Sum instance and the solution can be proven with an inner product argument. In some cases, that reduction may be more efficient than an arithmetic circuit.</em></p>
<h2>Bulletproofs in Practice</h2>
<p>The privarcy blockchain Monero uses the range proof described above to ensure that transactions do not have negative values in the input (i.e. an overflow in the finite field). ZCash uses Bulletproofs as a replacement for the SNARK polynomial commitment using a PLONKish circuit.</p>
<p>The linear runtime of Bulletproofs make them unsuitable for use in smart contracts on Ethereum mainnet. However, for protocols that need a fast proof generation and verification of a small problem — such as a range proof, Bulletproofs are hard to beat.</p>
<h2>The RareSkills ZK Book on Bulletproofs</h2>
<p>Our walkthrough of Bulletproofs is based on the original <a href="https://eprint.iacr.org/2017/1066.pdf">Bulletproofs paper</a>. The paper is very well organized, but it is extremely dense as it is targeted at professional cryptography researchers and assumes considerable background knowledge. Our collection of Bulletproof tutorials is largely a translation of the paper to a version senior web3 developers can understand. We create entire tutorials for the prerequisites the paper implicitly assumes the reader has.</p>
<p>As usual, we strive to provide an intuitive mental model of the algorithm, and not simply recite the steps the algorithm takes. Where suited, we include mathematical animations to make the explanation more efficient. At all costs, we avoid oversimplification so that you have a full intuition of what each step of the algorithm accomplishes.</p>
<h3>Reading this work</h3>
<p>We assume the reader has read and understood the first nine chapters of our <a href="https://rareskills.io/zk-book">ZK Book</a>. Familiarity with Groth16 or other ZK algorithms is not expected.</p>
<p>We include accompanying “fill in the blank” Python coding exercises so that you can practice what you learn.</p>
<p><strong>For the reader with the right prerequisites, it is possible, with a modicum of discipline, to code the Bulletproofs algorithm from scratch while setting aside not more than three hours per day for two weeks.</strong> This treatment of Bulletproofs provides a framework for how to do so without excessive hand-holding.</p>
<p>Bulletproofs are in a sense, “simpler” than SNARKs, so they are a great way to build confidence in understanding the field of ZK.</p>
<h2>Table of Contents</h2>
<p>Chapter 2 introduces the Pedersen Commitment, which is the foundational building block of Bulletproofs. Chapters 3-5 show how to accomplish an inner product proof with zero knowledge, but not succinctness (the proof size is $\mathcal{O}(n)$ where $n$ is the size of the vectors). Chapters 6-7 show how to prove knowledge of an inner product without ZK, but with a proof size logarithmic in $n$. Chapter 8 shows the core Bulletproofs algorithm. Chapter 9 and 10 are prerequisites for chapter 11 where we show how to construct a range proof without the use of an arithmetic circuit.</p>
<ol>
<li>Introduction to Bulletproofs (this chapter)</li>
<li><a href="https://rareskills.io/post/pedersen-commitment">Pedersen Commitments</a> Pedersen Commitments are what we called the “hash” at the beginning of this article. They are more composable than traditional hash functions, however, as they are additively homomorphic. That is, we can commit 2 to $A$ and 5 to $B$ and “reveal” 7 to $A + B$.</li>
<li><a href="https://rareskills.io/post/pedersen-polynomial-commitment">Polynomial Commitments via Pedersen Commitments</a> By creating Pedersen commitments of the coefficients of a polynomial, we can prove we 1) committed to a polynomial and 2) evaluated it correctly without revealing the original polynomial.</li>
<li><a href="https://rareskills.io/post/zk-multiplication">Zero Knowledge Multiplication</a> We can verify that two polynomials were multiplied together correctly by 1) committing them, 2) evaluating them, and 3) checking that the evaluations of the first two multiply to the third. By having a scheme for multiplying polynomials together (with zero knowledge), we also get scalar multiplication for free.</li>
<li><a href="https://rareskills.io/post/inner-product-argument">Inner Product Arguments</a> Now that we have the mechanism to do zero knowledge proofs for multiplication, it is only a small change to support zero knowledge proofs for the inner product. Specifically, we change the scalar coefficient polynomials to “vector polynomials” and do vector commitments instead of scalar commitments for the coefficients. We also define an operation of “vector polynomial inner product” that enables us to accomplish the inner product argument. While zero knowledge, this argument is not yet succinct.</li>
<li><a href="https://rareskills.io/post/outer-product-inner-product">Succinct Inner Product Arguments</a> Normally, to prove you correctly computed an inner product with two vectors of length $n$, you would need to send $2n$ elements (i.e. both of the vectors). This chapter shows a clever trick for only sending $n$ elements.</li>
<li><a href="https://rareskills.io/post/log-n-vector-commitment-proof">Logarithmic-Sized Proofs of Knowledge for Inner Products</a> This chapter uses the algorithm described in chapter 6 recursively to prove we correctly computed an inner product of two vectors of size $n$ with a proof size of only $\mathcal{O}(n)$ data.</li>
<li><a href="https://rareskills.io/post/bulletproofs-zkp">Bulletproof ZKP: the algorithm end-to-end</a> We combine the zero knowledge inner product argument in chapter 5 with the succint inner product in chapter 7 to produce the Bulletproof. At this point, you have sufficient knowledge to code the algorithm yourself by combining the previous exercises.</li>
<li><a href="https://rareskills.io/post/inner-product-algebra">Inner Product Algebra</a> This chapter introduces and proves various algebraic identities for adding inner products together, which will be useful when learning the Bulletproof range proof.</li>
<li><a href="https://rareskills.io/post/random-linear-combination">Random Linear Combinations</a> Instead of creating two proofs (or more) for two (or more) inner products, we can create one proof for multiple inner products by using the random linear combination trick.</li>
<li><a href="https://rareskills.io/post/range-proof">Range Proofs</a> A range proof is proof that a committed value lies in a certain range. Bulletproofs can accomplish this directly, without the need to arithmetize the problem. Bulletproofs range proofs prove that a number can be encoded as the inner product of a vector of powers of 2 $[1,2,4,…,2^{n-1}]$ and a binary vector, but they don’t reveal which bits are one or zero.</li>
</ol>
<p>We recommend that you fork <a href="https://github.com/RareSkills/ZK-bulletproofs">this Bulletproofs ZK repo</a> and do the exercises as you read, so that you can immediately practice what you learn.</p>
<h3>Acknowledgements</h3>
<p>The excellent <a href="https://doc-internal.dalek.rs/bulletproofs/">documentation of the Rust Bulletproofs crate</a> by Henry de Valence, Cathie Yun, and Oleg Andreev provided helpful pointers where the paper was unclear and we sometimes use their notation, which in some cases is more intuitive than the notation in the original paper. The reader may find that resource useful as an alternative angle on Bulletproofs.</p>
<div style='page-break-after: always;'></div>

<h1>What are Pedersen Commitments and How They Work</h1>
<p>Source: https://rareskills.io/post/pedersen-commitment</p>
<h1>What are Pedersen Commitments and How They Work</h1>
<p>Pedersen commitments allow us to encode arbitrarily large vectors with a single elliptic curve point, while optionally hiding any information about the vector.</p>
<p>It allows us to make claims about a vector without revealing the vector itself.</p>
<h2>Motivation</h2>
<p>When we discuss Bulletproof Zero Knowledge Proofs, they will generally be of the form “I have two vectors whose inner product is $v$.” This might seem basic, but you can actually use this mechanism to prove very non-trivial claims. We’ll get to that later.</p>
<p>But for such a proof to work, the vectors can’t just exist in the prover’s head – otherwise the prover can change them at will. They have to be mathematical entities in the real world. Generally, the prover does not want to just pass the two vectors to the verifier, but they still need to “pass something” to the verifier to represent that they’ve selected a pair of vectors and cannot change it.</p>
<p>This is where Pedersen commitments enter the picture.</p>
<p>In an inner product argument, the prover provides two <em>commitments</em> to two vectors, then provides a proof that the committed vectors have a certain inner product.</p>
<h2>Prerequisites</h2>
<p>We assume the reader is already familiar with <a href="https://rareskills.io/post/elliptic-curve-addition">elliptic curve point addition</a> and scalar multiplication and what it means for a point to be “on the curve.”</p>
<p>Notation-wise, capital letters are elliptic curve points, lowercase letters are finite field elements.</p>
<p>We say $A$ is an elliptic curve (EC) point, $a$ is a <a href="https://rareskills.io/post/finite-fields">finite field</a> element, and $aA$ is point multiplication between finite field element $a$ and EC point $A$. The expression $A + B$ denotes elliptic curve point addition.</p>
<h2>Traditional commitments</h2>
<p>When we design commit reveal functions in smart contracts, they are usually of the form</p>
<p>$$<br />
\text{commitment} = \mathsf{hash}(\text{value}, \text{salt})<br />
$$</p>
<p>where the $\text{salt}$ is a random value to prevent an attacker from brute-force guessing $\text{value}$.</p>
<p>For example, if we were committing a vote, there are only a limited number of choices, and thus the vote selection can be guessed by trying all the votes seeing which hash matches.</p>
<p>The academic terminology for the <em>salt</em> variable in the case of Pedersen commitments is the <em>blinding factor</em>. Because it is random, the attacker is “blinded” from being able to guess the value committed.</p>
<p>Because the value “commitment” cannot be guessed by an adversary, we say this commitment scheme is <em>hiding</em>.</p>
<p>During the reveal phase, the committer reveals the value and the salt, so the other party (or smart contract) can validate that it matches the original commitment. It isn’t possible to obtain another $(\text{value}, \text{salt})$ pair that can result in the same commitment, so we say this scheme is binding – the committer cannot change (i.e. is bound to) their committed value after the fact.</p>
<p>A $(\text{value}, \text{salt})$ pair that results in the hash is called the <em>opening</em>. To say that someone “knows an opening to the commitment” means they know (value, salt). To <em>reveal</em> $(\text{value}, \text{salt})$ means to <em>open</em> the commitment.</p>
<p>When discussing Pedersen commitments, there is a distinction between <em>knowing</em> the opening and <em>opening</em> the commitment. We usually want to prove we <em>know</em> the opening, but not necessarily <em>open</em> it.</p>
<h2>Terminology summary</h2>
<ul>
<li>A <strong>hiding</strong> commitment does not allow an adversary to know what value was selected by the commiter. This is usually accomplished by including a random term that the attacker cannot guess.</li>
<li>A <strong>blinding</strong> term is the random number that makes the commitment impossible to guess.</li>
<li>An <strong>opening</strong> is the values that will compute to the commitment.</li>
<li>A <strong>binding</strong> commitment does not allow the committer to compute a hash with different values. That is, they cannot find two (value, salt) pairs that hash to the same value.</li>
</ul>
<h2>Pedersen Commitments</h2>
<p>Pedersen commitments behave very similar to the commit-reveal scheme described earlier, except that they use elliptic curve groups instead of cryptographic hash functions.</p>
<p>Under the discrete logarithm assumption, given elliptic curve points $V$ and $U$, we cannot compute $x$ where $V$ = $xU$. That is to say, we do not know their <em>discrete log relationship</em>, i.e. how many times $U$ needs to be added to itself to get $V$.</p>
<p>We still refer to $x$ as the discrete logarithm of $U$ even though we cannot compute it, because we know it exists. All (cryptographic) elliptic curve points have a discrete logarithm, even if they cannot be computed.</p>
<p>In this sense, elliptic curve point multiplication behaves like a hash function. They are binding as long as we only allow openings within the curve order.</p>
<p>However, if the range of discrete logarithms is small and bound by the context of the application (such as vote choices), then the discrete logarithm might become guessable.</p>
<p>We can make a Pedersen commitment hiding in the following manner:</p>
<p>$$<br />
\text{commitment} = vG + sB<br />
$$</p>
<p>where $v$ is the value we are committing and $s$ is the salt (or blinding factor) and $B$ is another elliptic curve point such that the committer does not know the discrete logarithm relationship between $B$ and $G$.</p>
<p>We should emphasize that although the discrete logarithms are unknown, the points $G$ and $B$ are public and known to both the verifier and the committer.</p>
<h3>Why the committer must not know the discrete logarithm relationship between $B$ and $G$</h3>
<p>Suppose the committer knows $b$ such that $B = bG$.</p>
<p>In that case, they can open the commitment</p>
<p>$$<br />
\text{commitment} = vG + sB<br />
$$</p>
<p>to a different $(v’, s’)$ other than the value they originally committed.</p>
<p>Here’s how the committer could cheat if they know that $b$ is the discrete logarithm of $B$.<br />
$$<br />
B = bG<br />
$$</p>
<p>The committer can rewrite the commitment equation:<br />
$$<br />
\begin{align*}<br />
\text{commitment} &amp;= vG + sB \<br />
&amp;= vG + s(bG) \ \text{(substituting B = bG)} \<br />
&amp;= (v + sb)G<br />
\end{align*}<br />
$$</p>
<p>The committer picks a new value $v’$ and computes $s’$:</p>
<p>$$<br />
\begin{align*}<br />
v’ + s’b = v + sb \<br />
s’ = \frac{v + sb – v’}{b}<br />
\end{align*}<br />
$$</p>
<p>Then, the prover presents $(v’, s’)$ as the forged opening.</p>
<p>This works because<br />
$$<br />
\begin{align*}<br />
\text{commitment} &amp;= v’G + \frac{v + sb – v’}{b} B \<br />
\text{commitment} &amp;= v’G + (v + sb – v’)G \<br />
\text{commitment} &amp;= \cancel{v’G} + vG + s(bG) – \cancel{v’G} \<br />
\text{commitment} &amp;= vG + sB \<br />
\text{commitment} &amp;= \text{commitment} \<br />
\end{align*}<br />
$$<br />
<strong>Therefore, the committer must not know the discrete logarithm relationship between the elliptic curve points they are using.</strong></p>
<p>One way to accomplish this is to have a verifier supply the elliptic curve points for the committer. A simpler way, however, is to pick the elliptic curve points in a random and transparent way, such as by pseudorandomly selecting elliptic curve points. Given a random elliptic curve point, we do not know its discrete logarithm.</p>
<p>For example, we could start with the generator point, hash the $x$ and $y$ values, then use that to seed a pseudorandom but deterministic search for the next point.</p>
<h2>Why are Pedersen Commitments useful?</h2>
<p>It seems like Pedersen Commitments are just a normal commit-reveal with a different hash-function, so what’s the point?</p>
<p>This scheme has a couple advantages.</p>
<h3>Pedersen commitments are additively homomorphic</h3>
<p>Given a point $G$, we can add two commitments together $a_1G + a_2G$ = $(a_1 + a_2)G$.</p>
<p>If we include random blinding terms, we can still do a valid opening by adding the blinding terms together and providing that to the verifier. Let $C_1$ and $C_2$ be commitments. Now consider what happens when we add $C_1 + C_2$ together:</p>
<p>$$<br />
\begin{aligned}<br />
C_1 &amp;= a_1G + s_1B \<br />
C_2 &amp;= a_2G + s_2B \<br />
C_3 &amp;= C_1 + C_2 \<br />
\pi &amp;= s_1 + s_2 \<br />
\text{committer reveals…} \<br />
&amp;(a_1, a_2, \pi) \<br />
\text{and the verifier checks…} \<br />
C_3 &amp;\stackrel{?}{=} a_1G + a_2G + \pi B<br />
\end{aligned}<br />
$$</p>
<p>Alternatively, the verifier can check that</p>
<p>$$C_3 = (a_1 + a_2)G + \pi B$$</p>
<p>Regular hashes (such as SHA-256) cannot be added together in this manner.</p>
<p>Given two Pedersen commitments that use the same elliptic curve points to commit to, we can add the commitments together and still have a valid opening for them.</p>
<p>Pedersen commitments allow a prover to make claims about the sums of committed values.</p>
<h3>We can encode as many points as we like in a single point</h3>
<p>Our example of using $G$ and $B$ can also be thought of as a 2D vector commitment without a blinding term. But we can add as many elliptic curve points as we like $[G₁, G₂, …, Gₙ]$ and commit as many scalars as we like. (Here, $G_1$, $G_2$, etc mean different points in the same group, not generators of different groups).</p>
<h2>Pedersen Vector Commitments</h2>
<p>We can take the above scheme a step further and commit a set of values rather than a value and a blinding term.</p>
<h3>Vector commitment scheme</h3>
<p>Suppose we have a set of random elliptic curve points $(G₁,…,Gₙ)$ (that we do not know the discrete logarithm of), and we do the following:</p>
<p>$$C = \underbrace{v_1G_1 + v_2G_2 + … + v_nG_n}_\text{committed vector} + \underbrace{sB}_\text{blinding term}$$</p>
<p>This lets us commit $n$ values to $C$ and hide it with $s$.</p>
<p>Since the committer does not know the discrete logarithm of any $G_i$, they don’t know the discrete logarithm of $C$. Hence, this scheme is binding: they can only reveal $(v₁,…,vₙ)$ to produce $C$ later, they cannot produce another vector.</p>
<h3>Vector commitments can be combined</h3>
<p>We can add two Pedersen Vector Commitments to get one commitment to two vectors. This will still only allow the committer to open to the original vectors. The important implementation detail is that we have to use a different set of elliptic curve points to commit against.</p>
<p>$$<br />
\begin{align*}<br />
C_1 &amp;= v_1 G_1 + v_2 G_2 + \ldots + v_n G_n + r B \<br />
C_2 &amp;= w_1 H_1 + w_2 H_2 + \ldots + w_n H_n + s B \<br />
C_3 &amp;= C_1 + C_2<br />
\end{align*}<br />
$$</p>
<p>By adding $C_1$ and $C_2$ together, we are functionally committing one larger vector of size $2n$.</p>
<p>Here, $rB$ and $sB$ are the blinding terms. Even if the committer commits the zero vector, the commitment will still appear to be a random point.</p>
<p>The committer will later reveal the original vectors $(v₁…vₙ)$ and $(w₁…wₙ)$ and the blinding term $r + s$. This is binding: they cannot reveal another pair of vectors and blinding terms.</p>
<p>The fact that we are using $(G₁,…,Gₙ)$ for one vector and $(H₁,…,Hₙ)$ should not imply that there is a special relationship among the $G$ points and a special relationship among the $H$ points. All the points need to be selected pseudorandomly. This is merely notational convenience for saying “this vector of elliptic curve points goes with this vector of field elements, and this other vector of EC points goes with this other vector of field elements.”</p>
<p>There is no practical upper limit to the number of vectors we can commit.</p>
<p><strong>Exercise for the reader:</strong> If we used the same $G₁…Gₙ$ for both vectors before adding them, how could a committer open two different vectors for $C_3$? Give an example. How does using a different set of points $H₁…Hₙ$ prevent this?</p>
<p><strong>Exercise for the reader:</strong> What happens if the committer tries to switch the same elements inside the vector?</p>
<p>For example, they commit:</p>
<p>$$C_1 = v_1G_1 + v_2G_2 + \ldots + v_nG_n + rB$$</p>
<p>But open with the first two elements swapped:</p>
<p>$$[v_2, v_1, v_3, …, v_n]$$</p>
<p>That is, they switch the first two elements leaving everything else unchanged. Assume that the vector $G₁…Gₙ$ is unpermuted.</p>
<h2>Generating random points transparently</h2>
<p>How can we generate these random elliptic curve points? One obvious solution is to use a trusted setup, but this isn’t necessary. The committer is able to set up the points in a way they cannot know their discrete logarithm by randomly selecting the points in a transparent way.</p>
<p>They can pick the generator point, mix in a publicly chosen random number, and hash that result (and take it modulo the field modulus) to obtain another value. If that results in an $x$-value that lies on the elliptic curve, use that as the next generator and hash the $(x, y)$ pair again. Otherwise, if the $x$-value does not land on the curve, increment $x$ until it does. Because the committer is not generating the points, they don’t know their discrete log.</p>
<p><strong>Exercise:</strong> Adjust the following code to generate <code style='font-family: Arial'>n</code> points with unknown discrete logs:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import is_on_curve, FQ
from py_ecc.fields import field_properties
field_mod = field_properties[&quot;bn128&quot;][&quot;field_modulus&quot;]
from hashlib import sha256
from libnum import has_sqrtmod_prime_power, sqrtmod_prime_power

b = 3 # for bn128, y^2 = x^3 + 3
seed = &quot;RareSkills&quot;

x = int(sha256(seed.encode('ascii')).hexdigest(), 16) % field_mod 

entropy = 0

vector_basis = []
# modify the code below to generate n points
while not has_sqrtmod_prime_power((x**3 + b) % field_mod, field_mod, 1):
    # increment x, so hopefully we are on the curve
    x = (x + 1) % field_mod
    entropy = entropy + 1

# pick the upper or lower point depending on if entropy is even or odd
y = list(sqrtmod_prime_power((x**3 + b) % field_mod, field_mod, 1))[entropy &amp; 1 == 0]
point = (FQ(x), FQ(y))
assert is_on_curve(point, b), &quot;sanity check&quot;
vector_basis.append(point)

# new x value
x = int(sha256(str(x).encode('ascii')).hexdigest(), 16) % field_mod 
print(vector_basis)
</code></pre>
<p>At no point should you generate a point by picking a scalar and then multiplying it with the generator, as that would lead to the discrete logarithm being known. You need to select the $x$ values of the curve point pseudorandomly via a hash function and figure out if it is on the curve.</p>
<p>It is okay to start with the generator (which has a known discrete logarithm of 1) and generate the other points.</p>
<p><strong>Exercise for the reader:</strong> Suppose we commit a vector of values to the point points $G_1$ and $G_2$. The discrete logarithm for $G_1$ is known, but the discrete logarithm for $G_2$ is not known. We will ignore the blinding term for now. Can the committer open to two different vectors? Why or why not?</p>
<p>Learn More with RareSkills<br />
Check out our ZK bootcamp if you are looking to <a href="https://rareskills.io/zk-bootcamp">learn zero knowledge proofs</a>.</p>
<div style='page-break-after: always;'></div>

<h1>Polynomial Commitments Via Pedersen Commitments</h1>
<p>Source: https://rareskills.io/post/pedersen-polynomial-commitment</p>
<h1>Polynomial Commitments Via Pedersen Commitments</h1>
<p>A polynomial commitment is a mechanism by which a prover can convince a verifier a polynomial $p(x)$ has an evaluation $y = p(x)$ at point $x$ without revealing anything about $p(x)$. The sequence is as follows:</p>
<ol>
<li>The prover sends to the verifier a commitment $C$, “locking in” their polynomial.</li>
<li>The verifier responds with a value $u$ they want the polynomial evaluated at.</li>
<li>The prover responds with $y$ and $\pi$, where $y$ is the evaluation of $p(u)$ and $\pi$ is proof that the evaluation was correct.</li>
<li>The verifier checks $C$, $u$, $y$, $\pi$ and accepts or rejects that the evaluation of the polynomial was valid.</li>
</ol>
<p>This commitment scheme does not require a trusted setup. However, the communication overhead is $O(n)$ as the prover must send a commitment for each coefficient in their polynomial.</p>
<h2>Committing to the Polynomial</h2>
<p>The prover can commit to the polynomial by creating a <a href="https://rareskills.io/post/pedersen-commitment">Pedersen Commitment</a> of each coefficient. For a Pedersen Committment, the prover and verifier need to agree on two elliptic curve points with unknown discrete logs. We will use $G$ and $B$.</p>
<p>For example, if we have a polynomial</p>
<p>$$p(x) = c_0+c_1x+c_2x^2$$</p>
<p>we can create a Pedersen commitment for each coefficient. We will need three blinding terms $\gamma_0$, $\gamma_1$, $\gamma_2$. For convenience, any scalar used for blinding will use a lower-case Greek letter. We always use the elliptic curve point $B$ for the blinding term. Our commitments are produced as follows:</p>
<p>$$<br />
\begin{align*}<br />
C_0=c_0G+\gamma_0B \<br />
C_1=c_1G+\gamma_1B \<br />
C_2=c_2G+\gamma_2B \<br />
\end{align*}<br />
$$</p>
<p>The prover sends the tuple $(C_0, C_1, C_2)$ to the verifier.</p>
<h2>Verifier chooses $u$</h2>
<p>The verifier chooses their value for $x$ and sends that to the prover. We call that value $u$.</p>
<h2>Prover computes the proof</h2>
<h3>Prover evaluates the polynomial</h3>
<p>The prover computes the original polynomial as:</p>
<p>$$<br />
y = p(u) = c_0 + c_1u + c_2u^2<br />
$$</p>
<h3>Prover evaluates the blinding terms</h3>
<p>The proof that the evaluation was done correctly is given by the following polynomial, which uses the blinding terms multiplied by the associated power of $u$. The reason for this will be explained later.</p>
<p>$$<br />
\pi = \gamma_0 + \gamma_1u+\gamma_2u^2<br />
$$</p>
<p>The prover sends $(y, \pi)$ to the verifier. Note that the prover is only sending field elements (scalars) not elliptic curve points.</p>
<h2>Verification step</h2>
<p>The verifier runs the following check:</p>
<p>$$<br />
C_0+C_1u+C_2u^2\stackrel{?}{=}yG+\pi B<br />
$$</p>
<h2>Why the verification step works</h2>
<p>If we expand the elliptic curve points to their underlying values, we see the equation is balanced:</p>
<p>$$<br />
\begin{align*}<br />
C_0 + C_1u + C_2u^2 &amp;= yG + \pi B \<br />
(c_0G + \gamma_0B) + (c_1G + \gamma_1B)u + (c_2G + \gamma_2B)u^2 &amp;= (c_0 + c_1u + c_2u^2)G + (\gamma_0 + \gamma_1u + \gamma_2u^2)B \<br />
c_0G + \gamma_0B + c_1Gu + \gamma_1Bu + c_2Gu^2 + \gamma_2Bu^2 &amp;= (c_0 + c_1u + c_2u^2)G + (\gamma_0 + \gamma_1u + \gamma_2u^2)B \<br />
c_0G + c_1Gu + c_2Gu^2 + \gamma_0B + \gamma_1Bu + \gamma_2Bu^2 &amp;= (c_0 + c_1u + c_2u^2)G + (\gamma_0 + \gamma_1u + \gamma_2u^2)B \<br />
(c_0 + c_1u + c_2u^2)G + (\gamma_0 + \gamma_1u + \gamma_2u^2)B &amp;= (c_0 + c_1u + c_2u^2)G + (\gamma_0 + \gamma_1u + \gamma_2u^2)B \<br />
\end{align*}<br />
$$</p>
<p>In a sense, the prover is evaluating the polynomial using the polynomial’s coefficients and their choice of $u$. This will produce the evaluation of the original polynomial plus the blinding terms of the polynomial.</p>
<p>The proof of correct evaluation is that the prover can separate the blinding terms from the evaluation of the polynomial — even though the prover does not know the discrete logs of $yG$ and $\pi B$.</p>
<h3>An alternative illustration for why the verification works</h3>
<p>Recall that $p(x) = c_0 + c_1x + c_2x^2$. Therefore, the commitments to the coefficients are computed as follows:</p>
<p>$$<br />
\begin{matrix}<br />
&amp;\space\space\space c_0G&amp;\space\space\space c_1G&amp;\space\space\space c_2G\<br />
&amp;+\gamma_0B&amp;+\gamma_1B&amp;+\gamma_2B\<br />
&amp;\vcenter{\hbox{|}} \vcenter{\hbox{|}}&amp;\vcenter{\hbox{|}} \vcenter{\hbox{|}}&amp;\vcenter{\hbox{|}} \vcenter{\hbox{|}}\<br />
&amp;C_0&amp;C_1&amp;C_2<br />
\end{matrix}<br />
$$</p>
<p>When the verifier sends $\color{red}{u}$, the prover computes:</p>
<p>$$<br />
\begin{matrix}<br />
y=&amp; c_0&amp; +c_1\color{red}{u}&amp; +c_2\color{red}{u^2}\<br />
\pi=&amp;\gamma_0&amp;+\gamma_1\color{red}{u}&amp;+\gamma_2\color{red}{u^2}\<br />
\end{matrix}<br />
$$</p>
<p>In the final step, the verifier checks:</p>
<p>$$yG + \pi B\stackrel{?}=C_0 + C_1 {\color{red} u} + C_2 {\color{red} u^2}$$</p>
<p>If we expand the terms vertically, we see the equation is balanced if the prover was honest:</p>
<p>$$<br />
\begin{matrix}<br />
yG&amp;=&amp; c_0G&amp; c_1G\color{red}{u}&amp; c_2G\color{red}{u^2}\<br />
\pi B&amp;=&amp;\gamma_0B&amp;\gamma_1B\color{red}{u}&amp;\gamma_2B\color{red}{u^2}\<br />
\vcenter{\hbox{|}} \vcenter{\hbox{|}}&amp;&amp;\vcenter{\hbox{|}} \vcenter{\hbox{|}}&amp;\vcenter{\hbox{|}} \vcenter{\hbox{|}}&amp;\vcenter{\hbox{|}} \vcenter{\hbox{|}}\<br />
yG+\pi B&amp;\stackrel{?}=&amp;C_0&amp;+C_1\color{red}{u}&amp;+C_2\color{red}{u^2}<br />
\end{matrix}<br />
$$</p>
<p><strong>It is very important that you firmly grasp how this technique of proving correct evaluation of a polynomial, given blinding commitments to the coefficients, works because <a href="https://rareskills.io/post/bulletproofs-zk">Bulletproofs</a> use this technique everywhere.</strong></p>
<p><strong>Exercise:</strong> Write out the steps for how a prover would convince a verifier they correctly evaluated a degree 1 polynomial, without revealing the polynomial to the verifier.</p>
<p><strong>Exercise:</strong> Fill in the Python code below to implement the algorithm described in this chapter:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, add, FQ
from py_ecc.bn128 import curve_order as p
import random

def random_field_element():
    return random.randint(0, p)

# these EC points have unknown discrete logs:
G = (FQ(6286155310766333871795042970372566906087502116590250812133967451320632869759), FQ(2167390362195738854837661032213065766665495464946848931705307210578191331138))

B = (FQ(12848606535045587128788889317230751518392478691112375569775390095112330602489), FQ(18818936887558347291494629972517132071247847502517774285883500818572856935411))

# scalar multiplication example: multiply(G, 42)
# EC addition example: add(multiply(G, 42), multiply(G, 100))

# remember to do all arithmetic modulo p

def commit(f_0, f_1, f_2, gamma_0, gamma_1, gamma_2, G, B):
    # fill this in
    # return the commitments as a tuple (C0, C1, C2)
    pass

def evaluate(f_0, f_1, f_2, u):
    return (f_0 + f_1 * u + f_2 * u**2) % p

def prove(gamma_0, gamma_1, gamma_2, u):
    # fill this in
    # return pi
    pass

def verify(C0, C1, C2, G, B, f_u, pi):
    # fill this in
    # Return true or false
    pass

## step 0: Prover and verifier agree on G and B

## step 1: Prover creates the commitments
### f(x) = f_0 + f_1x + f_2x^2
f_0 = ...
f_1 = ...
f_2 = ...

### blinding terms
gamma_0 = ...
gamma_1 = ...
gamma_2 = ...
C0, C1, C2 = commit(f_0, f_1, f_2, gamma_0, gamma_1, gamma_2, G, B)

## step 2: Verifier picks u
u = ...

## step 3: Prover evaluates f(u) and pi

f_u = evaluate(f_0, f_1, f_2, u)
pi = prove(gamma_0, gamma_1, gamma_2, u)

## step 4: Verifier accepts or rejects
if verify(C0, C1, C2, G, B, f_u, pi):
    print(&quot;accept&quot;)
else:
    print(&quot;reject&quot;)
</code></pre>
<h2>Why the prover cannot cheat</h2>
<p>Cheating on the prover’s part means they don’t honestly evaluate $y = p(u)$ but still try to pass the final evaluation step.</p>
<p>Without loss of generality, let’s say the prover sends the correct commitments for the coefficients $C_0, C_1, C_2$.</p>
<p>We say without loss of generality because there is a mismatch between the coefficients sent in the commitments and the coefficients used to evaluate the polynomial.</p>
<p>To do so, the prover sends $y’$ where $y’ \neq c_0 + c_1u + c_2u^2$.</p>
<p>Using the final equation from the previous section, we see that the prover must satisfy:</p>
<p>$$<br />
(c_0 + c_1u + c_2u^2)G+(\gamma_0 + \gamma_1u+\gamma_2u^2)B=y’G+(\gamma_0 + \gamma_1u+\gamma_2u^2)B<br />
$$</p>
<p>The $G$ terms of the equation are clearly unbalanced. The other “lever” the prover can pull is adjusting the $\pi$ that they send.</p>
<p>$$<br />
(c_0 + c_1u + c_2u^2)G+(\gamma_0 + \gamma_1u+\gamma_2u^2)B=y’G + \boxed{\pi’}B<br />
$$</p>
<p>Since $y’ \neq c_0 + c_1u + c_2u^2$ the malicious prover must rebalance the equation by picking a term $\pi’$ that accounts for the mismatch in the $G$ terms. The prover can try to solve for $\pi’$ with</p>
<p>$$<br />
\pi’B = (c_0 + c_1u + c_2u^2)G+(\gamma_0 + \gamma_1u+\gamma_2u^2)B – y’G<br />
$$</p>
<p>But solving this equation requires the malicious prover to know the discrete logs of $G$ and $B$.</p>
<p>Let’s solve for $\pi’$ in the equation above:</p>
<p>$$<br />
\pi’ = \frac{(c_0 + c_1u + c_2u^2)G+(\gamma_0 + \gamma_1u+\gamma_2u^2)B – y’G}{B}<br />
$$</p>
<p>and then replace $B$ with $b$ and $G$ with $g$, where $b$ and $g$ are the discrete logs of $B$ and $G$ respectively:</p>
<p>$$\pi’ = \frac{(c_0 + c_1u + c_2u^2)g+(\gamma_0 + \gamma_1u+\gamma_2u^2)b – y’g}{b}$$</p>
<p>But again, this is not possible because computing the discrete log of $B$ and $G$ is infeasible.</p>
<h2>What the verifier learns</h2>
<p>The verifier learns that the commitments $C_0, C_1, C_2$ represent valid commitments to a polynomial that is at most degree 2, and that $y$ is the value of the polynomial evaluated at $u$.</p>
<div style='page-break-after: always;'></div>

<h1>Zero Knowledge Multiplication</h1>
<p>Source: https://rareskills.io/post/zk-multiplication</p>
<h1>Zero Knowledge Multiplication</h1>
<h2>Zero Knowledge Multiplication of Polynomials</h2>
<p>Using the polynomial commitment scheme from the previous chapter, a prover can show that they have three polynomials $l(x)$, $r(x)$, and $t(x)$ and prove that $t(x) = l(x)r(x)$.</p>
<p>For this algorithm to work, the verifier must believe that the polynomial evaluations are correct — but this is something we showed in the previous chapter. Most of the steps here are simply repeating the polynomial commitment algorithm we did previously.</p>
<p>At a high level, the prover commits to $l(x)$, $r(x)$, and $t(x)$ and sends the commitments to the verifier. Then, the verifier chooses a random value for $x$ as $u$ and asks the prover to evaluate the polynomials at $u$. The verifier then checks that the evaluations were done correctly and that the evaluation for $l(x)$ multiplied by the evaluation for $r(x)$ equals the evaluation for $t(x)$.</p>
<p>For example, suppose that the first polynomial is $l(x)=2x$ and the second is $r(x) = x + 1$. Then $t(x)=2x(x+1) = 2x^2+2$. The verifier can sample any random $x$ value, and the result of the product $l(x)r(x)$ will be $t(x)$. The plot below shows an example of the verifier choosing $x=2$:</p>
<p><img alt="random-polynomial-multiplication" src="assets/polynomial-multiplication.png" /></p>
<p>The verifier would then check that $3 \times 4 = 12$ and accept the prover’s claim.</p>
<p>The <a href="https://rareskills.io/post/schwartz-zippel-lemma">Schwartz-Zippel lemma</a> states that if $f(x) \neq g(x)$ then the probability that $f(u) = g(u)$ for some random value $u$ is less than $d/p$ where $d$ is the maximum degree of the two polynomials and $p$ is the order of the <a href="https://rareskills.io/post/finite-fields">finite field</a>. If $d \ll p$ ($d$ much much less than $p$), then the probability of $u$ being an intersection point of two non-equal polynomials is negligible.</p>
<p>Specifically, suppose the prover is lying and $l(x)r(x) \neq t(x)$. In that case, for a random $u$, $l(u)r(u) \neq t(u)$ with extremely high probability. If $l(x)r(x) \neq t(x)$, then $l(x)r(x)$ and $t(x)$ only intersect in at most $d$ points (the maximum degree of either $l(x)r(x)$ or $t(x)$), and it is extremely unlikely that the verifier would randomly pick a $u$ that is one of the $d$ intersection points.</p>
<p>To get a sense of scale, $d$ in our case is 2, but the curve order of our elliptic curves (and hence the order of the field) is about $2^{254}$. So if $t(x) \neq l(x)r(x)$, then the probability of $t(u) = l(u)r(u)$ is $1/2^{253}$ which is vanishingly small.</p>
<p>We now describe the algorithm in detail, and then show an optimization.</p>
<h2>Steps to prove knowledge of polynomial multiplication</h2>
<p>The prover commits two linear (degree 1) polynomials $l(x)$, $r(x)$, a quadratic (degree 2) polynomial $t(x)$, and sends the commitments to the verifier. The verifier responds with a random value $u$, and the prover evaluates $l_u = l(u)$, $r_u = r(u)$, and $t_u = t(u)$ along with the proofs of evaluation $\pi_l, \pi_r, \pi_t$. The verifier asserts that all the polynomials were evaluated properly and that $t_u = l_ur_u$.</p>
<h3>Setup</h3>
<p>The prover and verifier agree on elliptic curve points $G$ and $B$ with an unknown discrete log relationship (i.e. the points are chosen randomly).</p>
<h3>Prover commits to $l(x)$, $r(x)$, and $t(x)$</h3>
<p>The prover creates three polynomials:</p>
<p>$$<br />
\begin{align*}<br />
l(x) &amp;= a + s_Lx \<br />
r(x) &amp;= b + s_Rx \<br />
t(x) &amp;= l(x)r(x) = (a + s_Lx)(b + s_Rx) = ab+(as_R+bs_L)x+s_Ls_Rx^2\<br />
\end{align*}<br />
$$</p>
<p>So they need to produce a total of 7 Pedersen commitments for each of the coefficients, which will require seven blinding terms $\alpha_0, \alpha_1, \beta_0, \beta_1, \tau_0, \tau_1, \tau_2$</p>
<p>$$<br />
\begin{align*}<br />
L_0&amp;=aG + \alpha_0B &amp;&amp;\text{// constant coefficient of }l(x)\<br />
L_1&amp;=s_LG + \alpha_1B &amp;&amp;\text{// linear coefficient of }l(x)\<br />
\<br />
R_0&amp;=bG + \beta_0B &amp;&amp;\text{// constant coefficient of }r(x)\<br />
R_1&amp;=s_RG + \beta_1B &amp;&amp;\text{// linear coefficient of }r(x)\<br />
\<br />
T_0 &amp;= abG + \tau_0 B &amp;&amp;\text{// constant coefficient of }t(x)\<br />
T_1 &amp;=(as_R + bs_L)G + \tau_1B &amp;&amp;\text{// linear coefficient of }t(x)\<br />
T_2 &amp;= s_Ls_RG + \tau_2B &amp;&amp;\text{// quadratic coefficient of }t(x)<br />
\end{align*}<br />
$$</p>
<p>The prover sends $(L_0, L_1, R_0, R_1, T_0, T_1, T_2)$ to the verifier.</p>
<h3>Verifier generates random scalar $u$</h3>
<p>… and sends the field element $u$ to the prover.</p>
<h3>Prover evaluates the three polynomials and creates three proofs</h3>
<p>The prover plugs in $u$ to the polynomials and computes the sum of the blinding terms of the polynomial coefficient commitments when $u$ is applied.</p>
<p>$$<br />
\begin{align*}<br />
l_u &amp;= a + s_Lu\<br />
r_u &amp;= b + s_Ru\<br />
t_u &amp;= ab + (as_R + bs_l)u + s_Ls_Ru^2\<br />
\<br />
\pi_l &amp;= \alpha_0 + \alpha_1u \<br />
\pi_r &amp;= \beta_0 + \beta_1u \<br />
\pi_t &amp;= \tau_0 + \tau_1u + \tau_2u^2<br />
\end{align*}<br />
$$</p>
<p>The prover sends the values $(l_u, r_u, t_u, \pi_l, \pi_r, \pi_t)$ to the verifier. Note that these are all field elements, not elliptic curve points.</p>
<h3>Final verification step</h3>
<p>The verifier checks that each of the polynomials were evaluated correctly and that the evaluation of $t(u)$ is the product of the evaluation of $l(u)$ and $r(u)$. The first three checks are proofs that the polynomial was evaluated correctly with respect to the commitment to the coefficients, and the last check verifies that the output of the polynomials have the product relationship as claimed.</p>
<p>$$<br />
\begin{align*}<br />
l_uG + \pi_l B &amp;\stackrel{?}= L_0+L_1u &amp;&amp;\text{// Check that }l(u) \text{ was evaluated correctly}\<br />
r_uG + \pi_r B &amp;\stackrel{?}= R_0+R_1u &amp;&amp;\text{// Check that }r(u) \text{ was evaluated correctly}\<br />
t_uG + \pi_t B &amp;\stackrel{?}= T_0+T_1u+T_2u^2&amp;&amp;\text{// Check that }t(u) \text{ was evaluated correctly}\<br />
t_u &amp;\stackrel{?}= l_ur_u &amp;&amp;\text{// Check that }t(u)=l(u)r(u)<br />
\end{align*}<br />
$$</p>
<p>When we expand the terms, we see they balance if the prover was honest:<br />
$$<br />
\begin{align*}<br />
\underbrace{(a + s_Lu)}_{l_u}G + \underbrace{(\alpha_0 + \alpha_1u)}_{\pi_l} B &amp;\stackrel{?}= \underbrace{(aG + \alpha_0B)}_{L_0}+\underbrace{(s_LG + \alpha_1B)}_{L_1}u \<br />
\underbrace{(b + s_Ru)}_{r_u}G + \underbrace{(\beta_0 + \beta_1u)}_{\pi_r} B &amp;\stackrel{?}= \underbrace{(bG + \beta_0B)}_{R_0}+\underbrace{(s_RG + \beta_1B)}_{R_1}u \<br />
\underbrace{(ab + (as_R + bs_l)u + s_Ls_Ru^2)}_{t_u}G + \underbrace{(\tau_0 + \tau_1u + \tau_2u^2)}_{\pi_t} B &amp;\stackrel{?}= \underbrace{(abG + \tau_0 B)}_{T_0}+\underbrace{(as_R + bs_L+ \tau_1B)}_{T_1}u+\underbrace{(s_Ls_R+ \tau_2B)}_{T_2}u^2 \<br />
t_u &amp;\stackrel{?}= l_ur_u<br />
\end{align*}<br />
$$</p>
<h2>Optimization: sending fewer commitments</h2>
<p>In the first step, the prover sends 7 elliptic curve points, and in the final step, the verifier checks 4 equalities. We can improve the algorithm to only send 5 elliptic curve points and do 3 equality checks.</p>
<p>This is done by putting the constant coefficients of $l(x)$ and $r(x)$ into a single commitment and the linear coefficients of those polynomials into a separate commitment. By way of reminder, we defined $l(x)$ and $r(x)$ as</p>
<p>$$\begin{align*}<br />
l(x) &amp;= a + s_Lx \<br />
r(x) &amp;= b + s_Rx \<br />
\end{align*}$$</p>
<p>so $a$ and $b$ are the constant coefficients, and $s_L$ and $s_R$ are the linear coefficients.</p>
<p>This is similar to how we would commit a vector. We are in a sense committing the constant coefficients as a vector and the linear coefficients as another vector.</p>
<h3>Setup</h3>
<p>During the setup, we now need 3 elliptic curve points: $G$, $H$, and $B$.</p>
<h3>Polynomial commitment</h3>
<p>$$<br />
\begin{align*}<br />
A &amp;= aG + bH + \alpha B &amp;&amp;\text{// commit the constant terms}\<br />
S &amp;= s_LG + s_RH + \beta B &amp;&amp;\text{// commit the linear terms}\<br />
T_0 &amp;= abG + \tau_0 B &amp;&amp;\text{// commit the constant coefficient of } t(x) \<br />
T_1 &amp;=(as_R + bs_L)G + \tau_1B &amp;&amp;\text{// linear coefficient of }t(x)\<br />
T_2 &amp;= s_Ls_RG + \tau_2B &amp;&amp;\text{// quadratic coefficient of }t(x)<br />
\end{align*}<br />
$$</p>
<p>Note that the coefficients of $l(x)$ are applied to $G$ and the coefficients of $r(x)$ are applied to $H$. The prover sends $(A, S, T_0, T_1, T_2)$ to the verifier, who responds with $u$.</p>
<h3>Polynomial evaluation</h3>
<p>$$<br />
\begin{align*}<br />
l_u &amp;= l(u) = a + s_Lu\<br />
r_u &amp;= r(u) = b + s_Ru\<br />
t_u &amp;= t(u) = l(u)r(u) = ab + (as_R + bs_L)u + s_Ls_Ru^2\<br />
\pi_{lr} &amp;= \alpha + \beta u \<br />
\pi_t &amp;= \tau_0 + \tau_1u + \tau_2u^2<br />
\end{align*}<br />
$$</p>
<p>$l_u$, $r_u$, $t_u$, $\pi_{lr}$, and $\pi_t$ are computed as before, but the proof of evaluations for $l(x)$ and $r(x)$, which were formerly $\pi_l$ and $\pi_r$ are combined into a single one: $\pi_{lr}$.</p>
<h3>Final verification</h3>
<p>$$<br />
\begin{align*}<br />
A + Su &amp;\stackrel{?}= l_uG + r_uH+\pi_{lr}B \<br />
t_uG + \pi_tB &amp;\stackrel{?}= T_0 + T_1u + T_2u^2 \<br />
t_u &amp;\stackrel{?}= l_ur_u<br />
\end{align*}<br />
$$</p>
<p>The check $A + Su \stackrel{?}= l_uG + r_uH+\pi_{lr}B$ expands to</p>
<p>$$<br />
\underbrace{(aG + bH + \alpha B)}_A + \underbrace{(s_LG + s_RH + \beta B)u}_{Su} = \underbrace{(a + s_Lu)}_{l_u}G + \underbrace{(b + s_Ru)}_{r_u}H + \underbrace{(\alpha + \beta u)}_{\pi_{lr}}B<br />
$$</p>
<p>With some rearranging on the left-hand-side, we can see the equality check simultaneously checks that both $l(x)$ and $r(x)$ were evaluated correctly.</p>
<p>$$<br />
(a + s_Lu)G + (b + s_Ru)H + (\alpha + \beta u)B=\underbrace{(a + s_Lu)}_{l_u}G + \underbrace{(b + s_Ru)}_{r_u}H + \underbrace{(\alpha + \beta u)}_{\pi_{lr}}B<br />
$$</p>
<p>As another way of looking at the check $A + Su \stackrel{?}= l_uG + r_uH+\pi_{lr}B$, consider the following visualization:</p>
<p>$$<br />
\begin{align*}<br />
A &amp;= &amp;aG&amp; + &amp;bH &amp;+ &amp;\alpha B\<br />
Su &amp;= &amp;s_LuG&amp; + &amp;s_RuH &amp;+ &amp;\beta u B\<br />
&amp;&amp;\vcenter{\hbox{|}} \vcenter{\hbox{|}}&amp;&amp;\vcenter{\hbox{|}} \vcenter{\hbox{|}}&amp;&amp;\vcenter{\hbox{|}} \vcenter{\hbox{|}}\<br />
&amp;&amp;l(u)G&amp;&amp;r(u)H&amp;&amp;\pi_{lr}B<br />
\end{align*}<br />
$$</p>
<h2>Zero Knowledge Multiplication of Scalars</h2>
<p>Our proof that we multiplied two polynomials together correctly to obtain a third can be used to prove that we multiplied two <em>scalars</em> together to obtain a third. No changes to the algorithm are necessary, only a minor change in semantics (how we interpret the commitments).</p>
<p>Let’s say we want to prove that we carried out the multiplication $ab = v$.</p>
<h3>Problem statement</h3>
<p>$A$ is a commitment to $a$ and $b$, and $V$ is a commitment to $v$ where $v = ab$. We wish to prove that $A$ and $V$ are committed as claimed without revealing $a$, $b$, or $v$.</p>
<h3>Solution</h3>
<p>The high level idea is that a scalar can be turned into a polynomial by adding an arbitrarily chosen linear term, e.g. $a$ becomes $a + s_Lx$ and $b$ becomes $b + s_Rx$. $s_L$ and $s_R$ are chosen randomly by the prover.</p>
<p>When the polynomials $a + s_Lx$ and $b + s_Rx$ are multiplied together, the multiplication of $ab$ happens “inside” the polynomial multiplication.</p>
<p>$$(a + s_Lx)(b + s_Rx) = \boxed{ab} + (as_R + bs_L)x + s_Ls_rx^2$$</p>
<p>Recall the prover begins the algorithm by sending commitments:</p>
<p>$$<br />
\begin{align*}<br />
A &amp;= aG + bH + \alpha B &amp;&amp;\text{// commitment to }a\text{ and }b\<br />
S &amp;= s_LG + s_RH + \beta B &amp;&amp;\text{// commit the linear terms}\<br />
V &amp;= abG + \tau_0 B &amp;&amp;\text{// commit the product V} \<br />
T_1 &amp;=(as_R + bS_L)G + \tau_1B &amp;&amp;\text{// linear coefficient of }t(x)\<br />
T_2 &amp;= s_Ls_RG + \tau_2B &amp;&amp;\text{// quadratic coefficient of }t(x)<br />
\end{align*}<br />
$$</p>
<p>We simply change the “interpretation” of $A$ from being the constant terms of the polynomials to the constants $a$ and $b$ that we are multiplying. We change $T_0$ to $V$ to reflect the change of interpretation as a commitment to $V$ in the multiplication we are trying to prove we did correctly, i.e. $v = ab$.</p>
<p><strong>Exercise:</strong> Fill in the missing Python code to implement the algorithm described above.</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, add, FQ, eq
from py_ecc.bn128 import curve_order as p
import random

def random_element():
    return random.randint(0, p)

# these EC points have unknown discrete logs:
G = (FQ(6286155310766333871795042970372566906087502116590250812133967451320632869759), FQ(2167390362195738854837661032213065766665495464946848931705307210578191331138))

H = (FQ(13728162449721098615672844430261112538072166300311022796820929618959450231493), FQ(12153831869428634344429877091952509453770659237731690203490954547715195222919))

B = (FQ(12848606535045587128788889317230751518392478691112375569775390095112330602489), FQ(18818936887558347291494629972517132071247847502517774285883500818572856935411))

# utility function
def addd(A, B, C):
    return add(A, add(B, C))

# scalar multiplication example: multiply(G, 42)
# EC addition example: add(multiply(G, 42), multiply(G, 100))

# remember to do all arithmetic modulo p
def commit(a, sL, b, sR, alpha, beta, gamma, tau_1, tau_2):
    pass
    # return (A, S, V, T1, T2)


def evaluate(f_0, f_1, f_2, u):
    return (f_0 + f_1 * u + f_2 * u**2) % p

def prove(blinding_0, blinding_1, blinding_2, u):
    # fill this in
    # return pi
    pass

## step 0: Prover and verifier agree on G and B

## step 1: Prover creates the commitments
a = ...
b = ...
sL = ...
sR = ...
t1 = ...
t2 = ...

### blinding terms
alpha = ...
beta = ...
gamma = ...
tau_1 = ...
tau_2 = ...

A, S, V, T1, T2 = commit(a, sL, b, sR, alpha, beta, gamma, tau_1, tau_2)

## step 2: Verifier picks u
u = ...

## step 3: Prover evaluates l(u), r(u), t(u) and creates evaluation proofs
l_u = evaluate(a, sL, 0, u)
r_u = evaluate(b, sR, 0, u)
t_u = evaluate(a*b, t1, t2, u)

pi_lr = prove(alpha, beta, 0, u)
pi_t = prove(gamma, tau_1, tau_2, u)

## step 4: Verifier accepts or rejects
assert t_u == (l_u * r_u) % p, &quot;tu != lu*ru&quot;
assert eq(add(A, multiply(S, u)), addd(multiply(G, l_u), multiply(H, r_u), multiply(B, pi_lr))), &quot;l_u or r_u not evaluated correctly&quot;
assert eq(add(multiply(G, t_u), multiply(B, pi_t)), addd(V, multiply(T1, u), multiply(T2, u**2 % p))), &quot;t_u not evaluated correctly&quot;
</code></pre>
<h2>Learn more</h2>
<p>This article is part of a series on <a href="https://rareskills.io/post/bulletproofs-zk">Bulletproof ZKPs</a>.</p>
<div style='page-break-after: always;'></div>

<h1>A Zero Knowledge Proof for the Inner Product</h1>
<p>Source: https://rareskills.io/post/inner-product-argument</p>
<h1>A Zero Knowledge Proof for the Inner Product</h1>
<p>An inner product argument is a proof that the prover carried out the inner product computation correctly. This chapter shows how to construct a zero knowledge proof for an inner product argument.</p>
<p>In the previous chapter, we showed how to multiply two scalars together in a zero knowledge fashion: we commit to two degree-one polynomials, and prove that we correctly computed their product, and then show that the constant term of the degree one polynomials are the commitments to the secret factors we are multiplying.</p>
<p>If the coefficients of our polynomial are vectors instead of scalars, then we can prove that we computed the inner product of the vector correctly. We now introduce the <em>vector polynomial</em>.</p>
<h2>Vector polynomials: polynomials with vectors as coefficients</h2>
<p>The following are two polynomials with vector coefficients:</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{l}(x) &amp;= \begin{bmatrix} 1 \ 2 \end {bmatrix} x + \begin{bmatrix} 3 \ 4 \end{bmatrix} \<br />
\mathbf{r}(x) &amp;= \begin{bmatrix} 2 \ 3 \end{bmatrix} x +\begin{bmatrix} 7 \ 2 \end{bmatrix}<br />
\end{align*}<br />
$$</p>
<p>Evaluating a vector polynomial produces another vector. For example, $\mathbf{l}(2)$ produces</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{l}(2) &amp;= \begin{bmatrix} 1 \ 2 \end{bmatrix} (2) + \begin{bmatrix} 3 \ 4 \end{bmatrix}\<br />
&amp;= \begin{bmatrix} 2 \ 4 \end{bmatrix}+\begin{bmatrix} 3 \ 4 \end{bmatrix}\<br />
&amp;=\begin{bmatrix} 5 \ 8 \end{bmatrix}<br />
\end{align*}<br />
$$</p>
<p>and evaluating $\mathbf{r}$ at 2 returns:</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{r}(2) &amp;= \begin{bmatrix} 2 \ 3 \end{bmatrix} (2) + \begin{bmatrix} 7 \ 2 \end{bmatrix}\<br />
&amp;= \begin{bmatrix} 4 \ 6 \end{bmatrix} + \begin{bmatrix} 7 \ 2 \end{bmatrix}\<br />
&amp;= \begin{bmatrix} 11 \ 8 \end{bmatrix}<br />
\end{align*}<br />
$$</p>
<p>$\mathbf{l}(x)$ and $\mathbf{r}(x)$ are written as bold since they produce vectors when evaluated at some scalar $x$.</p>
<h2>Multiplying vector polynomials</h2>
<p>Vector polynomials can be multiplied together like scalar polynomials. For example, multiplying $\mathbf{l}(x)$ and $\mathbf{r}(x)$ yields:</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{l}(x)\mathbf{r}(x) &amp;=<br />
(\begin{bmatrix}<br />
1\2<br />
\end{bmatrix}x+<br />
\begin{bmatrix}<br />
3\4<br />
\end{bmatrix})(<br />
\begin{bmatrix}<br />
2 \ 3<br />
\end{bmatrix}x +<br />
\begin{bmatrix}<br />
7\2<br />
\end{bmatrix}<br />
)\<br />
&amp;=\begin{bmatrix}<br />
1\2<br />
\end{bmatrix}\circ<br />
\begin{bmatrix}<br />
2\3<br />
\end{bmatrix}x^2+<br />
\begin{bmatrix}<br />
1\2<br />
\end{bmatrix}\circ<br />
\begin{bmatrix}<br />
7\2<br />
\end{bmatrix}x+<br />
\begin{bmatrix}<br />
3\4<br />
\end{bmatrix}\circ<br />
\begin{bmatrix}<br />
2\3<br />
\end{bmatrix}x+<br />
\begin{bmatrix}<br />
3\4<br />
\end{bmatrix}\circ<br />
\begin{bmatrix}<br />
7\2<br />
\end{bmatrix}&amp;&amp;\<br />
&amp;=\begin{bmatrix}<br />
2\6<br />
\end{bmatrix}x^2+<br />
\begin{bmatrix}<br />
7\4<br />
\end{bmatrix}x+<br />
\begin{bmatrix}<br />
6\12<br />
\end{bmatrix}x+<br />
\begin{bmatrix}<br />
21\8<br />
\end{bmatrix}\<br />
&amp;=\begin{bmatrix}<br />
2\6<br />
\end{bmatrix}x^2+<br />
\begin{bmatrix}<br />
13\16<br />
\end{bmatrix}x+<br />
\begin{bmatrix}<br />
21\8<br />
\end{bmatrix}<br />
\end{align*}<br />
$$</p>
<p>When we multiply each of the vectors together, we take the Hadamard product (element-wise product, denoted with $\circ$).</p>
<p>Note that if we plug $x = 2$ into the resulting vector polynomial, we get the following:</p>
<p>$$<br />
\begin{bmatrix}<br />
2\6<br />
\end{bmatrix}(2)^2+<br />
\begin{bmatrix}<br />
13\16<br />
\end{bmatrix}(2)+<br />
\begin{bmatrix}<br />
21\8<br />
\end{bmatrix}=<br />
\begin{bmatrix}<br />
55\64<br />
\end{bmatrix}<br />
$$</p>
<p>This is the same as if we compute:</p>
<p>$$\mathbf{l}(2)\circ\mathbf{r}(2) = \begin{bmatrix}5\8\end{bmatrix}\circ\begin{bmatrix}11\8\end{bmatrix}=\begin{bmatrix}55\64\end{bmatrix}$$</p>
<p>In other words, multiplying two vector polynomials together and then evaluating the product at some point is the same as evaluating the vector polynomials separately and then taking the Hadamard product on the resulting vectors.</p>
<h2>Inner product of vector polynomials</h2>
<p>To compute the inner product of two vector polynomials, we multiply them together as described above, but then sum up the vector entries so the result becomes a scalar. We denote this operation as $\langle \mathbf{l}(x), \mathbf{r}(x) \rangle$. We can accomplish the same thing by using the inner product when we multiply vector coefficients instead of using the Hadamard product.</p>
<p>For the two example polynomials above, this would be:</p>
<p>$$\langle \mathbf{l}(x), \mathbf{r}(x) \rangle = \left\langle \begin{bmatrix} 1 \ 2 \end{bmatrix} x + \begin{bmatrix} 3 \ 4 \end{bmatrix}, \begin{bmatrix} 2 \ 3 \end{bmatrix} x + \begin{bmatrix} 7 \ 2 \end{bmatrix} \right\rangle<br />
$$</p>
<p>$$=\langle \begin{bmatrix} 1 \ 2 \end{bmatrix},\begin{bmatrix}2 \ 3 \end{bmatrix}\rangle x^2 + \langle \begin{bmatrix} 3 \ 4 \end{bmatrix},\begin{bmatrix}2 \ 3 \end{bmatrix}\rangle x + \langle \begin{bmatrix} 1 \ 2 \end{bmatrix},\begin{bmatrix}7 \ 2 \end{bmatrix}\rangle x+\langle \begin{bmatrix} 3 \ 4 \end{bmatrix},\begin{bmatrix}7 \ 2 \end{bmatrix}\rangle$$</p>
<p>$$=(1\cdot2+2\cdot3)x^2+(3\cdot2+4\cdot3)x + (1\cdot7+2\cdot2) x + (3\cdot7+4\cdot2)$$</p>
<p>$$=8x^2 + 18x + 11x + 29$$</p>
<p>$$=8x^2 + 29x + 29$$</p>
<p>Observe that $\langle\mathbf{l}(2), \mathbf{r}(2)\rangle$ is the same as $\langle \mathbf{l}(x), \mathbf{r}(x)\rangle$ evaluated at $x = 2$. That is, $\langle [5, 8], [11, 8]\rangle = 119$ and $8(2)^2 + 29(2) + 29 = 119$.</p>
<h3>Why this works in general</h3>
<p>Suppose we multiplied vector polynomials together the “normal” way — i.e. we take the elementwise-product (Hadamard product) of the coefficients instead of the inner product. The inner product of each of the coefficients is simply the sum of the terms of the Hadamard product of each of the coefficients.</p>
<p>Therefore, we can say that if we have two vector polynomials $\mathbf{l}(x)$ and $\mathbf{r}(x)$, and we multiply them together as $\mathbf{t}(x)=\mathbf{l}(x)\mathbf{r}(x)$ then the inner product of $\langle \mathbf{l}(x), \mathbf{r}(x) \rangle$ is equal to the element-wise sum of the coefficients of $\mathbf{t}$. Note that the multiplication of two vector polynomials results in a vector polynomial, but the inner product of two vector polynomials results in polynomial where all the coefficients are scalars.</p>
<h2>Zero knowledge inner product proof</h2>
<p>In the previous chapter on zero knowledge multiplication, we demonstrated that we have a valid multiplication by proving that the product of the constant terms of two linear polynomials equals the constant term in the product of the polynomials.</p>
<p>To prove correct computation of an inner product, we replace the polynomials with vector polynomials and we replace the multiplication of scalar polynomials with the inner product of vector polynomials.</p>
<p>Everything else remains the same.</p>
<h2>The algorithm</h2>
<p>The goal is for the prover to convince the verifier that $A$ is a commitment to $\mathbf{a}$ and $\mathbf{b}$, $V$ is a commitment to $v$, and that $\langle \mathbf{a}, \mathbf{b} \rangle = v$ without revealing $\mathbf{a}$, $\mathbf{b}$, or $v$.</p>
<h3>Setup</h3>
<p>The prover and verifier agree on:</p>
<ul>
<li><em>basis vectors</em> $\mathbf{G}$ and $\mathbf{H}$ with which the prover can commit the vectors</li>
<li>elliptic curve point $G$ (separate from $\mathbf{G}$) which will be used for committing the coefficients of $t(x)$ and the inner product committed to $V$</li>
<li>elliptic curve point $B$ for the blinding terms</li>
</ul>
<h3>Prover</h3>
<p>The prover generates the blinding terms $\alpha$, $\beta$, $\gamma$, $\tau_1$, and $\tau_2$ and computes:</p>
<p>$$<br />
\begin{align}<br />
A &amp;= \langle\mathbf{a},\mathbf{G}\rangle + \langle\mathbf{b},\mathbf{H}\rangle+\alpha B\<br />
S &amp;= \langle\mathbf{s}_L,\mathbf{G}\rangle + \langle\mathbf{s}_R,\mathbf{H}\rangle+\beta B\<br />
V &amp;= vG + \gamma B \<br />
T_1 &amp;= (\langle\mathbf{a},\mathbf{s}_R\rangle + \langle\mathbf{b},\mathbf{s}_L\rangle )G + \tau_1B\<br />
T_2 &amp;= \langle\mathbf{s}_L,\mathbf{s}_R\rangle G + \tau_2B<br />
\end{align}$$</p>
<p>Note that this time the linear coefficients $\mathbf{s}_L$ and $\mathbf{s}_R$ are vectors instead of scalars. The prover transmits $(A, S, V, T_1, T_2)$ to the verifier. After the verifier responds with a random $u$, the prover evaluates $\mathbf{l}(x)$, $\mathbf{r}(x)$, and their inner product $\mathbf{t}(x)$ at $u$.</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{l}_u &amp;= \mathbf{l}(u)=\mathbf{a} + \mathbf{s}_Lu \<br />
\mathbf{r}_u &amp;= \mathbf{r}(u)=\mathbf{b} + \mathbf{s}_Ru \<br />
t_u &amp;= v + (\langle\mathbf{a},\mathbf{s}_R\rangle + \langle\mathbf{b},\mathbf{s}_L\rangle)u + \langle\mathbf{s}_L,\mathbf{s}_R\rangle u^2\<br />
\pi_{lr} &amp;=\alpha+\beta u\<br />
\pi_t &amp;= \gamma + \tau_1u + \tau_2u^2\<br />
\end{align*}<br />
$$</p>
<h3>Final verification step</h3>
<p>First, the verifier checks that $t_u$ is the inner product of $\mathbf{l}_u$ and $\mathbf{r}_u$ evaluated at $u$.</p>
<p>$$t_u \stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{r}_u \rangle$$</p>
<p>This should hold if the prover is honest, because the inner product of the polynomials evaluated at $u$ is the same as the inner product of the vector polynomials $\mathbf{l}_x$ and $\mathbf{r}_x$ evaluated at $x = u$.</p>
<p>Second, the verifier checks that $A$ and $S$ are commitments to the constant and linear terms of $\mathbf{l}$ and $\mathbf{r}$ respectively.</p>
<p>$$A + Su \stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{G} \rangle + \langle \mathbf{r}_u, \mathbf{H} \rangle + \pi_{lr} B$$</p>
<p>Recall that $A$ and $S$ are commitments to the constant and linear terms of $\mathbf{l}$ and $\mathbf{r}$ and $\pi_{lr}$ is the sum of the blinding terms $\alpha$ and $\beta$ in $A$ and $S$ respectively.</p>
<p>Finally, the verifier checks that $t_u$ is the evaluation of the quadratic polynomial commited to $V, T_1, T_2$:</p>
<p>$$t_uG + \pi_tB \stackrel{?}{=} V + T_1 u + T_2 u^2$$</p>
<h2>Improving the proof size</h2>
<p>When the prover sends $(\mathbf{l}, \mathbf{r}, t, T_1, T_2, \pi)$, the prover sends over $2n$ elements (the length of $\mathbf{l}$ and $\mathbf{r}$) which is not succinct.</p>
<p>In the following chapters we will learn how to reduce the size of the proof. It is possible to create a proof of size $\log n$ that an inner product is correct. That is, if we wanted to prove we computed the inner product of two vectors of length $n$, then the proof would only be of size $\log n$ — exponentially smaller.</p>
<p>Specifically, we will optimize the step $t_u\stackrel{?}=\langle\mathbf{l}_u,\mathbf{r}_u\rangle$ and $A + Su \stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{G} \rangle + \langle \mathbf{r}_u, \mathbf{H} \rangle + \pi_{lr} B$ by sending a commitment to $\mathbf{l}_u$ and $\mathbf{r}_u$ instead of the actual vectors, along with a logarithmic size proof to show that the commitment holds the vectors whose inner product is $t_u$.</p>
<h2>Summary</h2>
<p>We have described a protocol that proves that $A$ is a commitment to $\mathbf{a}$ and $\mathbf{b}$, $V$ is a commitment to $v$, and that $\langle \mathbf{a}, \mathbf{b} \rangle = v$ without revealing $\mathbf{a}$, $\mathbf{b}$, or $v$. The proof size, however, is linear, as $\mathbf{l}_u$ and $\mathbf{r}_u$ in $(\mathbf{l}_u, \mathbf{r}_u, t, T_1, T_2, \pi_{lr},\pi_t)$ are each of size $n$.</p>
<p><strong>Exercise:</strong> Fill in the missing code to implement the algorithm in this lecture. Prove you know the inner product for an $n=4$ vector without revealing it. Note that numpy arrays allow for element-wise addition and multiplication. For example:</p>
<pre style='font-family: Arial'><code class="language-solidity">import numpy as np
a = np.array([1,2,3,4])
b = np.array([2,2,2,2])

print(a + b) # np.array([3,4,5,6])
print(a * b) # np.array([2,4,6,8])
print(numpy.inner(a, b)) # 20

# casting a numpy array to numpy doesn't do anything
print(np.array(a) + b) # np.array([3,4,5,6])
</code></pre>
<p>Fill in the following code:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, add, FQ, eq, Z1
from py_ecc.bn128 import curve_order as p
import numpy as np
from functools import reduce
import random

def random_element():
    return random.randint(0, p)

def add_points(*points):
    return reduce(add, points, Z1)

# if points = G1, G2, G3, G4 and scalars = a,b,c,d vector_commit returns
# aG1 + bG2 + cG3 + dG4
def vector_commit(points, scalars):
    return reduce(add, [multiply(P, i) for P, i in zip(points, scalars)], Z1)


# these EC points have unknown discrete logs:
G = [(FQ(6286155310766333871795042970372566906087502116590250812133967451320632869759), FQ(2167390362195738854837661032213065766665495464946848931705307210578191331138)),
     (FQ(6981010364086016896956769942642952706715308592529989685498391604818592148727), FQ(8391728260743032188974275148610213338920590040698592463908691408719331517047)),
     (FQ(15884001095869889564203381122824453959747209506336645297496580404216889561240), FQ(14397810633193722880623034635043699457129665948506123809325193598213289127838)),
     (FQ(6756792584920245352684519836070422133746350830019496743562729072905353421352), FQ(3439606165356845334365677247963536173939840949797525638557303009070611741415))]

H = [(FQ(13728162449721098615672844430261112538072166300311022796820929618959450231493), FQ(12153831869428634344429877091952509453770659237731690203490954547715195222919)),
    (FQ(17471368056527239558513938898018115153923978020864896155502359766132274520000), FQ(4119036649831316606545646423655922855925839689145200049841234351186746829602)),
    (FQ(8730867317615040501447514540731627986093652356953339319572790273814347116534), FQ(14893717982647482203420298569283769907955720318948910457352917488298566832491)),
    (FQ(419294495583131907906527833396935901898733653748716080944177732964425683442), FQ(14467906227467164575975695599962977164932514254303603096093942297417329342836))]

B = (FQ(12848606535045587128788889317230751518392478691112375569775390095112330602489), FQ(18818936887558347291494629972517132071247847502517774285883500818572856935411))

# scalar multiplication example: multiply(G, 42)
# EC addition example: add(multiply(G, 42), multiply(G, 100))

# remember to do all arithmetic modulo p
def commit(a, sL, b, sR, alpha, beta, gamma, tau_1, tau_2):
    pass
    # return (A, S, V, T1, T2)


def evaluate(f_0, f_1, f_2, u):
    return (f_0 + f_1 * u + f_2 * u**2) % p

def prove(blinding_0, blinding_1, blinding_2, u):
    # fill this in
    # return pi
    pass

## step 0: Prover and verifier agree on G and B

## step 1: Prover creates the commitments
a = np.array([89,15,90,22])
b = np.array([16,18,54,12])
sL = ...
sR = ...
t1 = ...
t2 = ...

### blinding terms
alpha = ...
beta = ...
gamma = ...
tau_1 = ...
tau_2 = ...

A, S, V, T1, T2 = commit(a, sL, b, sR, alpha, beta, gamma, tau_1, tau_2)

## step 2: Verifier picks u
u = ...

## step 3: Prover evaluates l(u), r(u), t(u) and creates evaluation proofs
l_u = evaluate(a, sL, 0, u)
r_u = evaluate(b, sR, 0, u)
t_u = evaluate(np.inner(a, b), t1, t2, u)

pi_lr = prove(alpha, beta, 0, u)
pi_t = prove(gamma, tau_1, tau_2, u)

## step 4: Verifier accepts or rejects
assert t_u == np.mod(np.inner(np.array(l_u), np.array(r_u)), p), &quot;tu !=〈lu, ru〉&quot;
assert eq(add(A, commit(S, u)), add_points(vector_commit(G, l_u), vector_commit(H, r_u), multiply(B, pi_lr))), &quot;l_u or r_u not evaluated correctly&quot;
assert eq(add(multiply(G, t_u), multiply(B, pi_t)), add_points(V, multiply(T1, u), multiply(T2, u**2 % p))), &quot;t_u not evaluated correctly&quot;
</code></pre>
<p>This tutorial is part of a series on <a href="staging.rareskills.io/post/bulletproofs-zk">ZK Bulletproofs</a>.</p>
<div style='page-break-after: always;'></div>

<h1>Succinct proofs of a vector commitment</h1>
<p>Source: https://rareskills.io/post/outer-product-inner-product</p>
<h1>Succinct proofs of a vector commitment</h1>
<p>If we have a <a href="https://rareskills.io/post/pedersen-commitment">Pedersen vector commitment</a> $A$ which contains a commitment to a vector $\mathbf{a}$ as $A = a_1G_1 + a_2G_2+\dots + a_nG_n$ we can prove we know the opening by sending $\mathbf{a}$ to the verifier who would check that $A \stackrel{?}= a_1G_1 + \dots + a_nG_n$. This requires sending $n$ elements to the verifier (assuming $\mathbf{a}$ is of length $n$).</p>
<p>In the previous chapter, we showed how to do this with zero knowledge. In this chapter, we show how to prove knowledge of an opening while sending less than $n$ elements, but without the zero knowledge property.</p>
<h2>Motivation</h2>
<p>The technique we develop here will be an important building block for proving a valid computation of an inner product with a proof of size $\log n$ where $n$ is the length of the vectors.</p>
<p>In the previous chapter, we showed how to prove we executed the inner product correctly, but without revealing the vectors or the result. However, the proof is of size $\mathcal{O}(n)$ because of the step where the prover sends $\mathbf{l}_u$ and $\mathbf{r}_u$.</p>
<p>The subroutine in this article will be important for reducing the size of the proof. This article isn’t concerned with zero knowledge because the previously discussed algorithm has the zero knowledge property. That is $\mathbf{l}_u$ and $\mathbf{r}_u$ weren’t secret to begin with, so there is no need to obfuscate them.</p>
<h2>Problem statement</h2>
<p>Given an agreed upon basis vector $\mathbf{G}=[G_1,\dots,G_n]$ the prover gives the verifier a Pedersen vector commitment $A$, where $A$ is a non-blinding commitment to $\mathbf{a}$, i.e. $A = \langle[a_1,\dots,a_n],[G_1,\dots,G_n]\rangle$ and wishes to prove they know the opening to the commitment while sending less than $n$ terms, i.e. sending the entire vector $[a_1,\dots,a_n]$.</p>
<h2>A proof smaller than $n$</h2>
<p>Shrinking the proof size relies on three insights:</p>
<h3>Insight 1: The inner product $\langle \mathbf{a},\mathbf{b}\rangle$ is the diagonal of the outer product</h3>
<p>The first insight we will leverage is that the inner product is the diagonal of the <em>outer product</em>. In other words, the outer product “contains” the inner product in a sense. The outer product, in the context of 1D vectors, is a 2D matrix formed by multiplying every element in the first 1D vector with every other element in the second vector. For example:</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{a}=[a_1, a_2],\space<br />
\mathbf{b}=[b_1, b_2],<br />
\end{align*}<br />
\space\space<br />
\mathbf{a} \otimes \mathbf{b} = \begin{pmatrix}<br />
\boxed{a_1 b_1} &amp; a_1 b_2 \<br />
a_2 b_1 &amp; \boxed{a_2 b_2} \<br />
\end{pmatrix}<br />
$$</p>
<p>This might seem like a step in the wrong direction because the outer product requires $\mathcal{O}(n^2)$ steps to compute. However, the following insight shows it is possible to <em>indirectly</em> compute the outer product in $\mathcal{O}(1)$ time.</p>
<h3>Insight 2: The sum of the outer product equals the product of the sum of the original vectors</h3>
<p>A second observation is that the sum of the terms of the outer product equals the product of the sum of the vectors. That is,</p>
<p>$$<br />
\sum_{i=1}^{n} a_i\sum_{i=1}^{n} b_i=\sum\mathbf{a} \otimes \mathbf{b}<br />
$$</p>
<p>For our example of vectors $[a_1,a_2]$ and $[b_1,b_2]$ this would be</p>
<p>$$<br />
\underbrace{(a_1 + a_2)(b_1 + b_2)}_{\sum a_i\sum b_i} = \underbrace{a_1b_1 + a_1b_2 + a_2b_1 + a_2b_2}_{\sum\mathbf{a} \otimes \mathbf{b}}<br />
$$</p>
<p>Graphically, this can be visualized as the area of the rectangle with dimensions $(a_1 + a_2) \times (b_1 + b_2)$ having the same “area” as $a_1 \times b_1 + a_1 \times b_2 + a_2 \times b_1 + a_2 \times b_2$</p>
<p>$$<br />
\begin{array}{|c|cc|}<br />
\hline<br />
&amp;a_1+a_2\<br />
\hline<br />
b_1&amp;a_1b_1 + a_1b_2\<br />
+b_2&amp; + a_2b_1 + a_2b_2\<br />
\hline<br />
\end{array}=<br />
\begin{array}{|c|c|c|}<br />
\hline<br />
&amp;a_1&amp;a_2\<br />
\hline<br />
b_1&amp;a_1b_1&amp;a_2b_1\<br />
\hline<br />
b_2&amp;a_1b_2&amp;a_2b_2\<br />
\hline<br />
\end{array}<br />
$$</p>
<p>In our case, the $\mathbf{b}$ vector is actually the basis vector of elliptic curve points, so we are saying that</p>
<p>$$<br />
(a_1 + a_2)(G_1 + G_2) = a_1G_1 + a_1G_2 + a_2G_1 + a_2G_2<br />
$$</p>
<p>Note that our original Pedersen commitment</p>
<p>$$<br />
A = \langle[a_1,a_2],[G_1,G_2]\rangle = a_1G_1 + a_2G_2<br />
$$</p>
<p>is embedded in the boxed terms of the outer product:</p>
<p>$$<br />
(a_1 + a_2)(G_1 + G_2) = \boxed{a_1G_1} + a_1G_2 + a_2G_1 + \boxed{a_2G_2}<br />
$$</p>
<p><strong>Therefore, by multiplying the sums of the vector entries together, we also compute the sum of outer product.</strong></p>
<p>Since the inner product is the diagonal of the outer product, we have <em>indirectly</em> computed the inner product by multiplying the sums of the vector entries together. To prove that we know the inner product, we need to prove we also know the terms of the outer product that are not part of the inner product.</p>
<p>For vectors of length $2$, let’s call the parts of the outer product that are not part of the inner product the <em>off-diagonal product</em>.</p>
<p>Below, we mark the terms that make up the off-diagonal product with $\square$ and the terms that make up the inner product with $\blacksquare$:</p>
<p>$$<br />
\begin{array}{|c|c|c|}<br />
\hline<br />
&amp;a_1&amp;a_2\<br />
\hline<br />
b_1&amp;\blacksquare&amp;\square\<br />
\hline<br />
b_2&amp;\square&amp;\blacksquare\<br />
\hline<br />
\end{array}<br />
$$</p>
<p>We can now formally state the identity we will rely on going forward. If $n=2$ then:<br />
$$\sum\mathbf{a}\otimes\mathbf{b}=\langle\mathbf{a},\mathbf{b}\rangle+\mathsf{off\_diagonal}(\mathbf{a},\mathbf{b})$$</p>
<p>The identity also holds if one of the vectors is a vector of elliptic curve points (even if their discrete logs are unknown).</p>
<p>For cases where $n &gt; 2$, proving knowledge of an inner product means the prover needs to convince the verifier they know the “area” of the purple-shaded region below.</p>
<p><img alt="a square matrix with every entry shaded except the main diagonal" src="assets/off-product-shade.png" /></p>
<p>Conveying this information succinctly when $n &gt; 2$ is trickier, so we will revisit this later.</p>
<p>In the case of $n = 2$, the area is simply the off-diagonals.</p>
<h3>Insight 3: If $n = 1$ then the inner product equals the outer product</h3>
<p>An important corner case is where we have a vector of length $1$. In that case, the prover simply sends the verifier $\mathbf{a}$ (which is of length $1$) and the verifier simply multiplies the single element of $\mathbf{a}$ with the single element of $\mathbf{G}$.</p>
<h2>Sketch of the algorithm</h2>
<p>We can now create a first draft of an algorithm for the case $n=2$ that proves we have computed the inner product of $\mathbf{a}$ and $\mathbf{G}$, which is equivalent to showing we know the commitment $A$.</p>
<p>The interaction between the prover and the verifier is as follows:</p>
<ol>
<li>The prover sends their commitment $A = a_1G_1 + a_2G_2$ to the verifier.</li>
<li>The prover adds up all the terms in $\mathbf{a}$ and sends that as $a’ = a_1 + a_2$ to the verifier (note that the sum of the components of a vector is a scalar, hence summing the elements of $\mathbf{a}$ results in scalar $a’$). Furthermore, the prover computes the off-diagonal terms of $\mathbf{a} \otimes \mathbf{G}$ (i.e. $R = a_2G_1$, $L = a_1G_2$) and sends $L$ and $R$ to the verifier.</li>
</ol>
<p>Graphically, $L$ and $R$ can be seen as follows:</p>
<p>$$<br />
\begin{array}{|c|c|c|}<br />
\hline<br />
&amp;a_1&amp;a_2\<br />
\hline<br />
G_1&amp;&amp;R\<br />
\hline<br />
G_2&amp;L&amp;\<br />
\hline<br />
\end{array}<br />
$$</p>
<ol start="3">
<li>The verifier indirectly computes $\mathbf{a} \otimes \mathbf{G}$ by computing $a’G’$ where $G’ = G_1 + G_2$ and checks that</li>
</ol>
<p>$$\underbrace{a’G’}_\text{outer product sum} = \underbrace{A}_\text{inner product} + \underbrace{L + R}_\text{off-diagonal terms}$$</p>
<p>In expanded form, the above equation is:<br />
$$\underbrace{(a_1+a_2)(G_1+G_2)}_\text{outer product} = \underbrace{a_1G_1 + a_2G_2}_\text{inner product} + \underbrace{a_1G_2 + a_2G_1}_\text{off-diagonal terms}$$</p>
<p>Note that the check above is equivalent to the identity from earlier:</p>
<p>$$\sum\mathbf{a}\otimes\mathbf{G}=\langle\mathbf{a},\mathbf{G}\rangle+\mathsf{off\_diagonal}(\mathbf{a},\mathbf{G})$$</p>
<h4>Security bug: multiple openings</h4>
<p>However, there is security issue — the prover can find multiple proofs for the same commitment. For example, the prover could chose $L$ randomly, then compute</p>
<p>$$<br />
R = a’G’ – L<br />
$$</p>
<p>To prevent this, we re-use a similar idea from our discussion of zero knowledge multiplication — the prover must include verifier-provided randomness $u$ in their computation. They must also send $L$ and $R$ <em>in advance</em> of getting $u$ so $L$ and $R$ cannot be “advantageously” selected.</p>
<p>The reason the prover must send $L$ and $R$ individually, instead of the sum $L + R$ is that the prover is able to hack the protocol by moving value between $L$ and $R$ with no restriction. That is, since</p>
<p>$$L + R = a’G’$$</p>
<p>the prover could pick some elliptic curve point $S$ and compute a fraudulent $L’$ and $R’$ as</p>
<p>$$\underbrace{(L + S)}_{L’} + \underbrace{(R – S)}_{R’} = a’G’$$</p>
<p>We need to force the prover to keep $L$ and $R$ separate.</p>
<p>Here is the updated algorithm that corrects this bug:</p>
<ol>
<li>The prover and verifier agree on a basis vector $[G_1, G_2]$ where the points are chosen randomly and their discrete logs are unknown.</li>
<li>The prover computes and sends to the verifier $(A, L, R)$:<br />
   $$<br />
   \begin{align*}<br />
   A &amp;= a_1G_1 + a_2G_2 &amp;&amp; \text{// vector commitment we are proving knowledge of}\<br />
   L &amp;= a_1G_2 &amp;&amp; \text{// left diagonal term}\<br />
   R &amp;= a_2G_1 &amp;&amp; \text{// right diagonal term}\<br />
   \end{align*}<br />
   $$</li>
<li>The verifier responds with a random scalar $u$.</li>
<li>The prover computes and sends $a’$:</li>
</ol>
<p>$$a’ = a_1 + a_2u$$</p>
<ol start="5">
<li>The verifier, now in possession of $(A, L, R, a’, u)$ checks that:<br />
   $$<br />
   L + u A + u^2R \stackrel{?}= a'(u G_1 + G_2)<br />
   $$</li>
</ol>
<p>Under the hood this is:<br />
$$<br />
\underbrace{a_1G_2}_L + \underbrace{u(a_1G_1 + a_2G_2)}_{uA} + \underbrace{a_2u G_1}_{u^2R} = \underbrace{(a_1 + u a_2)}_{a’}(u G_1 + G_2)<br />
$$</p>
<p>which is identically correct if the prover correctly computed $a’$, $L$, and $R$.</p>
<p>Note that the verifier applied $u$ to $G_2$ whereas the prover applied $u$ to $a_1$. This causes the terms of the original inner product to be the linear coefficients of the resulting polynomial.</p>
<p>The fact that $L$ and $R$ are separated by $u^2$, which the verifier controls, prevents a malicious prover from doing the attack described earlier. That is, the prover cannot shift value from $R$ to $L$ because the value they shift must be scaled by $u^2$, but the prover must send $L$ and $R$ before they receive $u$.</p>
<h2>An alternative interpretation of the algorithm: halving the dimension of $n$</h2>
<p>The verifier is only carrying out a single multiplication, $a’$ times $(uG_1 + G_2)$. Even though we started with vectors of length $2$, the verifier only carries out $n/2=1$ point multiplications.</p>
<p>The operation $a_1 + a_2u$ turned a vector $\mathbf{a}$ of length $2$ into a vector of length $1$. Hence, the prover and verifier are both jointly constructing a new vector of length $1$ given the prover’s vectors and the verifier’s randomness $u$.</p>
<p>Since they have both compressed the original vector to a vector of length $1$, the verifier can use the identity $\langle\mathbf{a}’,\mathbf{G}’\rangle=\mathbf{a}’\otimes\mathbf{G}’$ when $n = 1$. Here, $\mathbf{a}’ = a_1 + a_2u$ and $\mathbf{G’} = G_1u + G_2$.</p>
<h2>Security of the algorithm</h2>
<h3>Algorithm summary</h3>
<p>As a quick summary of the algorithm,</p>
<ol>
<li>The prover sends the $(A, L, R)$ to the verifier.</li>
<li>The verifier responds with $u$.</li>
<li>The prover computes and sends $a’$.</li>
<li>The verifier checks that:</li>
</ol>
<p>$$L + uA + u^2R \stackrel{?}= a'(uG_1 + G_2)$$</p>
<p>Now let’s see why the prover cannot cheat.</p>
<p>The only “degree of freedom” the prover has on step 3 is $a’$.</p>
<p>To come up with an $a’$ that satisfies</p>
<p>$$L + uA + u^2R = a'(uG_1 + G_2)$$</p>
<p>the prover needs to know the discrete logs of $G_1$ and $G_2$. Specifically, they would have to solve</p>
<p>$$a’=\frac{l + ua + u^2r}{ug_1 + g_2}$$</p>
<p>where</p>
<ul>
<li>$l$ and $r$ are the discrete logs of $L$ and $R$</li>
<li>$g_1$ and $g_2$ are the discrete logs of $G_1$ and $G_2$ respectively, and $a$ is the discrete log of $A$.</li>
<li>$l$ and $r$ are known the prover, since the prover produced $L$ and $R$ in step 1.</li>
</ul>
<p>However, the prover does not know the discrete logs $g_1$ and $g_2$, so they cannot compute $a’$.</p>
<h3>The variable $a’$ has only two valid solutions</h3>
<p>There are only two values valid for $a’$ that satisfy $L + uA + u^2R = a'(uG_1 + G_2)$. Note that the equation $L + uA + u^2R$ forms a quadratic polynomial with respect to variable $u$ and $a'(uG_1+G_2)$ forms a linear polynomial. By the <a href="https://rareskills.io/post/schwartz-zippel-lemma">Schwartz-Zippel Lemma</a>, the equation has at most two solutions. As long as the order of the field is $\gg 2$, then the probability of the prover finding $a’$ such that it results in an intersection point of $L + uA + u^2R = a'(uG_1 + G_2)$ is negligible.</p>
<h2>Bulletproofs paper approach to injecting randomness</h2>
<p>Instead of combining $a_1$ and $a_2$ together as $a_1 + a_2u$, the prover combines them as $a’ = a_1u + a_2u^{-1}$ and the verifier does $G’ = u^{-1}G_1 + u G_2$. Note that the powers of the two vectors are applied in the opposite order. When we compute the outer product, the inner product terms will have the $uu^{-1}$ cancel:</p>
<p>$$<br />
[a_1u, u^{-1}a_2] \otimes [u^{-1}G_1, uG_2]=<br />
\begin{array}{|c| c c|}<br />
\hline<br />
&amp; ua_1 &amp; u^{-1}a_2 \<br />
\hline<br />
G_1u^{-1} &amp; \color{green}{a_1G_1} &amp; a_1G_2u^2 \<br />
G_2u &amp; a_2G_1u^{-2} &amp; \color{green}{a_2G_2} \<br />
\hline<br />
\end{array}<br />
$$</p>
<p>Arguably, this approach is “cleaner” so we will use that going forward.</p>
<h3>Introducing $\mathsf{fold}(\mathbf{a},x)$</h3>
<p>The computation $a_1x + a_2x^{-1}$ happens so frequently in Bulletproofs that it is handy to give it a name, which we call $\mathsf{fold}(\mathbf{a},x)$. The first argument $\mathbf{a}$ is the vector we are folding (which must be of even length, if not we pad it with a $0$). Fold splits the vector $\mathbf{a}$ of length $n$ into $n/2$ pairs, and returns a vector of length $n/2$ as follows:</p>
<p>$$\mathsf{fold}(\mathbf{a}, x)=[a_1x+a_2x^{-1},a_3x+a_4x^{-1},\dots,a_{n-1}x+a_nx^{-1}]$$</p>
<p>If we do $\mathsf{fold}(\mathbf{a},x^{-1})$ we mean:</p>
<p>$$\mathsf{fold}(\mathbf{a}, x^{-1})=[a_1x^{-1}+a_2x,a_3x^{-1}+a_4x,\dots,a_{n-1}x^{-1}+a_nx]$$</p>
<p>When $n=2$, $\mathsf{fold}(\mathbf{a},x)$ is simply $a_1x+a_2x^{-1}$ and $\mathsf{fold}(\mathbf{a},x^{-1})=a_1x^{-1}+a_2x$.</p>
<h3>Algorithm description with $\mathsf{fold}$</h3>
<p>We now restate the algorithm using the Bulletproofs paper’s approach to randomness:</p>
<ol>
<li>The prover sends their commitment to $\mathbf{a}$ as $A = a_1G_1 + a_2G_2$ to the verifier, along with $L$ and $R$ computed as<br />
   $$<br />
   \begin{align*}<br />
   L &amp;= a_1G_2 \<br />
   R &amp;= a_2G_1 \<br />
   \end{align*}<br />
   $$</li>
<li>The verifier responds with a random scalar $u$.</li>
<li>The prover computes and sends $a’$<br />
   $$a’ = \mathsf{fold}(\mathbf{a},u) = a_1u + a_2u^{-1}$$</li>
<li>The verifier computes:<br />
   $$<br />
   \begin{align*}<br />
   P &amp;= a’\cdot\mathsf{fold}(\mathbf{G},u^{-1})= a’\cdot(u^{-1} G_1 + uG_2)\<br />
   P &amp;\stackrel{?} = Lu^2 + A + u^{-2}R<br />
   \end{align*}<br />
   $$</li>
</ol>
<p>Assuming the prover was honest, the final check under the hood expands to:</p>
<p>$$<br />
\begin{align*}<br />
(a_1G_2)u^2 + (a_1G_1 + a_2G_2) + u^{-2}(a_2G_1) &amp;= (a_1u + a_2u^{-1})(u^{-1} G_1 + uG_2)\<br />
(a_1G_2)u^2 + (a_1G_1 + a_2G_2) + u^{-2}(a_2G_1) &amp;=a_1G_1+a_1u^2G_2+a_2u^{-2}G_1+a_2G_2\<br />
a_1u^2G_2 + a_1G_1 + a_2G_2 + a_2u^{-2}G_1 &amp;=a_1G_1+a_1u^2G_2+a_2u^{-2}G_1+a_2G_2\<br />
a_1G_1 + a_1u^2G_2 + a_2u^{-2}G_1 + a_2G_2 &amp;=a_1G_1+a_1u^2G_2+a_2u^{-2}G_1+a_2G_2\<br />
\end{align*}<br />
$$</p>
<h2>How to handle cases when $n &gt; 2$</h2>
<p>Assuming array $\mathbf{a}$ has even length (if not, we can add a zero element to make it even length), we can pairwise-partition the array. Below is an example of a pairwise-partition:</p>
<p>$$\mathbf{a} = [a_1, a_2, a_3, a_4, a_5, a_6, a_7, a_8]=[a_1, a_2] [a_3, a_4] [a_5, a_6] [a_7, a_8]$$</p>
<p>Similarly, we can pair-wise partition $\mathbf{G}$.</p>
<p>$$\mathbf{G} = [G_1, G_2, G_3, G_4, G_5, G_6, G_7, G_8]=[G_1, G_2] [G_3, G_4] [G_5, G_6] [G_7, G_8]$$</p>
<p>Each of the sub-pairs can then be treated as instances of computing the inner product using the $n=2$ case from earlier:<br />
<img alt="outer product of pairwise partitions" src="assets/pairwise-outer-product.png" /></p>
<p>We could then prove we know the four $n=2$ commitments $a_1G_1 + a_2G_2$, $a_3G_3 + a_4G_4$, $a_5G_5 + a_6G_6$, and $a_7G_7 + a_8G_8$ and this would be equivalent to proving we know the opening to the original commitment.</p>
<p>However, that would create four extra $(L, R)$ terms for pairs we are proving — i.e. no efficiency gain in terms of the size of the data the prover transmits.</p>
<p>The naive solution would be for the prover to commit and send:</p>
<p>$$\begin{align*}<br />
A_1 &amp;= a_1G_1+a_2G_2\<br />
A_2 &amp;= a_3G_3+a_4G_4\<br />
A_3 &amp;= a_5G_5+a_6G_6\<br />
A_4 &amp;= a_7G_7+a_8G_8\<br />
L_1 &amp;= a_1G_2\<br />
R_1 &amp;= a_2G_1\<br />
L_2 &amp;= a_3G_4\<br />
R_2 &amp;= a_4G_3\<br />
L_3 &amp;= a_5G_6\<br />
R_3 &amp;= a_6G_5\<br />
L_4 &amp;= a_7G_8\<br />
R_4 &amp;= a_8G_7\<br />
\end{align*}$$</p>
<p>Graphically, that can be seen as follows:<br />
$$\begin{array}{c|c|}<br />
&amp;a_1 &amp; a_2 &amp; a_3 &amp; a_4 &amp; a_5 &amp; a_6 &amp; a_7 &amp; a_8\<br />
\hline<br />
G_1&amp;&amp;R_1\<br />
\hline<br />
G_2&amp;L_1\<br />
\hline<br />
G_3&amp;&amp;&amp;&amp;R_2\<br />
\hline<br />
G_4&amp;&amp;&amp;L_2\<br />
\hline<br />
G_5&amp;&amp;&amp;&amp;&amp;&amp;R_3\<br />
\hline<br />
G_6&amp;&amp;&amp;&amp;&amp;L_3\<br />
\hline<br />
G_7&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;R_4\<br />
\hline<br />
G_8&amp;&amp;&amp;&amp;&amp;&amp;&amp;L_4\<br />
\hline<br />
\end{array}$$</p>
<p>As a (key!) optimization, we add up all the $A_i$, $L_i$ and $R_i$ terms from each of the pairs to become the single points $A$, $L$, $R$. In other words, the prover only sends:</p>
<p>$$\begin{align*}<br />
A &amp;= A_1 + A_2 + A_3 + A_4\<br />
L &amp;= L_1 + L_2 + L_3 + L_4\<br />
R &amp;= R_1 + R_2 + R_3 + R_4\<br />
\end{align*}$$</p>
<p>The operation described is shown in the animation below:</p>
<p>[</p>
<p>](https://r2media.rareskills.io/bulletproofs-06/outerproduct-anim.mp4)</p>
<h3>Security of adding all the commitments and off-diagonals together</h3>
<p>An initial concern with such an optimization is that since the prover is adding more terms together, there is more opportunity to hide a dishonest computation.</p>
<p>We now show that once the prover sends $A$ (and $L$ and $R$) they can only create one unique proof that they know the opening to $A$.</p>
<p>Observe that $L$ is computed as $L = a_1G_2 + a_3G_4 +a_5G_6+a_7G_8$ and $R$ is computed as $R = a_2G_1 + a_4G_3 +a_6G_5+a_8G_7$. They do not have any common elliptic curve points. Thus, the prover cannot “shift value” from $L$ to $R$ because they do not know the discrete logs of any of the points. Effectively, $L$ is a Pedersen vector commitment of $[a_2, a_4, a_6, a_8]$ to the basis vector $[G_1, G_3, G_5, G_7]$. The security assumption of a Pedersen vector commitment is that the prover can only produce one possible vector opening. “Shifting values around” after they send the commitment would mean the prover can compute a different vector other than $[a_2, a_4, a_6, a_8]$ which produces the same commitment. But that contradicts our assumption that a prover can only produce a single valid vector for a Pedersen commitment. A similar argument can be made for $R$.</p>
<p>$A$ is the addition of four Pedersen commitments (the commitments to the vectors $[a_1, a_2]$, $[a_3, a_4]$, $[a_5, a_6]$, $[a_7, a_8]$). However, the fact that several Pedersen commitments are added together is immaterial from a security perspective. It makes no difference if the commitments are computed separately and then added, or $A$ is computed as a vector of $n = 8$. Consider that:</p>
<p>$$\begin{align*}<br />
&amp;\space a_1G_1 + a_2G_2\space + \space a_3G_3 + a_4G_4\space\space + \space\space a_5G_5 + a_6G_6\space + \space a_7G_7 + a_8G_8\<br />
=&amp;(a_1G_1 + a_2G_2) + (a_3G_3 + a_4G_4) + (a_5G_5 + a_6G_6) + (a_7G_7 + a_8G_8)\end{align*}$$</p>
<p>For example, the prover might “shift value” from $a_1G_1$ to $a_2G_1$.</p>
<p>The only remaining concern is that the prover could shift value from $a_1G_1$ in $A$ to $a_2G_1$ in $L$ since they share a common elliptic curve point. However, this is prevented by the random $u$ from the verifier as shown previously.</p>
<p>Hence, once the prover sends $(A, L, R)$ computed in the manner described in this section, they can only create one possible opening, and thus create only one possible proof.</p>
<h2>Proving we know an opening to $A$ while sending $n/2$ data</h2>
<ol>
<li>The prover sends $A = \langle\mathbf{a},\mathbf{G}\rangle$ to the verifier. The prover also sends $L = a_1G_2 + a_3G_4 + … a_{n-1}G_n$ and $R = a_2G_1 + a_4G_3 + … + a_nG_{n-1}$.</li>
<li>The verifier sends a random $u$.</li>
<li>The prover computes $\mathbf{a}’=\mathsf{fold}(\mathbf{a},u)$ and sends $\mathbf{a}’$ to the verifier.</li>
<li>The verifier checks that $Lu^2 + A + Ru^{-2} \stackrel{?}=\langle\mathbf{a}’,\mathsf{fold}(\mathbf{G},u^{-1})\rangle$.</li>
</ol>
<p>We leave it as an exercise for the reader to work out an example to check that the final verification check is algebraically identical if the prover was honest. We suggest using a small example such as $n=4$.</p>
<h2>Yet another interpretation of $\mathsf{fold}$</h2>
<p>$P$ is a commitment to the original vector $\mathbf{a}$ with respect to the basis vector $\mathbf{G}$. $L$ is a commitment to the vector made up of the left off-diagonals of the pairwise outer products and $R$ is a commitment to the components of the right off-diagonals of the pairwise outer products.</p>
<p>The sum $Lu^2 + P + Ru^{-2}$ is itself a vector commitment of the vector $\mathsf{fold}(\mathbf{a},u)$ to the basis $\mathsf{fold}(\mathbf{G}, u^{-1})$, which has size $n/2$.</p>
<p>We show the relationship graphically below:</p>
<p>To prove we know the opening to a commitment of size $n/2$, we can simply send the vector of size $n/2$, which in this case is $\mathsf{fold}(\mathbf{a},u)$.</p>
<p>Using this interpretation, the algorithm is doing the following:</p>
<ol>
<li>Prover sends $A = \langle\mathbf{a},\mathbf{G}\rangle$, $L$, and $R$.</li>
<li>The verifier sends $u$.</li>
<li>Now the verifier has a commitment $A’ = Lu^2 + A + Ru^{-2}$ with respect to the basis vector $\mathsf{fold}(\mathbf{G},u^{-1})$.</li>
<li>The prover proves they know the opening to $A’$ by sending $\mathbf{a}’ =\mathsf{fold}(\mathbf{a}, u)$.</li>
</ol>
<h2>Limitations on verification speed</h2>
<p>Because the verifier needs to compute $\mathsf{fold}(\mathbf{G}, u^{-1})$, this will require iterating over the entire $\mathbf{G}$ vector, which will take $\mathcal{O}(n)$ time. Although the proof size can be smaller than the original vectors, verifing the proof will still take linear time.</p>
<h2>Summary</h2>
<p>We have shown how the prover can show they know an opening to a Pedersen vector commitment $A$ while sending only $n/2$ elements ($\mathbf{a}$ folded).</p>
<p>In the next chapter, we show how to recursively apply this algorithm so that the prover only sends $\mathcal{O}(\log n)$ elements.</p>
<p><strong>Exercise:</strong> Implement the algorithm described in this chapter. Use the following code as a starting point:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, add, FQ, eq, Z1
from py_ecc.bn128 import curve_order as p
import numpy as np
from functools import reduce
import random

def random_element():
    return random.randint(0, p)

def add_points(*points):
    return reduce(add, points, Z1)

# if points = G1, G2, G3, G4 and scalars = a,b,c,d vector_commit returns
# aG1 + bG2 + cG3 + dG4
def vector_commit(points, scalars):
    return reduce(add, [multiply(P, i) for P, i in zip(points, scalars)], Z1)

# these EC points have unknown discrete logs:
G_vec = [(FQ(6286155310766333871795042970372566906087502116590250812133967451320632869759), FQ(2167390362195738854837661032213065766665495464946848931705307210578191331138)),
     (FQ(6981010364086016896956769942642952706715308592529989685498391604818592148727), FQ(8391728260743032188974275148610213338920590040698592463908691408719331517047)),
     (FQ(15884001095869889564203381122824453959747209506336645297496580404216889561240), FQ(14397810633193722880623034635043699457129665948506123809325193598213289127838)),
     (FQ(6756792584920245352684519836070422133746350830019496743562729072905353421352), FQ(3439606165356845334365677247963536173939840949797525638557303009070611741415))]

# return a folded vector of length n/2 for scalars
def fold(scalar_vec, u):
    pass
    # your code here

# return a folded vector of length n/2 for points
def fold_points(point_vec, u):
    pass
    # your code here

# return L, R as a tuple
def compute_secondary_diagonal(G_vec, a):
    pass
    # your code here

a = [9,45,23,42]

# prover commits
A = vector_commit(G_vec, a)
L, R = compute_secondary_diagonal(G_vec, a)

# verifier computes randomness
u = random_element()

# prover computes fold(a)
aprime = fold(a, u)

# verifier computes fold(G)
Gprime = fold_points(G_vec, pow(u, -1, p))

# verification check
assert eq(vector_commit(Gprime, aprime), add_points(multiply(L, pow(u, 2, p)), A, multiply(R, pow(u, -2, p)))), &quot;invalid proof&quot;
assert len(Gprime) == len(a) // 2 and len(aprime) == len(a) // 2, &quot;proof must be size n/2&quot;
</code></pre>
<div style='page-break-after: always;'></div>

<h1>Logarithmic sized proofs of commitment</h1>
<p>Source: https://rareskills.io/post/log-n-vector-commitment-proof</p>
<h1>Logarithmic sized proofs of commitment</h1>
<p>In a previous chapter, we showed that multiplying the sums of elements of the vectors $\mathbf{a}$ and $\mathbf{G}$ computes the sum of the outer product terms, i.e. $\sum \mathbf{a}\otimes\mathbf{G}=\sum\mathbf{a}\sum\mathbf{G}$. We also showed that the outer product “contains” the inner product along its main diagonal.</p>
<p>To “extract” the inner product $\langle\mathbf{a},\mathbf{G}\rangle$, one must subtract from the outer product all terms that are not part of the inner product, i.e. the purple shaded region below:</p>
<p><img alt="off product shade" src="assets/off-diagonal-shade.png" /></p>
<p>There are $\mathcal{O}(n^2)$ such terms, so doing this directly is not efficient.</p>
<p>However, observe that we can “fill up the outer product” in the manner shown in the animation below:</p>
<p><a href="https://r2media.rareskills.io/bulletproofs-07/MatrixFoldingAnimation.mp4"></a></p>
<p>In the animation above, after the prover sends the off-diagonal terms, the prover folds both $\mathbf{G}$ and $\mathbf{a}$, reducing their length by half.</p>
<p>At the end, after we have folded $\mathbf{a}$ and $\mathbf{G}$ $\log n$ times, the size of the vectors is 1. <strong>When $n=1$ the outer product equals the inner product</strong> and we simply reveal both vectors, which will be of constant size.</p>
<h2>Recap — and how this relates to the previous chapter’s algorithm</h2>
<p>Previously, we showed how to prove we know the opening to a commitment $P$ while sending $n/2$ elements instead of $n$. As a recap:</p>
<p>The prover commits the vector $\mathbf{a}$ to $P$ as $P = \langle\mathbf{a},\mathbf{G}\rangle$. The prover then sends the off-diagonal terms $L=a_1G_2 + a_3G_4 + \dots a_{n-1}G_n$ and $R = a_2G_1 + a_4G_3 + \dots a_nG_{n-1}$. The verifier responds with $u$ and the prover folds the vector $\mathbf{a}$ over $u$ as $\mathbf{a}’=\mathsf{fold}(\mathbf{a},u)=[a_1u+a_2u^{-1}, a_3u+a_4u^{-1}, \dots, a_{n-1}u+a_nu^{-1}]$ and sends $\mathbf{a}’$ to the verifier. Since $\mathbf{a}’$ is of length $n/2$, we have cut the size of the data to be transmitted in half.</p>
<p>The verifier then checks that $\langle\mathsf{fold}(\mathbf{G},u^{-1}),\mathbf{a}’\rangle\stackrel{?}=Lu^2+P+Ru^{-2}$</p>
<p>The original intent of this algorithm was to prove that we know the commitment to $P$ by showing that we know the sum of the inner product $P$ and the off-diagonal terms $L$ and $R$ equals the sum of the elements of the outer products of the pairwise partitions of $\mathbf{a}$ and $\mathbf{G}$.</p>
<p>If we recursively apply this algorithm, we get the $\log n$ algorithm described at the beginning of this chapter.</p>
<p>However, we could also interpret this procedure as us proving we know the opening to the commitment $P’$ where $P’=(Lu^2+P+Ru^{-2})$ with respect to the basis vector $\mathsf{fold}(\mathbf{G},u^{-1})$ of length $n’=n/2$. We could naïvely prove we know the opening of $P’$ by sending $\mathbf{a}’$ and the verifier checking that $P’\stackrel{?}=\langle\mathbf{a’},\mathsf{fold}(\mathbf{G},u^{-1})\rangle$.</p>
<p>But rather than proving we know the opening to $P’$ by sending $\mathbf{a}’$, we can recursively apply the algorithm to prove we know the opening to $P’$ by sending a vector $\mathbf{a}{\prime\prime}$ of size $n/4$. In fact, we can keep recursing until $a^{\prime\dots\prime}$ is of size $n=1$.</p>
<p>The animation below provides an intuition of what is happening. The next section describes the animation in detail.</p>
<p><img alt="iterative commitment animation" src="assets/iterative-folding.gif" /></p>
<p>For this algorithm to work, the length of the vectors must be a power of two. However, if the length is not a power of two, we can pad the vectors with zeros until the length is a power of two.</p>
<h2>Proving we know the opening to $P$ with $\mathcal{O}(\log n)$ data</h2>
<h3>The algorithm</h3>
<p>The prover and verifier agree on a basis vector $\mathbf{G}$ of length $n$. The prover sends the verifier $P$ which is $\langle\mathbf{a},\mathbf{G}\rangle$. The prover wishes to convince the verifier that they know the opening to $P$ while sending only logarithmic-sized data.</p>
<p>The prover and verifier then engage in the following algorithm below. The arguments after the | mean they are only known to the prover.</p>
<p>In the algorithm description below $n$ is the length of the vectors in the input, which are all of the same length.</p>
<h4>$\texttt{prove_commitments_log}(P, \mathbf{G}, | \mathbf{a})$</h4>
<h5>Case 1: $n = 1$</h5>
<ol>
<li>The prover sends $a$ and the verifier checks that $aG \stackrel{?}= P$ and the algorithm ends.</li>
</ol>
<h5>Case 2: $n &gt; 1$</h5>
<ol>
<li>The prover computes and sends to the verifier $(L, R)$ where<br />
   $$\begin{align*}<br />
   L &amp;= a_1G_2 + a_3G_4 + \dots a_{n-1}G_n\<br />
   R &amp;= a_2G_1 + a_4G_3 + \dots a_nG_{n-1}<br />
   \end{align*}<br />
   $$</li>
<li>The verifier sends randomness $u$</li>
<li>The prover and verifier both compute:<br />
   $$<br />
   \begin{align*}<br />
   \mathbf{G}’&amp;=\mathsf{fold}(\mathbf{G},u^{-1})\<br />
   P’ &amp;= Lu^2+P+Ru^{-2}<br />
   \end{align*}<br />
   $$</li>
<li>The prover computes<br />
   $$\mathbf{a}’=\mathsf{fold}(\mathbf{a},u)$$</li>
<li>$\texttt{prove_commitments_log}(P’, \mathbf{G}’, \mathbf{a}’)$</li>
</ol>
<h3>Commentary on the algorithm</h3>
<p>The prover is recursively proving that, given values $P$ and $\mathbf{G}$, they know the $\mathbf{a}$ such that $P=\langle\mathbf{a},\mathbf{G}\rangle$. Both parties recursively fold $\mathbf{G}$ until it is a single point, and the prover recursively folds $\mathbf{a}$ until it is a single point.</p>
<p>The prover transmits a constant amount of data on each iteration, and the recursion will run at most $\log n$ times, so the prover sends $\mathcal{O}(\log n)$ data.</p>
<p>We stress that this algorithm is not zero knowledge because in the case $n=1$, the verifier learns the entire vector. The verifier could also send non-random values for $u$ to try to learn something about $\mathbf{a}$.</p>
<p>However, recall that our motivation for this algorithm is to reduce the size of the check $t_u=\langle\mathbf{l}_u,\mathbf{r}_u\rangle$, and $\mathbf{l}_u$ and $\mathbf{r}_u$ were not secret to begin with.</p>
<p>In fact, we have not shown how to prove we know the inner product with logarithmic-sized data, we have only shown that we know the opening to a commitment with a logarithmic-sized data. However, it is straightforward to update our algorithm to show we know the inner product of two vectors, as we will do later in this article.</p>
<h4>Runtime</h4>
<p>The verifier carries out the computation $\mathbf{G}’ = \mathsf{fold}(\mathbf{G}, u^{-1})$ $\log n$ times, and the first $\mathsf{fold}$ takes $\mathcal{O}(n)$ time. At first glance, it seems that the verifier’s runtime is $\mathcal{O}(n \log n)$. However, notice that with each iteration, $n$ is halved, resulting in a runtime or $n + \frac{n}{2} + \frac{n}{4} + … + 1 = 2n$, leading to an overall runtime for the verifier of $\mathcal{O}(n)$.</p>
<h2>Proving we know the inner product $\langle\mathbf{a},\mathbf{b}\rangle=v$</h2>
<p>We now adjust the algorithm above to prove that we conducted the inner product between two scalar vectors, as opposed to a vector of field elements and a vector of elliptic curve points.</p>
<p>Specifically, we must prove that $P$ holds a commitment to the inner product $\langle\mathbf{a},\mathbf{b}\rangle$. This inner product is a scalar, so we do a normal Pedersen commitment instead of a vector commitment. For this we use a random elliptic curve point (with unknown discrete log) $Q$. Thus, $P = \langle\mathbf{a},\mathbf{b}\rangle Q$.</p>
<p>However, we cannot simply re-use our previous algorithm because the prover can provide multiple openings to $P$. For example, if $\mathbf{a} = [1,2]$ and $\mathbf{b} = [3,4]$, the prover can open also open with vectors $\mathbf{a}’ = [3,2]$ and $\mathbf{b}’ = [1, 4]$.</p>
<p>To create a <em>secure</em> proof of knowledge of an inner product, the prover must also compute and send a commitment for $\mathbf{a}$ and $\mathbf{b}$.</p>
<p>The naive solution is to run the commitment algorithm twice. The first two times are to prove $\mathbf{a}$ and $\mathbf{b}$ are properly committed to $P_1 = \langle\mathbf{a},\mathbf{G}\rangle$ and $P_2 =\langle\mathbf{b},\mathbf{H}\rangle$ and the third time is to show that $\langle\mathbf{a},\mathbf{b}\rangle Q=P_3$ was properly computed. In the next section, we show how to modify our algorithm to compute the inner product when both vectors are field elements.</p>
<h3>Converting a scalar inner product to a scalar-point inner product</h3>
<p>Let $\mathbf{Q}^n$ be the vector consisting of $n$ copies of the point $Q$, i.e.</p>
<p>$$\mathbf{Q}^n=[\underbrace{Q,\dots, Q}_n]$$</p>
<p>Thus,</p>
<p>$$\mathbf{b}\circ\mathbf{Q}^n=[b_1Q, b_2Q,\dots,b_nQ]$$</p>
<p>Note that $\langle\mathbf{a},\mathbf{b}\rangle Q$ is equal to $\langle\mathbf{a},\mathbf{b}\circ\mathbf{Q}^n\rangle$.</p>
<p>That is, we can multiply each entry of $\mathbf{b}$ by $Q$ and take the inner product of that vector with $\mathbf{a}$ and the result will be the same as $\langle\mathbf{a},\mathbf{b}\rangle Q$. For example, if $\mathbf{a} = [1,2]$ and $\mathbf{b} = [3,4]$, then $\langle[1,2],[3,4]\rangle Q=(1\cdot 3+2\cdot 4)Q= \langle[1,2],[3Q,4Q]\rangle=1\cdot 3Q+2\cdot 4Q=11Q$.</p>
<p>From there, we could prove the following:</p>
<ol>
<li>$P_1 =\langle\mathbf{a},\mathbf{G}\rangle$</li>
<li>$P_2 =\langle\mathbf{b},\mathbf{H}\rangle$</li>
<li>$P_3 =\langle\mathbf{a},\mathbf{b}\circ\mathbf{Q}\rangle$</li>
</ol>
<h3>One proof instead of three</h3>
<p>We can do better than sending three commitments and run the algorithm three times.</p>
<p>Because the points in $\mathbf{G}$, $\mathbf{H}$ and $Q$ have an unknown discrete log relationship, they can be combined as a single commitment $P = \langle\mathbf{a},\mathbf{G}\rangle +\langle\mathbf{b},\mathbf{H}\rangle + \langle\mathbf{a},\mathbf{b}\rangle Q = P_1 + P_2 + P_3$.</p>
<p>We will slightly re-arrange the commitment $P = \langle\mathbf{a},\mathbf{G}\rangle +\langle\mathbf{b},\mathbf{H}\rangle + \langle\mathbf{a},\mathbf{b}\rangle Q$ as $P = \langle\mathbf{a},\mathbf{G}\rangle+\langle\mathbf{a},\mathbf{b}\circ\mathbf{Q}^n\rangle +\langle\mathbf{b},\mathbf{H}\rangle$ to make the next trick more obvious.</p>
<p>To prove this entire commitment at once, instead of three inner products, observe that</p>
<p>$$P=\langle\mathbf{a},\mathbf{G}\rangle+\langle\mathbf{a},\mathbf{b}\circ\mathbf{Q}^n\rangle +\langle\mathbf{b},\mathbf{H}\rangle=\langle\mathbf{a}\oplus\mathbf{a}\oplus\mathbf{b},\mathbf{G}\oplus b\circ\mathbf{Q}^n\oplus\mathbf{H}\rangle$$</p>
<p>where $\oplus$ means vector concatenation.</p>
<p>Effectively, we are proving we committed vector $\mathbf{a}\oplus\mathbf{a}\oplus\mathbf{b}$ to elliptic curve vector basis $\mathbf{G}\oplus\mathbf{Q}^n\oplus\mathbf{H}$.</p>
<p>In practice, we don’t <em>actually</em> concatenate the vectors because the total length would generally not be a power of two. Rather, we compute the $\mathbf{G}$, $\mathbf{H}$, and $\mathbf{b}\circ\mathbf{Q}^n$ components separately, but compute $L$ and $R$ as if they were concatenated.</p>
<p>We show the algorithm in the animation below:</p>
<p><a href="https://r2media.rareskills.io/bulletproofs-07/ThreeWayInnerProduct.mp4"></a></p>
<h2>The algorithm</h2>
<p>Given $v = \langle\mathbf{a},\mathbf{b}\rangle$ and commitment $P = vQ+\langle\mathbf{a},\mathbf{G}\rangle + \langle\mathbf{b},\mathbf{H}\rangle$ we wish to prove that $P$ is committed as claimed. That is, $v$, $\mathbf{a}$, and $\mathbf{b}$ are committed to $P$ and $\langle\mathbf{a},\mathbf{b}\rangle=v$.</p>
<h4>$\texttt{prove_commitments_log}(P, \mathbf{G}, \mathbf{H},Q, |\mathbf{a}, \mathbf{b})$</h4>
<h5>Case 1: $n = 1$</h5>
<ol>
<li>The prover sends $(a,b)$ and the verifier checks that $P \stackrel{?}= aG + bH + abQ$. The algorithm ends.</li>
</ol>
<h5>Case 2: $n &gt; 1$</h5>
<ol>
<li>The prover computes and sends to the verifier $(L, R)$ which are simply the off-diagonal terms of all the vectors concatenated together (see the animation above):<br />
   $$\begin{align*}<br />
   L &amp;= (a_1b_2 + a_3b_4 + \dots a_{n-1}b_n)Q+(a_1G_2 + a_3G_4 + \dots a_{n-1}G_n)+(b_2H_1 + b_4H_3 + \dots b_nH_{n-1})\<br />
   R &amp;= (a_2b_1 + a_4b_3 + \dots a_nb_{n-1})Q+(a_2G_1 + a_4G_3 + \dots a_nG_{n-1})+(b_1H_2 + b_3H_4 + \dots b_{n-1}H_n)<br />
   \end{align*}<br />
   $$</li>
<li>The verifier sends randomness $u$.</li>
<li>The prover and verifier both compute:<br />
   $$<br />
   \begin{align*}<br />
   P’ &amp;= Lu^2+P+Ru^{-2}\<br />
   \mathbf{G}’&amp;=\mathsf{fold}(\mathbf{G}, u^{-1})\<br />
   \mathbf{H}’&amp;=\mathsf{fold}(\mathbf{H}, u)\\<br />
   \end{align*}<br />
   $$</li>
<li>The prover computes:<br />
   $$<br />
   \begin{align*}<br />
   \mathbf{a}’&amp;=\mathsf{fold}(\mathbf{a},u)\<br />
   \mathbf{b}’&amp;=\mathsf{fold}(\mathbf{b},u^{-1})<br />
   \end{align*}<br />
   $$</li>
<li>$\texttt{prove_commitments_log}(P’, G’, H’, \mathbf{a}’, \mathbf{b}’)$</li>
</ol>
<p>The following exercises can be found in our <a href="https://github.com/RareSkills/ZK-bulletproofs/tree/main">ZK Bulletproofs GitHub Repo</a>:</p>
<p><strong>Exercise 1:</strong> Fill in the missing code below to implement the algorithm to prove that $\mathbf{a}$ is committed to $\mathbf{G}$ to produce point $P$:</p>
<pre style='font-family: Arial'><code class="language-solidity">from py_ecc.bn128 import G1, multiply, add, FQ, eq, Z1
from py_ecc.bn128 import curve_order as p
import numpy as np
from functools import reduce
import random

def random_element():
    return random.randint(0, p)

def add_points(*points):
    return reduce(add, points, Z1)

# if points = G1, G2, G3, G4 and scalars = a,b,c,d vector_commit returns
# aG1 + bG2 + cG3 + dG4
def vector_commit(points, scalars):
    return reduce(add, [multiply(P, i) for P, i in zip(points, scalars)], Z1)

# these EC points have unknown discrete logs:
G_vec = [(FQ(6286155310766333871795042970372566906087502116590250812133967451320632869759), FQ(2167390362195738854837661032213065766665495464946848931705307210578191331138)),
     (FQ(6981010364086016896956769942642952706715308592529989685498391604818592148727), FQ(8391728260743032188974275148610213338920590040698592463908691408719331517047)),
     (FQ(15884001095869889564203381122824453959747209506336645297496580404216889561240), FQ(14397810633193722880623034635043699457129665948506123809325193598213289127838)),
     (FQ(6756792584920245352684519836070422133746350830019496743562729072905353421352), FQ(3439606165356845334365677247963536173939840949797525638557303009070611741415))]

# return a folded vector of length n/2 for scalars
def fold(scalar_vec, u):
    pass

# return a folded vector of length n/2 for points
def fold_points(point_vec, u):
    pass

# return (L, R)
def compute_secondary_diagonal(G_vec, a):
    pass

a = [4,2,42,420]

P = vector_commit(G_vec, a)

L1, R1 = compute_secondary_diagonal(G_vec, a)
u1 = random_element()
aprime = fold(a, u1)
Gprime = fold_points(G_vec, pow(u1, -1, p))

L2, R2 = compute_secondary_diagonal(Gprime, aprime)
u2 = random_element()
aprimeprime = fold(aprime, u2)
Gprimeprime = fold_points(Gprime, pow(u2, -1, p))

assert len(Gprimeprime) == 1 and len(aprimeprime) == 1, &quot;final vector must be len 1&quot;
assert eq(vector_commit(Gprimeprime, aprimeprime), add_points(multiply(L2, pow(u2, 2, p)), multiply(L1, pow(u1, 2, p)), P, multiply(R1, pow(u1, -2, p)), multiply(R2, pow(u2, -2, p)))), &quot;invalid proof&quot;
</code></pre>
<p><strong>Exercise 2:</strong> Modify the code above to implement the algorithm that proves $P$ holds a commitment to $\mathbf{a}$, $\mathbf{b}$ and $v$, and that $\langle\mathbf{a},\mathbf{b}\rangle=v$. Use the following basis vector for $\mathbf{H}$ and EC point $Q$:</p>
<pre style='font-family: Arial'><code class="language-solidity">H = [(FQ(13728162449721098615672844430261112538072166300311022796820929618959450231493), FQ(12153831869428634344429877091952509453770659237731690203490954547715195222919)),
    (FQ(17471368056527239558513938898018115153923978020864896155502359766132274520000), FQ(4119036649831316606545646423655922855925839689145200049841234351186746829602)),
    (FQ(8730867317615040501447514540731627986093652356953339319572790273814347116534), FQ(14893717982647482203420298569283769907955720318948910457352917488298566832491)),
    (FQ(419294495583131907906527833396935901898733653748716080944177732964425683442), FQ(14467906227467164575975695599962977164932514254303603096093942297417329342836))]

Q = (FQ(11573005146564785208103371178835230411907837176583832948426162169859927052980), FQ(895714868375763218941449355207566659176623507506487912740163487331762446439))
</code></pre>
<p>This tutorial is part of a series on <a href="https://rareskills.io/post/bulletproofs-zk">Bulletproof ZKP</a>.</p>
<div style='page-break-after: always;'></div>

<h1>Bulletproofs ZKP: Zero Knowledge and Succinct Proofs for Inner Products</h1>
<p>Source: https://rareskills.io/post/bulletproofs-zkp</p>
<h1>Bulletproofs ZKP: Zero Knowledge and Succinct Proofs for Inner Products</h1>
<p>Bulletproofs ZKPs allow a prover to prove knowledge of an inner product with a logarithmic-sized proof. Bulletproofs do not require a trusted setup.</p>
<p>In the previous chapters, we showed how to prove knowledge of an inner product without revealing the vectors or the inner product, albeit with a proof of size $\mathcal{O}(n)$ where $n$ is the length of the vector. We also showed how to prove knowledge of an inner product using logarithmic-sized data, but without the zero knowledge property.</p>
<p>In this chapter, we combine the algorithms together to demonstrate the Bulletproof ZK algorithm.</p>
<p>(This work if part of a series on <a href="https://rareskills.io/post/bulletproofs-zk">ZK Bulletproofs</a>.)</p>
<h2>Problem Statement</h2>
<p>The prover and verifier agree on two elliptic curve basis vectors $\mathbf{G}$ and $\mathbf{H}$ of length $n$ and elliptic curve points $Q$ and $B$. The discrete log relationships between all these points are unknown.</p>
<p>The prover has vectors $\mathbf{a}$ and $\mathbf{b}$ with inner product $v$. The prover commits $\mathbf{a}$ and $\mathbf{b}$ to $A$ as $A =\langle\mathbf{a},\mathbf{G}\rangle + \langle\mathbf{b},\mathbf{H}\rangle + \alpha B$ where $\alpha$ is a blinding term. The prover commits $V = \langle\mathbf{a},\mathbf{b}\rangle Q + \gamma B$.</p>
<p>The prover sends $(A, V)$ to to the verifier and aims to prove that $\mathbf{a}$ and $\mathbf{b}$ are committed to $A$ and their inner product is committed to $V$. The verifier does not learn the vectors or the inner product.</p>
<p>The size of the proof must be $\mathcal{O}(\log n)$.</p>
<h2>The Bulletproof ZK Algorithm</h2>
<p>The prover generates random scalars $\set{\alpha, \beta,\gamma,\tau_1,\tau_2}$ and random vectors $\set{\mathbf{s}_L,\mathbf{s}_R}$ and computes the commitments</p>
<p>$$<br />
\begin{align}<br />
A &amp;= \langle\mathbf{a},\mathbf{G}\rangle + \langle\mathbf{b},\mathbf{H}\rangle+\alpha B\<br />
S &amp;= \langle\mathbf{s}_L,\mathbf{G}\rangle + \langle\mathbf{s}_R,\mathbf{H}\rangle+\beta B\<br />
V &amp;= vQ + \gamma B \<br />
\end{align}$$</p>
<p>The prover prepares (but does not send) vector polynomials</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{l}(x) &amp;= \mathbf{a} + \mathbf{s}_Lx\<br />
\mathbf{r}(x) &amp;= \mathbf{b} + \mathbf{s}_Rx\<br />
t(x)&amp;=\langle\mathbf{l}(x),\mathbf{r}(x)\rangle = \langle\mathbf{a},\mathbf{b}\rangle+(\langle\mathbf{a},\mathbf{s}_R\rangle + \langle\mathbf{b},\mathbf{s}_L\rangle) x+(\langle\mathbf{s}_L,\mathbf{s}_R\rangle)x^2<br />
\end{align*}<br />
$$</p>
<p>$A$ is a commitment to the constant terms of the vector polynomial, $S$ is a commitment to the linear terms, and $V$ is a commitment to the inner product.</p>
<p>The prover creates commitments to the coefficients of $t(x)$ as</p>
<p>$$<br />
\begin{align*}<br />
T_1 &amp;= (\langle\mathbf{a},\mathbf{s}_R\rangle + \langle\mathbf{b},\mathbf{s}_L\rangle)Q + \tau_1B\<br />
T_2 &amp;= \langle\mathbf{s}_L,\mathbf{s}_R\rangle Q + \tau_2B<br />
\end{align*}<br />
$$</p>
<p>Note that $V$ is a commitment to the constant coefficient of $t(x)$, and $T_1$ and $T_2$ are commitments to the linear and quadratic coefficients of $t(x)$, respectively.</p>
<p>The prover sends $(A, S, V, T_1, T_2)$ to the verifier.</p>
<p>The verifier responds with random value $u$.</p>
<p>The prover then evaluates the polynomials at $u$ and creates proofs that they were evaluated correctly:</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{l}_u &amp;= \mathbf{l}(u) = \mathbf{a} + \mathbf{s}_Lu \<br />
\mathbf{r}_u &amp;= \mathbf{r}(u) = \mathbf{b} + \mathbf{s}_Ru \<br />
t_u &amp;= t(u) =v + (\langle\mathbf{a},\mathbf{s}_R\rangle + \langle\mathbf{b},\mathbf{s}_L\rangle)u + \langle\mathbf{s}_L,\mathbf{s}_R\rangle u^2\<br />
\pi_{lr} &amp;=\alpha+\beta u\<br />
\pi_t &amp;= \gamma + \tau_1u + \tau_2u^2\<br />
\end{align*}<br />
$$</p>
<p>Previously, the prover transmits $(\mathbf{l}_u, \mathbf{r}_u, t_u, \pi_{lr}, \pi_t)$ so the verifier could check that</p>
<p>$$<br />
\begin{align*}<br />
t_u&amp;\stackrel{?}=\langle\mathbf{l}_u,\mathbf{r}_u\rangle\<br />
A + Su &amp;\stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{G} \rangle + \langle \mathbf{r}_u, \mathbf{H} \rangle + \pi_{lr} B\<br />
t_u Q &amp;\stackrel{?}{=} V + T_1 u + T_2 u^2 – \pi_t B\<br />
\end{align*}<br />
$$</p>
<p>but this would be linear in size due to the vectors $\mathbf{l}_u$ and $\mathbf{r}_u$. Instead, the prover commits $\mathbf{l}_u$ and $\mathbf{r}_u$ as</p>
<p>$$C=\langle\mathbf{l}_u,\mathbf{G}\rangle+\langle\mathbf{r}_u,\mathbf{H}\rangle$$</p>
<p>and sends $(C, t_u, \pi_{lr}, \pi_t)$.</p>
<p>We can re-arrange the first two verifier checks as follows:</p>
<p>$$<br />
\begin{align*}<br />
t_u&amp;\stackrel{?}=\langle\mathbf{l}_u,\mathbf{r}_u\rangle\<br />
A + Su -\pi_{lr}B&amp;\stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{G} \rangle + \langle \mathbf{r}_u, \mathbf{H} \rangle\<br />
\end{align*}<br />
$$</p>
<p>Observe that if we set $P = A + Su -\pi_{lr}B$, then this is the same problem statement as proving $P$ holds commitments to two vectors $\mathbf{l}_u$ and $\mathbf{r}_u$ with respect to the basis vectors $\mathbf{G}$ and $\mathbf{H}$, and that $\mathbf{l}_u$ and $\mathbf{r}_u$ have an inner product of $t_u$. Therefore, we can reuse the logarithmic-size proof of knowledge of a commitment opening to $P$.</p>
<p>For this proof, we do not need secrecy because $\mathbf{l}_u$ and $\mathbf{r}_u$ were made public anyway in the previous algorithm.</p>
<p>Now that the verifier has all the necessary data, the prover engages in an interactive proof to prove that $C$ holds the commitments to $\mathbf{l}_u$ and $\mathbf{r}_u$ and that their inner product is $t_u$:</p>
<p>$$\texttt{prove_commitments_log}(C + t_uQ, \mathbf{G}, \mathbf{H}, Q, \mathbf{l}_u, \mathbf{r}_u)$$</p>
<p>That subroutine will prove that $t_u\stackrel{?}=\langle\mathbf{l}_u,\mathbf{r}_u\rangle$ and $C\stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{G} \rangle + \langle \mathbf{r}_u, \mathbf{H} \rangle$ without having to send $\mathbf{l}_u$ and $\mathbf{r}_u$. It also only sends logarithmic-sized data. Note that the recursive algorithm from the previous chapter uses a commitment $P = \langle \mathbf{a}, \mathbf{G} \rangle + \langle \mathbf{b}, \mathbf{H} \rangle + \langle\mathbf{a},\mathbf{b}\rangle Q$, so the verifier needs to add in the “$Q$ portion” themselves. Now the verifier can be assured that</p>
<p>$$<br />
\begin{align*}<br />
t_u&amp;\stackrel{?}=\langle\mathbf{l}_u,\mathbf{r}_u\rangle\<br />
C&amp;\stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{G} \rangle + \langle \mathbf{r}_u, \mathbf{H} \rangle\<br />
\end{align*}<br />
$$</p>
<p>Finally, the verifier checks that</p>
<p>$$<br />
\begin{align*}<br />
C&amp;\stackrel{?}=A + Su-\pi_{lr}B\<br />
t_u Q &amp;\stackrel{?}{=} V + T_1 u + T_2 u^2 – \pi_t B<br />
\end{align*}<br />
$$</p>
<p>Recall that $A$ and $S$ are commitments to the constant and linear terms of the vector polynomials $\mathbf{l}(x)$ and $\mathbf{r}(x)$, respectively. The first check ensures that the vectors committed to $C$ are evaluations of those polynomials at $u$.</p>
<p>The second check is to ensure that $t(u)$ was evaluated correctly, given the commitments to the coefficients $V$, $T_1$, and $T_2$.</p>
<h2>Non-interactivity via the Fiat Shamir Transform</h2>
<p>In practice, this algorithm is made non-interactive via the <em>Fiat Shamir Transform</em>. Instead of asking the verifier for randomness, the prover generates randomness by concatenating all the data it has transmitted so far and hashing that. The verifier then re-hashes the data to ensure the prover generated the randomness correctly.</p>
<p>It is critical that the hash include all previous data transmissions, otherwise the implementation will have a <a href="https://blog.trailofbits.com/2022/04/15/the-frozen-heart-vulnerability-in-bulletproofs/">frozen heart vulnerability</a>.</p>
<h2>Next steps</h2>
<p>In practice, problems of practical interest consist of multiple inner products. For example, a Rank 1 Constraint System:</p>
<p>$$L\mathbf{a}\circ R\mathbf{a} = O\mathbf{a}$$</p>
<p>is really $3n$ inner products (e.g. multiplying $\mathbf{a}$ by the $n$ rows of $L$ and so on for $R$ and $O$) and a single Hadamard product. Therefore, it will be handy to know some mathematical tricks for combining multiple inner products into a single one so that we don’t have to send $3n$ Bulletproofs. We will learn how to accomplish this in our upcoming chapter on random linear combinations.</p>
<p>Furthermore, some useful problems can be directly encoded as an inner product, notably the range proof or the subset sum problem. In those situations, we can skip encoding the problem as an <a href="https://rareskills.io/post/arithmetic-circuit">arithmetic circuit</a> and directly encode it as an inner product. To increase the flexibility of our inner product representation, as well as to lay the ground work for understanding random linear combinations, we will learn some inner product algebra in the next chapter.</p>
<p><strong>Exercise:</strong> Combine the previous exercises to prove that $A =\langle\mathbf{a},\mathbf{G}\rangle + \langle\mathbf{b},\mathbf{H}\rangle + \alpha B$ where $\mathbf{a}$ and $\mathbf{b}$ are vectors of length 4. Your proof should be both succinct and zero knowledge. Create an interactive proof for the sake of simplicity. Refer to <a href="https://github.com/RareSkills/ZK-bulletproofs">this repository</a> for the exercises.</p>
<div style='page-break-after: always;'></div>

<h1>Inner Product Algebra</h1>
<p>Source: https://rareskills.io/post/inner-product-algebra</p>
<h1>Inner Product Algebra</h1>
<p>In this article, we give some useful algebraic tricks for inner products that will be useful in deriving range proofs (and encoding circuits as inner products) later. Each rule will be accompanied by a simple proof.</p>
<h2>Notation</h2>
<p>Variables in bold, like $\mathbf{a}$, denote a vector. Variables not in bold, like $v$, denote a scalar. The operator $\circ$ is the Hadamard product (elementwise multiplication) of two vectors, i.e. $[a_1, \dots, a_n]\circ[b_1, \dots, b_n] = [a_1b_1, \dots, a_nb_n]$. We use the shorthand “lhs” and “rhs” to refer to the “left-hand side” and “right-hand side” of an equation, respectively. A “summand” is an element of an addition, e.g. if $a + b = c$, then $a$ and $b$ would be called summands. The $\mathbf{1}$ vector is a vector of all ones, i.e. $[1, 1, \dots, 1]$. All vectors are implied to be of the same length $n$ unless otherwise stated.</p>
<h2>Rule 1: An inner product where one of the vectors is a sum of vectors can be expanded</h2>
<p>Suppose we’re calculating an inner product where one of the vectors is a sum of two vectors – for example $\langle\mathbf{a} + \mathbf{b}, \mathbf{c}\rangle$. We can split this up into the sum of two inner products:<br />
$\langle \mathbf{a} + \mathbf{b}, \mathbf{c} \rangle = \langle \mathbf{a}, \mathbf{c}\rangle + \langle \mathbf{b}, \mathbf{c} \rangle$</p>
<p>Proof:<br />
The lhs can be written as<br />
$$<br />
\sum_{i=1}^n(a_i+b_i)c_i<br />
$$</p>
<p>The rhs can be written as</p>
<p>$$<br />
\begin{align*}<br />
\sum_{i=1}^na_ic_i+\sum_{i=1}^nc_ib_i &amp;=\sum_{i=1}^n(a_ic_i+c_ib_i) \<br />
&amp;=\sum_{i=1}^n(a_i+b_i)c_i<br />
\end{align*}<br />
$$</p>
<h2>Rule 2: Inner products with common terms can be combined</h2>
<p>The two inner products on the lhs below have a common vector of $\mathbf{c}$. Therefore, they can be combined:<br />
$$\langle \mathbf{a}, \mathbf{c}\rangle + \langle \mathbf{b}, \mathbf{c} \rangle = \langle \mathbf{a} + \mathbf{b}, \mathbf{c} \rangle$$</p>
<p>This is really Rule 1 with the lhs and the rhs swapped.</p>
<p>The proof is the same as Rule 1.</p>
<h2>Rule 3: Moving vectors to the other side of the inner product</h2>
<p>An inner product can be re-written as the $\mathbf{1}$ vector with the Hadamard product of the original vectors:<br />
$$\langle \mathbf{a}, \mathbf{b} \rangle= \langle \mathbf{1}, \mathbf{a\circ b} \rangle$$</p>
<p>Proof:</p>
<p>$$\begin{align*}<br />
\langle \mathbf{a}, \mathbf{b} \rangle&amp;=\sum_{i=1}^na_ib_i \<br />
\langle \mathbf{1}, \mathbf{a\circ b} \rangle&amp;=\sum_{i=1}^n1*(a_ib_i)\<br />
\sum_{i=1}^na_ib_i &amp;= \sum_{i=1}^n1*(a_ib_i)\<br />
\end{align*}$$</p>
<h2>Rule 4: We can add vectors to one of the terms of the inner product to force two inner products to have common terms</h2>
<p>Suppose we’re adding an inner product $\langle\mathbf{x}, \mathbf{b}+\mathbf{c}\rangle$ and an inner product $\langle\mathbf{y}, \mathbf{b}\rangle$, and the sum of the inner products is $v$. Note that they have different components, so we can’t add them with Rule 2. Nevertheless, the following equality</p>
<p>$$\langle \mathbf{x}, \mathbf{b} + \mathbf{c}\rangle + \langle \mathbf{y}, \mathbf{b}\rangle = v$$</p>
<p>can be written as</p>
<p>$$\langle \mathbf{x} + \mathbf{y}, \mathbf{b} + \mathbf{c}\rangle = v + \langle\mathbf{y},\mathbf{c}\rangle$$</p>
<p>In the above scenario, we can add $\langle\mathbf{y},\mathbf{c}\rangle$ to both sides.</p>
<p>$$\begin{align*}<br />
\langle \mathbf{x}, \mathbf{b} + \mathbf{c}\rangle + \langle \mathbf{y}, \mathbf{b}\rangle + \boxed{\langle\mathbf{y},\mathbf{c}\rangle}&amp;= v + \boxed{\langle\mathbf{y},\mathbf{c}\rangle}\<br />
\langle \mathbf{x}, \mathbf{b} + \mathbf{c}\rangle + \langle \mathbf{y}, \mathbf{b}\rangle + \langle\mathbf{y},\mathbf{c}\rangle&amp;= v + \langle\mathbf{y},\mathbf{c}\rangle<br />
\end{align*}$$</p>
<p>We now have common $\mathbf{y}$ terms we can combine using Rule 2:</p>
<p>$$\begin{align*}<br />
\langle \mathbf{x}, \mathbf{b} + \mathbf{c}\rangle + \langle \mathbf{\fbox{y}}, \mathbf{b}\rangle + \langle\mathbf{\fbox{y}},\mathbf{c}\rangle&amp;= v + \langle\mathbf{y},\mathbf{c}\rangle\<br />
\langle \mathbf{x}, \mathbf{b} + \mathbf{c}\rangle + \langle \mathbf{\fbox{y}}, \mathbf{b} + \mathbf{c}\rangle &amp;= v + \langle\mathbf{y},\mathbf{c}\rangle\<br />
\langle \mathbf{x}, \mathbf{b} + \mathbf{c}\rangle + \langle \mathbf{y}, \mathbf{b} + \mathbf{c}\rangle &amp;= v + \langle\mathbf{y},\mathbf{c}\rangle\<br />
\end{align*}$$</p>
<p>Now that we have forced the two inner products to have common term $\langle \mathbf{b} + \mathbf{c} \rangle$ on the lhs, we can combine them into one vector using Rule 2 again:</p>
<p>$$\begin{align*}<br />
\langle \mathbf{x}, \boxed{\mathbf{b} + \mathbf{c}}\rangle + \langle \mathbf{y}, \boxed{\mathbf{b} + \mathbf{c}}\rangle &amp;= v + \langle\mathbf{y},\mathbf{c}\rangle\<br />
\langle \mathbf{x} + \mathbf{y}, \boxed{\mathbf{b} + \mathbf{c}}\rangle &amp;= v + \langle\mathbf{y},\mathbf{c}\rangle\<br />
\langle \mathbf{x} + \mathbf{y}, \mathbf{b} + \mathbf{c}\rangle &amp;= v + \langle\mathbf{y},\mathbf{c}\rangle\<br />
\end{align*}$$</p>
<p>Therefore,</p>
<p>$$\langle \mathbf{x}, \mathbf{b} + \mathbf{c}\rangle + \langle \mathbf{y}, \mathbf{b}\rangle = v$$</p>
<p>can be rewritten as</p>
<p>$$\langle \mathbf{x} + \mathbf{y}, \mathbf{b} + \mathbf{c}\rangle = v + \langle\mathbf{y},\mathbf{c}\rangle$$</p>
<h2>Rule 5: Adding two inner products with unrelated vectors</h2>
<p>We can add $\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle$ (which have no vectors in common) and obtain:</p>
<p>$$\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle=\langle\mathbf{a}_1+\mathbf{a}_2,\mathbf{b}_1+\mathbf{b}_2\rangle-\langle\mathbf{a_1},\mathbf{b_2}\rangle-\langle\mathbf{a_2},\mathbf{b_1}\rangle$$</p>
<p>Proof:</p>
<p>$$\begin{align*}<br />
\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle&amp;=\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle\<br />
\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle+\langle\mathbf{a}_1,\mathbf{b}_2\rangle&amp;=\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle+\langle\mathbf{a}_1,\mathbf{b}_2\rangle&amp;&amp;\text{add }\langle\mathbf{a}_1,\mathbf{b}_2\rangle \text{ to both sides}\<br />
\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle+\langle\mathbf{a}_1,\mathbf{b}_2\rangle&amp;=\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_1+\mathbf{a}_2,\mathbf{b}_2\rangle&amp;&amp;\text{combine }\mathbf{b}_2 \text{ terms}\<br />
\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle+\langle\mathbf{a}_1,\mathbf{b}_2\rangle+\langle\mathbf{a}_2,\mathbf{b}_1\rangle&amp;=\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_1+\mathbf{a}_2,\mathbf{b}_2\rangle+\langle\mathbf{a}_2,\mathbf{b}_1\rangle&amp;&amp;\text{add }\langle\mathbf{a}_2,\mathbf{b}_1\rangle\text{ to both sides}\<br />
\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle+\langle\mathbf{a}_1,\mathbf{b}_2\rangle+\langle\mathbf{a}_2,\mathbf{b}_1\rangle&amp;=\langle\mathbf{a}_1+\mathbf{a}_2,\mathbf{b}_1\rangle+\langle\mathbf{a}_1+\mathbf{a}_2,\mathbf{b}_2\rangle&amp;&amp;\text{combine }\mathbf{b}_1\text{ terms}\<br />
\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle+\langle\mathbf{a}_1,\mathbf{b}_2\rangle+\langle\mathbf{a}_2,\mathbf{b}_1\rangle&amp;=\langle\mathbf{a}_1+\mathbf{a}_2,\mathbf{b}_1+\mathbf{b}_2\rangle&amp;&amp;\text{combine right-hand side}\<br />
\langle\mathbf{a}_1,\mathbf{b}_1\rangle+\langle\mathbf{a}_2,\mathbf{b}_2\rangle&amp;=\langle\mathbf{a}_1+\mathbf{a}_2,\mathbf{b}_1+\mathbf{b}_2\rangle-\langle\mathbf{a}_1,\mathbf{b}_2\rangle-\langle\mathbf{a}_2,\mathbf{b}_1\rangle&amp;&amp;\text{subtract }\langle\mathbf{a}_1,\mathbf{b}_2\rangle+\langle\mathbf{a}_2,\mathbf{b}_1\rangle<br />
\end{align*}$$</p>
<p>The proof illustrates that it may be handy sometimes to be creative about finding inner products to add to both sides of the equation.</p>
<h2>Rule 6: Scalars can be brought inside and outside of an inner product</h2>
<p>$z\cdot\langle\mathbf{a},\mathbf{b}\rangle = \langle z\cdot\mathbf{a},\mathbf{b}\rangle = \langle\mathbf{a},z\cdot\mathbf{b}\rangle$</p>
<p>The proof for this statement is left as an exercise for the reader. As a hint, constant terms can be brought in and out of a summation.</p>
<p>This tutorial is part of our series on <a href="staging.rareskills.io/post/bulletproofs-zk">ZK Bulletproofs</a>.</p>
<div style='page-break-after: always;'></div>

<h1>Reducing the number of equality checks (constraints) through random linear combinations</h1>
<p>Source: https://rareskills.io/post/random-linear-combination</p>
<h1>Reducing the number of equality checks (constraints) through random linear combinations</h1>
<p>Random linear combinations are a common trick in zero knowledge proof algorithms to enable $m$ equality checks to be probabilistically checked with a single equality check. Suppose we have $m$ inner products we are trying to prove. Instead of creating $m$ proofs, we create a random linear combination of the equalities and prove that.</p>
<h2>Equality of Pedersen Commitments</h2>
<p>First, let’s consider how we might prove the equality of multiple Pedersen commitments.</p>
<p>If we have elliptic curve points $G$ and $B$ with unknown discrete logs, and blinding terms $\alpha$ and $\beta$ we can construct <a href="https://rareskills.io/post/pedersen-commitment">Pedersen committments</a> $L$ and $R$ where</p>
<p>$L = aG + \alpha B$<br />
$R = bG + \beta B$</p>
<p>The verifier can check if $a = b$ if the prover provides the difference in the blinding terms. The verifier cannot simply check $L = R$ because the blinding terms will generally not be equal to each other, i.e. $\alpha \neq \beta$.</p>
<p>If the prover wishes to convince the verifier that $a$ and $b$ are committed to $L$ and $R$ respectively, but without revealing $a$ and $b$, the prover can compute</p>
<p>$\pi = \alpha – \beta$</p>
<p>And give $\pi$ to the verifier. The verifier computes</p>
<p>$L \stackrel{?}{=} R + \pi B$</p>
<p>Under the hood, this expands to</p>
<p>$(aG + \alpha B) = (bG + \beta B) + (\alpha – \beta) B$</p>
<p>All the blinding terms will cancel out leaving $aG \stackrel{?}{=} bG$.</p>
<p>But suppose the prover wishes to establish equality for several commitments, i.e. $L_1 = L_2, L_2 = R_2, …, L_m = R_m$. The naïve solution is to send $m$ blinding terms $\pi_1,…,\pi_m$ and the verifier will run $m$ equality checks. This will require sending $m$ field elements ($\pi_1,…,\pi_m$) and the verifier’s algorithm will run in $\mathcal{O}(m)$ time.</p>
<h2>Why the prover cannot simply add up all the commitments</h2>
<p>Suppose we have $l_1, l_2, r_1, r_2$ with commitments $L_1, L_2, R_1, R_2$ respectively. Suppose also the prover wants to show that $l_1 = r_1$ and $l_2 = r_2$ without revealing them.</p>
<p>The following check is not secure:</p>
<p>$$L_1 + L_2 = R_1 + R_2 + \pi B$$</p>
<p>where $\pi$ is the difference in the blinding terms. As a counterexample, consider the case where $l_1 = 1, r_1 = 2, l_2 = 2, r_2 = 1$. The <em>sums</em> are balanced, but the original claim is incorrect.</p>
<h2>Random linear combinations</h2>
<p>However, if the prover is required to show that</p>
<p>$$L_1 + L_2z = R_1 + R_2z + \pi B$$</p>
<p>for a random value $z$ they cannot predict, then the scheme is secure.</p>
<p>Specifically, the prover and verifier do the following algorithm:</p>
<h3>Randomized proof of equality</h3>
<h4>Setup</h4>
<p>The prover and verifier agree on elliptic curve points $G$ and $B$, where the discrete logs are unknown.</p>
<h4>Prover sends commitments</h4>
<p>The prover generates blinding terms $\alpha_1, \alpha_2, \beta_1, \beta_2$ and creates the Pedersen commitments</p>
<p>$L_1 = l_{1}G + \alpha_1 B$ \<br />
$R_1 = r_{1}G + \beta_1 B$ \<br />
$L_2 = l_{2}G + \alpha_2 B$ \<br />
$R_2 = r_{2}G + \beta_2 B$</p>
<p>and sends $(L_1, L_2, R_1, R_2)$ to the verifier.</p>
<h4>Verifier picks a random $z$</h4>
<p>The verifier chooses a random field element $z$ and sends it to the prover.</p>
<h4>Prover computes the difference in blinding terms</h4>
<p>The prover computes $\pi = \alpha_1+\alpha_2\cdot z-\beta_1-\beta_2\cdot z$ and sends $\pi$ to the verifier.</p>
<h4>Final verification check</h4>
<p>The verifier checks that</p>
<p>$$L_1+L_2z\stackrel{?}=R_1+R_2z+\pi B$$</p>
<h3>Security analysis</h3>
<p>If $l_1 = r_1$ and $l_2 = r_2$ then the equation will be balanced regardless of the choice of $z$, assuming the prover computed $\pi$ correctly.</p>
<p>Now suppose $l_1\neq r_1$ or $l_2 \neq r_2$. The prover still will not be able to produce a valid $\pi$ because doing so would require solving for the discrete logs of $G$ and $B$.</p>
<h2>Generalizing to $m$ checks</h2>
<p>If we have $m$ equality checks, $L_1 = R_1, L_2 = R_2, …, L_m = R_m$, the verifier could send $m$ random elements $z_1,\dots,z_m$ and the prover could provide $\pi$ such that</p>
<p>$L_1 + L_2z_1 + L_3z_2 + … L_mz_{m-1} \stackrel{?}{=}R_1 + R_2z_1+R_3z_2+\dots+R_mz_{m-1} + \pi B$</p>
<p>However, this requires the verifier to send $m$ elements, leading to a linear communication overhead. The communication overhead can be reduced to constant if the verifier only sends $z$ and the prover and verifier separate the commitments by successive powers of $z$:</p>
<p>$L_1 + L_2z + L_3z^2 + … L_mz^{m-1} \stackrel{?}{=}R_1+R_2z+R_3z^2\dots+R_mz^{m-1} + \pi B$</p>
<h3>Security analysis</h3>
<p>The left-hand-side and right-hand-side are both polynomials of degree $m-1$. If they are unequal to each other, then they intersect in at most $m-1$ points by the <a href="https://rareskills.io/post/schwartz-zippel-lemma">Schwartz Zippel Lemma</a>. If $m\ll p$ where $p$ is the order of the finite field, then again the probability of $z$ being an intersection point is negligible.</p>
<h2>Random linear combinations of inner products</h2>
<p>We can generalize the above technique to combine multiple inner products together.</p>
<p>Suppose we have two inner products</p>
<p>$\langle \mathbf{a}_L, \mathbf{a}_R\rangle = v_1$ and $\langle \mathbf{a}_L, \mathbf{a}_W\rangle=v_2$</p>
<p>Because the two inner products share a common term, it is algebraically possible to combine them as follows:</p>
<p>$\langle\mathbf{a}_L, \mathbf{a}_R + \mathbf{a}_W\rangle = v_1 + v_2$</p>
<p>However, this is not secure from a soundness perspective because it is possible that $\langle \mathbf{a}_L, \mathbf{a}_R\rangle \neq v_1$ and $\langle \mathbf{a}_L, \mathbf{a}_W\rangle\neq v_2$ but $\langle\mathbf{a}_L, \mathbf{a}_R + \mathbf{a}_W\rangle = v_1 + v_2$.</p>
<p>As expected, we can solve this by using a random linear combination.</p>
<p>$$\begin{align*}<br />
&amp;\langle \mathbf{a}_L, \mathbf{a}_R\rangle = v_1 &amp;&amp;\text{ // first inner product}\<br />
&amp;z\langle \mathbf{a}_L, \mathbf{a}_W\rangle=z\cdot v_2 &amp;&amp;\text{ // second inner product}\<br />
&amp;\langle \mathbf{a}_L, z\cdot\mathbf{a}_W\rangle=z\cdot v_2 &amp;&amp;\text{ // bring } z \text{ inside}\<br />
&amp;\langle\mathbf{a}_L, \mathbf{a}_R + z\cdot\mathbf{a}_W\rangle = v_1 + z\cdot v_2&amp;&amp;\text{ // combine into one inner product}<br />
\end{align*}<br />
$$</p>
<p>We only need to create an inner product proof for a single inner product instead of two. It is crucial that the prover receives $z$ after they have sent the relevant commitments, but we leave the exact details for the next chapter when we see an example of an algorithm using this technique: range proofs.</p>
<p><em>This tutorial is part of a series on <a href="staging.rareskills.io/post/bulletproofs-zk">ZK Bulletproofs</a>.</em></p>
<div style='page-break-after: always;'></div>

<h1>Range Proof</h1>
<p>Source: https://rareskills.io/post/range-proof</p>
<h1>Range Proof</h1>
<p>A range proof in the context of inner product arguments is a proof that the scalar $v$ has been committed to $V$ and $v$ is less than $2^n$ for some non-negative integer $n$.</p>
<p>This article shows how the Bulletproofs paper constructs such a proof. The high level idea is that if we can prove that a vector $\mathbf{a}_L$ consists only of ones and zeros and that $\mathbf{a}_L$ is the binary representation of $v$, then $v$ must be less than $2^n$. This is analogous to saying that a number that fits in an 8 bit unsigned integer must be less than 256.</p>
<p>The advantage of using Bulletproofs for range proofs is that the range proof can be directly constructed without the need of an <a href="https://rareskills.io/post/arithmetic-circuit">arithmetic circuit</a>.</p>
<p><a href="https://github.com/monero-project/monero/blob/master/src/ringct/bulletproofs.cc">Monero uses Bulletproof Range Proofs</a> (the algorithm presented here) to ensure that the sum of transactions is not negative (in a <a href="https://rareskills.io/post/finite-fields">finite field</a>, the negative numbers are the elements greater than $p/2$ as they are additive inverses of the elements less than or equal $p/2$ where $p$ is the field order).</p>
<p><em>This article is part of a series on <a href="staging.rareskills.io/post/bulletproofs-zk">ZK Bulletproofs</a>.</em></p>
<h2>Notation</h2>
<p>$\mathbf{0}^n$ is an $n$ dimensional vector of all zeros.</p>
<p>$\mathbf{1}^n$ is an $n$ dimensional vector of all ones.</p>
<p>$\mathbf{2}^n$ is an $n$ dimensional vector $[1,2,4,8,…,2^{n-1}]$</p>
<p>$\mathbf{y}^n$ is an $n$ dimensional vector $[1, y, y^2, y^3, …, y^{n-1}]$</p>
<p>$\mathbf{y}^{-n}$ is an $n$ dimensional vector $[1, y^{-1}, y^{-2}, …, y^{-(n-1)}]$</p>
<p>Note that $\mathbf{y}^n\circ\mathbf{y}^{-n}=\mathbf{1}^n$.</p>
<h2>Range proof overview</h2>
<p>Proving that $V$ is a commitment to a scalar with a value less than $2^n$ requires proving the following:</p>
<ol>
<li>$\mathbf{a}_L$ is binary (only holds values $0$ and $1$).</li>
<li>The inner product $\langle \mathbf{a}_L,\mathbf{2}^n\rangle=v$.</li>
</ol>
<p>The second point is easy to prove, we do a normal inner product proof then reveal $\mathbf{2}^n$ is one of the vectors in the commitment — or have the verifier construct the commitment of $\mathbf{2}^n$ themselves. However, proving that $\mathbf{a}_L$ is binary without an arithmetic circuit requires a couple algebraic tricks.</p>
<h2>Four useful tricks</h2>
<p>The bulletproofs paper implicitly uses four algebraic tricks that are best taught explicitly before looking at the range proof algorithm directly.</p>
<h3>1. Proving $\mathbf{a}_L$ is binary</h3>
<p>The statement $\mathbf{a}_L$ is binary is equivalent to the following two assertions:</p>
<ul>
<li>$\mathbf{a}_R = \mathbf{a}_L-\mathbf{1}^n$</li>
<li>$\mathbf{a}_L \circ \mathbf{a}_R = \mathbf{0}^n$</li>
</ul>
<p>For example, if $\mathbf{a}_L = [1,0,0,1]$ then $\mathbf{a}_R=[0,-1,-1,0]$.</p>
<p>In this case, $\mathbf{a}_L \circ\mathbf{a}_R=\mathbf{0}^n$ because</p>
<p>$$[1,0,0,1] \circ [0,-1,-1,0] = [0,0,0,0] = \mathbf{0}^n$$</p>
<p>Now consider a case where $\mathbf{a}_L$ is not binary, for example $[2, 1, 0, 0]$. $\mathbf{a}_R$ will be $[1,0,-1,-1]$ The Hadamard product of $\mathbf{a}_L$ and $\mathbf{a}_R$ will be $[2,0,0,0] \neq \mathbf{0}^n$.</p>
<p>More generally, if $\mathbf{a}_L$ has a non-binary entry, that entry will be subtracted by $1$, and the resulting entry in $\mathbf{a}_R$ will be non-zero. When the Hadamard product is computed, then at that particular index, $\mathbf{a}_L$ and $\mathbf{a}_R$ will both be non-zero and the product will be non-zero, meaning $\mathbf{a}_L \circ \mathbf{a}_R \neq \mathbf{0}^n$.</p>
<p>However, if a particular entry in $\mathbf{a}_L$ is $1$, then $\mathbf{a}_R$ will be $0$ at that index so that the Hadamard product at that index will be zero, too.</p>
<p>Finally, if a particular entry in $\mathbf{a}_L$ is $0$, then $\mathbf{a}_R$ will be $-1$ at that index and their element-wise product will still be zero at that index.</p>
<p>Therefore, if $\mathbf{a}_L$ is binary and $\mathbf{a}_R$ is computed as $\mathbf{a}_R = \mathbf{a}_L-\mathbf{1}$, then $\mathbf{a}_L \circ \mathbf{a}_R = \mathbf{0}^n$.</p>
<h3>2. Proving a vector is all zero</h3>
<p>Suppose we wish to prove that the Pedersen commitment $A$ holds a zero vector. We create the Pedersen commitment $A = \langle\mathbf{a},\mathbf{G}\rangle + \alpha B$ and wish to prove to a verifier that $\mathbf{a}=\mathbf{0}^n$.</p>
<p>It might seem sufficient to simply send the blinding term $\alpha$, but to make our solution more composable, we do not want to reveal the blinding term because that might affect other commitments we have created.</p>
<p>Instead, the prover sends $A$ to the verifier, and the verifier responds a vector full of random values $\mathbf{r}$. The prover must now prove that</p>
<p>$$\langle\mathbf{a},\mathbf{r}\rangle = 0$$</p>
<p>Note that this is a probabilistic test. It is possible, with negligible probability, that $\langle\mathbf{a},\mathbf{r}\rangle=0$ for $\mathbf{a}\neq\mathbf{0}^n$, but it is not possible for the prover to forge such an $\mathbf{a}$ because they do not know in advance what $\mathbf{r}$ will be.</p>
<p>However, transmitting $\mathbf{r}$ requires $\mathcal{O}(n)$ communication overhead, so the verifier instead only sends a single random element $y$ and the prover computes $\mathbf{y}^n$ and uses $\mathbf{y}^n$ as a the random vector.</p>
<p>Then, the prover proves that $\langle\mathbf{a},\mathbf{y}^n\rangle=0$.</p>
<h3>3. Proving an inner product is of the form $\langle\mathbf{a}_L,\mathbf{a}_R\circ\mathbf{y}^n\rangle$ where $y$ is chosen by the verifier and the prover computes $\mathbf{y}^n$</h3>
<p>We don’t yet have a mechanism to prove that $\mathbf{a}_L\circ\mathbf{a}_R=\mathbf{0}^n$, as that is a Hadamard product, not an inner product. However, stating the vector $\mathbf{a}_L\circ\mathbf{a}_R$ is identically $\mathbf{0}^n$ is the same as stating that $\langle\mathbf{a}_L\circ\mathbf{a}_R,\mathbf{y}^n\rangle=0$. By the inner product rules, we can move $\mathbf{a}_R$ to the other side of the inner product and we now have $\langle\mathbf{a}_L,\mathbf{a}_R\circ\mathbf{y}^n\rangle=0$.</p>
<p>The verifier will receive commitments to $\mathbf{a}_L$ and $\mathbf{a}_R$, not $\mathbf{a}_R\circ\mathbf{y}^n$. It will be up to the verifier to construct a commitment to $\mathbf{a}_R\circ\mathbf{y}^n$ so they are convinced the prover used $\mathbf{a}_R\circ\mathbf{y}^n$ as the second vector in the inner product.</p>
<p>The key trick we rely on is that the prover uses the basis vectors $\mathbf{G}$ and $\mathbf{H}$ to commit their vectors, but the verifier uses $\mathbf{G}$ and $\mathbf{H}\circ\mathbf{y}^{-n}$.</p>
<p>When the prover sends the evaluation $\mathbf{r}_u$, the prover must ensure that $\mathbf{y}^n$ terms will cancel with the $\mathbf{y}^{-n}$ in the verifier’s basis vector $\mathbf{y}^{-n}\circ\mathbf{H}$.</p>
<p>Specifically, the prover constructs the commitments</p>
<p>$$\begin{align*}<br />
A &amp;= \langle\mathbf{a}_L,\mathbf{G}\rangle+\langle\mathbf{a}_R,\mathbf{H}\rangle+\alpha B\<br />
S &amp;= \langle\mathbf{s}_L,\mathbf{G}\rangle+\langle\mathbf{s}_R,\mathbf{H}\rangle+\beta B<br />
\end{align*}<br />
$$</p>
<p>And sends $(A, S)$ to the verifier. There is no need to commit and send $V$ because it is zero in this case.</p>
<p>The prover’s polynomials will be</p>
<p>$$\begin{align*}<br />
\mathbf{l}(x)&amp;=\mathbf{a}_L + \mathbf{s}_Lx\<br />
\mathbf{r}(x)&amp;=\mathbf{a}_R\circ\mathbf{y}^n + \boxed{\mathbf{s}_R\circ\mathbf{y}^n}x<br />
\end{align*}<br />
$$</p>
<p>Crucially, the prover has Hadamard multiplied $\mathbf{s}_R$ by $\mathbf{y}^n$. Previously, $\mathbf{r}(x)$ was computed as $\mathbf{r}(x)=\mathbf{a}_R\circ\mathbf{y}^n + \mathbf{s}_Rx$ (without the $\mathbf{y}^n\circ\mathbf{s}_R$. This will later allow all of the $\mathbf{y}^n$ terms to be canceled when the verifier computes the commitment $\langle\mathbf{r}_u,\mathbf{y}^{-n}\circ\mathbf{H}\rangle$. Under the hood, $\mathbf{r}_u$ is $(\mathbf{a}_R+\mathbf{s}_Ru)\circ\mathbf{y}^n$ so the $\mathbf{y}^n$ will cancel when the verifier computes $\langle(\mathbf{a}_R+\mathbf{s}_Ru)\circ\mathbf{y}^n,\mathbf{y}^{-n}\circ\mathbf{H}\rangle$, i.e.</p>
<p>$$\begin{align*}<br />
&amp;\langle(\mathbf{a}_R+\mathbf{s}_Ru)\circ\mathbf{y}^n,\mathbf{y}^{-n}\circ\mathbf{H}\rangle\<br />
&amp;=\langle(\mathbf{a}_R+\mathbf{s}_Ru),\mathbf{y}^n\circ\mathbf{y}^{-n}\circ\mathbf{H}\rangle\<br />
&amp;=\langle(\mathbf{a}_R+\mathbf{s}_Ru),\mathbf{1}^n\circ\mathbf{H}\rangle\<br />
&amp;=\langle(\mathbf{a}_R+\mathbf{s}_Ru),\mathbf{H}\rangle<br />
\end{align*}$$</p>
<p>However, the prover cannot compute $\mathbf{l}(x)$ or $\mathbf{r}(x)$ yet because the verifier hasn’t sent $y$ yet. Therefore, after receiving $(A, S)$ the verifier sends $y$ and the prover computes $\mathbf{y}^n$ and computes the polynomial $t(x)$:</p>
<p>$$t(x)=\langle\mathbf{l}(x),\mathbf{r}(x)\rangle=\langle\mathbf{a}_L,\mathbf{a}_R\circ\mathbf{y}^n\rangle+t_1x+t_2x^2$$</p>
<p>where</p>
<p>$$\begin{align*}<br />
t_1&amp;=\langle\mathbf{a}_L,\mathbf{s}_R\circ\mathbf{y}^n\rangle + \langle\mathbf{s}_L,\mathbf{a}_R\circ\mathbf{y}^n\rangle\<br />
t_2&amp;=\langle\mathbf{s}_L,\mathbf{s}_R\circ\mathbf{y}<br />
^n\rangle<br />
\end{align*}$$</p>
<p>The prover commits to the coefficients $t_1$ and $t_2$ as</p>
<p>$$\begin{align*}<br />
T_1&amp;=t_1G+\tau_1B\<br />
T_2&amp;=t_2G+\tau_2B<br />
\end{align*}$$</p>
<p>and sends $(T_1, T_2)$ to the verifier. The verifier responds with $u$ and the prover evaluates the vector polynomials $\mathbf{l}(x)$ and $\mathbf{r}(x)$:</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{l}_u&amp;= \mathbf{a}_L\circ\mathbf{y}^n+(\mathbf{s}_L\circ\mathbf{y}^n)u\<br />
\mathbf{r}_u&amp;= \mathbf{a}_R+\mathbf{s}_Ru\<br />
t_u&amp;=\langle\mathbf{l}_u,\mathbf{r}_u\rangle\<br />
\pi_{lr}&amp;=\alpha + \beta u\<br />
\pi_t&amp;=\tau_1u+\tau_2u^2<br />
\end{align*}<br />
$$</p>
<p>Note that $\pi_t$ only includes the blinding terms for $t_1$ and $t_2$. In the previous implementation, $\pi_t$ was computed as $\gamma + \tau_1u+\tau_2u^2$, where $\gamma$ is the blinding term for $V$, which is also the constant coefficient of the polynomial $t(x)$.</p>
<p>There is no blinding term $\gamma$ because there is no commitment to $V$, i.e. $v$ is not secret — it is $0$. The prover sends $(\mathbf{l}_u, \mathbf{r}_u, t_u, \pi_{lr}, \pi_t)$ and the verifier checks that:</p>
<p>$$<br />
\begin{align*}<br />
t_u&amp;\stackrel{?}=\langle\mathbf{l}_u,\mathbf{r}_u\rangle\<br />
A+Su&amp;\stackrel{?}=\langle\mathbf{l}_u,\mathbf{G}\rangle+\langle\mathbf{r}_u,\mathbf{y}^{-n}\circ\mathbf{H}\rangle+\pi_{lr}B\<br />
t_uG&amp;\stackrel{?}{=} T_1 u + T_2 u^2 – \pi_t B\<br />
\end{align*}<br />
$$</p>
<p>The first crucial difference is that the commitment to $\mathbf{r}_u$ is done with respect to the basis vector $\mathbf{y}^{-n}\circ\mathbf{H}$ instead of $\mathbf{H}$ for the reasons discussed earlier.</p>
<p>Second, $t_uG\stackrel{?}{=} T_1 u + T_2 u^2 + \pi_t B$ has no constant commitment. Normally, the equation is $t_uG\stackrel{?}{=} \boxed{V}+T_1 u + T_2 u^2 + \pi_t B$, but $V$ is a commitment to $0$ in this case.</p>
<p>In general, if $V$ contains values known to the verifier, the verifier can construct the commitment to $V$ as we show in the next section.</p>
<h3>4. Proving an inner product when an additive public constant is involved</h3>
<p>As alluded in the section above, the verifier can reconstruct commitments if the verifier knows the underlying vector.</p>
<p>For example, suppose we are proving that</p>
<p>$\langle\mathbf{a}_L + \mathbf{j},\mathbf{a}_R\circ\mathbf{y}^n + \mathbf{k}\rangle=vz$</p>
<p>where $\mathbf{j}$ and $\mathbf{k}$ are a vectors known to the verifier and $z$ is a scalar known to the verifier in advance. Unlike $\mathbf{y}^n$, these vectors and scalar are known before the proof begins. Note that $\mathbf{k}$ is not Hadamard multiplied by $\mathbf{y}^n$ in this example.</p>
<p>The prover still only commits to the secret values $\mathbf{a}_L$, $\mathbf{a}_R$ and $v$ as usual:</p>
<p>$$\begin{align*}<br />
A &amp;= \langle\mathbf{a}_L,\mathbf{G}\rangle+\langle\mathbf{a}_R,\mathbf{H}\rangle+\alpha B\<br />
S &amp;= \langle\mathbf{s}_L,\mathbf{G}\rangle+\langle\mathbf{s}_R,\mathbf{H}\rangle+\beta B\<br />
V &amp;= vG + \gamma B<br />
\end{align*}$$</p>
<p>As usual, the polynomials $\mathbf{l}(x)$ and $\mathbf{r}(x)$ are such that the constant term is the vector from the original inner product and the linear terms are $\mathbf{s}_L$ and $\mathbf{s}_R$. Upon receiving $y$ from the verifier, the prover computes $\mathbf{y}^n$ and crafts but does not evaluate $\mathbf{l}(x)$ and $\mathbf{r}(x)$:</p>
<p>$$\begin{align*}<br />
\mathbf{l}(x)&amp;=\underbrace{\mathbf{a}_L + \mathbf{j}}_\text{left vector} + \mathbf{s}_Lx\<br />
\mathbf{r}(x)&amp;=\underbrace{\mathbf{a}_R\circ\mathbf{y}^n + \mathbf{k}}_\text{right vector} + \boxed{\mathbf{s}_R\circ\mathbf{y}^n}x<br />
\end{align*}<br />
$$</p>
<p>Note that $\mathbf{k}$ is not Hadamard multiplied with $\mathbf{y}^n$, but the linear term $\mathbf{s}_R$ still is. We will show how the verifier handles this later.</p>
<p>For now, we compute $t(x)$ as</p>
<p>$$t(x)=vz+t_1x+t_2x^2$$</p>
<p>where</p>
<p>$$\begin{align*}<br />
t_1&amp;=\langle\mathbf{a}_L+\mathbf{j},\mathbf{s}_R\circ\mathbf{y}^n\rangle + \langle\mathbf{s}_L,\mathbf{a}_R\circ\mathbf{y}^n+\mathbf{k}\rangle\<br />
t_2&amp;=\langle\mathbf{s}_L,\mathbf{s}_R\circ\mathbf{y}<br />
^n\rangle<br />
\end{align*}$$</p>
<p>Note that the constant term in $t(x)$ is $vz$ and not $v$. The commitments are computed as</p>
<p>$$\begin{align*}<br />
T_1 &amp;= t_1G+\tau_1B\<br />
T_2 &amp;= t_2G+\tau_2B\<br />
\end{align*}$$</p>
<p>and sent to the verifier who then sends the random value $u$.</p>
<p>The prover computes:</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{l}_u&amp;= \mathbf{a}_L\circ\mathbf{y}^n+\mathbf{j}+(\mathbf{s}_L\circ\mathbf{y}^n)u\<br />
\mathbf{r}_u&amp;= \mathbf{a}_R\circ\mathbf{y}^n+\mathbf{k}+\mathbf{s}_Ru\<br />
t_u&amp;=\langle\mathbf{l}_u,\mathbf{r}_u\rangle\<br />
\pi_{lr}&amp;=\alpha + \beta u\<br />
\pi_t&amp;=vz+\tau_1u+\tau_2u^2<br />
\end{align*}<br />
$$</p>
<p>Note that the constant term in $\pi_t$ is $\gamma z$. The prover sends $(\mathbf{l}_u,\mathbf{r}_u,t_u,\pi_{lr},\pi_t)$. Finally, the verifier computes:</p>
<p>$$<br />
\begin{align*}<br />
t_u&amp;\stackrel{?}=\langle\mathbf{l}_u,\mathbf{r}_u\rangle\<br />
A+Su+\underbrace{\langle\mathbf{j,\mathbf{G}}\rangle}_{\text{commitment to } \mathbf{j} \text{ in } \mathbf{l}(x)}+\underbrace{\langle\mathbf{k},\mathbf{y}^{-n}\circ\mathbf{H}\rangle}_{\text{commitment to } \mathbf{k} \text{ in } \mathbf{r}(x)}&amp;\stackrel{?}=\langle\mathbf{l}_u,\mathbf{G}\rangle+\langle\mathbf{r}_u,\mathbf{y}^{-n}\circ\mathbf{H}\rangle+\pi_{lr}B\<br />
t_uG&amp;\stackrel{?}{=} Vz+ T_1 u + T_2 u^2 – \pi_t B\<br />
\end{align*}<br />
$$</p>
<p>$\mathbf{l}_u$ and $\mathbf{r}_u$ contain $\mathbf{j}$ and $\mathbf{k}$ respectively, but $A$ and $S$ do not. Hence, the verifier computes commitments to those vectors and adds them to the commitments $A$ and $S$. In the case of $\mathbf{k}$, the basis vector $\mathbf{y}^{-n}\circ\mathbf{H}$ will cause $\mathbf{k}$ to become $\mathbf{k}\circ\mathbf{y}^{-n}$, so the commitment must be computed with respect to $\mathbf{y}^{-n}\circ\mathbf{H}$. Finally, the blinding term $\pi_t$ contains $vz$ but $V$ does not contain $z$. Therefore, the prover must multiply $V$ by $z$.</p>
<p>By computing $\langle\mathbf{j},\mathbf{G}\rangle$, $\langle\mathbf{k},\mathbf{y}^n\circ\mathbf{H}\rangle$ and $Vz$, the verifier can be sure the inner product computation actually included those terms.</p>
<h2>Range proof</h2>
<p>To prove that $V$ is a value less than $2^n$ we have three things to prove:</p>
<ul>
<li>the inner product $\langle \mathbf{a}_L,\mathbf{2}^n\rangle = V$, i.e. $\mathbf{a}_L$ is the binary representation of $v$</li>
<li>$\mathbf{a}_R=\mathbf{a}_L-\mathbf{1}$</li>
<li>$\mathbf{a}_L \circ \mathbf{a}_R = \mathbf{0}$</li>
</ul>
<p>The last two claims are not directly in the form of an inner product. However, we can modify them slightly to accomplish this. What we are really saying is that the vectors</p>
<ul>
<li>$\mathbf{a}_R – \mathbf{a}_L+\mathbf{1}$</li>
<li>$\mathbf{a}_L\circ\mathbf{a}_R$</li>
</ul>
<p>are both $\mathbf{0}^n$. We can use the trick from a previous section to prove that they are zero. That is, the the prover needs to establish that</p>
<p>$$\langle\mathbf{a}_L\circ\mathbf{a}_R,\mathbf{y}^n\rangle=\mathbf{0}$$</p>
<p>and</p>
<p>$$\langle \mathbf{a}_R – \mathbf{a}_L+\mathbf{1},\mathbf{y}^n\rangle=\mathbf{0}$$</p>
<p>where $\mathbf{y}^n$ is the random vector derived from the $y$ value sent from the verifier.</p>
<p>The original bulletproofs paper slightly modifies the first claim as follows so that we can use the third trick in the previous section:</p>
<p>$$\langle\mathbf{a}_L,\mathbf{a}_R\circ\mathbf{y}^n\rangle=\mathbf{0}$$</p>
<p>Therefore, the prover has three inner products to establish:</p>
<ol>
<li>$\langle \mathbf{a}_L,\mathbf{2}^n\rangle = v$</li>
<li>$\langle\mathbf{a}_L,\mathbf{a}_R\circ\mathbf{y}^n\rangle=\mathbf{0}^n$</li>
<li>$\langle\mathbf{a}_L -\mathbf{1}^n- \mathbf{a}_R,\mathbf{y}^n\rangle=\mathbf{0}^n$</li>
</ol>
<h3>Combining three inner products into one</h3>
<p>The three inner products can be combined into a single one using a random linear combination with randomness $z$ provided from the verifier.</p>
<p>$$z^2 \cdot \langle \mathbf{a_L}, 2^n \rangle + z^1 \cdot \langle \mathbf{a_L} – \mathbf{1}^n – \mathbf{a_R}, \mathbf{y}^n \rangle + z^0\langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle = z^2 \cdot v + z^1\cdot\mathbf{0}^n+z^0\cdot\mathbf{0}^n$$</p>
<p>$$z^2 \cdot \langle \mathbf{a_L}, 2^n \rangle + z \cdot \langle \mathbf{a_L} – \mathbf{1}^n – \mathbf{a_R}, \mathbf{y}^n \rangle + \langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle = z^2 \cdot v$$</p>
<p>With some very hefty inner product algebra, we can combine all the inner products as follows. We show the derivation in the appendix.</p>
<p>$$\left\langle \mathbf{a_L} – z \cdot \mathbf{1}^n, \mathbf{y}^n \circ \mathbf{a_R} + \mathbf{y}^n\cdot z + z^2 \cdot 2^n \right\rangle = z^2 \cdot v + (z – z^2) \cdot \langle \mathbf{1}^n, y^n \rangle – z^3 \langle \mathbf{1}^n, 2^n \rangle<br />
$$</p>
<p>The terms in boxes below contain values known to the verifier, so we will construct our verification algorithm to explicitly check for those values. That is, the verifier will compute commitments to the values in the boxed terms, not the prover:</p>
<p>$$\left\langle \mathbf{a_L} – \boxed{z \cdot \mathbf{1}^n}, \mathbf{y}^n \circ \mathbf{a_R} + \boxed{\mathbf{y}^n\cdot z + z^2 \cdot 2^n }\right\rangle = \boxed{z^2} \cdot v + \boxed{(z – z^2) \cdot \langle \mathbf{1}^n, y^n \rangle – z^3 \langle \mathbf{1}^n, 2^n \rangle}<br />
$$</p>
<p>To save space, the Bulletproofs paper refers to the term $(z – z^2) \cdot \langle \mathbf{1}^n, y^n \rangle – z^3 \langle \mathbf{1}^n, 2^n \rangle$ as $\delta(y,z)$, so the inner product can be written as</p>
<p>$$\left\langle \mathbf{a_L} – z \cdot \mathbf{1}^n, \mathbf{y}^n \circ \mathbf{a_R} + \mathbf{y}^n\cdot z + z^2 \cdot 2^n \right\rangle = z^2 \cdot v + \delta(y,z)<br />
$$</p>
<p>Note that $\delta(y,z)$ is a value the verifier can compute.</p>
<h2>Range Proof Algorithm</h2>
<p>The prover chooses $v$ and it’s binary representation $\mathbf{a}_L$ and computes $\mathbf{a}_R = \mathbf{a}_L – \mathbf{1}$.</p>
<p>The prover then randomly chooses the blinding term $\alpha$ and computes the combined commitment of $\mathbf{a}_L$ and $\mathbf{a}_R$ using basis vectors $\mathbf{G}$ and $\mathbf{H}$ as</p>
<p>$$A = \langle\mathbf{a}_L,\mathbf{G}\rangle+\langle\mathbf{a}_R,\mathbf{H}\rangle+\alpha B$$</p>
<p>The prover then chooses the linear terms of the soon-to-be-created vector polynomials $\mathbf{l}(x)$ and $\mathbf{r}(x)$ as $\mathbf{s}_L$ and $\mathbf{s}_R$ and commits to them</p>
<p>$$S = \langle\mathbf{s}_L,\mathbf{G}\rangle+\langle\mathbf{s}_R,\mathbf{H}\rangle+\beta B$$</p>
<p>The prover commits the inner product to $V$ as with respect to $G$ of an unknown discrete log (unrelated to $\mathbf{G}$):</p>
<p>$$V = vG+\gamma B$$</p>
<p>The prover sends $(A, S, V)$ to the verifier.</p>
<p>The verifier responds with random values $(y, z)$ which the prover will use to combine the three inner products into a single one.</p>
<p>$$\left\langle \mathbf{a_L} – z \cdot \mathbf{1}^n, \mathbf{y}^n \circ \mathbf{a_R} + \mathbf{y}^n\cdot z + z^2 \cdot 2^n \right\rangle = z^2 \cdot v + \delta(y,z)<br />
$$</p>
<p>The left part of the inner product $\mathbf{a}_L-z\cdot\mathbf{1}$ will be the constant term of $\mathbf{l}(x)$ and $\mathbf{a_R} + \mathbf{y}^n\cdot z + z^2 \cdot 2^n$ will be the constant term of $\mathbf{r}(x)$.</p>
<p>Thus, we construct $\mathbf{l}(x)$ as</p>
<p>$$\mathbf{l}(x)=\underbrace{\mathbf{a}_L-z\cdot\mathbf{1}}_\text{constant term}+\mathbf{s}_Lx$$</p>
<p>and we construct $\mathbf{r}(x)$ as</p>
<p>$$\mathbf{r}(x)=\underbrace{\mathbf{y}^n\circ(\mathbf{a}_R+z\cdot\mathbf{1})+z^2\mathbf{2}^n}_\text{constant term}+\mathbf{y}^n\circ\mathbf{s}_Rx$$</p>
<p>Note that we element-wise multiplied $\mathbf{s}_Rx$ with $\mathbf{y}^n$ for the reasons we discussed in part 3 of the prerequisites section above.</p>
<p>The prover can now construct $t(x) = \langle\mathbf{l}(x),\mathbf{r}(x)\rangle$ with constant coefficient $t_0$, linear coefficient $t_1$ and quadratic coefficient $t_2$ as:</p>
<p>$$\begin{align*}<br />
t_0 &amp;= \left\langle \mathbf{a_L} – z \cdot \mathbf{1}^n, \mathbf{y}^n \circ \mathbf{a_R} + \mathbf{y}^n\cdot z + z^2 \cdot 2^n \right\rangle\<br />
t_1 &amp;= \langle(\mathbf{a}_L-z\cdot\mathbf{1}),\mathbf{y}^n\circ\mathbf{s}_R\rangle+\langle\mathbf{y}^n\circ(\mathbf{a}_R+z\cdot\mathbf{1})+z^2\mathbf{2}^n,\mathbf{s_L}\rangle\<br />
t_2 &amp;= \langle\mathbf{s}_L,\mathbf{y}^n\circ\mathbf{s}_R\rangle<br />
\end{align*}<br />
$$</p>
<p>where $t(x) = t_0 + t_1x + t_2x^2$</p>
<p>The prover sends commitments to $t_1$ and $t_2$ as</p>
<p>$$<br />
\begin{align*}<br />
T_1 &amp;= t_1G+\tau_1B\<br />
T_2 &amp;= t_2G+\tau_2B<br />
\end{align*}<br />
$$</p>
<p>There is no need to commit to $t_0$ — observe that it is exactly the inner product we are trying to prove, so the verifier already has the commitment as $V$.</p>
<p>The verifier sends randomness $u$ and the prover computes</p>
<p>$$<br />
\begin{align*}<br />
\mathbf{l}_u &amp;= \mathbf{l}(u) \<br />
\mathbf{r}_u &amp;= \mathbf{r}(u) \<br />
t_u &amp;= \mathbf{t}(u)\<br />
\pi_{lr} &amp;=\alpha+\beta u\<br />
\pi_t &amp;= z^2\gamma + \tau_1u + \tau_2u^2\<br />
\end{align*}<br />
$$</p>
<p>Note that the constant term of $\pi_t$ is multiplied by $z^2$ to reflect the $z^2\cdot v$ term of the original inner product.</p>
<p>The verifier then computes a new basis vector $\mathbf{H}_{\mathbf{y}^{-1}}=\mathbf{y}^{-n}\circ\mathbf{H}$ and runs the following checks:</p>
<p>$t_u \stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{r}_u \rangle$</p>
<p>$A + Su + \boxed{\langle -z\cdot\mathbf{1}^n,\mathbf{G}\rangle} + \boxed{\langle z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle}\stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{G} \rangle + \langle \mathbf{r}_u, \mathbf{H}_{y^{-1}} \rangle + \pi_{lr} B$</p>
<p>$t_uG+\pi_tB=V\boxed{z^2}+\boxed{\delta(y,z)}\cdot G+T_1u+T_2u^2$</p>
<p>Recall that the prover did not commit the entire vectors they used for the left and right side of the inner product, but only $\mathbf{a}_L$ and $\mathbf{b}_L$. The rest of the vectors were additive public vectors known to the verifier, so the verifier reconstructed the commitments to the vectors by constructing commitments to the constant terms and adding them to the commitment of the secret vectors supplied by the prover.</p>
<p>By way of reminder, here is the original inner product with the values known to the verifier boxed:</p>
<p>$$\left\langle \mathbf{a_L} + \boxed{-z \cdot \mathbf{1}^n}, \mathbf{y}^n \circ \mathbf{a_R} + \boxed{\mathbf{y}^n\cdot z + z^2 \cdot 2^n }\right\rangle = \boxed{z^2} \cdot v + \boxed{\delta(y,z)}<br />
$$</p>
<p>The reader is encouraged to verify that the boxed terms (values known to the verifier) in the original product were reconstructed by the verifier in the boxed terms in the set of equality checks above.</p>
<p>By replicating a portion of the prover’s computation, the verifier asserts that the prover actually carried out the computation as claimed.</p>
<h3>Correctness of the verification algorithm</h3>
<p>We now show that the final verification checks are identically correct if the prover was honest.</p>
<p>Below we show the exact algebra, but intuitively the verifier is “reconstructing” the left vector in the inner product $\mathbf{a_L} – z \cdot \mathbf{1}^n$, the right vector in the inner product $\mathbf{y}^n \circ \mathbf{a_R} + \mathbf{y}^n\cdot z + z^2 \cdot 2^n$ and the output $z^2v+\delta(y,z)$.</p>
<p>The verifier is not given commitments to $\mathbf{a_L} – z \cdot \mathbf{1}^n$ and $\mathbf{y}^n \circ \mathbf{a_R} + \mathbf{y}^n\cdot z + z^2 \cdot 2^n$ but to $\mathbf{a}_L$ and $\mathbf{a}_R$. Similarly, the verifier is not given a commitment to the output $z^2v + \delta(y,z)$ but only to $v$.</p>
<p>The additive terms and the terms element-wise multiplied by $\mathbf{y}^n$ must be reconstructed by the verifier.</p>
<h4>Correctness of $t_u = \mathbf{t}(u)$</h4>
<p>For the $t_u\stackrel{?}<br />
=\langle\mathbf{l}_u,\mathbf{r}_u\rangle$ check, this is true by definition, as that is how the prover computed $t_u$.</p>
<h4>Correctness of the committed $\mathbf{l}(x)$ and $\mathbf{r}(x)$ with respect to $A$ and $S$</h4>
<p>For<br />
$A + Su + \langle -z\cdot\mathbf{1}^n,\mathbf{G}\rangle + \langle z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle\stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{G} \rangle + \langle \mathbf{r}_u, \mathbf{H}_{y^{-1}} \rangle + \pi_{lr} B$</p>
<p>we make the following substitution:</p>
<p>$\underbrace{\langle\mathbf{a}_L,\mathbf{G}\rangle+\langle\mathbf{a}_R,\mathbf{H}\rangle+\alpha B}_A + \underbrace{(\langle\mathbf{s}_L,\mathbf{G}\rangle+\langle\mathbf{s}_R,\mathbf{H}\rangle+\beta B)}_Su + \langle -z\cdot\mathbf{1}^n,\mathbf{G}\rangle + \langle z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle\stackrel{?}{=} \langle \underbrace{\mathbf{a}_L-z\cdot\mathbf{1}+\mathbf{s}_Lu}_{\mathbf{l}_u}, \mathbf{G} \rangle + \langle \underbrace{\mathbf{y}^n\circ(\mathbf{a}_R+z\cdot\mathbf{1})+z^2\mathbf{2}^n+\mathbf{y}^n\circ\mathbf{s}_Rx}_{\mathbf{r}_u}, \mathbf{H}_{y^{-1}} \rangle + \underbrace{(\alpha+\beta u)}_{\pi_{lr}} B$</p>
<p>All the $\mathbf{G}$ terms cancel as follows:</p>
<p>$\cancel{\langle\mathbf{a}_L,\mathbf{G}\rangle}+\langle\mathbf{a}_R,\mathbf{H}\rangle+\alpha B + (\cancel{\langle\mathbf{s}_L,\mathbf{G}\rangle}+\langle\mathbf{s}_R,\mathbf{H}\rangle+\beta B)u + \cancel{\langle -z\cdot\mathbf{1}^n,\mathbf{G}\rangle} + \langle z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle \stackrel{?}{=} \cancel{\langle \mathbf{a}_L-z\cdot\mathbf{1}+\mathbf{s}_L u, \mathbf{G} \rangle} + \langle \mathbf{y}^n\circ(\mathbf{a}_R+z\cdot\mathbf{1})+z^2\mathbf{2}^n+\mathbf{y}^n\circ\mathbf{s}_R x, \mathbf{H}_{y^{-1}} \rangle + (\alpha+\beta u) B$</p>
<p>$\langle\mathbf{a}_R,\mathbf{H}\rangle+\alpha B + (\langle\mathbf{s}_R,\mathbf{H}\rangle+\beta B)u + \langle z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle \stackrel{?}{=} \langle \mathbf{y}^n\circ(\mathbf{a}_R+z\cdot\mathbf{1})+z^2\mathbf{2}^n+\mathbf{y}^n\circ\mathbf{s}_R x, \mathbf{H}_{y^{-1}} \rangle + (\alpha+\beta u) B$</p>
<p>The blinding terms related to $B$ cancel as follows:</p>
<p>$\langle\mathbf{a}_R,\mathbf{H}\rangle+\cancel{\alpha B} + (\langle\mathbf{s}_R,\mathbf{H}\rangle+\cancel{\beta B)u} + \langle z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle \stackrel{?}{=} \langle \mathbf{y}^n\circ(\mathbf{a}_R+z\cdot\mathbf{1})+z^2\mathbf{2}^n+\mathbf{y}^n\circ\mathbf{s}_R x, \mathbf{H}_{y^{-1}} \rangle + \cancel{(\alpha+\beta u) B}$</p>
<p>$\langle\mathbf{a}_R,\mathbf{H}\rangle + (\langle\mathbf{s}_R,\mathbf{H}\rangle u) + \langle z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle \stackrel{?}{=} \langle \mathbf{y}^n\circ(\mathbf{a}_R+z\cdot\mathbf{1})+z^2\mathbf{2}^n+\mathbf{y}^n\circ\mathbf{s}_R u, \mathbf{H}_{y^{-1}} \rangle$</p>
<p>The $\mathbf{H}_{y^{-1}}$ cancels with the $\mathbf{y}^n$ terms:</p>
<p>$\langle\mathbf{a}_R,\mathbf{H}\rangle + (\langle\mathbf{s}_R,\mathbf{H}\rangle u) + \langle z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle \stackrel{?}{=} \langle \mathbf{a}_R+z\cdot\mathbf{1},\mathbf{H}\rangle+\langle z^2\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle+\langle\mathbf{s}_R u, \mathbf{H} \rangle$</p>
<p>Split the inner products:</p>
<p>$\langle\mathbf{a}_R,\mathbf{H}\rangle + (\langle\mathbf{s}_R,\mathbf{H}\rangle u) + \langle z\cdot\mathbf{y}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle+\langle z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle \stackrel{?}{=} \langle \mathbf{a}_R,\mathbf{H}\rangle+\langle z\cdot\mathbf{1},\mathbf{H}\rangle+\langle z^2\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle+\langle\mathbf{s}_R u, \mathbf{H} \rangle$</p>
<p>Cancel terms that appear on both sides of the equation:</p>
<p>$$\cancel{\langle\mathbf{a}_R,\mathbf{H}\rangle} + (\langle\mathbf{s}_R,\mathbf{H}\rangle u) + \langle z\cdot\mathbf{y}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle+\langle z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle \stackrel{?}{=} \cancel{\langle \mathbf{a}_R,\mathbf{H}\rangle}+\langle z\cdot\mathbf{1},\mathbf{H}\rangle+\langle z^2\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle+\langle\mathbf{s}_R u, \mathbf{H} \rangle$$</p>
<p>$$\cancel{(\langle\mathbf{s}_R,\mathbf{H}\rangle u)} + \langle z\cdot\mathbf{y}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle+\langle z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle \stackrel{?}{=}\langle z\cdot\mathbf{1},\mathbf{H}\rangle+\langle z^2\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle+\cancel{\langle\mathbf{s}_R u, \mathbf{H} \rangle}$$</p>
<p>$$\langle z\cdot\mathbf{y}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle+\cancel{\langle z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle} \stackrel{?}{=}\langle z\cdot\mathbf{1},\mathbf{H}\rangle+\cancel{\langle z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle}$$</p>
<p>$$\langle z\cdot\mathbf{y}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle \stackrel{?}{=}\langle z\cdot\mathbf{1},\mathbf{H}\rangle$$</p>
<p>Move $\mathbf{y}^n$ to the other side:</p>
<p>$$\langle z\cdot\mathbf{1},\mathbf{H}\rangle \stackrel{?}{=}\langle z\cdot\mathbf{1},\mathbf{H}\rangle$$</p>
<p>$$\cancel{\langle z\cdot\mathbf{1},\mathbf{H}\rangle} \stackrel{?}{=}\cancel{\langle z\cdot\mathbf{1},\mathbf{H}\rangle}$$</p>
<h4>Correctness of the evaluation of $t(u)$</h4>
<p>To see that</p>
<p>$$t_uG+\pi_tB=Vz^2+\delta(y,z)G+T_1u+T_2u^2$$</p>
<p>is correct, we could substitute the terms as follows:</p>
<p>$$\underbrace{\langle \mathbf{l}_u, \mathbf{r}_u \rangle}_{t_u}G+\underbrace{(z^2\gamma + \tau_1u + \tau_2u^2)}_{\pi_t}B=\underbrace{(vG+\gamma B)}_{V}z^2+\delta(y,z)G+\underbrace{(t_1G+\tau_1B}_{T_1})u+\underbrace{(t_2G+\tau_2B)}_{T_2}u^2$$</p>
<p>with $\mathbf{l}_u$, $\mathbf{r}_u$, $t_1$, $t_2$:<br />
$$\begin{align*}<br />
\mathbf{l}_u &amp;= \mathbf{a_L} – z \cdot \mathbf{1}^n\<br />
\mathbf{r}_u &amp;= \mathbf{y}^n \circ \mathbf{a_R} + \mathbf{y}^n\cdot z + z^2 \cdot 2^n\<br />
t_1 &amp;= \langle(\mathbf{a}_L-z\cdot\mathbf{1}),\mathbf{y}^n\circ\mathbf{s}_R\rangle+\langle\mathbf{y}^n\circ(\mathbf{a}_R+z\cdot\mathbf{1})+z^2\mathbf{2}^n,\mathbf{s_L}\rangle\<br />
t_2 &amp;= \langle\mathbf{s}_L,\mathbf{y}^n\circ\mathbf{s}_R\rangle<br />
\end{align*}<br />
$$</p>
<p>However, such algebra would be extremely messy. Instead, we observe that $z^2v+\delta(y,z)G$ is the constant term of the vector polynomial inner product of of $\langle\mathbf{l}(x),\mathbf{r}(x)\rangle$. To cancel out the blinding term in $\gamma B$ in $V$, observe that $\pi_t$ contains $z^2\gamma$, so this will cancel with the gamma term in $Vz^2 = (vG + \gamma B)z^2$.</p>
<p>Since Pedersen commitments are additively homomorphic, the verifier can simply compute and add $\delta(y,z)G$ to $Vz^2$ to compute the commitment to the constant term of the polynomial $t(x)$.</p>
<h2>Logarithmic-sized range proof</h2>
<p>We can reduce the size of the data transmission by sending a commitment $C$ to $\mathbf{l}_u$ and $\mathbf{r}_u$ and proving that the committed vectors have inner product $t_u$ using the logarithmic-sized proof, and then verifying that</p>
<p>$$A + Su + \langle -z\cdot\mathbf{1}^n,\mathbf{G}\rangle + \langle z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n,\mathbf{H}_{\mathbf{y}^{-1}}\rangle-\pi_{lr}B\stackrel{?}{=} C$$</p>
<p>and</p>
<p>$$t_u \stackrel{?}{=} \langle \mathbf{l}_u, \mathbf{r}_u \rangle$$</p>
<p>with respect to the basis vectors $\mathbf{G}$ and $\mathbf{H}_{\mathbf{y}^{-1}}$.</p>
<h2>Using the range proof algorithm for the subset sum</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Subset_sum_problem">subset sum problem</a> asks, “given a set of numbers, does a subset (possibly including the entire set) sum up to $k$? For example if $k = 16$ and the set is $\set{3,5,7,11}$ the answer is yes because $5 + 11 = 16$. However if $k=13$, then the answer is no.</p>
<p>The subset sum problem is NP-Complete, meaning that, similar to a Boolean circuit or arithmetic circuit, it can represent any problem in <a href="https://rareskills.io/post/p-vs-np">NP</a>. That is, any problem in NP can be rewritten (the technical word is “<a href="https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/npcomplete.pdf">reduced</a>“) to a subset sum instance.</p>
<p>By replacing $\mathbf{2}^n$ with $[3,5,7,11]$, we can prove we know a solution to a subset sum without revealing the answer. Specifically, the prover would know that $\mathbf{a}_L= [0,1,0,1]$ if $k=16$. In general, a one entry in $\mathbf{a}_L$ means we include that element in the subset and a zero means it is not included in the subset.</p>
<p>Therefore, Bulletproofs are capable of proving knowledge of any witness for any problem in NP.</p>
<h2>Appendix: Derivation of combining three inner products into one</h2>
<p>Starting with the three inner products</p>
<p>$$z^2 \cdot \langle \mathbf{a_L}, \mathbf{2}^n \rangle + z \cdot \langle \mathbf{a_L} – \mathbf{1}^n – \mathbf{a_R}, \mathbf{y}^n \rangle + \langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle = z^2 \cdot v$$</p>
<p>we show how to derive the final result</p>
<p>$$\langle \mathbf{a_L}-z\cdot\mathbf{1}^n, \mathbf{a}_R\circ\mathbf{y}^n+z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle= z^2 \cdot v+(z-z^2)\cdot\langle\mathbf{1}^n,\mathbf{y}^n\rangle-z^3\langle\cdot\mathbf{1}^n,\mathbf{2}^n\rangle$$</p>
<p>using the inner product algebra we learned previously.</p>
<ol>
<li>The middle term can be split into separate inner products:</li>
</ol>
<p>$$z^2 \cdot \langle \mathbf{a_L}, \mathbf{2}^n \rangle + \boxed{z \cdot \langle \mathbf{a_L} – \mathbf{1}^n – \mathbf{a_R}, \mathbf{y}^n \rangle} + \langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle = z^2 \cdot v$$</p>
<p>$$z^2 \cdot \langle \mathbf{a_L}, \mathbf{2}^n \rangle+\boxed{z\cdot\langle\mathbf{a}_L,\mathbf{y}^n\rangle+z\cdot\langle-\mathbf{1}^n,\mathbf{y}^n\rangle+z\cdot\langle-\mathbf{a}_R,\mathbf{y}^n\rangle}+\langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle=z^2\cdot v$$</p>
<ol start="2">
<li>We can move the constant $z$ terms inside the inner products:<br />
   $$\langle \mathbf{a_L}, z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_L,z\cdot\mathbf{y}^n\rangle+\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle+\langle-\mathbf{a}_R,z\cdot\mathbf{y}^n\rangle+\langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle= z^2 \cdot v$$</li>
<li>Move the values known to the verifier to the right:<br />
   $$\langle \mathbf{a_L}, z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_L,z\cdot\mathbf{y}^n\rangle+\boxed{\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle}+\langle-\mathbf{a}_R,z\cdot\mathbf{y}^n\rangle+\langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle= z^2 \cdot v$$</li>
</ol>
<p>$$\langle \mathbf{a_L}, z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_L,z\cdot\mathbf{y}^n\rangle+\langle-\mathbf{a}_R,z\cdot\mathbf{y}^n\rangle+\langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle= z^2 \cdot v-\boxed{\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle}$$</p>
<ol start="4">
<li>Convert the $\mathbf{a}_R$ terms to both be $\mathbf{a}_R\circ\mathbf{y}^n$:</li>
</ol>
<p>$$\langle \mathbf{a_L}, z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_L,z\cdot\mathbf{y}^n\rangle+\boxed{\langle-\mathbf{a}_R\circ\mathbf{y}^n,z\cdot\mathbf{1}^n\rangle}+\langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<p>$$\langle \mathbf{a_L}, z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_L,z\cdot\mathbf{y}^n\rangle+\boxed{\langle\mathbf{a}_R\circ\mathbf{y}^n,-z\cdot\mathbf{1}^n\rangle}+\langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<p>$$\langle \mathbf{a_L}, z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_L,z\cdot\mathbf{y}^n\rangle+{\langle\mathbf{a}_R\circ\mathbf{y}^n,-z\cdot\mathbf{1}^n\rangle}+\boxed{\langle \mathbf{a_L}, \mathbf{a_R} \circ \mathbf{y}^n \rangle}= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<p>$$\langle \mathbf{a_L}, z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_L,z\cdot\mathbf{y}^n\rangle+\langle\mathbf{a}_R\circ\mathbf{y}^n,-z\cdot\mathbf{1}^n\rangle+\langle \mathbf{a_R} \circ \mathbf{y}^n,\mathbf{a_L} \rangle = z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<ol start="5">
<li>Combine the $\mathbf{a}_R\circ\mathbf{y}^n$ terms into one:</li>
</ol>
<p>$$\langle \mathbf{a_L}, z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_L,z\cdot\mathbf{y}^n\rangle+\underbrace{\langle\mathbf{a}_R\circ\mathbf{y}^n,-z\cdot\mathbf{1}^n\rangle+\langle \mathbf{a_R} \circ \mathbf{y}^n,\mathbf{a_L} \rangle} = z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<p>$$\langle \mathbf{a_L}, z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_L,z\cdot\mathbf{y}^n\rangle+\langle\mathbf{a}_R\circ\mathbf{y}^n,\mathbf{a}_L-z\cdot\mathbf{1}^n\rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<ol start="6">
<li>Combine the two $\mathbf{a}_L$ terms on the left:</li>
</ol>
<p>$$\langle \boxed{\mathbf{a_L}}, z^2\cdot\mathbf{2}^n \rangle+\langle\boxed{\mathbf{a}_L},z\cdot\mathbf{y}^n\rangle+\langle\mathbf{a}_R\circ\mathbf{y}^n,\mathbf{a}_L-z\cdot\mathbf{1}^n\rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<p>$$\langle \mathbf{a_L}, z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_R\circ\mathbf{y}^n,\mathbf{a}_L-z\cdot\mathbf{1}^n\rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<ol start="7">
<li>Split the last left-hand side term into two inner products:<br />
   $$\langle \mathbf{a_L}, z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle+\boxed{\langle\mathbf{a}_R\circ\mathbf{y}^n,\mathbf{a}_L-z\cdot\mathbf{1}^n\rangle}= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</li>
</ol>
<p>$$\langle \mathbf{a_L}, z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_R\circ\mathbf{y}^n,\mathbf{a}_L\rangle+\langle\mathbf{a}_R\circ\mathbf{y}^n,-z\cdot\mathbf{1}^n\rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<ol start="8">
<li>Combine the $\mathbf{a}_L$ terms:<br />
   $$\langle \boxed{\mathbf{a_L}}, z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_R\circ\mathbf{y}^n,<br />
   \boxed{\mathbf{a}_L}\rangle+\langle\mathbf{a}_R\circ\mathbf{y}^n,-z\cdot\mathbf{1}^n\rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</li>
</ol>
<p>$$\langle \mathbf{a_L}, \mathbf{a}_R\circ\mathbf{y}^n+z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle+\langle\mathbf{a}_R\circ\mathbf{y}^n,-z\cdot\mathbf{1}^n\rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle$$</p>
<ol start="9">
<li>We can use the rule $\langle \mathbf{x}, \mathbf{b} + \mathbf{c}\rangle + \langle \mathbf{b}, \mathbf{y}\rangle = v \rightarrow \langle \mathbf{x} + \mathbf{y}, \mathbf{b} + \mathbf{c}\rangle = v + \langle\mathbf{y},\mathbf{c}\rangle$ to combine the terms that contain $\mathbf{a}_R\circ\mathbf{y}^n$. Here $\mathbf{b}$ is $\mathbf{a}_R\circ\mathbf{y}^n$, $\mathbf{c}$ is $z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n$, and $\mathbf{y}$ is $-z\cdot\mathbf{1}^n$.</li>
</ol>
<p>$$\langle \underbrace{\mathbf{a_L}}_\mathbf{x}, \underbrace{\mathbf{a}_R\circ\mathbf{y}^n}_\mathbf{b}+\underbrace{z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n}_\mathbf{c} \rangle+\langle\underbrace{\mathbf{a}_R\circ\mathbf{y}^n}_\mathbf{b},\underbrace{-z\cdot\mathbf{1}^n}_\mathbf{y}\rangle= \underbrace{z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle}_v$$</p>
<p>$$\langle \underbrace{\mathbf{a_L}}_\mathbf{x}-\underbrace{z\cdot\mathbf{1}^n}_\mathbf{y}, \underbrace{\mathbf{a}_R\circ\mathbf{y}^n}_\mathbf{b}+\underbrace{z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n}_\mathbf{c} \rangle= \underbrace{z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle}_v+\langle\underbrace{-z\cdot\mathbf{1}^n}_\mathbf{y},\underbrace{z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n}_\mathbf{c}\rangle$$</p>
<p>$$\langle \mathbf{a_L}-z\cdot\mathbf{1}^n, \mathbf{a}_R\circ\mathbf{y}^n+z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle+\langle-z\cdot\mathbf{1}^n,z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n\rangle$$</p>
<ol start="10">
<li>We now break up the terms on the right-hand side:</li>
</ol>
<p>$$\langle \mathbf{a_L}-z\cdot\mathbf{1}^n, \mathbf{a}_R\circ\mathbf{y}^n+z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle= z^2 \cdot v-\langle-z\cdot\mathbf{1}^n,\mathbf{y}^n\rangle+\langle-z\cdot\mathbf{1}^n,z\cdot\mathbf{y}^n\rangle+\langle-z\cdot\mathbf{1}^n,z^2\cdot\mathbf{2}^n\rangle$$</p>
<ol start="11">
<li>Take the scalars out of the inner products on the right:</li>
</ol>
<p>$$\langle \mathbf{a_L}-z\cdot\mathbf{1}^n, \mathbf{a}_R\circ\mathbf{y}^n+z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle= z^2 \cdot v+z\cdot\langle\mathbf{1}^n,\mathbf{y}^n\rangle-z^2\cdot\langle\mathbf{1}^n,\mathbf{y}^n\rangle-z^3\langle\cdot\mathbf{1}^n,\mathbf{2}^n\rangle$$</p>
<ol start="12">
<li>Factor out $\langle\mathbf{1}^n,\mathbf{y}^n\rangle$:</li>
</ol>
<p>$$\langle \mathbf{a_L}-z\cdot\mathbf{1}^n, \mathbf{a}_R\circ\mathbf{y}^n+z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle= z^2 \cdot v+(z-z^2)\cdot\langle\mathbf{1}^n,\mathbf{y}^n\rangle-z^3\langle\cdot\mathbf{1}^n,\mathbf{2}^n\rangle$$</p>
<p>Since $\delta(y,z)=(z-z^2)\cdot\langle\mathbf{1}^n,\mathbf{y}^n\rangle-z^3\langle\cdot\mathbf{1}^n,\mathbf{2}^n\rangle$, we have:</p>
<p>$$\langle \mathbf{a_L}-z\cdot\mathbf{1}^n, \mathbf{a}_R\circ\mathbf{y}^n+z\cdot\mathbf{y}^n+z^2\cdot\mathbf{2}^n \rangle= z^2 \cdot v+\delta(y,z)$$</p>
<p>This completes the derivation. $\blacksquare$</p>
<div style='page-break-after: always;'></div>

<h1>Multiplication of Polynomials in Point Form</h1>
<p>Source: https://rareskills.io/post/polynomial-multiplication-point-form</p>
<h1>Multiplication of Polynomials in Point Form</h1>
<p>Polynomial multiplication is widely used in <a href="https://rareskills.io/zk-book">zero-knowledge proofs</a> and mathematical cryptography. But the brute force or traditional approach for multiplying polynomials runs in $\mathcal{O}(n^2)$, which is fine for small inputs but becomes quite expensive as the degree of the polynomial increases. This article takes a detailed look at polynomial multiplication in order to explore ways of making it faster.</p>
<ul>
<li>We begin with a review of the schoolbook/traditional polynomial arithmetic</li>
<li>Followed by a study of different forms of representation of polynomials</li>
<li>We examine and compare polynomial arithmetic in these different forms</li>
<li>Finally, we look at how these forms can potentially speed up polynomial multiplication – and how they form the grounds for an algorithm called <strong>Number Theoretic Transform (NTT)</strong></li>
</ul>
<h2>Polynomial Multiplication – Traditional approach</h2>
<p>Consider two polynomials $p_1(x)$ and $p_2(x)$ of degree $n$ each:</p>
<p>$$<br />
p_1(x) = a_0 + a_1x + a_2x^2 + \dots + a_nx^n \p_2(x) = b_0 + b_1x + b_2x^2 + \dots + b_nx^n$$</p>
<p>Multiplying these two polynomials using the simple way of distribution of multiplication over addition takes $\mathcal{O}(n^2)$. Here, each term of $p_1(x)$ is multiplied with each term of $p_2(x)$:</p>
<p>$$<br />
\begin{aligned} p_1(x) \cdot p_2(x) &amp;= p_1(x)\cdot (b_0 + b_1x + \dots + b_nx^n) \ &amp;= (a_0 + a_1x + \dots + a_nx^n)\cdot (b_0 + b_1x + \dots + b_nx^n) \ &amp;= a_0\cdot (b_0 + b_1x + \dots + b_nx^n) \ &amp;+ a_1x\cdot (b_0 + b_1x + \dots + b_nx^n) \ &amp; \vdots\ &amp;+ a_nx^n\cdot (b_0 + b_1x + \dots + b_nx^n) \end{aligned}$$</p>
<p>For example,<br />
let $p_1(x) = 1 + 2x$,<br />
and $p_2(x) = 3 + 4x$.</p>
<p>Then,</p>
<p><img alt="Step-wise multiplication of p1(x) and p2(x) by distribution of multiplication over addition." src="assets/ryO52KiOle.png" /></p>
<p>When programmed, this is implemented in the form of nested loops.</p>
<pre style='font-family: Arial'><code class="language-solidity"># Let A be array representing the coefficients of p1(x)
A = [a0, a1, ..., an]
# Let B be array representing the coefficients of p2(x)
B = [b0, b1, ..., bn]
# Let C be the array storing coefficients of p1(x).p2(x)

function multiply_polynomials(A, B):
    n = len(A)
    m = len(B)
    C = array of zeros of length (n + m - 1)

    for i from 0 to n - 1:
        for j from 0 to m - 1:
            C[i + j] += A[i] * B[j]
    return C
</code></pre>
<p>You would get the result as:</p>
<p>$$<br />
\begin{array}{c|cc} \times &amp; 3 &amp; 4x \ \hline 1 &amp; 3 &amp; 4x \ 2x &amp; 6x &amp; 8x^2 \end{array}$$</p>
<p>For each $n$ iterations of the outer loop, the inner loop executes n times (assuming equal degree $m=n$), thus giving n times n, i.e. runtime of $\mathcal{O}(n^2)$.</p>
<p>We now want to see if we can optimize this and do better. Or simply, is there a way to make polynomial multiplication faster?</p>
<h2>Ways to represent a Polynomial</h2>
<p>There are two ways in which we can represent a polynomial: <strong>coefficient form</strong> and <strong>point form</strong>.</p>
<h3>Coefficient Form</h3>
<p>Polynomials are usually expressed in what is called the monomial basis, or the coefficient form, meaning it’s written as a linear combination of powers of the variable.</p>
<p>For instance, a polynomial of degree $n$, when expressed as</p>
<p>$$<br />
p(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n$$</p>
<p>is expressed in the monomial basis, since it is using $[1, x, x^2, …, x^n]$ as the basis for its coefficients, which are $a_0, a_1, a_2, .., a_x.$</p>
<p>In this representation, the coefficients of $p(x)$ can be written as a vector or an array, like $[a_0, a_1, …, a_n]$, where the first element corresponds to the constant term or coefficient of $x^0$, while the last element corresponds to the coefficient of $x^n$.</p>
<p><em>You should note that the earlier method of distributive multiplication we looked at (runtime</em> $\mathcal{O}(n^2)$<em>) was applied over the coefficient form of polynomials.</em></p>
<h3>Point (or Value) Form</h3>
<p>The point (or value) form representation is based on the fact that every polynomial of degree $n$ can be represented by a set of $n+1$ distinct points that lie on it.</p>
<p>For example, consider a quadratic polynomial (degree $2$):</p>
<p>$$<br />
2x^2-3x+1$$</p>
<p>Now take any $3$ (because $n=2$) lying on this curve, say $(0,1)$, $(1,0)$ and $(2,3)$. We can say that these $3$ points represent the given polynomial. Alternatively, if we are just given these points, it is possible to recover the polynomial $2x^2 -3x+1$ from that information. Why does this work? Or how are we able to equivalently represent a degree $n$ polynomial with $n+1$ points?</p>
<p>This is because:</p>
<blockquote>
<p>For every set of $n+1$ distinct points, there exists a unique lowest degree polynomial of degree at most $n$ that passes through all of them.</p>
</blockquote>
<p>This lowest degree polynomial is called the <strong>Lagrange Polynomial</strong>.</p>
<p>For example,<br />
given two points $(1, 2)$ and $(3, 4)$, there exists a unique polynomial of degree $1$ (a line) passing through these points:</p>
<p>$$<br />
p(x) = 1 + x$$</p>
<p>Similarly,<br />
given three points $(0, 1)$, $(1, 4)$, and $(2, 9)$, the Lagrange polynomial of degree $2$ passing through these points is:</p>
<p>$$<br />
p(x) = 1 +2x+ x^2 \<br />
1 + 2(0) + (0)^2 = 1 \<br />
1 + 2(1) + (1)^2 = 4 \<br />
1 + 2(2) + (2)^2 = 9$$</p>
<p>How this special polynomial is calculated given a set of points is something that is discussed in this article on <a href="https://www.rareskills.io/post/python-lagrange-interpolation">Lagrange interpolation</a>.</p>
<h3>Uniqueness of the Lagrange Polynomial</h3>
<p>You must note that for a set of points, there are multiple polynomials of a given degree that pass through all of them. But only the lowest degree polynomial is unique.</p>
<p>For instance, in the above example of points $(1, 2)$ and $(3, 4)$, there exist many polynomials passing through these two points:<br />
$x^2 -3x +4$ and $2x^2 -7x +7$ are two of many polynomials of degree $2$ (quadratic) that pass through these two points.</p>
<p><img alt="$x^2 -3x +4$ and $2x^2 -7x +7$, two of the many quadratic polynomials that pass through $(1, 2)$ and $(3, 4)$." src="assets/ByDce_A7xe.png" /></p>
<p>Similarly, if you consider degree $3$, two example polynomials passing through points $(1,2)$ and $(3,4)$ are $x^3 -5x^2 +8x-2$ and $-x^{3}+4x^{2}-2x+1$.</p>
<p><img alt="$x^3-5x^2+8x-2$ and $-x^3 +4x^2-2x+1$, two of the many cubic polynomials that pass through $(1, 2)$ and $(3, 4)$." src="assets/image3.png" /></p>
<p>But for the lowest degree, here $1$, there exists only one polynomial of the lowest degree, and that is $p(x) = 1 + x$. It is unique, and there is no other polynomial of degree $1$ that can pass through these two given points.</p>
<p><img alt="The unique linear polynomial $1+x$ passing through $(1,2)$ and $(3,4)$." src="assets/BJE4fd0mge.png" /></p>
<h3>How is this lowest degree determined?</h3>
<p>For every set of $n+1$ distinct points, there is a unique polynomial of degree at most $n$ that passes through them. The degree of the unique polynomial is less than $n$ if some of the points are <strong>collinear</strong> or <strong>lie on a lower degree polynomial</strong>. Therefore, we use the term “at most $n$” to cover the case where the degree is exactly $n$, as well as cases where the degree is lower.</p>
<p>For example,<br />
given the points $(1,2),(2,4),$ and $(3,6)$, the polynomial of lowest degree passing through them is $y=2x$. This is because the points are collinear, i.e. they lie on a line. One can check that the slope between any pair of them is the same:</p>
<p>$$<br />
\frac{4-2}{2-1} = \frac{6-4}{3-2} = 2$$</p>
<p>Therefore, the lowest degree is $1$, and $y=2x$ is the unique Lagrange polynomial.</p>
<p>Similarly,<br />
Given five points, $(-2,1), (-1,0), (0,1), (1,4), (2,9)$, we find that all of them lie on a parabola with the equation</p>
<p>$$<br />
y = x^2 + 2x + 1<br />
$$</p>
<p>This is a case where all the given points lie on a lower degree polynomial, here degree $2$, and thus the Lagrange polynomial has degree $2$, which is less than $n$ (Here, $n=4$).</p>
<p>Refer to the Appendix at the end for proof of uniqueness of the lowest degree polynomial.</p>
<h2>Conversion Between Coefficient Form and Point Form</h2>
<p>Since coefficient form and point form are equivalent, we can readily convert between them as we show now.</p>
<h3>Interpolation (Point Form → Coefficient Form)</h3>
<p>The conversion from point form to coefficient form, called interpolation, is calculating the polynomial of lowest degree which passes through all the given points. One of the most well-known methods it is done is using <strong>Lagrange Interpolation</strong>, that we mentioned previously. If you are unfamiliar with it, you may go through <a href="https://www.rareskills.io/post/python-lagrange-interpolation">this article</a>.</p>
<p>In short, given a set of $(n+1)$ distinct points</p>
<p>$$<br />
{(x_0, y_0), (x_1, y_1), \dots, (x_n, y_n)}$$</p>
<p>we can find the unique lowest degree polynomial $p(x)$ of degree at most $n$ using the formula for Lagrange interpolation, such that:</p>
<p>$$<br />
p(x_i) = y_i \quad \text{for all } i = 0, 1, \dots, n<br />
$$</p>
<p>You should keep in mind that the runtime of Lagrange interpolation is $\mathcal{O}(n^2)$.</p>
<h3>Evaluation (Coefficient Form → Point Form)</h3>
<p>The conversion from coefficient form to point form, called evaluation, is evaluating the polynomial at values of $x$ to obtain the corresponding values of $y$, and thus a set of $(x_i, y_i)$ points, which represent the polynomial. One common way this can be done is by using <strong>Horner’s rule</strong> (to be discussed in detail in a future article).</p>
<p>In short, given coefficients $[a_0, a_1, \dots, a_n]$ of a polynomial $p(x)$ and a value $x_i$, Horner’s Method evaluates $p(x_i)$ as follows:</p>
<p>$$<br />
p(x_i) = a_0 + x_i(a_1 + x_i(a_2 + \dots + x_i a_n) \dots)$$</p>
<p>This method factors out common powers of $x$, one at a time until all the terms are processed. Let us look at an example to understand it better.</p>
<p>Given the polynomial $p(x) = 2 + 3x + 5x^2 + x^3$ and a value $x = 2$, we will review how Horner’s rule evaluates $p(2)$.</p>
<p>We can rewrite $p(x)$ as follows (as shown in the generalized expression above):</p>
<p>$$<br />
p(x) = 2 + x(3 + x(5 + x(1)))$$</p>
<p>Substituting $x = 2$:</p>
<p><img alt="Step-wise multiplication using Horner’s method with alternative multiplication and addition operations." src="assets/H1T_8RVYgx.png" /></p>
<p>Observe how the steps involve alternating multiplications and additions. Steps 1, 3 and 5 of the above calculation are multiplications, while steps 2, 4 and 6 are additions. In total, there are $n$ multiplications and $n$ additions (here $n=3$), giving a total runtime of $\mathcal{O}(n)$. This is how Horner’s rule evaluates a polynomial of degree $n$ at a given value of $x$ in $\mathcal{O}(n).$</p>
<p>Therefore, evaluating the polynomial at $n+1$ distinct $x$-values – converting from coefficient to point form using this rule – takes $n$ times $n+1$, i.e. $\mathcal{O}(n^2)$.</p>
<h2>Coefficient form VS Point form</h2>
<p>We said that coefficient form and point form of a polynomial are equivalent, and one can be converted to the other. That is, there is no difference in the final results of addition and multiplication when done in either form. Let us examine this, with an example of addition first.</p>
<h3>Addition in coefficient form</h3>
<p>Consider two polynomials given in coefficient form,</p>
<p>$$<br />
p_1(x) = 1 + 2x + 3x^2 \p_2(x) = 4 + 0x + 1x^2 $$</p>
<p>Or their respective arrays of coefficients:</p>
<p>$$<br />
p_1: [1,\ 2,\ 3] \p_2: [4,\ 0,\ 1]$$</p>
<p>Now, adding the two polynomials is simply adding the two arrays element-wise, and the resultant coefficient array represents the final polynomial. Let’s verify this:</p>
<p>$$<br />
p_{\text{sum}}(x) = (1+4) + (2+0)x + (3+1)x^2 = 5 + 2x + 4x^2<br />
$$</p>
<p>Or simply, $[(1+4), (2+0),(3+1)] = [5,\ 2,\ 4]$</p>
<p>For two polynomials of degree $n$, we perform $n+1$ additions to get the sum’s representation. Therefore, the runtime of addition in coefficient form is $\mathcal{O}(n)$.</p>
<h3>Addition in point form</h3>
<p>Consider the same two polynomials,</p>
<p>$$<br />
p_1(x) = 1 + 2x + 3x^2 \p_2(x) = 4 + 0x + 1x^2 $$</p>
<p>First, we need to convert them from coefficient form to point form. Since the degree of both polynomials is $2$, the degree of their sum will be at most $2$ as well. Therefore, we need three points to represent the sum (degree plus one: $n + 1$), which requires $3$ evaluations each of $p_1(x)$ and $p_2(x)$.</p>
<p>Let us evaluate $p_1(x)$ and $p_2(x)$ at $x = 0, 1, 2$ to get our points.</p>
<p><em>Note: We are just choosing</em> $x=0,1,2$ <em>for simplicity. You could pick any other</em> $3$ <em>points for evaluation.</em></p>
<ul>
<li>$p_1(0) = 1, \quad p_1(1)= 6, \quad p_1(2)=17$</li>
<li>$p_2(0) = 4, \quad p_2(1)= 5, \quad p_2(2)=8$</li>
</ul>
<p>Now, adding the two polynomials requires adding the corresponding evaluations element-wise, that is:</p>
<ul>
<li>$p_{\text{sum}}(0) = (1+4) =5$</li>
<li>$p_{\text{sum}}(1) = (6+5) =11$</li>
<li>$p_{\text{sum}}(2) = (17+8) =25$</li>
</ul>
<p>These three points $(0, 5), (1, 11)$ and $(2, 25)$ give us the point representation of the sum. Let us verify whether they satisfy the polynomial we calculated earlier:</p>
<p>$$<br />
p_{\text{sum}}(x) = 5 + 2x + 4x^2$$</p>
<ul>
<li>$p_{\text{sum}}(0) = 5$</li>
<li>$p_{\text{sum}}(1) = 5 + 2 + 4 = 11$</li>
<li>$p_{\text{sum}}(2) = 5 + 4 + 16 = 25$</li>
</ul>
<p>Therefore, you see that addition in both forms gives the same result, or the same polynomial, just represented in different ways.</p>
<p>In point form addition, for two polynomials of degree $n$, there are $n+1$ points representing each of them, and thus $n+1$ element-wise additions that we perform to get the sum’s representative points. Therefore, the runtime of addition in point form is $\mathcal{O}(n)$.</p>
<p>Now, let us also look at multiplication closely.</p>
<h3>Multiplication in coefficient form</h3>
<p>Consider two polynomials given in coefficient form:</p>
<p>$$<br />
p_1(x) = 1 + 2x\p_2(x) = 3 + 4x$$</p>
<p>Or their respective coefficient arrays:<br />
$p_1 = [1,\ 2]$ and $p_2 = [3,\ 4]$</p>
<p>Multiplying them using the distributive way <a href="https://hackmd.io/_9-URR4GTpKqHij6eH5NIg#Polynomial-Multiplication--Traditional-approach">discussed earlier</a> gives:</p>
<p>$$<br />
\begin{aligned}<br />
p_{\text{prod}}(x) &amp;= (1)(3+4x) + (2x)(3 + 4x) \ &amp;= 3 + 4x + 6x + 8x^2 \&amp;= 3 + 10x + 8x^2<br />
\end{aligned}<br />
$$</p>
<p>The resulting polynomial is $p_{\text{prod}}(x) = 3 + 10x + 8x^2$, represented by the coefficient array $[3,\ 10,\ 8]$. The distributive method of coefficient form multiplication takes $\mathcal{O}(n^2)$, as we saw at the start of this article.</p>
<h3>Multiplication in point form</h3>
<p>We now consider the same polynomials and convert them to their point forms.</p>
<p>$$<br />
p_1(x) = 1 + 2x\p_2(x) = 3 + 4x<br />
$$</p>
<p>Since both polynomials have degree $1$, their product will have a degree of at most $2$, meaning we need $3$ points to represent it, which requires 3 evaluations of each of $p_1(x)$ and $p_2(x)$.</p>
<p>So, let’s evaluate $p_1(x)$ and $p_2(x)$ at $x = 0, 1, 2$.</p>
<ul>
<li>$p_1(0) = 1,\quad p_1(1) = 3,\quad p_1(2) = 5$</li>
<li>$p_2(0) = 3,\quad p_2(1) = 7,\quad p_2(2) = 11$</li>
</ul>
<p>Now, to get the points that represent their product, we multiply the evaluations element-wise:</p>
<ul>
<li>$p_{\text{prod}}(0) = 1 \cdot 3 = 3$</li>
<li>$p_{\text{prod}}(1) = 3 \cdot 7 = 21$</li>
<li>$p_{\text{prod}}(2) = 5 \cdot 11 = 55$</li>
</ul>
<p>So the three points $(0, 3), (1, 21)$ and $(2, 55)$ give us the point representation of the resultant product.</p>
<p>Let’s verify if they satisfy the polynomial product we got earlier:</p>
<p>$$<br />
p_{\text{prod}}(x) = 3 + 10x + 8x^2<br />
$$</p>
<ul>
<li>$p_{\text{prod}}(0) = 3$</li>
<li>$p_{\text{prod}}(1) = 3 + 10 + 8 = 21$</li>
<li>$p_{\text{prod}}(2) = 3 + 20 + 32 = 55$</li>
</ul>
<p>Therefore, you can see that multiplication in both forms gives the same polynomial, just represented in different ways.</p>
<p>In summary, for two polynomials $p_1(x)$ and $p_2(x)$ of degree $n$, their product will have a degree of at most $2n$, meaning we need $2n+1$ points to represent it. Thus, we perform $2n+1$ evaluations for each polynomial at common values of $x$ to convert them into point form:</p>
<p>$$<br />
p_1(x_0), p_1(x_1), p_1(x_2), \dots p_1(x_{2n})\p_2(x_0), p_2(x_1), p_2(x_2), \dots p_2(x_{2n})$$</p>
<p>We then perform point form multiplication by multiplying these two sets element-wise, which takes $2n+1$ multiplications, i.e., runtime of $\mathcal{O}(n)$.</p>
<p>$$<br />
p_1(x_0) \cdot p_2(x_0), \; p_1(x_1) \cdot p_2(x_1), \dots, \; p_1(x_{2n}) \cdot p_2(x_{2n})<br />
$$</p>
<p>This gives us the $2n+1$ points which represent the product $p_1(x).p_2(x)$:</p>
<p>$$<br />
{(x_0,p_1(x_0) \cdot p_2(x_0)), \; (x_1, p_1(x_1) \cdot p_2(x_1)), \dots, \; (x_{2n}, p_1(x_{2n}) \cdot p_2(x_{2n}))}<br />
$$</p>
<p>The amazing thing to note here is that, while addition in both coefficient and point form takes the same time $\mathcal{O}(n)$, multiplication in point form is significantly faster than in coefficient form. In point form, we perform $2n+1$ element-wise multiplications, giving a runtime of $\mathcal{O}(n)$, which is way better than the $\mathcal{O}(n^2)$ required for coefficient form multiplication!</p>
<p>However, there is still an issue- we haven’t considered the overhead of converting to the point form and vice versa.</p>
<p>So, lets look at the complete process of multiplication in point form, which involves three steps:</p>
<ol>
<li><strong>Conversion of coefficient to point form</strong><br />
   We evaluate the two polynomials of degree $n$, to be multiplied, at $(2n+1)$ values of $x$, to get a set of $(2n+1)$ evaluations each. This takes $\mathcal{O}(n^2)$ using Horner’s Method.</li>
<li><strong>Element-wise multiplication in point form representation</strong><br />
   We multiply these two sets element-wise to get $(2n+1)$ evaluations that gives the point form representation of their product. This takes $\mathcal{O}(n)$.</li>
<li><strong>Conversion of point to coefficient form</strong><br />
   We calculate the unique lowest degree polynomial (coefficient form) that passes through all the resultant $(2n+1)$ points. This takes $\mathcal{O}(n^2)$ using Lagrange interpolation.</li>
</ol>
<p>Therefore, the overall runtime for the steps above is:</p>
<p>$$<br />
\mathcal{O}(n^2) + \mathcal{O}(n) + \mathcal{O}(n^2) \approx \mathcal{O}(n^2)$$</p>
<p>which is no better than where we started from. Thus, we need to explore whether any optimizations can make this process faster.</p>
<h2>Optimizing conversion</h2>
<p>The key point to keep in mind is that multiplication in coefficient form takes $\mathcal{O}(n^2)$, whereas multiplication in point form (element-wise) takes $\mathcal{O}(n)$. Therefore, if we can find a way to convert coefficient form to point form and vice versa (steps 1 and 3 mentioned above) faster than $\mathcal{O}(n^2)$, we can optimize multiplication to run in sub-quadratic time.</p>
<p><em>It is important to note that we cannot optimize polynomial addition, because addition in both coefficient form and point form runs in</em> $\mathcal{O}(n)$ <em>each.</em></p>
<p>So now let us brainstorm a few ways we might make the conversion from coefficient to point form faster.</p>
<p>What if we knew a point whose evaluation could give us the values of several related points, saving us from repeated calculations?</p>
<p>For example, if we had a polynomial with a symmetric graph, evaluating one point would tell us the evaluation for its corresponding symmetric point as well.</p>
<p>Consider the polynomial $p(x)= x^2$.<br />
Observe how,</p>
<p>$$<br />
p(-2) = 4\space\space\space \text{and} \space\space\space p(2) =4$$</p>
<p><img alt="A graph of $x^2$ showing symmetric values of $x=-2$ and $x=2$." src="assets/image1.png" /></p>
<p>Or, more simply, observe how for all $x_i$,</p>
<p>$$<br />
p(-x_i)=p(x_i)$$</p>
<p>This is not just true for $p(x)=x^2$, but generalizes to all polynomials that contain only even powered coefficients, which are also called as even polynomials.</p>
<p>For example, consider the even polynomial (containing only terms with even powers of $x$:</p>
<p>$$<br />
q(x) =x^{10}+3x^{8}-2x^{6}+3x^{4}-2x^{2}-x^{0} $$</p>
<p><img alt="A graph of $x^{10}+3x^{8}-2x^{6}+3x^{4}-2x^{2}-x^{0}$ showing symmetric curve about y-axis." src="assets/B1T5Y2Vmxx.png" /></p>
<p>In the graph above, it is easy to observe that</p>
<p>$$<br />
q(x) =q(-x)$$</p>
<p>Visually speaking, the graphs of even polynomials are mirrored about the $y$-axis, and they evaluate to the same $y$ for both positive and negative values of any given $x$.</p>
<p>What about odd polynomials that contain only odd powered coefficients?<br />
Consider $p(x) = x^3$.<br />
Observe how</p>
<p>$$<br />
p(-2) = -8 =-p(2)$$</p>
<p><img alt="A graph of $x^3$ showing symmetricity about origin." src="assets/SykB6eQQge.png" /></p>
<p>In the graph above, observe how, for all $x_i$,</p>
<p>$$<br />
p(-x_i) = -p(x_i)$$</p>
<p>Again, this is not just true for $p(x)=x^3$, but generalizes to all odd polynomials, i.e. polynomials containing only terms with odd powers of $x$. For example, consider the polynomial</p>
<p>$$<br />
q(x)=-x^{7}+3x^{5}+x^{3}-x$$</p>
<p><img alt="A graph of $x^{7}+3x^{5}+x^{3}-x$ showing symmetricity about origin." src="assets/r19332NXgx.png" /></p>
<p>In the graph above, observe that</p>
<p>$$<br />
q(x) = -q(-x)$$</p>
<p>Visually, the graphs of all odd polynomials are symmetric about the origin, which makes the above equality true for all of them.</p>
<p>Now you can see that after evaluating certain points, we can get the evaluation at other points without any extra calculation. For instance, in the examples above, for even and odd polynomials, knowing the evaluation for $p(x)$ also gives us the evaluation for $p(-x)$.</p>
<p>We can exploit this fact to make polynomial multiplication faster, which is exactly what a beautiful algorithm called the <strong>Number Theoretic Transform (NTT)</strong> allows us to do. NTT enables evaluation and interpolation in $\mathcal{O}(n \log n)$ by recursively using properties of symmetries of certain points, thereby making the conversion sub-quadratic.</p>
<p>But since NTT operates over a finite field, there are no negative values of $x$ we can work with. This is where the concepts of multiplicative subgroups, cyclicity and roots of unity come into the picture. These concepts will allow us to exploit the symmetries present in finite fields to perform polynomial multiplication more efficiently. We’ll explore how NTT works in detail in upcoming articles.</p>
<h2>Appendix</h2>
<h3>Uniqueness of lowest degree polynomial proof</h3>
<p>We show that if there are two polynomials $p(x)$ and $q(x)$ of equal degree which interpolate a set of points, then a polynomial $r(x)$ must exist such that $r(x) = p(x)-q(x)$.</p>
<p>We will then show that the only possible solution for $r(x)$ is $r(x) = 0$, otherwise we end up with a polynomial that has more roots than its degree, which we show is impossible. Let us look at these steps in detail now.</p>
<p><strong>Let us assume that the lowest degree Lagrange polynomial is not unique.</strong> Then there are at least two distinct polynomials of lowest degree that pass through all given $n+1$ points. Let these two polynomials be $p(x)$ and $q(x)$. Now, define the polynomial $r(x)$ as the difference between $p(x)$ and $q(x)$.</p>
<p>$$<br />
r(x) = p(x) – q(x)$$</p>
<p>Now, if we show that $r(x)$ is $0$ for all values of $x$, then we will have shown that $p(x)$ equals $q(x)$, and therefore the Lagrange polynomial is unique.</p>
<p>Since the degree of both $p(x)$ and $q(x)$ is at most $n$, it follows from simple algebraic subtraction that $r(x)$ must also have degree at most $n$.<br />
Also, since both $p(x)$ and $q(x)$ pass through the same $n+1$ points, they will evaluate to the same $y$-value for each of the $x$-values.</p>
<p><em>Note: Graphically, when two </em><em>different polynomials</em><em> evaluate to the same</em> $y$<em>-value for a given</em> $x$<em>, it means that they intersect at that point. For example:</em></p>
<p>$$<br />
p(x) = x^2 \quad \text{and} \quad q(x) = 2x.<br />
$$</p>
<p><img alt="A graph showing how $x^2$ and $2x$ intersect at $(0,0)$ and $(2,4)$." src="assets/SJ1siOxKex.png" /></p>
<p><em>At</em> $x=2$<em>, both give</em> $y=4$<em>, so they intersect at the point</em> $(2,4)$<em>. In this case, we’re dealing with different polynomials. Another possibility for having the same</em> $y$ <em>for a given</em> $x$ <em>is that they are actually the same polynomial! In that case, they will have the same</em> $y$ <em>for all values of</em> $x$<em>, not just for some particular values of</em> $x$<em>.</em></p>
<p>In the case of $p(x)$ and $q(x)$, they are equal for at least n + 1 different values of $x$. This can be mathematically expressed as:</p>
<p>$$<br />
p(x_i)=q(x_i) \space\space\forall i \in {0,1,\dots, n}<br />
$$</p>
<p>So, the difference between $p(x)$ and $q(x)$ at all $n+1$ points will be zero. That is,</p>
<p>$$<br />
r(x_i) =p(x_i)- q(x_i) = 0 \space\space\forall i \in {0,1,\dots, n}$$</p>
<p>Therefore, $r(x)$ evaluates to zero at $n+1$ points, which implies that it is a zero polynomial. Let us see more clearly why.</p>
<p>A zero polynomial is one that evaluates to zero for all values of $x$. The simplest example of a zero polynomial is:</p>
<p>$$<br />
p(x) = 0$$</p>
<p>Another way of looking at this is, if our domain of polynomial evaluation – the set of points at which the polynomial can be evaluated – is, say, $S={x_0, x_1 \cdots, x_{n-1}}$, then a zero polynomial for this domain can be:</p>
<p>$$<br />
p(x)=(x-x_0)(x-x_1)\cdots (x-x_{n-1})$$</p>
<p>Because,</p>
<p>$$<br />
p(x) = 0 \quad \forall \quad x\in {x_0,x_1,\cdots x_{n-1}}$$</p>
<p>We could have many more zero polynomials for the domain $S$ such as:</p>
<p>$$<br />
f_1(x)=p(x)\f_2(x)=p(x)^2\f_3(x)=0\cdot p(x)$$</p>
<p>Notice that each of $f_1(x), f_2(x)$ and $f_3(x)$ evaluates to zero for the domain $S$, and thus is a zero polynomial. We can have many more. The most primitive one being $f_3(x)=0$, i.e. the constant zero itself.</p>
<p><em>Note: If the domain</em> $S$ <em>is taken as the set of all real numbers, then the only zero polynomial we can have is</em> $f(x)=0$, <em>since no other polynomial will evaluate to zero for all real numbers.</em></p>
<p>Now observe the number of roots and degree for each of the example zero polynomials we looked at.</p>
<p><em>Roots of a polynomial are the values in the domain for which the polynomial evaluates to zero, whereas the degree of a polynomial is the highest power of the variable as you well know.</em></p>
<ul>
<li>$f_1(x):$ Roots- $n$, Degree- $n$</li>
<li>$f_2(x):$ Roots- $n$, Degree- $2n$</li>
<li>$f_3(x):$ Roots- $n$, Degree- $0$</li>
</ul>
<p>All of them evaluate to zero on $S$; therefore, the number of roots is $n$, whereas the degree can be varied by modifying $p(x)$, whose degree is $n$.</p>
<p>Also note that $f_3(x)=0$ has the number of roots greater than its degree. This is only possible in the case of a zero polynomial, specifically the primitive one $f_3(x)=0$. Otherwise, the number of roots is always less than or equal to the degree.</p>
<p>Consider a non-zero polynomial of degree $n$; it can have at most $n$ roots (or intersections with the $x$-axis). The primitive zero polynomial is the only exception, as it has more roots than its degree.</p>
<p>For example,<br />
a quadratic equation of degree $2$, such as $q(x)=x^2-3x+2$, has two roots, i.e. $x=1$ and $x=2$.</p>
<p><img alt="A graph of a upward-opening parabola, $x^2-3x+2$, intersecting the x-axis at points $x = 1$ and $x = 2$." src="assets/image2.png" /></p>
<p>For a quadratic equation to have more than two roots, it must be equal to zero, i.e. $q(x)=0$.</p>
<p>Now, coming back to our argument: since $r(x)$ evaluates to zero at $n+1$ points, it must have at least $n+1$ roots, which is greater than its degree $n$. Therefore $r(x)$ must be equal to zero.</p>
<p>$$<br />
p(x)-q(x)=r(x)=0$$</p>
<p>This implies that $p(x)=q(x)$, meaning they are the same polynomial. Therefore, the polynomial of lowest degree that interpolates a set of $n+1$ distinct points is <strong>unique.</strong></p>
<div style='page-break-after: always;'></div>

<h1>Multiplicative Subgroups and Primitive Elements</h1>
<p>Source: https://rareskills.io/post/multiplicative-subgroups</p>
<h1>Multiplicative Subgroups and Primitive Elements</h1>
<h2>Introduction</h2>
<p>This chapter continues our study of group theory by exploring <strong>subgroups</strong> and <strong>generators</strong>. The concept of a <strong>primitive element</strong> will be introduced at the end. We assume you are already familiar with the definition of a group. If you need a refresher, check out <a href="https://www.rareskills.io/post/group-theory">this article</a>.</p>
<p>To build intuition, we begin with <strong>additive groups</strong>, which are straightforward and help clarify core concepts like subgroups and generators.</p>
<p>We then shift to <strong>multiplicative groups of integers modulo $n$</strong>. The integers themselves, under multiplication, do not form a group—only $1$ and $-1$ have multiplicative inverses in $\mathbb{Z}$, so the group axioms fail. To address this, we consider multiplication modulo $n$, focusing on the integers less than $n$ that are <strong>coprime</strong> to it. These coprime integers <strong>do</strong> have multiplicative inverses modulo $n$, and together they form a well-defined group. This construction plays a central role in number theory and is foundational to many cryptographic systems.</p>
<blockquote>
<p><strong>Coprime:</strong> Two numbers are coprime if their Greatest Common Divisor (GCD) is 1.</p>
<p><strong>Example 1:</strong> 8 and 15 are coprime because<br />
Factors of 8: 1, 2, 4, 8<br />
Factors of 15: 1, 3, 5, 15<br />
Common factor: 1<br />
The gcd is 1.</p>
<p><strong>Example 2:</strong> 12 and 18 are NOT coprime because<br />
Factors of 12: 1, 2, 3, 4, 6, 12<br />
Factors of 18: 1, 2, 3, 6, 9, 18<br />
Common factor: 1, 2, 3, 6<br />
The gcd is 6.</p>
</blockquote>
<p>Finally, we examine <strong>generators</strong>—elements that can produce an entire group or subgroup through repeated multiplication. Understanding generators reveals important subgroup structures, especially when $n$ is prime, and highlights their critical role in cryptographic applications.</p>
<h2>1. Additive Groups</h2>
<p>Additive groups use addition (often modulo some number) as the operation, with the identity element being $0$ and the inverse of an element $a$ being $-a$, making their structure relatively straightforward. Let’s dive into examples to see how they work.</p>
<h3>1.1 Example: $(\mathbb{Z}_6, +)$</h3>
<p>To warm up, consider $\mathbb{Z}_6 = {0, 1, 2, 3, 4, 5}$ under addition. Start with <em>closure</em>:<br />
$$3 + 4 = 7, \text{ or } 5 + 5 = 10$$<br />
takes us outside the set. To fix this, we use **addition modulo 6**:<br />
$$3 + 4 = 7 \equiv 1 \pmod{6} \text{ and } 5 + 5 = 10 \equiv 4 \pmod{6}.$$<br />
Now every result stays in ${0, 1, 2, 3, 4, 5}$. Here is the addition table:</p>
<table>
<thead>
<tr>
<th>$+ \mod 6$</th>
<th>$0$</th>
<th>$1$</th>
<th>$2$</th>
<th>$3$</th>
<th>$4$</th>
<th>$5$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$0$</td>
<td>$0$</td>
<td>$1$</td>
<td>$2$</td>
<td>$3$</td>
<td>$4$</td>
<td>$5$</td>
</tr>
<tr>
<td>$1$</td>
<td>$1$</td>
<td>$2$</td>
<td>$3$</td>
<td>$4$</td>
<td>$5$</td>
<td>$6 \equiv 0$</td>
</tr>
<tr>
<td>$2$</td>
<td>$2$</td>
<td>$3$</td>
<td>$4$</td>
<td>$5$</td>
<td>$6 \equiv 0$</td>
<td>$7 \equiv 1$</td>
</tr>
<tr>
<td>$3$</td>
<td>$3$</td>
<td>$4$</td>
<td>$5$</td>
<td>$6 \equiv 0$</td>
<td>$7 \equiv 1$</td>
<td>$8 \equiv 2$</td>
</tr>
<tr>
<td>$4$</td>
<td>$4$</td>
<td>$5$</td>
<td>$6 \equiv 0$</td>
<td>$7 \equiv 1$</td>
<td>$8 \equiv 2$</td>
<td>$9 \equiv 3$</td>
</tr>
<tr>
<td>$5$</td>
<td>$5$</td>
<td>$6 \equiv 0$</td>
<td>$7 \equiv 1$</td>
<td>$8 \equiv 2$</td>
<td>$9 \equiv 3$</td>
<td>$10 \equiv 4$</td>
</tr>
</tbody>
</table>
<p>Every result is in ${0, 1, 2, 3, 4, 5}$, so closure holds. Check the other properties:</p>
<ul>
<li><strong>Associativity:</strong> Grouping does not change the result:</li>
<li>$(2 + 3) + 4 = 5 + 4 = 9 \equiv 3 \pmod{6}$,</li>
<li>$2 + (3 + 4) = 2 + (7 \equiv 1) = 3$.</li>
<li><strong>Identity</strong>: $0$ works as identity element, since $3 + 0 = 3$ (see the table’s first row or column).</li>
<li><strong>Inverses</strong>: Each element has a pair summing to $0 \pmod{6}$:</li>
</ul>
<table>
<thead>
<tr>
<th>Element</th>
<th>Inverse</th>
<th>Check</th>
</tr>
</thead>
<tbody>
<tr>
<td>$0$</td>
<td>$0$</td>
<td>$0 + 0 = 0$</td>
</tr>
<tr>
<td>$1$</td>
<td>$5$</td>
<td>$1 + 5 = 6 \equiv 0$</td>
</tr>
<tr>
<td>$2$</td>
<td>$4$</td>
<td>$2 + 4 = 6 \equiv 0$</td>
</tr>
<tr>
<td>$3$</td>
<td>$3$</td>
<td>$3 + 3 = 6 \equiv 0$</td>
</tr>
<tr>
<td>$4$</td>
<td>$2$</td>
<td>$4 + 2 = 6 \equiv 0$</td>
</tr>
<tr>
<td>$5$</td>
<td>$1$</td>
<td>$5 + 1 = 6 \equiv 0$</td>
</tr>
</tbody>
</table>
<p>Thus, $(\mathbb{Z}_6, +)$ is a group with <strong>order</strong> (the number of elements in a set) $|\mathbb{Z}_6| = 6$.</p>
<p><strong>Exercise 1.1</strong>: Check whether $(\mathbb{Z}_{9}, +)$ is a group.</p>
<p><em>Hint:</em> Try building the addition table like we did for $\mathbb{Z}_6$.</p>
<p>Doing this by hand can be a bit tedious, so here’s a Python script that will generate the full addition table for any $\mathbb{Z}_n$:</p>
<pre style='font-family: Arial'><code class="language-solidity">
def print_addition_table(mod):
    header = [&quot;+ mod &quot; + str(mod)] + list(range(mod))
    print(&quot; | &quot;.join(str(h).rjust(4) for h in header))
    print(&quot;-&quot; * (6 * (mod + 1)))
    for row in range(mod):
        line = [str(row).rjust(4)]
        for col in range(mod):
            value = row + col
            result = value % mod
            if value &gt;= mod:
                line.append(f&quot;{value} ≡ {result}&quot;.rjust(6))
            else:
                line.append(str(result).rjust(6))
        print(&quot; | &quot;.join(line))

# Try it with Z_9
print_addition_table(9)
</code></pre>
<p><strong>Follow-up:</strong> After analyzing $\mathbb{Z}_9$, try generating the table for $\mathbb{Z}_{10}$. Can you determine whether $(\mathbb{Z}_{10}, +)$ is also a group?</p>
<p>Thus, $(\mathbb{Z}_n, +)$ is a group with order $n$. Finite groups like this are key in modular arithmetic and cryptography. Next, we turn our attention to how certain elements and subsets within a group can reveal deeper structure—through subgroups and generators.</p>
<h2>2. Subgroups and Generators</h2>
<h3>2.1 Understanding Subgroups</h3>
<p>When studying groups, we often encounter subsets that retain the group’s structure under the same operation. These special subsets, called <strong>subgroups</strong>, behave like miniature versions of the parent group. Not every subset qualifies, though—let’s explore this with examples to see what makes a subgroup.</p>
<h4>Example 2.1.1: Subgroups in $(\mathbb{Z}, +)$</h4>
<p>Consider $(\mathbb{Z}, +)$, the group of all integers under addition, and two familiar subsets:</p>
<ul>
<li>The set of <strong>even integers</strong>: ${\ldots, -4, -2, 0, 2, 4, \ldots}$</li>
<li>The set of <strong>odd integers</strong>: ${\ldots, -3, -1, 1, 3, 5, \ldots}$</li>
</ul>
<p>Let’s check the even integers:</p>
<ul>
<li><strong>Closure</strong>: The sum of two even numbers is even (e.g., $-2 +4= 2$).</li>
<li><strong>Identity</strong>: $0$ is even and included.</li>
<li><strong>Inverses</strong>: The inverse of any even number is also even (e.g., the inverse of $2$ is $-2$).</li>
<li><strong>Associativity</strong>: Inherited from $\mathbb{Z}$.</li>
</ul>
<p>The even integers satisfy all group properties—this is a valid subgroup.</p>
<p>Now check the odd integers:</p>
<ul>
<li><strong>Closure</strong>: $1 + 3= 4$, which is even—not part of the set. So closure fails.</li>
<li><strong>Identity</strong>: $0$ is not odd, so the identity element is missing.</li>
<li><strong>Inverses</strong>: For $1$, $-1$ is odd—but since closure and identity already fail, it is not a subgroup.</li>
</ul>
<p>The odd integers under addition do not satisfy the subgroup criteria—they’re just a subset, not a subgroup.</p>
<h4>Example 2.1.2: Subgroups in $(\mathbb{Z}_8, +)$</h4>
<p>Now take $(\mathbb{Z}_8, +) = {0, 1, 2, 3, 4, 5, 6, 7}$ and test two subsets:</p>
<ul>
<li><strong>Evens modulo 8</strong>: ${0, 2, 4, 6}$</li>
<li><strong>First half</strong>: ${0, 1, 2, 3}$</li>
</ul>
<p>For ${0, 2, 4, 6}$:</p>
<ul>
<li><strong>Closure</strong>: $2 +4= 6$, $4 +6= 10 \equiv2\pmod{8}$ (both in set).</li>
<li><strong>Identity</strong>: $0$ is present.</li>
<li><strong>Inverses</strong>: $2 +6= 8 \equiv 0$, $4 +4= 8 \equiv 0$, $0 + 0 = 0$ (all pairs work).</li>
<li><strong>Associativity</strong>: Holds from $\mathbb{Z}_8$.</li>
</ul>
<p>This is a subgroup!</p>
<p>For ${0, 1, 2, 3}$ :</p>
<ul>
<li><strong>Closure</strong>: $2+3= 5$ (not in set).</li>
<li><strong>Identity</strong>: $0$ is present.</li>
<li><strong>Inverses</strong>: For $1$, no element in ${0, 1, 2, 3}$ gives $0$ (e.g., $1 +3=4\pmod{8}$).</li>
</ul>
<p>This fails to be a group, so it is only a subset, not a subgroup.</p>
<p><strong>Definition</strong>: A subset $H$ of a group $G$ is a <strong>subgroup</strong> if it is a group under the same operation, meaning it satisfies closure, contains the identity, has inverses for all its elements, and inherits associativity from $G$.</p>
<p><strong>Exercise 2.1.1:</strong> Find all subgroups of $(\mathbb{Z}_{5}, +)$.</p>
<h3>2.2 Generators in Additive Groups</h3>
<p>Now that we have seen subgroups, let’s explore how single elements can generate them—or even the whole group. We’ll revisit $(\mathbb{Z}_6, +) = {0, 1, 2, 3, 4, 5}$, the additive group under modulo $6$ addition, and examine what happens when we repeatedly add an element to itself, like <em>"walking</em>" around the numbers modulo $6$.</p>
<h4>Example 2.2.1: Generators in $(\mathbb{Z}_6, +)$</h4>
<p><em>Try 1:</em></p>
<ul>
<li>$1$</li>
<li>$1 + 1 = 2$</li>
<li>$2 + 1 = 3$</li>
<li>$3 + 1 = 4$</li>
<li>$4 + 1 = 5$</li>
<li>$5 + 1 = 6 \equiv 0 \pmod{6}$</li>
</ul>
<p>This gives ${0, 1, 2, 3, 4, 5} = \mathbb{Z}_6$. Repeatedly adding $1$ cycles through the <strong>entire group</strong>.</p>
<p><em>Try 2:</em></p>
<ul>
<li>$2$</li>
<li>$2 + 2 = 4$</li>
<li>$4 + 2 = 6 \equiv 0 \pmod{6}$</li>
<li>$0 + 2 = 2$</li>
</ul>
<p>This gives ${0, 2, 4}$—only <strong>half the group</strong>.</p>
<p><em>Try 3:</em></p>
<ul>
<li>$3$</li>
<li>$3 + 3 = 6 \equiv 0 \pmod{6}$</li>
<li>$0 + 3 = 3$</li>
</ul>
<p>This gives ${0, 3}$, even smaller!</p>
<p><em>Try 5:</em></p>
<ul>
<li>$5$</li>
<li>$5 + 5 = 10 \equiv 4 \pmod{6}$</li>
<li>$4 + 5 = 9 \equiv 3 \pmod{6}$</li>
<li>$3 + 5 = 8 \equiv 2 \pmod{6}$</li>
<li>$2 + 5 = 7 \equiv 1 \pmod{6}$</li>
<li>$1 + 5 = 6 \equiv 0 \pmod{6}$</li>
</ul>
<p>This gives ${0, 1, 2, 3, 4, 5} = \mathbb{Z}_6$, covering everything, just like $1$.</p>
<p><strong>Exercise 2.2.1</strong>: In $(\mathbb{Z}_9, +)$, which elements generate the entire $\mathbb{Z}_9$? Which elements generate proper subgroups?</p>
<h4>2.2.2 Subgroups Generated by Elements</h4>
<p>Now consider any group $G$ with a binary operation (like addition or multiplication), and let $g$ be an element of $G$. By repeatedly applying the group operation to $g$, we can form the set of elements it generates. This set is denoted $\langle g \rangle$ and is called the <strong>cyclic subgroup generated by $g$</strong>.</p>
<p>For example in additive groups:</p>
<p>$$\langle g \rangle = { g, g + g, g + g + g, \ldots } = { n \cdot g \mid n \in \mathbb{Z} }$$</p>
<p>For many elements, $\langle g \rangle$ is a <strong>proper subgroup</strong>. But when $\langle g \rangle = G$, we say that $g$ is a <strong>generator</strong> of $G$, and that $G$ is a <strong>cyclic group</strong>.<br />
As shown in Example 2.2.1, we computed the subgroups generated by elements in $(\mathbb{Z}_6, +) = {0, 1, 2, 3, 4, 5}$ under addition modulo 6. The subgroups generated by each element are as follows:</p>
<ul>
<li>$\langle 0 \rangle = {0}$ (proper subgroup)</li>
<li>$\langle 1 \rangle = {0, 1, 2, 3, 4, 5} = \mathbb{Z}_6$</li>
<li>$\langle 2 \rangle = {0, 2, 4}$ (proper subgroup)</li>
<li>$\langle 3 \rangle = {0, 3}$ (proper subgroup)</li>
<li>$\langle 4 \rangle = {0, 4, 2}$ (proper subgroup)</li>
<li>$\langle 5 \rangle = {0, 1, 2, 3, 4, 5} = \mathbb{Z}_6$</li>
</ul>
<p>Elements $1$ and $5$ generate the entire group $\mathbb{Z}_6$, making them generators, while $0$, $2$, $3$, and $4$ generate proper subgroups.</p>
<h4>Example 2.2.3: Generators in $(\mathbb{Z}_5, +)$</h4>
<p>Now test $\mathbb{Z}_5 = {0, 1, 2, 3, 4}$ under addition modulo 5:</p>
<ul>
<li>$\langle1\rangle = {0, 1, 2, 3, 4}=\mathbb{Z}_5$</li>
<li>$\langle2\rangle ={2, 4, 6 \equiv 1 \pmod 5 , 8 \equiv 3 \pmod 5} ={1,2,3,4}=\mathbb{Z}_5$</li>
<li>$\langle3\rangle= {3, 6 \equiv 1 \pmod 5, 9 \equiv 4\pmod 5 , 7 \equiv 2\pmod 5 }={1,2,3,4}=\mathbb{Z}_5$</li>
<li>$\langle4\rangle ={4, 8 \equiv 3 \pmod 5, 12 \equiv 2\pmod 5 , 16 \equiv 1\pmod 5 }={1,2,3,4}=\mathbb{Z}_5$</li>
</ul>
<p>Every non-zero element is a generator.</p>
<p>This happens because $\gcd(g, 5) = 1$ for all $g \neq 0$, and $5$ is prime.</p>
<p><strong>Conclusion</strong>: In $(\mathbb{Z}_n, +)$, an element $g$ generates the entire group if and only if $\gcd(g, n) = 1$. For prime $n$, all non-zero elements are generators. For composite $n$, only some elements qualify.</p>
<h3>2.3 Code: Exploring Additive Generators in $(\mathbb{Z}_n, +)$</h3>
<pre style='font-family: Arial'><code class="language-solidity">def additive_closure(a, n):
    &quot;&quot;&quot;Generates {0, a, 2a, 3a, ...} mod n until it repeats.&quot;&quot;&quot;
    result = []
    current = 0
    while current not in result:
        result.append(current)
        current = (current + a) % n
    return sorted(result)  # Sorted for readability

# Show generated sets in (Z_6 , +)
print(&quot;Z_6:&quot;)
for a in range(6):
    print(f&quot;Generated by {a}: {additive_closure(a, 6)}&quot;)

# Show generated sets in (Z_5, +)
print(&quot;\nZ_5:&quot;)
for a in range(5):
    print(f&quot;Generated by {a}: {additive_closure(a, 5)}&quot;)
</code></pre>
<p><strong>Output:</strong></p>
<pre style='font-family: Arial'><code class="language-solidity">(Z_6, +):
Generated by 0: [0]
Generated by 1: [0, 1, 2, 3, 4, 5]
Generated by 2: [0, 2, 4]
Generated by 3: [0, 3]
Generated by 4: [0, 2, 4]
Generated by 5: [0, 1, 2, 3, 4, 5]

(Z_5, +):
Generated by 0: [0]
Generated by 1: [0, 1, 2, 3, 4]
Generated by 2: [0, 1, 2, 3, 4]
Generated by 3: [0, 1, 2, 3, 4]
Generated by 4: [0, 1, 2, 3, 4]
</code></pre>
<p>Having explored additive groups, we now turn to their multiplicative counterparts.</p>
<h2>3. Multiplicative Groups</h2>
<p>Multiplicative groups operate under multiplication (often modulo some number), with the identity element being $1$, though finding inverses can be more nuanced, especially in finite settings. To illustrate this, we’ll examine a concrete example using multiplication modulo $7$.</p>
<h3>3.1 Example:</h3>
<p>Consider $\mathbb{Z}_7 = {0, 1, 2, 3, 4, 5, 6}$:</p>
<table>
<thead>
<tr>
<th>$\times\mod 7$</th>
<th>$0$</th>
<th>$1$</th>
<th>$2$</th>
<th>$3$</th>
<th>$4$</th>
<th>$5$</th>
<th>$6$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$0$</td>
<td>$0$</td>
<td>$0$</td>
<td>$0$</td>
<td>$0$</td>
<td>$0$</td>
<td>$0$</td>
<td>$0$</td>
</tr>
<tr>
<td>$1$</td>
<td>$0$</td>
<td>$1$</td>
<td>$2$</td>
<td>$3$</td>
<td>$4$</td>
<td>$5$</td>
<td>$6$</td>
</tr>
<tr>
<td>$2$</td>
<td>$0$</td>
<td>$2$</td>
<td>$4$</td>
<td>$6$</td>
<td>$8 \equiv 1$</td>
<td>$10 \equiv 3$</td>
<td>$12 \equiv 5$</td>
</tr>
<tr>
<td>$3$</td>
<td>$0$</td>
<td>$3$</td>
<td>$6$</td>
<td>$9 \equiv2$</td>
<td>$12 \equiv5$</td>
<td>$15 \equiv1$</td>
<td>$18 \equiv4$</td>
</tr>
<tr>
<td>$4$</td>
<td>$0$</td>
<td>$4$</td>
<td>$8 \equiv1$</td>
<td>$12 \equiv5$</td>
<td>$16 \equiv2$</td>
<td>$20 \equiv6$</td>
<td>$24 \equiv3$</td>
</tr>
<tr>
<td>$5$</td>
<td>$0$</td>
<td>$5$</td>
<td>$10 \equiv3$</td>
<td>$15 \equiv1$</td>
<td>$20 \equiv6$</td>
<td>$25 \equiv4$</td>
<td>$30 \equiv2$</td>
</tr>
<tr>
<td>$6$</td>
<td>$0$</td>
<td>$6$</td>
<td>$12 \equiv5$</td>
<td>$18 \equiv4$</td>
<td>$24 \equiv3$</td>
<td>$30 \equiv2$</td>
<td>$36 \equiv1$</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Closure</strong> holds, e.g., $2 \times5= 10 \equiv 3$, is in the set.</li>
<li><strong>Identity</strong>: The identity element is $1$, but this fails for $0$, which has no inverse;</li>
<li><strong>Inverses</strong>: Each nonzero element has an inverse:</li>
<li>$1 \times1= 1$</li>
<li>$2 \times4= 8 \equiv1\pmod{7}$</li>
<li>$3 \times5= 15 \equiv1\pmod{7}$</li>
<li>$4 \times2= 8 \equiv1\pmod{7}$</li>
<li>$5 \times3= 15 \equiv1\pmod{7}$</li>
<li>$6 \times6= 36 \equiv1\pmod{7}$</li>
</ul>
<p>Since $0$ has no inverse, $(\mathbb{Z}_7, \times)$ is <strong>not</strong> a group. But if we remove $0$ and consider only the nonzero elements — that is, $\mathbb{Z}_7 \setminus {0} = {1, 2, 3, 4, 5, 6}$ — we do get a group under multiplication. This set is often denoted $\mathbb{Z}_7^*$ and is a group of order 6.</p>
<p>You can use this code to generate the multiplication table for any modulus $n$:</p>
<pre style='font-family: Arial'><code class="language-solidity">def print_multiplication_table(mod):
    header = [&quot;× mod &quot; + str(mod)] + list(range(mod))
    print(&quot; | &quot;.join(str(h).rjust(4) for h in header))
    print(&quot;-&quot; * (6 * (mod + 1)))

    for row in range(mod):
        line = [str(row).rjust(4)]
        for col in range(mod):
            value = row * col
            result = value % mod
            if value &gt;= mod:
                line.append(f&quot;{value} ≡ {result}&quot;.rjust(6))
            else:
                line.append(str(result).rjust(6))
        print(&quot; | &quot;.join(line))

print_multiplication_table(5)
</code></pre>
<h3>3.2 Example: $(\mathbb{Z}_6\setminus {0}, \times),$ Is Not a Group</h3>
<p>Let’s now test the set ${1, 2, 3, 4, 5}$ under multiplication mod 6.</p>
<table>
<thead>
<tr>
<th>$\times \mod 6$</th>
<th>$1$</th>
<th>$2$</th>
<th>$3$</th>
<th>$4$</th>
<th>$5$</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1</strong></td>
<td>$1$</td>
<td>$2$</td>
<td>$3$</td>
<td>$4$</td>
<td>$5$</td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>$2$</td>
<td>$4$</td>
<td>$6 \equiv 0$</td>
<td>$8 \equiv 2$</td>
<td>$10\equiv 4$</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>$3$</td>
<td>$6\equiv 0$</td>
<td>$9 \equiv 3$</td>
<td>$12 \equiv 0$</td>
<td>$15 \equiv 3$</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>$4$</td>
<td>$8\equiv 2$</td>
<td>$12 \equiv 0$</td>
<td>$16 \equiv 4$</td>
<td>$20 \equiv 2$</td>
</tr>
<tr>
<td><strong>5</strong></td>
<td>$5$</td>
<td>$10\equiv 4$</td>
<td>$15 \equiv 3$</td>
<td>$20 \equiv 2$</td>
<td>$25 \equiv1$</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Closure</strong>: Holds.</li>
<li><strong>Identity</strong>:$1$ still works.</li>
<li><strong>Inverses</strong>: Inverses fail. Only $1$ and $5$ are invertible:</li>
<li>$1 \times1= 1$</li>
<li>$5 \times5= 25 \equiv1\mod 6$</li>
</ul>
<p>Elements $2$, $3$, and $4$ have no counterparts that yield $1$. This failure arises because $6$ is not prime: it factors as $6 =2\times 3$. The factorization introduces zero divisors—nonzero elements that, when multiplied by other nonzero elements, yield zero modulo $n$. For example, $4 \times3= 12 \equiv 0 \pmod{6}$, even though both $3$ and $4$ are nonzero in $\mathbb{Z}_6$. Since $2$, $3$, and $4$ share common factors with $6$ (namely $2$, $3$, and $2$, respectively), they fail to have multiplicative inverses.</p>
<h3>3.3 Example: $(\mathbb{Z}_8\setminus{0}, \times)$</h3>
<p>As another example, let’s now consider multiplication in $\mathbb{Z}_8\setminus{0} = {1, 2, 3, 4, 5, 6, 7}$:</p>
<table>
<thead>
<tr>
<th>$\times$</th>
<th>$1$</th>
<th>$2$</th>
<th>$3$</th>
<th>$4$</th>
<th>$5$</th>
<th>$6$</th>
<th>$7$</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>$1$</strong></td>
<td>$1$</td>
<td>$2$</td>
<td>$3$</td>
<td>$4$</td>
<td>$5$</td>
<td>$6$</td>
<td>$7$</td>
</tr>
<tr>
<td><strong>$2$</strong></td>
<td>$2$</td>
<td>$4$</td>
<td>$6$</td>
<td>$8 \equiv 0$</td>
<td>$10 \equiv 2$</td>
<td>$12 \equiv 4$</td>
<td>$14 \equiv 6$</td>
</tr>
<tr>
<td><strong>$3$</strong></td>
<td>$3$</td>
<td>$6$</td>
<td>$9 \equiv 1$</td>
<td>$12 \equiv 4$</td>
<td>$15 \equiv 7$</td>
<td>$18 \equiv 2$</td>
<td>$21 \equiv 5$</td>
</tr>
<tr>
<td><strong>$4$</strong></td>
<td>$4$</td>
<td>$8 \equiv 0$</td>
<td>$12 \equiv 4$</td>
<td>$16 \equiv 0$</td>
<td>$20 \equiv 4$</td>
<td>$24 \equiv 0$</td>
<td>$28 \equiv 4$</td>
</tr>
<tr>
<td><strong>$5$</strong></td>
<td>$5$</td>
<td>$10 \equiv 2$</td>
<td>$15 \equiv 7$</td>
<td>$20 \equiv 4$</td>
<td>$25 \equiv 1$</td>
<td>$30 \equiv 6$</td>
<td>$35 \equiv 3$</td>
</tr>
<tr>
<td><strong>$6$</strong></td>
<td>$6$</td>
<td>$12 \equiv 4$</td>
<td>$18 \equiv 2$</td>
<td>$24 \equiv 0$</td>
<td>$30 \equiv 6$</td>
<td>$36 \equiv 4$</td>
<td>$42 \equiv 2$</td>
</tr>
<tr>
<td><strong>$7$</strong></td>
<td>$7$</td>
<td>$14 \equiv 6$</td>
<td>$21 \equiv 5$</td>
<td>$28 \equiv 4$</td>
<td>$35 \equiv 3$</td>
<td>$42 \equiv 2$</td>
<td>$49 \equiv 1$</td>
</tr>
</tbody>
</table>
<p>Only the elements ${1, 3, 5, 7}$—those that are coprime with 8—have multiplicative inverses modulo 8:</p>
<ul>
<li>$1 \times1= 1 \pmod 8$</li>
<li>$3 \times3= 9 \equiv 1 \pmod 8$</li>
<li>$5 \times5= 25 \equiv 1 \pmod 8$</li>
<li>$7 \times7= 49 \equiv 1 \pmod 8$</li>
</ul>
<p>The remaining elements, ${2, 4, 6}$, all share a common factor with 8 and therefore do not have inverses. Thus, $\mathbb{Z}_8 \setminus {0}$ is <strong>not</strong> a group under multiplication.</p>
<h3>3.4. What Is $\mathbb{Z}_n^*$?</h3>
<p>The set $\mathbb{Z}_n^*$ is formally defined as:</p>
<p>$$<br />
\mathbb{Z}_n^* = { a \in \mathbb{Z}_n \mid \gcd(a, n) =1}$$</p>
<p>That is, $\mathbb{Z}_n^*$ consists of all elements in $\mathbb{Z}_n$ that have a <strong>multiplicative inverse modulo $n$</strong>.</p>
<ul>
<li>When <strong>$n$ is prime</strong>, every nonzero element is invertible, so:<br />
  $$\mathbb{Z}_n^* = \mathbb{Z}_n \setminus {0}$$</li>
<li>
<p>When <strong>$n$ is composite</strong>, only elements <strong>coprime</strong> to $n$ are invertible. For example:</p>
</li>
<li>
<p>$\mathbb{Z}_6^* = {1, 5}$</p>
</li>
<li>$\mathbb{Z}_8^* = {1, 3, 5, 7}$</li>
</ul>
<p>In both cases, $\mathbb{Z}_n^*$ forms a <strong>group</strong> under multiplication. However, the full set $\mathbb{Z}_n \setminus {0}$ does <strong>not</strong> always form a group—unless $n$ is prime.</p>
<p>Earlier, we used $\mathbb{Z}_n \setminus {0}$ to demonstrate why the absence of inverses (and presence of zero divisors) breaks group structure when $n$ is composite. That distinction is more than technical: it plays a central role in understanding modular arithmetic, cryptographic algorithms, and number theory.</p>
<p>In the next section, we’ll explore how the structure of $\mathbb{Z}_n^*$ changes when $n$ is prime.</p>
<h3>3.5 Why Prime Moduli Work</h3>
<p>These failures motivate <strong>prime moduli</strong>. In ${1,2,\ldots, p-1}$, where $p$ is prime, every element has an inverse. In fact, this is guaranteed by a classic result from number theory.</p>
<p>Consider the pattern for ${1, 2, \ldots, n-1}$ modulo $n$:</p>
<ul>
<li>If $n$ is <strong>not prime</strong>, it is <strong>not</strong> a group due to zero divisors and missing inverses (e.g., $n = 6, 8$).</li>
<li>If $n$ <strong>is prime</strong>, it is a group.</li>
</ul>
<h3>3.6 Python Code for Computing Inverse</h3>
<p>One of the simplest and most efficient methods is to use Python’s built-in<br />
<code style='font-family: Arial'>pow(a, -1, p)</code> to compute modular inverses. Behind the scenes, this works because of a famous number theory result called <strong>Fermat’s Little Theorem</strong>, which guarantees that an inverse exists and tells us:</p>
<p>$$<br />
a^{-1} \equiv a^{p – 2} \pmod{p}$$</p>
<p>when the modulus $p$ is a prime number and $a$ is not divisible by $p$.</p>
<p>Here’s an example:</p>
<pre style='font-family: Arial'><code class="language-solidity">def mod_inverse(a, p):  
    return pow(a, -1, p)

# Test for Z_7^*
def test_inverses(p):
    print(f&quot;Testing inverses modulo {p}:&quot;)
    for a in range(1, p):
        inv = mod_inverse(a, p)
        print(f&quot;Inverse of {a} mod {p} is {inv} (check: {a} * {inv} = {a * inv % p})&quot;)

if __name__ == &quot;__main__&quot;:
    p = 7
    test_inverses(p)
</code></pre>
<p><strong>Output:</strong></p>
<pre style='font-family: Arial'><code class="language-solidity">Testing inverses modulo 7:
Inverse of 1 mod 7 is 1 (check: 1 * 1 = 1)
Inverse of 2 mod 7 is 4 (check: 2 * 4 = 1)
Inverse of 3 mod 7 is 5 (check: 3 * 5 = 1)
Inverse of 4 mod 7 is 2 (check: 4 * 2 = 1)
Inverse of 5 mod 7 is 3 (check: 5 * 3 = 1)
Inverse of 6 mod 7 is 6 (check: 6 * 6 = 1)
</code></pre>
<h3>3.7 Exercises</h3>
<h4>Math Exercises:</h4>
<ol>
<li>Find the modular inverse (if it exists) for the following, using Python’s <code style='font-family: Arial'>pow(a, -1, n)</code> function or a calculator. For prime moduli, you may optionally use Fermat’s Little Theorem:</li>
</ol>
<ul>
<li>$3^{-1} \mod 11$</li>
<li>$7^{-1} \mod 26$</li>
<li>$4^{-1} \mod 15$<br />
2. For which values of $a$ in $\mathbb{Z}_{12}^*$ does the modular inverse exist?</li>
</ul>
<h4>Coding Exercises:</h4>
<ol>
<li>Write a function <code style='font-family: Arial'>list_all_inverses(n)</code> that returns a dictionary of all elements in $\mathbb{Z}_n^*$ with their inverses (if they exist).</li>
<li>Write a program that takes user input <code style='font-family: Arial'>a</code> and <code style='font-family: Arial'>n</code>, and checks whether a modular inverse exists. If it does, print the inverse. Try it with different values and see what you discover.</li>
<li><strong>Challenge:</strong> Pick a few small primes <code style='font-family: Arial'>p</code>, and write a program that checks whether<br />
<code style='font-family: Arial'>pow(a, p - 1, p) == 1</code> for all <code style='font-family: Arial'>a</code> in <code style='font-family: Arial'>{1, 2, ..., p - 1}</code>.<br />
   Does this hold for every <code style='font-family: Arial'>a</code>? What happens if <code style='font-family: Arial'>p</code> isn’t prime?</li>
</ol>
<h2>4. Generators in Multiplicative Groups</h2>
<p>We now turn our attention to generators in multiplicative groups. To build intuition, we begin with concrete examples in $\mathbb{Z}_7^* = {1, 2, 3, 4, 5, 6}$, exploring how individual elements can generate the full group—or just a part of it—through repeated multiplication</p>
<h4>Example 4.1: $(\mathbb{Z}_7^*, \times)$</h4>
<p><em>Try 3:</em></p>
<ul>
<li>$3^1 = 3$</li>
<li>$3^2 = 9 \equiv 2 \pmod{7}$</li>
<li>$3^3 = 27 \equiv 6 \pmod{7}$</li>
<li>$3^4 = 81 \equiv 4 \pmod{7}$</li>
<li>$3^5 = 243 \equiv 5 \pmod{7}$</li>
<li>$3^6 = 729 \equiv 1 \pmod{7}$</li>
</ul>
<p>$\langle 3 \rangle = {1, 2, 3, 4, 5, 6} = \mathbb{Z}_7^*$. Element $3$ is a generator.</p>
<p><em>Try 2:</em></p>
<ul>
<li>$2^1 = 2$</li>
<li>$2^2 = 4$</li>
<li>$2^3 = 8 \equiv 1 \pmod{7}$</li>
</ul>
<p>$\langle 2 \rangle = {1, 2, 4}$, a subgroup, not the full group.</p>
<p><em>Try Other Elements:</em></p>
<ul>
<li>$4$: $4, 16 \equiv 2, 8 \equiv 1$ $\Rightarrow {1, 2, 4}$</li>
<li>$5$: $5, 25 \equiv 4, 20 \equiv 6, 30 \equiv 2, 10 \equiv 3, 15 \equiv 1$ $\Rightarrow \mathbb{Z}_7^*$</li>
<li>$6$: $6, 36 \equiv 1$ $\Rightarrow {1, 6}$</li>
</ul>
<p><strong>Summary</strong>:</p>
<table>
<thead>
<tr>
<th>Element</th>
<th>Generated Set</th>
<th>Size</th>
<th>Generator?</th>
</tr>
</thead>
<tbody>
<tr>
<td>$2$</td>
<td>${1, 2, 4}$</td>
<td>$3$</td>
<td>❌</td>
</tr>
<tr>
<td>$3$</td>
<td>${1, 2, 3, 4, 5, 6}$</td>
<td>$6$</td>
<td>✅</td>
</tr>
<tr>
<td>$4$</td>
<td>${1, 2, 4}$</td>
<td>$3$</td>
<td>❌</td>
</tr>
<tr>
<td>$5$</td>
<td>${1, 2, 3, 4, 5, 6}$</td>
<td>$6$</td>
<td>✅</td>
</tr>
<tr>
<td>$6$</td>
<td>${1, 6}$</td>
<td>$2$</td>
<td>❌</td>
</tr>
</tbody>
</table>
<h3>4.1 Primitive Elements</h3>
<p>As we see, in the additive group $(\mathbb{Z}_p, +)$, where $p$ is prime, <strong>every non-zero element is guaranteed to be a generator</strong>. That is, repeatedly adding any non-zero element will eventually cycle through all elements of the group. However, in multiplicative groups $(\mathbb{Z}_p^*, \times)$, <strong>only some elements are generators</strong>—these are called <strong>primitive elements</strong>. This contrast highlights a fundamental difference: additive groups over primes are always cyclic with all non-zero elements as generators, while multiplicative groups over primes are cyclic but have only a subset of elements that serve as generators.</p>
<blockquote>
<p><strong>Definition</strong>: An element $g \in \mathbb{Z}_p^*$ is a <strong>primitive element</strong> if $$\langle g \rangle = {g^1, g^2, \ldots, g^{p-1}} \pmod{p} = \mathbb{Z}_p^*.$$</p>
</blockquote>
<p>Take, for example, $\mathbb{Z}_7^*$: $3$ and $5$ are primitive elements, while $2$, $4$, and $6$ are not.</p>
<p>A <a href="https://en.wikipedia.org/wiki/Primitive_root_modulo_n">foundational result</a> in number theory guarantees that for any prime $p$, the group $\mathbb{Z}_p^*$ is <strong>cyclic</strong>, which means it always contains at least one primitive element. This stands in contrast to the additive group $(\mathbb{Z}_p, +)$, where <strong>every non-zero element</strong> generates the full group.</p>
<h4>Example 4.1.1: $\mathbb{Z}_5^* = {1, 2, 3, 4}$</h4>
<ul>
<li><strong>Element 2</strong>:<br />
  $$\begin{align*}<br />
  2^1 &amp;= 2 \pmod{5}, \<br />
  2^2 &amp;= 4 \pmod{5}, \<br />
  2^3 &amp;= 8 \equiv 3 \pmod{5}, \<br />
  2^4 &amp;= 16 \equiv 1 \pmod{5}<br />
  \end{align*}$$<br />
  $\langle 2 \rangle = {1, 2, 3, 4} = \mathbb{Z}_5^*$, so $2$ is a primitive element.</li>
<li><strong>Element 3</strong>:<br />
  $$\begin{align*}<br />
  3^1 &amp;= 3 \pmod{5}, \<br />
  3^2 &amp;= 9 \equiv 4 \pmod{5}, \<br />
  3^3 &amp;= 27 \equiv 2 \pmod{5}, \<br />
  3^4 &amp;= 81 \equiv 1 \pmod{5}<br />
  \end{align*}$$<br />
  $\langle 3 \rangle = {1, 2, 3, 4} = \mathbb{Z}_5^*$, so $3$ is a primitive element too.</li>
</ul>
<h4>Example 4.1.2: $\mathbb{Z}_{11}^* = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}$</h4>
<ul>
<li><strong>Element 2</strong>:<br />
  $$\begin{align*}<br />
  2^1 &amp;= 2 \pmod{11}, \<br />
  2^2 &amp;= 4 \pmod{11}, \<br />
  2^3 &amp;= 8 \pmod{11}, \<br />
  2^4 &amp;= 16 \equiv 5 \pmod{11}, \<br />
  2^5 &amp;= 10 \pmod{11}, \<br />
  2^6 &amp;= 20 \equiv 9 \pmod{11}, \<br />
  2^7 &amp;= 18 \equiv 7 \pmod{11}, \<br />
  2^8 &amp;= 14 \equiv 3 \pmod{11}, \<br />
  2^9 &amp;= 6 \pmod{11}, \<br />
  2^{10} &amp;= 12 \equiv 1 \pmod{11}<br />
  \end{align*}$$<br />
  $\langle 2 \rangle = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} = \mathbb{Z}_{11}^*$, so $2$ is a primitive element.</li>
<li><strong>Element 3</strong>:<br />
  $$\begin{align*}<br />
  3^1 &amp;= 3 \pmod{11}, \<br />
  3^2 &amp;= 9 \pmod{11}, \<br />
  3^3 &amp;= 27 \equiv 5 \pmod{11}, \<br />
  3^4 &amp;= 15 \equiv 4 \pmod{11}, \<br />
  3^5 &amp;= 12 \equiv 1 \pmod{11}<br />
  \end{align*}$$<br />
  $\langle 3 \rangle = {1, 3, 4, 5, 9}$, a subgroup, not the full group.</li>
</ul>
<h4>Example 4.1.3: $\mathbb{Z}_{17}^*$</h4>
<ul>
<li><strong>Element 3</strong>:<br />
  $$\begin{align*}<br />
  3^1 &amp;= 3 \pmod{17}, \<br />
  3^2 &amp;= 9 \pmod{17}, \<br />
  3^3 &amp;= 27 \equiv 10 \pmod{17}, \<br />
  3^4 &amp;= 81 \equiv 13 \pmod{17}, \<br />
  3^5 &amp;= 243 \equiv 5 \pmod{17}, \<br />
  3^6 &amp;= 729 \equiv 15 \pmod{17}, \<br />
  3^7 &amp;= 2187 \equiv 11 \pmod{17}, \<br />
  3^8 &amp;= 6561 \equiv 16 \pmod{17}, \<br />
  3^9 &amp;= 19683 \equiv 14 \pmod{17}, \<br />
  3^{10} &amp;= 59049 \equiv 8 \pmod{17}, \<br />
  3^{11} &amp;= 177147 \equiv 7 \pmod{17}, \<br />
  3^{12} &amp;= 531441 \equiv 4 \pmod{17}, \<br />
  3^{13} &amp;= 1594323 \equiv 12 \pmod{17}, \<br />
  3^{14} &amp;= 4782969 \equiv 2 \pmod{17}, \<br />
  3^{15} &amp;= 14348907 \equiv 6 \pmod{17}, \<br />
  3^{16} &amp;= 43046721 \equiv 1 \pmod{17}<br />
  \end{align*}$$</li>
</ul>
<p>$\langle 3 \rangle = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16} = \mathbb{Z}_{17}^*$, order $16$ (full group). So, $3$ is a primitive element of $\mathbb{Z}_{17}^*$.</p>
<p><strong>Additional Note:</strong> Although we mentioned earlier that $\mathbb{Z}_n^*$ is always a group, it is <strong>not always cyclic</strong> when $n$ is not prime. In contrast, when $n = p$ is prime, $\mathbb{Z}_p^*$ is <strong>always cyclic</strong>. This distinction matters in practice—especially in cryptography—where we prefer to work with <strong>cyclic groups</strong>, so we often choose prime moduli to ensure that $\mathbb{Z}_p^*$ has this cyclic structure.</p>
<h3>4.2 Python Code: Finding Primitive Elements Modulo a Prime</h3>
<p>To find primitive elements in $\mathbb{Z}_p^*$, we compute the subgroup generated by a candidate element and check whether it includes <strong>all</strong> elements of $\mathbb{Z}_p^*$. The following code verifies whether a given element $g$ is primitive by performing this check. You can use it to explore and test more examples of primitive elements:</p>
<pre style='font-family: Arial'><code class="language-solidity">def is_primitive_element(g, p):
    &quot;&quot;&quot;Check if g is a primitive element modulo p.&quot;&quot;&quot;
    required = set(range(1, p))
    generated = set()
    val = 1
    for _ in range(1, p):
        val = (val * g) % p
        generated.add(val)
    return generated == required

def primitive_elements(p):
    &quot;&quot;&quot;Find all primitive elements of prime p.&quot;&quot;&quot;
    elements = []
    for g in range(2, p):
        if is_primitive_element(g, p):
            elements.append(g)
    return elements

# Example usage:
prime = 11
print(f&quot;Primitive elements modulo {prime}: {primitive_elements(prime)}&quot;)
</code></pre>
<p><strong>Sample Output:</strong></p>
<pre style='font-family: Arial'><code class="language-solidity">Primitive elements modulo 11: [2, 6, 7, 8]
</code></pre>
<p>For a more efficient approach, the galois library can directly compute primitive elements in the multiplicative group of a finite field, which for a prime $p$ corresponds to $\mathbb{Z}_p^*$. First, install the library with <code style='font-family: Arial'>pip install galois</code>, then use:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
print(galois.GF(7).primitive_elements)  # Output: [3, 5]
</code></pre>
<p>This method is optimized for large primes, making it ideal for practical applications, while the manual code above helps understand the concept of primitive elements.</p>
<h4>Exercise</h4>
<p>Let’s apply what we’ve learned about generators and primitive elements.</p>
<ol>
<li>Use the Python code above to find a generator of $(\mathbb{Z}_{13}^*, \times)$.<br />
<em>(Hint: You’re looking for an element whose powers generate all elements of $\mathbb{Z}_n^*$.)*</em></li>
<li>Use your generator $g$ to list all elements of $\mathbb{Z}_{13}^*$ in the form $g^k$.<br />
<em>(Hint: You should get 12 distinct powers.)</em></li>
<li>For each of the following values of $k \in {1, 2, 3, 4, 6}$, write out the subgroup generated by $g^k$.<br />
<em>(Hint: This approach helps you find all subgroups without brute force. For example, consider the case $k = 2$ below)</em></li>
</ol>
<blockquote>
<p><strong>Example:</strong><br />
If $g = 2$ (a generator of $\mathbb{Z}_{13}^*$), then:</p>
<ul>
<li>$g^2 = 2^2 = 4$</li>
<li>The subgroup generated by $4$ is:<br />
  $$ \langle4\rangle = {4, 4^2 =16\equiv3\pmod{13}, 4^3=64\equiv12 \pmod{13}, \dots} = {4, 3, 12, 9, 10, 1} $$</li>
</ul>
</blockquote>
<p>Now try this for $k = 1, 3, 4, 6$.<br />
<em>(Remember: compute $g^k$ first, then list its successive powers modulo 13 until you loop back to 1.)</em></p>
<ol start="4">
<li>Use your work above to list all <em>distinct</em> subgroups of $(\mathbb{Z}_{13}^*, \times)$.<br />
<em>(Hint: Some powers of $g$ generate the same subgroup.)</em></li>
</ol>
<p><strong>Conclusion:</strong><br />
This chapter focused on understanding multiplicative groups modulo a prime and their subgroup structures. We saw that these groups are cyclic, meaning they can be generated by a single element — called a primitive element. We also explored how the powers of a primitive element generate cyclic subgroups, and how different powers produce different subgroups.</p>
<p>Because we haven’t yet introduced formal tools like Lagrange’s Theorem, we approached subgroup discovery manually — especially in the final exercise, where we tried a few values of $k$ to see which subgroups they generate. In the next chapter, you’ll learn the underlying principles that allow you to determine subgroup sizes and generators systematically.</p>
<div style='page-break-after: always;'></div>

<h1>The Fundamental Theorem of Finite Cyclic Groups</h1>
<p>Source: https://rareskills.io/post/fundamental-theorem-cyclic-groups</p>
<h1>The Fundamental Theorem of Finite Cyclic Groups</h1>
<p>The Fundamental Theorem of Cyclic Groups provides guarantees about the existence of cyclic subgroups within a cyclic group.</p>
<p>In the context of the <strong>Number Theoretic Transform</strong> (NTT) of polynomials over a finite field, and also the FRI operation in ZK-STARKs, we need multiplicative subgroups whose order (number of elements) is a power of two. The Fundamental Theorem of Cyclic Groups enables us to quickly determine if a particular finite field has a multiplicative subgroup with that order or not.</p>
<p>We will cover the following as preliminaries, then get to the Fundamental Theorem of Cyclic Groups:</p>
<ol>
<li>Definition of a Subgroup</li>
<li>Order of a Group or Subgroup</li>
<li>Powers of an Element in a Multiplicative Group</li>
<li>Cyclic Subgroups, Cyclic Groups, and Their Generators</li>
</ol>
<h2>1. Definition of a Subgroup</h2>
<p>Given a group $G$, a subset $H$ of $G$ is a <strong>subgroup</strong> of $G$ if $H$ forms a group with respect to the group operation in $G$. In other words, if we restrict ourselves to elements of $H$ but keep using the group operation of $G$, the criteria for a group all still hold. Most importantly, the operation must be <em>closed</em> in $H$, so whenever we apply the operation on two elements of $H$, we must get out another element of $H$.</p>
<h3>Example of Subgroups</h3>
<p>The set of integers modulo 8 under addition form a group $\mathbb{Z}_8 = ({0, 1, 2, 3, 4, 5, 6, 7}, +)$. $H_1=({0, 2, 4, 6}, +)$ and $H_2=({0, 4}, +)$ are both subgroups of $\mathbb{Z}_8$. On the other hand, $H_3=({0, 2}, +)$ is not, because $(2+2) \not \in H_3$</p>
<h2>2. Order of a Group or Subgroup</h2>
<p>The <strong>order</strong> of a group $G$, denoted by $|G|$, is the number of its elements. If a group is not finite, its order is said to be infinite.</p>
<h3>Example for the order of a group and subgroup</h3>
<p>Consider the group $\mathbb{Z}_8$ of the example above. The order of $\mathbb{Z}_8$ is 8, meaning $|\mathbb{Z}_8| = 8$ (since there are exactly 8 elements in the group). Moreover, the order of the subgroup $H = ({0, 2, 4, 6}, +)$ is 4 (i.e. $|H| = 4$).</p>
<h2>3. Powers of an element in a multiplicative group</h2>
<p>If $g \in G$, the powers of the element $g$, denoted by $\langle g\rangle$, form the set ${g^i: i\in\mathbb{Z}}$. For example, consider the group of non-zero integers under multiplication modulo 7, $G ={1, 2, 3, 4, 5, 6}$. The powers of the element $3$ in $G$ are as follows:</p>
<p>$$<br />
\begin{aligned} \text{Powers of $3$} = \langle 3\rangle = {3^0, 3^1, 3^2, 3^3, 3^4, 3^5, 3^6, \dots}.<br />
\end{aligned}$$</p>
<p>Since:</p>
<p>$$<br />
\begin{aligned} &amp;3^0 = 1,\ &amp;3^1 = 3,\ &amp;3^2 = 9 \equiv 2,\ &amp;3^3 = 9 * 3 \equiv 2 * 3 \equiv 6,\ &amp;3^4 = 9 * 9 \equiv 2 * 2 \equiv 4,\ &amp;3^5 = 9 * 9 * 3 \equiv 2 * 2 * 3 \equiv 12 \equiv 5,\ &amp;3^6 = 9 * 9 * 9 \equiv 2 * 2 * 2 \equiv 8 \equiv 1.<br />
\end{aligned}$$</p>
<p>How about $3^7$?</p>
<p>$$<br />
3^7 = 3^6 * 3 \equiv 1 * 3 \equiv 3 \pmod{7}.$$</p>
<p>Thus, $\langle 3\rangle = {1, 2, 3, 4, 5, 6}$. For every power $3^i$ with $i&gt;6$, the result will be one of the elements already in this list.</p>
<p><strong>Exercise</strong>: Find the set generated by the element $4$.</p>
<h3>The powers of an arbitrary element form a Subgroup</h3>
<p>Let $g$ be an arbitrary element in $G$. The set of powers of the element $g$ forms a subgroup. In other words,</p>
<p>$$<br />
\langle g\rangle = {g^i: i\in\mathbb{Z}},$$</p>
<p>is a subgroup of $G$.</p>
<p><strong>Proof.</strong> See the appendix</p>
<h2>4. <strong>Cyclic Subgroups, Cyclic Groups, and Their Generators</strong></h2>
<p>Let $g$ be an arbitrary element in $G$, then the subgroup $\langle g\rangle = {g^i: i\in\mathbb{Z}}$ is called the cyclic subgroup of $G$ generated by $g$. For example, consider the group of non-zero integers under multiplication modulo 7, $G ={1, 2, 3, 4, 5, 6}$. The cyclic subgroup generated by the element $2$ is:</p>
<p>$$<br />
\langle 2 \rangle = {2^0=\boxed{1}, 2^1=\boxed{2}, 2^2=\boxed{4},2^3=\boxed{1},2^4=\boxed{2},2^5=\boxed{4},2^6=\boxed{1}}={1, 2, 4}$$</p>
<p><strong>Exercise</strong>: Find the subgroup generated by the element $6$.</p>
<h3>Definition of a Cyclic Group</h3>
<p>Let $g$ be an arbitrary element in $G$. If $G = \langle g\rangle$, then we say that $G$ is a cyclic group and that $g$ is a generator of $G$. In other words, a group is cyclic if it contains at least one element that generates all the elements of the group.</p>
<h3>Example of Cyclic Group and Generators</h3>
<p>Consider the group of non-zero integers under multiplication modulo 7, $G ={1, 2, 3, 4, 5, 6}$. Since $\langle 3 \rangle = G$, as shown above, then $G$ is a cyclic group and $3$ is a generator of $G$.</p>
<p>Note that $\langle 2 \rangle = {1, 2, 4}\neq G$, which means that the element $2$ is <strong>not</strong> a generator of $G$, but rather 2 generates a subgroup of $G$.</p>
<p><strong>Exercise</strong>: Determine whether $5$ is a generator of $G$</p>
<h3><strong>Generators of Cyclic Groups Are Not Necessarily Unique</strong></h3>
<p>Cyclic groups have at least one generator, but the generator does not have to be unique. In other words, a cyclic group can have multiple generators, any of which generates the entire group. The same is true for cyclic subgroups.</p>
<p>As you saw in the example and exercise above, the elements $3$ and $5$ are both <strong>generators</strong> of the group $G ={1, 2, 3, 4, 5, 6}$.</p>
<p>For another example, consider the group of non-zero integers under multiplication modulo 5. i.e, ${1, 2 ,3, 4}$. The elements $2$ and $3$ generate whole group.</p>
<p>$$<br />
\begin{aligned}<br />
&amp;2^0 =1,\qquad\qquad\qquad\qquad 3^0 =1,\<br />
&amp;2^1 =2,\qquad\qquad\qquad\qquad 3^1 =3,\<br />
&amp;2^2 =4,\qquad\qquad\qquad\qquad 3^2 =4,\<br />
&amp;2^3 =3,\qquad\qquad\qquad\qquad3^3 =2.\<br />
\end{aligned}$$</p>
<h2>The Fundamental Theorem of Finite Cyclic Groups</h2>
<p>The Fundamental Theorem of Finite Cyclic Groups makes three claims. Let $G$ be a finite cyclic group, and let $H$ be a subgroup of $G$.</p>
<ol>
<li>$H$ is necessarily finite and cyclic (meaning $H$ has a generator).</li>
<li>The order of $H$ (the number of elements it has) is a factor of the order of $G$. In other words, the order of $H$ divides the order of $G$. For example, suppose the order of $G$ is 6. We automatically know a subgroup of order 5 cannot exist, since 5 does not divide 6.</li>
<li>If $q$ is the order of $G$ and $k$ divides $q$, then a subgroup of size $k$ <strong>necessarily</strong> <strong>exists</strong> and is <strong>unique</strong>. In fact, we can immediately find a generator for it: if $g$ is a generator of $G$, then $g^\frac{q}{k}$ is a generator for a subgroup of size $k$. This subgroup of size $k$ is equal to $\langle g^\frac{q}{k} \rangle$. We will show examples of this shortly.</li>
</ol>
<h3><strong>Connection to Lagrange’s Theorem</strong></h3>
<p>Statement (2) of the Fundamental Theorem of Finite Cyclic Groups holds for any finite group $G$, even if $G$ is not cyclic. This general result is known as <strong>Lagrange’s Theorem</strong>.</p>
<p>For brevity, we will call this the Fundamental Theorem of Cyclic Groups, with the understanding that we are dealing with finite groups.</p>
<h3>Examples of Using the Fundamental Theorem of Cyclic Groups</h3>
<p>Let’s use $G = {1,2,3,4,5,6}$ under multiplication modulo 7 as our cyclic group. We’ve already shown that $g = 3$ generates the whole group.</p>
<p><strong>The subgroup of order 1</strong></p>
<p>Using the Fundamental Theorem of Cyclic Groups, we know that $G$ has a subgroup of order 1, since 1 divides 6. Now let’s find a generator for this subgroup. Since $k = 1$, we have $3^{\frac{6}{1}} \equiv 1\pmod{7}$. The identity element $1$ is clearly a generator of the subgroup of size one.</p>
<p><strong>The subgroup of order 2</strong></p>
<p>Now let’s find the subgroup of size 2. We know it exists because 2 divides 6. Here, we have $g = 3$, $q = 6$, and $k = 2$. Plugging into the formula, we get $g^{\frac{q}{k}} = 3^{\frac{6}{2}} = 3^3 \equiv 6\mod{7}$. The element $6$ generates the subgroup of order 2 with elements ${1, 6}$.</p>
<p><strong>The subgroup of order 3</strong></p>
<p>Since 3 divides 6, there exists a subgroup of order 3. Moreover, the element $3^{\frac{q}{k}} = 3^{\frac{6}{3}} = 3^{2} \equiv 2\pmod{7}$ is a generator for this subgroup, $\langle 2\rangle = {2^0, 2^1, 2^2} ={1, 2, 4}.$</p>
<p><strong>The subgroup of order 4, 5</strong></p>
<p>There are no subgroups of order 4 and 5 because these numbers do not divide 6.</p>
<p><strong>The subgroup of order 6</strong></p>
<p>Since 6 divides 6, there exists a subgroup of order 6. Moreover, $3^{\frac{q}{k}} = 3^{\frac{6}{6}} = 3^{1} = 3$ is a generator of this subgroup:</p>
<p>$$<br />
\langle 3 \rangle ={1, 2, 3, 4, 5, 6} = G.$$</p>
<p><strong>Exercise</strong>: Consider the group with elements $G = {1,2,3,4,5,6,7,8,9,10}$ under multiplication module 11.</p>
<ol>
<li>Find a generator for this group.</li>
<li>The group has a subgroup of order 5, since 5 divides the order of $G$, which is 10. Find a generator for this subgroup and calculate the subgroup it generates.</li>
</ol>
<h2>Multiplicative Subgroup of Order $k$ in a Finite Field</h2>
<p>We can find multiplicative subgroups of specific order in a finite field. First, let’s see the definition of finite fields:</p>
<p>By definition, a field $(\mathbb{F}, + , *)$ consists of two Abelian (binary operators are commutative) groups:</p>
<ol>
<li><strong>Additive group</strong>: $(\mathbb{F}, +)$, which is Abelian and finite in a finite field.</li>
<li><strong>Multiplicative group</strong>: $(\mathbb{F^*}, *)$  (where $\mathbb{F}^* = \mathbb{F}\setminus{0}$), which is also Abelian and finite in a finite field.</li>
</ol>
<p>Note that $\mathbb{F}_q$ denotes a field with $q$ elements, while $\mathbb{F}^*_q$ (the multiplicative group of $\mathbb{F}_q$) has $q-1$ elements.</p>
<p>In the following theorem, we see that every finite field necessarily contains a multiplicative subgroup of order $q – 1$ (with $0$ omitted).</p>
<h3>Theorem 1: The Multiplicative Group $\mathbb{F}^*_q$ is Cyclic of Order $q-1$.</h3>
<p>If $\mathbb{F}_q$ is a <strong>finite field</strong> with order $q$, then its multiplicative group $\mathbb{F}^*_q$ is cyclic of order $q-1$.</p>
<p>That $\mathbb{F}^*_q$ forms a group of order $q-1$ follows immediately from the definition of a finite field.</p>
<p>Since proving the cyclicity of the multiplicative group would take us beyond the scope of this article, we will take it as a given fact.</p>
<p>A cyclic group has a generator, therefore we know that some $g\in \mathbb{F}^*_q$ exists such that</p>
<p>$$<br />
\mathbb{F}^*_q = {1, g^1, g^2, \dots, g^{q-2}} = \langle g \rangle.$$</p>
<h3>Primitive Elements in a Finite Field (Generators)</h3>
<p>In field theory, a <em>primitive element</em> of a finite field $\mathbb{F}_q$ is a generator of the multiplicative group of the field. The element $g$ in Theorem 1 is a primitive element in $\mathbb{F}_q$.</p>
<p>The Python code below uses the <code style='font-family: Arial'>galois</code> library to identify primitive elements (generators) in $\mathbb{F}_7$.</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois

GF = galois.GF(7)

primitive_elements = GF.primitive_elements
print(&quot;Primitive elements:&quot;, primitive_elements)
# Primitive elements: [3 5]

# Alternatively, find a single primitive element
# suitable when there are a lot of primitive elements
primitive_element = GF.primitive_element
print(&quot;A primitive element:&quot;, primitive_element)
# A primitive element: 3
</code></pre>
<p><strong>Exercise</strong>: Use Python to find all the primitive elements of the multiplicative group of the field $\mathbb{F}_{11}$.</p>
<h3>Corollary 1: <strong>Testing for Subgroup of Order $k$ — Existence via Divisors in a Finite Field</strong></h3>
<p>A useful corollary is that we can quickly check whether a subgroup of a certain size exists or not by listing all the divisors of $q-1$. For example, consider the field $\mathbb{F}_{41}$, which has a multiplicative group $\mathbb{F}_{41}^*$ with order 40. We can quickly check that $\mathbb{F}_{41}^*$ has a subgroup of size 8 because 8 divides 40.</p>
<p>As another example, consider the field $\mathbb{F}_{17}$, whose multiplicative group $\mathbb{F}_{17}^*$ has order 16. Since 8 divides 16, $\mathbb{F}_{17}^*$ has a subgroup of size 8. Similarly, it has a subgroup of size 4, because 4 divides 16, and, as you might guess, $\mathbb{F}_{17}$ also has a subgroup of size 2.</p>
<p>To find a generator for a multiplicative subgroup of a given size, we can use statement (3) of the Fundamental Theorem above. The size of $\mathbb{F}_{q}^*$ is $q-1$, so if we have a primitive element $g$ and want a multiplicative subgroup of order $k$, we can calculate $g^\frac{q-1}{k}$ for any primitive element $g$.</p>
<h3>All In One Example</h3>
<p>Consider the finite field $\mathbb{F}_{17} ={0, 1, 2, \dots, 16}$. For a given $k$, we want to find a multiplicative subgroup of order $k$ in $\mathbb{F}_{17}$ using the Fundamental Theorem of Finite Cyclic Groups.</p>
<p>Since the multiplicative subgroup $\mathbb{F}_{17}^*$ has order $q-1=17-1=16$, we know from statement (2) of the Fundamental Theorem of Cyclic Groups that it has subgroups of orders 1, 2, 4, 8, and 16, with the last one being the group itself.</p>
<p>These subgroups are cyclic, so each has at least one generator. From statement (3) of the theorem, we know that a generator for each subgroup is given by $g^\frac{q-1}{k}$ , where $g$ is a generator of the full multiplicative group and $k$ is the size of our desired subgroup.</p>
<p>Therefore, to generate the subgroups, the first step is to find a generator of $\mathbb{F}_{17}^*$, which is a primitive element of $\mathbb{F}_{17}$.</p>
<h2>The generator of $\mathbb{F}^*_{17}$</h2>
<p>Recall from Theorem 1 that $\mathbb{F}^*_{17}$ <em>is a cyclic group</em>. To show that <em>$mathbb{F}^</em>_{17}$* can be generated by the element $3$, we can calculate all powers of $3$ as follows:</p>
<p>$$<br />
\begin{aligned}<br />
&amp;3^0 = \boxed{1},\<br />
&amp;3^1 = \boxed{3},\<br />
&amp;3^2 = \boxed{9},\<br />
&amp;3^3 \equiv 9 * 3 \equiv \boxed{10},\space\space (27 – 17 = 10)\<br />
&amp;3^4 \equiv 10 * 3\equiv \boxed{13},\space\space(30 – 17 = 13)\<br />
&amp;3^5 \equiv 13 * 3 \equiv \boxed{5},\space\space(39 – 2*17 = 5)\<br />
&amp;3^6 \equiv 5 * 3 \equiv \boxed{15},\<br />
&amp;3^7 \equiv 15 * 3 \equiv \boxed{11},\space\space(45 – 2*17 = 11)\<br />
&amp;3^8 \equiv 11 * 3 \equiv \boxed{16},\space\space(33 – 17 = 16)\<br />
&amp;3^9 \equiv 16 * 3 \equiv \boxed{14},\space\space(48 – 2*17 = 14)\<br />
&amp;3^{10} \equiv 14 * 3 \equiv \boxed{8},\space\space(42 – 2*17 = 8)\<br />
&amp;3^{11} \equiv 8 * 3 \equiv \boxed{7},\space\space(24 – 17 = 7)\<br />
&amp;3^{12} \equiv 7 * 3 \equiv \boxed{4},\space\space(21 – 17 = 4)\<br />
&amp;3^{13} \equiv 4 * 3 \equiv \boxed{12},\<br />
&amp;3^{14} \equiv 12 * 3 \equiv \boxed{2},\space\space(36 – 2*17 = 2)\<br />
&amp;3^{15} \equiv 2 * 3 \equiv \boxed{6},\<br />
&amp;3^{16} = 6 * 3 \equiv \boxed{1},\space\space(18 – 17 = 1).<br />
\end{aligned}$$</p>
<p>How about $3^{17}$?</p>
<p>$$<br />
3^{17} = 3^{16} * 3 \equiv 1 * 3 = 3.$$</p>
<p>You can see that for all $i \ge 17$, the element $3^i$ is equal to one of the $3^0, 3^1, 3^2, \dots, 3^{16}$. You can also see that every value in {1,2,…,16} shows up somewhere in the list of powers of 3. Then, $\langle 3 \rangle = {1, 2, \dots, 16} = \mathbb{F}^*_{17}$.</p>
<p>This generator can be found in Python code with <code style='font-family: Arial'>galois.primitive_elements</code>.</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois

GF = galois.GF(17)

primitive_element = GF.primitive_element
print(&quot;A primitive element:&quot;, primitive_element)
# A primitive element: 3
</code></pre>
<h2>Finding the subgroup of order $k$ in $\mathbb{F}^*_{17}$</h2>
<p>Suppose we want to determine whether a multiplicative subgroup of order $k=4$ exists in the finite field $\mathbb{F}_q = \mathbb{F}_{17}$. Note that the multiplicative group of $\mathbb{F}_q$ has size $q-1$. For $\mathbb{F}_{17}$, the multiplicative group $\mathbb{F}_{17}^*$ has $q-1 = 17-1 = 16$ elements.</p>
<p>Recall from the Fundamental Theorem of Finite Cyclic Groups that since $k$ divides $q-1$ (explicitly, $4$ divides 16), then $g^{\frac{q-1}{k}}$ is a generator and $\langle g^{\frac{q-1}{k}}\rangle$ is a subgroup of size 4.</p>
<p>$$<br />
g^{\frac{q-1}{k}} = 3^{\frac{16}{4}} = 3^4 \equiv 13\pmod{17}.$$</p>
<p>Thus, 13 is a generator for the subgroup of size 4 in $\mathbb{F}^*_{17}$ and</p>
<p>$$<br />
\langle 13 \rangle = {13^0 = \boxed{1}, 13^1 = \boxed{13}, 13^2\equiv\boxed{16}, 13^3\equiv\boxed{4}, 13^4\equiv\boxed{1}, 13^5\equiv\boxed{13}, \dots}.$$</p>
<p>Explicitly, ${1, 13, 16, 4}$ is the subgroup.</p>
<p><strong>Exercise:</strong> Compute the multiplicative subgroups of order $2$ and $8$.</p>
<h2>Summary</h2>
<ul>
<li>The goal is to find the multiplicative subgroups of size $k$ in the field $\mathbb{F}_q$.</li>
<li>If $G = \langle g\rangle = {g^i: i\in\mathbb{Z}}$, then $G$ is a cyclic group, and $g$ is a generator of $G$.</li>
<li>If $\mathbb{F}_q$ is a <strong>finite field</strong> of order $q$, then $(\mathbb{F}^*_q, .)$ is a cyclic group of order $q-1$.</li>
<li>The Fundamental Theorem of Cyclic Groups states that the finite field $\mathbb{F}_q$ has a subgroup of size $k$ if and only if $k$ divides $q-1$. Moreover, a generator of this subgroup is $g^{\frac{q-1}{k}}$, where $g$ is a generator of the multiplicative group $\mathbb{F}^*_q$.</li>
</ul>
<h2>Appendix</h2>
<p>We will check the three conditions necessary for $\langle g \rangle$ to be a subgroup of $G$.</p>
<p><strong>Identity</strong>: Since $1 = g^0$, then $1 \in \langle g \rangle$.</p>
<p><strong>Closure:</strong> Suppose $a, b \in \langle g \rangle$. Then we know $a = g^n$ and $b = g^m$ for some $n, m$. Applying the group operation gives</p>
<p>$$<br />
ab = g^ng^m = g^{n+m}.$$</p>
<p>Since $n+m\in\mathbb{Z}$, hence $ab\in \langle g \rangle$.</p>
<p><strong>Inverse:</strong> Suppose $a\in \langle g \rangle$. Then $a = g^m$ for some $m$,</p>
<p>$$<br />
a^{-1} = (g^m)^{-1} = g^{-m}.$$</p>
<p>Since $-m\in\mathbb{Z}$, so $a^{-1} \in \langle g \rangle$.</p>
<h2>Exercises</h2>
<p>What are all the orders of the multiplicative subgroups of $\mathbb{F}_{31}$ and a generator for each subgroup?</p>
<p>What are all the orders of the multiplicative subgroups of $\mathbb{F}_{5}$ and a generator for each subgroup?</p>
<p>What are all the orders of the multiplicative subgroups of $\mathbb{F}_{51}$ and a generator for each subgroup?</p>
<div style='page-break-after: always;'></div>

<h1>Roots of Unity in Finite Fields</h1>
<p>Source: https://rareskills.io/post/roots-of-unity-finite-field</p>
<h1>Roots of Unity in Finite Fields</h1>
<p>This article explains what Roots of Unity in a Finite Field are and how they are intertwined with multiplicative subgroups. The reader is expected to be familiar with the prior chapter about the <a href="https://rareskills.io/post/fundamental-theorem-cyclic-groups">Fundamental Theorem of Cyclic Groups</a>.</p>
<p>That theorem states that, given a multiplicative group $\mathbb{F}_q^*$ of order $q-1$, there exists a unique subgroup of order $k$ when $k$ divides $q-1$. Otherwise, if $k$ does not divide $q-1$, no subgroup of order $k$ exists.</p>
<p>It also states that, if $g$ is a generator of $\mathbb{F}_q^*$, then $g^\frac{q-1}{k}$ generates the subgroup of order $k$.</p>
<p>In this article, we will show that all elements of this subgroup are what are called $k$-th roots of unity, and that the generator $\omega$ is what is called a primitive $k$-th root of unity.</p>
<h2>Motivation and goal for this chapter</h2>
<p>The square roots of $1$ in a finite field are easy to compute: they are the numbers congruent to $1$ and negative $1$ (which are $1$ and $q – 1$ respectively. Remember that, in a field $\mathbb{F}_q$, the number $-x$ is congruent to $q-x$ ). In other words, $\sqrt{1} = {1, q-1 }$. This set is the set of solutions of the equation $a^2 \equiv 1$, where $a$ is an element of the finite field.</p>
<p>But what if we want to compute the cube roots of $1$, or more generally, the $k$-th roots of 1, i.e., $\sqrt[^k]{1}$? By definition, $k$-th roots of unity are the elements that satisfy the equation $a^k \equiv 1$. But how can we find them?</p>
<p>We could try all elements one by one — a brute-force approach — but that would be infeasible when the field contains many elements. Fortunately, there is a simple way to find them all. <strong>In the finite field $\mathbb{F}_q$, the $k$-th roots of unity are precisely the elements of the multiplicative subgroup of order $k$.</strong> This definition assumes $k$ divides <strong>$q-1.$</strong></p>
<p>To state it in the strongest possible terms: <strong>If an element $a$ in a finite field is part of a multiplicative subgroup of order $k$, then it is a $k$-th root of unity, and $a^k\equiv1$. And if an element $a$ is a $k$-th root of unity (meaning $a^k\equiv1)$, then it is part of a multiplicative subgroup of order k.</strong></p>
<h3>Equivalence of terminology</h3>
<p>It may seem like we are just adding a new terminology for the same entity (an element in a multiplicative group) and pointing out that raising it to the $k$-th power equals 1.</p>
<p>In a certain sense, yes, we are introducing a new term for the same entity: a $k$-th root of unity is an element in a multiplicative subgroup of order $k$ and vice-versa. Normally, giving the same entity two names leads to confusion, so we need to justify introducing the term “root of unity.”</p>
<p>When we speak about cyclic groups of order $k$ in the most general sense, we do not have the guarantee that taking an element in that group and applying the binary operator to that element and itself $k$ times results in the identity element, i.e., $a^k\equiv1$ , or more generally for a binary operator $\star$:</p>
<p>$$<br />
\underbrace{a\star a\star\dots\star a}_k=\text{identity}$$</p>
<p>For cyclic groups in general, the above property is not guaranteed to hold, but for roots of unity in a finite field, it is guaranteed.</p>
<p>Therefore, we can say that roots of unity have all the properties the Fundamental Theorem of Cyclic groups says they should have, <em>and</em> they have the property that $a^k\equiv1.$</p>
<p>So, to put the reader’s mind at ease, you already know a good bit about roots of unity in a finite group simply because you understand the Fundamental Theorem of Cyclic Groups. Since roots of unity in a finite field are a cyclic subgroup, the Fundamental Theorem of Cyclic Groups applies to them.</p>
<p>However, the added guarantee that $a^k\equiv1$ unlocks additional properties that efficient ZK algorithms directly leverage. These properties enable us to create efficient algorithms like the Number Theoretic Transform and other <a href="rareskills.io/zk-book">Zero-Knowledge Proof</a> algorithms, such as PLONK and ZK-STARKs.</p>
<p>We study these additional properties of roots of unity in later chapters. This chapter focuses on definitions and examples to build the understanding that an element $a$ in a finite field has the property $a^k\equiv1$ if and only if it is part of a multiplicative subgroup of order $k.$</p>
<h3>Computing k-th roots of unity is the same as finding the multiplicative subgroup of order k</h3>
<p>In the article on the Fundamental Theorem of Cyclic Groups, we learned how to find all the elements of a multiplicative subgroup of order $k$. First, we obtain a generator of this subgroup from the generator of the multiplicative group $\mathbb{F}_q^*$. Then, using this generator, we can find all the elements of the subgroup of order $k$.</p>
<p><strong>Thus, finding the $k$-th roots of unity of $\mathbb{F}_q$ is no different from finding the multiplicative subgroup of order $k$, if $k$ divides $q-1$, which we already know how to do.</strong></p>
<p>Our aim in this chapter is to prove this equivalence – between the group of all $k$-th roots of unity and the multiplicative subgroup of order $k$, if $k$ divides $q-1$.</p>
<p>To do this, we need to prove the following two statements:</p>
<ol>
<li>Every element $a$ within the multiplicative subgroup of order $k$ satisfies $a^k\equiv 1$.</li>
<li>Suppose $k$ divides $q-1$. Then, every element $a$ in $\mathbb{F}_q^*$ that satisfies $a^k\equiv 1$ belongs to the unique subgroup of order $k$.</li>
</ol>
<p>We will explore these two statements through examples to illustrate that they hold true. Since the formal proofs can be somewhat mathematically demanding, we will defer some of them to the appendix for the interested reader, though we have made the effort to make the proofs as widely understandable as possible.</p>
<h2>1. Every element $a$ in the subgroup of order $k$ satisfies $a^k\equiv 1$</h2>
<p>This first statement shows that all elements of the multiplicative subgroup of order $k$ are $k$-th roots of unity.</p>
<p>However, this is NOT sufficient to establish the equivalence between the group of all $k$-th roots of unity and the multiplicative subgroup of order $k$ (when $k$ divides $q-1$) since it does not guarantee that all $k$-th roots of unity belong to this subgroup — which will be discussed in Statement 2.</p>
<p>We will begin this section with a proof of this statement, and then show through examples that this statement holds true.</p>
<h3>Proof of Statement 1</h3>
<p>Recall from the article on the Fundamental Theorem of Cyclic Groups that a unique subgroup of order $k$ is generated by $\omega = g^{\frac{q-1}{k}}$, where $g$ is a generator of the multiplicative group $\mathbb{F}_{q}^*$. We refer to $\langle\omega\rangle$ as the elements generated by taking successive powers of $\omega$ modulo $q$:</p>
<p>$$<br />
\langle\omega\rangle = {\omega^{0},\omega^{1},\omega^{2},\dots ,\omega^{k-1}}.$$</p>
<p>Let $\omega^m$ be an arbitrary element in $\langle\omega\rangle$ for some $0\le m\le k-1$. The goal is to prove that $(\omega^m)^k\equiv 1$.</p>
<p><em>The proof below assumes that the generator has the property $\omega^k \equiv 1$; see Appendix A for the proof of this fact.</em></p>
<p>Let’s compute $(\omega^m)^k$ as follows:</p>
<p>$$<br />
\begin{aligned}<br />
(\omega^m)^k<br />
&amp;= (\omega^{mk}) &amp;&amp;\qquad\text{Power of a power rule<br />
}\<br />
&amp;= (\omega^{km}) &amp;&amp;\qquad\text{Commutativity of exponents}\<br />
&amp;=(\omega^k)^m&amp;&amp;\qquad\text{Factoring the exponent}\<br />
&amp;\equiv (1)^m&amp;&amp;\qquad\text{Based on the discussion in the appendix A: $\omega^k\equiv 1$}\<br />
&amp;\equiv 1&amp;&amp;\qquad\text{One raised to an arbitrary power is 1}.<br />
\end{aligned}$$</p>
<p><strong>Therefore, every element in the subgroup of order $k$, when raised to the power $k$, equals 1.</strong></p>
<h3>Example of Statement 1 in $\mathbb{F}_7 = {0,1,2,3,4,5,6}$</h3>
<p>In this example, we have $q-1 = 7-1 = 6$. Also, the element $g = 3$ is a generator of the multiplicative group $\mathbb{F}_q^* = {1,2,3,4,5,6}$.</p>
<p><em>How to find the generator of the multiplicative group will not be covered in this article, but the <code style='font-family: Arial'>galois</code> library provides a quick way to do it. Given the field $\mathbb{F}_q$, one way to find the generator is by using the <code style='font-family: Arial'>primitive_element</code> property, as shown below.</em></p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
GF = galois.GF(7) # define the field
GF.primitive_element # GF(3, order=7)
</code></pre>
<p><strong>Subgroup of order 3</strong></p>
<p>Since 3 divides $q-1 = 6$, the Fundamental Theorem of Cyclic Groups guarantees the existence of a unique subgroup of order 3.</p>
<p>This subgroup is generated by $3^{\frac{q-1}{k}} = 3^{\frac{6}{3}} = 3^{2} = 9\equiv 2\pmod{7}$. Thus,</p>
<p>$$<br />
\langle 2\rangle = {2^0,2^1,2^2} = {1, 2, 4}.$$</p>
<p>Now, we want to verify that every element $a$ in $\langle 2\rangle$ satisfies $a^3\equiv 1$. This is done below:</p>
<p>$$<br />
\begin{aligned}<br />
&amp;1^3 \equiv \boxed{1}\pmod{7}\<br />
&amp;2^3 \equiv 8 \equiv \boxed{1}\pmod{7}\<br />
&amp;4^3 \equiv 16 \cdot 4 \equiv 2 \cdot 4 \equiv 8 \equiv \boxed{1}\pmod{7}\<br />
\end{aligned}$$</p>
<p>So, the elements $1, 2$ and $4$ satisfy $a^3\equiv 1\pmod{7}$. Therefore, the condition is satisfied.</p>
<p><strong>Subgroup of order 2</strong></p>
<p>Since 2 divides $q-1 = 6$, the Fundamental Theorem of Cyclic Groups guarantees the existence of a unique subgroup of order 2.</p>
<p>This subgroup is generated by $3^{\frac{q-1}{k}} = 3^{\frac{6}{2}} = 3^{3} = 27\equiv 6\pmod{7}$. Thus,</p>
<p>$$<br />
\langle 6\rangle = {6^0,6^1} = {1, 6}.$$</p>
<p>Now, we want to verify that every element $a$ in $\langle 6\rangle$ satisfies $a^2\equiv 1$.</p>
<p>$$<br />
\begin{aligned}<br />
&amp;1^2 \equiv \boxed{1}\pmod{7}\<br />
&amp;6^2 = 36 \equiv \boxed{1}\pmod{7}<br />
\end{aligned}$$</p>
<p>So, the elements $1$ and $6$ satisfy $a^2\equiv 1\pmod{7}$. Therefore, the condition is satisfied.</p>
<p><strong>Exercise.</strong> Verify that every element $a$ in $\mathbb{F}_7^*$ satisfies $a^6\equiv 1$.</p>
<h2>2. If $k$ divides $q-1$, then every element $a$ in $\mathbb{F}_q^*$ with $a^k\equiv 1$ belongs to the unique subgroup of order $k$</h2>
<p>This statement asserts that ALL $k$-th roots of unity belong to the multiplicative subgroup of order $k$, provided that $k$ divides $q-1$.</p>
<p>In this section, we illustrate this claim through a few examples. The full proof is provided in Appendix B for the interested reader.</p>
<p><em>Before moving on, let’s consider the case where $k$ does not divide $q-1$. In this situation, the Fundamental Theorem of Cyclic Groups tells us that there is no subgroup of order $k$, and therefore no equivalence can exist.</em></p>
<h3>Example in $\mathbb{F}_7 = {0,1,2,3,4,5,6}$</h3>
<p>In this example, we have $q-1 = 7-1 = 6$. Also, the element $g = 3$ is a generator of the multiplicative group $\mathbb{F}_q^* = {1,2,3,4,5,6}$.</p>
<p><strong>Subgroup of order $k=3$</strong></p>
<p>Since 3 divides $q-1 = 6$, there exists a unique subgroup of order 3. This subgroup is generated by $3^{\frac{q-1}{k}} = 3^{\frac{6}{3}} = 3^{2} = 9\equiv 2\pmod{7}$. Thus,</p>
<p>$$<br />
\langle 2\rangle = {2^0,2^1,2^2} = {1, 2, 4}.$$</p>
<p>We want to verify that every element $a$ in $\mathbb{F}_{7}^*$ satisfying $a^3\equiv 1$ belongs to this unique subgroup of order $k = 3$ in $\mathbb{F}_{7}^*$. Let’s find all such elements $a$ in $\mathbb{F}_{7}^*$, as follows:</p>
<p>$$<br />
\begin{aligned}<br />
&amp;\boxed{1^3 \equiv 1}\pmod{7}\<br />
&amp;\boxed{2^3 \equiv 8 \equiv 1}\pmod{7}\<br />
&amp;3^3 \equiv 9 \cdot 3 \equiv 2 \cdot 3 \equiv 6\pmod{7}\<br />
&amp;\boxed{4^3 \equiv 16 \cdot 4 \equiv 2 \cdot 4 \equiv 8 \equiv 1}\pmod{7}\<br />
&amp;5^3 \equiv 25 \cdot 5 \equiv 4 \cdot 5 \equiv 20 \equiv 6\pmod{7}\<br />
&amp;6^3 \equiv 36 \cdot 6 \equiv 1 \cdot 6 \equiv 6\pmod{7}.<br />
\end{aligned}$$</p>
<p>The elements $1, 2$ and $4$ satisfy $a^3\equiv 1\pmod{7}$. These three elements are exactly the members of the subgroup of order $k =3$ in $\mathbb{F}_{7}^*$. Therefore, every element $a$ in $\mathbb{F}_{7}^*$ with $a^3\equiv 1$ belongs to the unique subgroup of order $k = 3$.</p>
<p><strong>Exercise.</strong> Verify that every element $a$ in $\mathbb{F}_7^*$ satisfying $a^2\equiv 1$ belongs to the unique subgroup of order $k = 2$.</p>
<h3>Example in $\mathbb{F}_{17} = {0,1,2,\dots,16}$</h3>
<p>In this example, we have $q-1 = 17-1 = 16$. Also, the element $g = 3$ is a generator of the multiplicative group $\mathbb{F}_q^* = {1,2,\dots,16}$, which can be verified using the <code style='font-family: Arial'>galois</code> library:</p>
<pre style='font-family: Arial'><code class="language-solidity">GF = galois.GF(17) # define the field
GF.primitive_element # GF(3, order=17)
</code></pre>
<p><strong>Subgroup of order $k=4$</strong></p>
<p>Since 4 divides $q-1 = 16$, there exists a unique subgroup of order 4. This subgroup is generated by $3^{\frac{q-1}{k}} = 3^{\frac{16}{4}} = 3^{4} = 81\equiv 13\pmod{17}$. Thus,</p>
<p>$$<br />
\langle 13\rangle = {13^0,13^1,13^2, 13^3} = {1, 13, 16, 4}.$$</p>
<p>We want to verify that every element $a$ in $\mathbb{F}_{17}^*$ satisfying $a^4\equiv 1$ belongs to this unique subgroup of order $k = 4$ in $\mathbb{F}_{17}^*$. Let’s find all such elements $a$ in $\mathbb{F}_{17}^*$, as follows:</p>
<p>$$<br />
\begin{aligned}<br />
&amp;\boxed{1^4 = 1}\<br />
&amp;2^4 = 16\not\equiv 1\<br />
&amp;3^4 = 81 \equiv 13\not\equiv 1\<br />
&amp;\boxed{4^4 \equiv 16 \cdot 16 \equiv (-1) \cdot (-1) \equiv 1}\<br />
&amp;5^4 \equiv 25 \cdot 25 \equiv 7 \cdot 7 \equiv 49 \equiv 15\not\equiv 1\<br />
&amp;6^4 \equiv 36 \cdot 36 \equiv 2 \cdot 2 \equiv 4\not\equiv 1\<br />
&amp;7^4 \equiv 49 \cdot 49 \equiv 15 \cdot 15 \equiv (-2) \cdot (-2)\equiv 4\not\equiv 1\<br />
&amp;8^4 \equiv (2^3)^4 = (2^4)^3 \equiv (-1)^3 \equiv -1\equiv 16\not\equiv 1\qquad\qquad\pmod{17}\<br />
&amp;9^4 = 81 \cdot 81 \equiv 13 \cdot 13 \equiv (-4) \cdot (-4) \equiv 16\not\equiv 1\<br />
&amp;10^4 \equiv 100 \cdot 100 \equiv 15 \cdot 15 \equiv (-2) \cdot (-2) \equiv 4\not\equiv 1\<br />
&amp;11^4 \equiv 121 \cdot 121 \equiv 2 \cdot 2 \equiv 4\not\equiv 1\<br />
&amp;12^4 \equiv 144 \cdot 144 \equiv 8 \cdot 8 \equiv 13\not\equiv 1\<br />
&amp;\boxed{13^4 \equiv 169 \cdot 169 \equiv 16 \cdot 16 \equiv (-1) \cdot (-1)\equiv 1}\<br />
&amp;14^4 \equiv (-3)^4 \equiv 9 \cdot 9\equiv 13\not\equiv 1\<br />
&amp;15^4\equiv (-2)^4\equiv 16\not\equiv 1\<br />
&amp;\boxed{16^4 \equiv (-1)^4 \equiv 1}.<br />
\end{aligned}$$</p>
<p>The elements $1, 4, 13$ and $16$ satisfy $a^4\equiv 1\pmod{17}$. These four elements are exactly the members of the subgroup of order $k =4$ in $\mathbb{F}_{17}^*$. Therefore, every element $a$ in $\mathbb{F}_{17}^*$ with $a^4\equiv 1$ belongs to the unique subgroup of order $k = 4$.</p>
<p><strong>Exercise.</strong> Verify that every element $a$ in $\mathbb{F}_{17}^*$ satisfying $a^8\equiv 1$ belongs to the unique subgroup of order $k = 8$.</p>
<h2>Primitive $k$-th root of unity</h2>
<p>A primitive $k$-th root of unity is a special $k$-th root of unity: <strong>it is a $k$-th root of unity that generates all other $k$-th roots of unity.</strong></p>
<p><strong>Since the group of $k$-th roots of unity we are interested in is the same as the subgroup of order $k$, the primitive $k$-th roots of unity are exactly the generators of this subgroup.</strong></p>
<p><em>Note: In the case where $k$ does not divide $q-1$ (considering finite fields $\mathbb{F}_q$), there may still be $k$-th roots of unity, but in this case there are NO primitive $k$-th roots of unity.</em></p>
<p>Thus, finding a primitive $k$-th root of unity is straightforward: it is the same as finding a generator of the subgroup of order $k$, and the Fundamental Theorem of Cyclic Groups tells us how to do this.</p>
<p><em>A formal definition of a primitive $k$-th root of unity is that it is a $k$-th root of unity with order $k$, where the order of an element $a$ is the smallest positive integer greater than zero $r$ such that $a^r \equiv 1$.</em></p>
<p>For example, $4$ is a 6th root of unity in $\mathbb{F}_7$ because $4^6\equiv 1\pmod{7}$, but it is not a primitive 6th root because there is a power lower than 6 that makes it $1$, specifically 3, i.e. $4^3 \equiv 1\pmod{7}$.</p>
<h3>The number of primitive $k$-th roots of unity</h3>
<p>Just as a subgroup of order $k$ can have more than one generator, there can be more than one primitive $k$-th root of unity. The number of primitive $k$-th roots of unity is the same as the number of generators of the subgroup of order $k$.</p>
<p>The number of primitive $k$-th roots of unity (and generators) is given by Euler’s totient function $\phi(k)$. The proof of this fact is beyond the scope of this article. For the application we are considering—the Number Theoretic Transform—we only need a primitive $k$-th root of unity, which can be found using the Fundamental Theorem, assuming we know a generator for $\mathbb{F}_q^*$.</p>
<p>In the rest of this chapter, we will present examples showing how to use the Fundamental Theorem to find a primitive $k$-th root of unity, and then all $k$-th roots of unity for a given $k$.</p>
<h3>Example of 4th roots of unity in $\mathbb{F}_{17}^*$</h3>
<p>A generator of the multiplicative group $\mathbb{F}_{17}^*$ is the element $3$, which can be found using the <code style='font-family: Arial'>galois</code> library.</p>
<p>A generator for the subgroup of order 4 is then $\omega = 3^{\frac{16}{4}} = 3^4 \equiv 13$.</p>
<p>Based on our discussion, this generator is a primitive 4th root of unity. Let us check that using the definition of a primitive $k$-th root of unity. We need to show that:</p>
<ol>
<li>Element $13$ is a 4th root of unity. This can be seen by checking that $13^4 \equiv 1 \pmod{17}$.</li>
<li>The order of element $13$ is 4. This means that 4 is the smallest positive integer $r$ such that $13^r \equiv 1 \pmod{17}$. Let us check that below:</li>
</ol>
<p>$$<br />
\begin{aligned}<br />
&amp;13^1 = 13,\<br />
&amp;13^2 \equiv 16,&amp;&amp;\pmod{17}\<br />
&amp;13^3 \equiv 13 \cdot 16 \equiv 4,\<br />
&amp;\boxed{13^4 \equiv 16 \cdot 16\equiv -1 \cdot -1 \equiv 1}.<br />
\end{aligned}$$</p>
<p>Therefore, element $13$ is a primitive 4th root of unity, and we can use element $13$ to generate the subgroup of all 4th roots of unity, as follows:</p>
<p>$$<br />
\begin{aligned}<br />
\langle\omega\rangle &amp;= { \omega^0, \omega^1, \omega^2, \omega^3, \omega^4}\<br />
&amp;= {13^0, 13^1, 13^2, 13^3}\<br />
&amp;={1, 13, 16, 4}.<br />
\end{aligned}$$</p>
<h3>Example of 8th roots of unity in $\mathbb{F}_{17}^*$</h3>
<p>Since $g =3$ is a generator of the multiplicative group $\mathbb{F}_{17}^*$, a generator for the subgroup of order $8$ is $\omega = 3^{\frac{16}{8}} = 3^2 \equiv 9$.</p>
<p>Let us check that it is also a primitive 8th root of unity. We need to check that:</p>
<ol>
<li>Element $9$ is a 8th root of unity. This can be seen by checking that $9^8 \equiv 1 \pmod{17}$.</li>
<li>The order of element $9$ is 8. This means that 8 is the smallest positive integer $r$ such that $9^r \equiv 1 \pmod{17}$. Let us check that below:</li>
</ol>
<p>$$<br />
\begin{aligned}<br />
&amp;9^1 = 9,\<br />
&amp;9^2 \equiv 13,\<br />
&amp;9^3 \equiv 9 \cdot 13 \equiv 15,\<br />
&amp;9^4 \equiv 13 \cdot 13 \equiv (-4) \cdot (-4)\equiv 16,\qquad\pmod{17}\<br />
&amp;9^5 \equiv 9 \cdot 16 \equiv 8,\<br />
&amp;9^6 \equiv 9 \cdot 8 \equiv 4,\<br />
&amp;9^7 \equiv 9 \cdot 4 \equiv 2,\<br />
&amp;\boxed{9^8 \equiv 9 \cdot 2 = 18\equiv 1}.<br />
\end{aligned}$$</p>
<p>Therefore, element $9$ is a primitive 8th root of unity, and we can use element $9$ to generate the subgroup of all 8th roots of unity, as follows:</p>
<p>$$<br />
\begin{aligned}<br />
\langle\omega\rangle &amp;= { \omega^0, \omega^1, \omega^2, \omega^3, \omega^4, \omega^5, \omega^6, \omega^7}\<br />
&amp;= {9^0, 9^1, 9^2, 9^3, 9^4, 9^5, 9^6, 9^7}\<br />
&amp;={1, 9, 13, 15, 16, 8, 4, 2}.<br />
\end{aligned}$$</p>
<p><strong>Exercise.</strong> Find a primitive 2nd root of unity in $\mathbb{F}_{17}^*$ and the subgroup of all 2nd roots of unity.</p>
<h2>Conclusion and summary</h2>
<p>An efficient method is needed to find all $k$-th roots of unity, which is what we studied in this article. In summary, we established that:</p>
<ul>
<li>An element $a$ of a finite field $\mathbb{F}_q$ is a $k$-th root of unity if $a^k \equiv 1$.</li>
<li>If $k$ divides $q-1$, then the subgroup of $\mathbb{F}_q^*$ containing all $k$-th roots of unity is the unique subgroup of order $k$ guaranteed by the Fundamental Theorem of Cyclic Groups.</li>
<li>A primitive $k$-th root of unity generates the subgroup of all $k$-th roots of unity. If $g$ is a generator of the multiplicative group $\mathbb{F}_q^*$ and $k$ divides $q-1$, then the element $\omega = g^{\frac{q-1}{k}}$ is a primitive $k$-th root of unity.</li>
</ul>
<h2>Appendix A</h2>
<h3>The generator $\omega$ of the subgroup of order $k$ satisfies<strong>: $\omega^k \equiv 1$</strong></h3>
<p>Let $g$ be a generator of the multiplicative group $\mathbb{F}_q^*$. Recall from the Fundamental Theorem of Cyclic Groups that the element $\omega = g^{\frac{q-1}{k}}$ generates the unique subgroup of order $k$. We aim to prove that $\omega^k \equiv 1$.</p>
<p><strong>Proof.</strong> <a href="https://en.wikipedia.org/wiki/Fermat%27s_little_theorem#:~:text=In%20number%20theory%2C%20Fermat's%20little,an%20integer%20multiple%20of%207">Fermat’s Little Theorem</a> states that if $q$ is a prime number, then for any integer $g$:</p>
<p>$$<br />
g^{q}\equiv g\pmod{q}.$$</p>
<p>For example, if $g = 2$ and $q=7$, then $2^7\equiv 2\pmod{7}$.</p>
<p>If $g$ is not divisible by $q$, we can divide both sides of the equation above by $g$. This shows that Fermat’s Little Theorem is equivalent to the statement:</p>
<p>$$<br />
g^{q-1}\equiv 1\pmod{q}.$$</p>
<p>We now compute $\omega^k$ as follows:</p>
<p>$$<br />
\begin{aligned}<br />
\omega^k &amp;= (g^{\frac{q-1}{k}})^k &amp;&amp;\qquad\text{By definition of $\omega$}\<br />
&amp;= g^{q-1} &amp;&amp;\qquad\text{Simplifying the exponent}\<br />
&amp;\equiv 1\pmod{q}&amp;&amp;\qquad\text{Applying Fermat’s Little Theorem}<br />
\end{aligned}$$</p>
<h2>Appendix B</h2>
<h3>If $a\in\mathbb{F}_q^*$ and $a^k\equiv1$ then $a$ is a member of a unique cyclic subgroup of order $k$.</h3>
<p>Let $g$ be a generator of $\mathbb{F}_q^*$. This equivalently means that $g$ is a $(q-1)$-th root of unity.</p>
<p>Let $\omega=g^\frac{q-1}{k}$ be a generator for the multiplicative subgroup of order $k$ ($\omega=g^\frac{q-1}{k}$ is directly from the Fundamental Theorem of Cyclic Groups).</p>
<p>If $a\in\mathbb{F}_q^*$ and $a^k\equiv1$ then we must prove that there exists an integer $s$ such that $\omega^s=a$. The existence of integer $s$ in $\omega^s\equiv a$ proves that $a$ can be generated by $\omega$ and thus is part of the unique subgroup of order $k$.</p>
<p>To find such an $s$, we will substitute $\omega$ with $g^\frac{q-1}{k}$and $a$ with $g^m$ to convert $\omega^s=a$ into:</p>
<p>$$<br />
{\left(g^\frac{q-1}{k}\right)}^s\stackrel{?}{=}g^m$$</p>
<p>Can we come up with an $s$ that makes the equation true? If we strategically pick an $s$ such that the exponent $\frac{q-1}{k}$ will cancel and leave $m$ we have:</p>
<p>$$<br />
s=\frac{mk}{q-1}$$</p>
<p>Plugging $s=\frac{mk}{q-1}$ into the left-hand side of this equation ${\left(g^\frac{q-1}{k}\right)}^s\stackrel{?}{=}g^m$, we obtain</p>
<p>$$<br />
{\left(g^\frac{q-1}{k}\right)}^{(\frac{mk}{q-1})}$$</p>
<p>We can see that the $q-1$ and $k$ terms cancel, leaving us with $g^m$.</p>
<p>$$<br />
g^{\frac{\cancel{q-1}}{\cancel{k}}\frac{m\cancel{k}}{\cancel{q-1}}}\implies g^m$$</p>
<p>We can substitute back the original definitions for $g^\frac{q-1}{k}=\omega$, $\frac{mk}{q-1}=s$, and $g^m=a$ and see that</p>
<p>$$<br />
\omega^s=a$$</p>
<p>Therefore, if $a^k\equiv1$, then there does in fact exist an $s$ such that $\omega^s=a$. $s$ is simply</p>
<p>$$<br />
s=\frac{mk}{q-1}$$</p>
<p>where $m$ is the solution to $g^m=a$, $k$ is the order of the subgroup, and $q$ is the modulus of our finite field.</p>
<p>However, we must still prove that $s$ is an integer because generating a value by raising $\omega$ to a fraction doesn’t qualify as being a member of the subgroup generated by $\omega$.</p>
<h3>Showing $s$ is an integer</h3>
<p>To show that $\frac{mk}{q-1}$ is an integer, we need to show that dividing $mk$ by $q-1$ has no remainder.</p>
<p>When we carry out the division, we should get a quotient $n$ and a remainder $r$. We want to show that $r$ is necessarily 0.</p>
<p>$$<br />
\frac{mk}{q-1}=n+r$$</p>
<p>A remainder cannot be larger than the divisor (e.g., $\frac{11}{5}$ cannot have a remainder 5 or larger), so we also have the condition that:</p>
<p>$$<br />
0\leq r\lt q-1$$</p>
<p>We can isolate $mk$ in $\frac{mk}{(q-1)}=n+r$ by translating it from the form <code style='font-family: Arial'>dividend / divisor = quotient + remainder</code> to <code style='font-family: Arial'>dividend = quotient⋅divisor + remainder</code>. (To illustrate this re-write, consider that 6/4=1 remainder 2 can be written as 6 = 4⋅1 + 2). In re-written form, we have:</p>
<p>$$<br />
mk = n\cdot(q-1) + r\qquad\text{where $0\le r &lt; q-1$}$$</p>
<p>In order to show that $r=0$, we will set aside $mk=n\cdot(q-1)+r$ for a moment, and derive another property of $mk$.</p>
<p><strong>Fact:</strong> $g^{mk}\equiv1$</p>
<p>$g^{mk}\equiv1$ can be derived from the following facts:</p>
<ul>
<li>$a^k\equiv1$</li>
<li>$a=g^m$</li>
</ul>
<p>So by substitution, $(g^m)^k\equiv1$ and by the power of a power rule, $g^{mk}\equiv1$.</p>
<p><strong>Substitute $mk = n\cdot(q-1) + r$ into $g^{mk}\equiv1$</strong></p>
<p>We now have enough tools to show that the remainder $r$ in $mk = n\cdot(q-1) + r$ is zero. We remind the reader that $g^{q-1}\equiv1$ since $g$ is a primitive $(q-1)$-th root of unity.</p>
<p>We now show that using the definitions</p>
<ul>
<li>$g^{mk}\equiv1$</li>
<li>$mk = n\cdot(q-1) + r$</li>
<li>$g^{q-1}\equiv1$</li>
</ul>
<p>are sufficient to show that $g^{mk}\equiv g^r$:</p>
<p>$$<br />
\begin{aligned}<br />
g^{mk} &amp;= g^{n\cdot (q-1) + r}&amp;&amp;\qquad\text{Substitution}\<br />
&amp;= g^{n\cdot(q-1)}\cdot g^r&amp;&amp;\qquad\text{Power multiplication rule}\<br />
&amp;= (g^{q-1})^n\cdot g^r&amp;&amp;\qquad\text{Power of a power rule}\<br />
&amp;\equiv 1\cdot g^r&amp;&amp;\qquad\text{Because $g^{q-1}\equiv 1$}\<br />
&amp;= g^r<br />
\end{aligned}$$</p>
<p>As a consequence of $g^{mk}\equiv g^r$ and $g^{mk}\equiv 1$, we have that</p>
<p>$$<br />
g^r\equiv1$$</p>
<p>Since $g$ is a primitive $(q-1)$-th root of unity, there are only two solutions for $r$:</p>
<ul>
<li>$r=0$</li>
<li>$r\equiv q-1$</li>
</ul>
<p>Recall that $r$ is defined as the solution to</p>
<p>$$<br />
\frac{mk}{q-1}=n+r\qquad\text{where $0\le r &lt; q-1$}$$</p>
<p>We know that any valid division $\frac{mk}{q-1}$ cannot result in a remainder $r$ of $q-1$ or higher, specifically, the remainder must be in the range $0\le r \lt q-1$. That range restriction on the remainder implies $r\neq q-1$.</p>
<p>So ruling out the possibility that $r=q-1$, the only solution for $g^r\equiv1$ is $r=0$ (i.e. $g^0\equiv1$).</p>
<p>Since $r=0$, the result of dividing $mk$ by $q-1$ is an integer.</p>
<p>Finally, since $s$ is defined as</p>
<p>$$<br />
s=\frac{mk}{q-1}$$</p>
<p>$s$ is an integer.</p>
<div style='page-break-after: always;'></div>

<h1>Roots of unity ω have the property ω^(k/2) ≡ −1</h1>
<p>Source: https://rareskills.io/post/roots-of-unity-additive-inverse</p>
<h1>Roots of unity ω have the property ω^(k/2) ≡ −1</h1>
<p>In previous articles, we established that in the <a href="https://rareskills.io/post/finite-fields">finite field</a> $\mathbb{F}_q$, if $k$ divides $q-1$:</p>
<ul>
<li><strong>There exists a unique subgroup of order $k$ – the $k$-th roots of unity.</strong></li>
<li><strong>A generator $\omega$ of this subgroup is a primitive $k$-th root of unity and is given by $\omega = g^{\frac{q-1}{k}}$, where $g$ is a generator of $\mathbb{F}_q^*$.</strong></li>
<li><strong>$k$ is the smallest positive integer for which $\omega^k\equiv 1$.</strong></li>
</ul>
<p>In this article, we explore a key property of a <a href="https://rareskills.io/post/roots-of-unity-finite-field">primitive root of unity</a> $\omega$ in $\mathbb{F}_q$: <strong>As long as $k$ is even, $\omega^{\frac{k}{2}}$ is congruent to</strong> $-1$.</p>
<h2>Motivation</h2>
<p>For some applications, we want to find relationships among different $k$-th roots of unity for some $k$. More precisely, we want to determine which roots of unity are additive inverses of others.</p>
<p>In a field $\mathbb{F}_q$, if $k$ divides $q-1$, the $k$-th roots of unity can be written as</p>
<p>$$<br />
{ 1, \omega, \omega^2, …, \omega^{k-1} }$$</p>
<p>where $\omega$ is a primitive $k$-th root of unity.</p>
<p>One might ask: can we easily find $-\omega$ or $- \omega^2$? Yes, we can. Let’s take the following fact, which we will prove shortly: If $k$ is even, then $\omega^\frac{k}{2} \equiv -1$.</p>
<p>Let us use this fact. Since $-\omega$ is the same as $(-1) \omega$, knowing that $-1 \equiv \omega^\frac{k}{2}$, we have that</p>
<p>$$<br />
-\omega = (-1) \omega = \omega^\frac{k}{2} \omega = \omega^{\frac{k}{2} + 1}$$</p>
<p>The same can be used to find $- \omega^2$. We have that</p>
<p>$$<br />
-\omega^2 = (-1) \omega^2 = \omega^\frac{k}{2} \omega^2 = \omega^{\frac{k}{2} + 2}$$</p>
<p>This can be generalized by any $i$ as</p>
<p>$$<br />
– \omega^i = \omega^{\frac{k}{2} + i}$$</p>
<p>This establishes a relationship between the $k$-th roots of unity.</p>
<p>As an example, let us consider the 8th roots of unity,</p>
<p>$$<br />
{ 1, \omega, \omega^2, \omega^3, \omega^4, \omega^5, \omega^6, \omega^7 }$$</p>
<p>Using the relationship obtained, the 8th roots of unity can be written as</p>
<p>$$<br />
{ 1, \omega, \omega^2, \omega^3, -1, -\omega, – \omega^2, -\omega^3}$$</p>
<p>For this to hold, we just need to show that $\omega^\frac{k}{2} \equiv -1$ for any $k$. Let’s proceed to do that now.</p>
<h2>What is the meaning of $-1$ in a finite field $\mathbb{F}_q$</h2>
<p>In $\mathbb{F}_q$, the notation $-a$ denotes the <strong>additive inverse</strong> of $a$, satisfying $a + (-a) = 0$.</p>
<p>For example, in $\mathbb{F}_7$, since $6+1 = 7\equiv 0\pmod{7}$, we say $6$ is the additive inverse of $1$, and write</p>
<p>$$<br />
-1 \equiv 6\pmod{7}.$$</p>
<p>For any finite field $\mathbb{F}_q$, since $(q-1) + 1 = q \equiv 0\pmod{q}$, the additive inverse of $1$ is always $q-1$:</p>
<p>$$<br />
-1 \equiv q-1\pmod{q}.$$</p>
<p>Let’s now look at examples of $\omega^{\frac{k}{2}}$.</p>
<h2>Example of $\omega^{\frac{k}{2}}$ among the $k$-th roots of unity in $\mathbb{F}_{17}$</h2>
<p>In the following examples, we use the generator $g =3$ for the multiplicative group $\mathbb{F}_{17}^*$. Since $16$ is the additive inverse of $1$ in $\mathbb{F}_{17}$, we have:</p>
<p>$$<br />
-1 \equiv 16\pmod{17}$$</p>
<h3>Case $k=4$</h3>
<p>A <strong>primitive 4th root of unity</strong> is $\omega = g^{\frac{q-1}{k}} = g^{\frac{16}{4}} = 3^4 \equiv 13\pmod{17}$.</p>
<p>Here, $\omega^{\frac{k}{2}} = \omega^{\frac{4}{2}} = \omega^2 = 13^2\equiv 16 \equiv -1$.</p>
<p><strong>Thus, we conclude that $\omega^{\frac{k}{2}} \equiv -1$ for $k=4$.</strong></p>
<h3>Case $k = 8$</h3>
<p>Now $\omega = g^{\frac{q-1}{k}} = g^{\frac{16}{8}} = 3^2 \equiv 9\pmod{17}$ is a <strong>primitive 8th root of unity.</strong></p>
<p>For $k=8$, $\frac{k}{2} = 4$, and we have $\omega^{\frac{k}{2}} = 9^4\equiv 16 \equiv -1$.</p>
<h2>Example of $\omega^{\frac{k}{2}}$ among the $k$-th roots of unity in $\mathbb{F}_{97}$</h2>
<p>In the finite field $\mathbb{F}_{97}$, we have $q-1 = 97-1 = 96$. The additive inverse of $1$ is:</p>
<p>$$<br />
-1\equiv 96\pmod{97}$$</p>
<p>The element $g = 5$ is a generator of the multiplicative group $\mathbb{F}_{97}^* = {1,2,\dots,96}$. The <code style='font-family: Arial'>galois</code> library provides a convenient way to find this generator using the <code style='font-family: Arial'>primitive_element</code> property, as shown below:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
GF = galois.GF(97) # Define the field
GF.primitive_element # Returns GF(5, order=97)
</code></pre>
<p>For $k=32$, we obtain $\omega$ as</p>
<p>$$<br />
\omega = g^{\frac{q-1}{k}} = g^{\frac{96}{32}} = 5^3 = 125 \equiv 28\pmod{97}.$$</p>
<p>Letting $\frac{k}{2} = 16$, we calculate $\omega^{\frac{k}{2}} = 28^{16}$ with the following Python code:</p>
<pre style='font-family: Arial'><code class="language-solidity">result = 28**16 % 97
print(f&quot;28^16 % 97 = {result}&quot;)  # Output: 96
</code></pre>
<p>Thus:</p>
<p>$$<br />
\omega^{16} = 28^{16} \equiv 96 \equiv -1\pmod{97}.$$</p>
<p>We conclude that, for $k = 32$, $\omega^{\frac{k}{2}} \equiv -1$ in $\mathbb{F}_{97}$.</p>
<h2>Python code</h2>
<p>The following Python code checks if $\omega^\frac{k}{2} \equiv – 1$ for a field $\mathbb{F}_q$. It is used to test this property in $\mathbb{F}_{17}$ for $k = 8$ and $\omega = 9$. You can test it for $k = 32$ with $\omega = 28$ in $\mathbb{F}_{97}$ or another valid combination of your choice.</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois

def check_omega_half_is_minus_one(q, omega, k):
    GF = [galois.GF](http://galois.gf/)(q)
    if k % 2 != 0:
        raise ValueError(&quot;k must be even&quot;)

    omega_half = GF(omega) ** (k // 2)

    return omega_half == GF(q-1)

# Example usage:
q = 17
k = 8
omega = 9
result = check_omega_half_is_minus_one(q, omega, k)
print(f&quot;For ω={omega} and k={k}: ω^(k/2) == -1 is {result} in F_{q}&quot;)
</code></pre>
<h2><strong>The mathematical proof</strong></h2>
<p>Let $g$ be a generator of $\mathbb{F}_q^*$. This equivalently means that $g$ is a <strong>primitive $(q-1)$-th root of unity</strong>.</p>
<p>Let $\omega=g^\frac{q-1}{k}$ be a primitive $k$-th root of unity in the finite field $\mathbb{F}_q$. We will prove that $\omega^{\frac{k}{2}} \equiv -1$.</p>
<p>The idea of the proof is to show that $\omega^\frac{k}{2}$ can only be $1$ or $−1$. We will exclude the possibility that $\omega^\frac{k}{2}$ is $1$, leaving $−1$ as the only option.</p>
<p><strong>Proof</strong>:</p>
<p>Let’s take the square of $\omega^\frac{k}{2}$. It is given by</p>
<p>$$<br />
\left(\omega^\frac{k}{2} \right)^2 = \omega^k \equiv 1$$</p>
<p>The last equality follows from the fact that $\omega$ is a primitive $k$-th root of unity.</p>
<p>Since the square of $\omega^\frac{k}{2}$ is $1$, that is, $\left(\omega^\frac{k}{2} \right)^2 \equiv 1$, then $\omega^\frac{k}{2}$ can only be $1$ or $-1$, because only $1^2$ or $(-1)^2$ is equal to $1$.</p>
<p>Let us show that it cannot be $1$.</p>
<p>Replace $\omega = g^{\frac{q-1}{k}}$ into $\omega^{\frac{k}{2}}$. This gives us that</p>
<p>$$<br />
\omega^{\frac{k}{2}} = \left(g^{\frac{q-1}{k}}\right)^{\frac{k}{2}} = g^{\frac{q-1}{2}}$$</p>
<p>Since $g$ is a <strong>primitive</strong> $(q-1)$-th root of unity, the smallest positive integer $r$ for which $g^r\equiv 1$ is $r=q−1$.</p>
<p>In other words, there is no integer $r$ smaller than $q-1$ such that $g^{r} \equiv 1$. Since $\frac{q-1}{2} &lt; q-1$, $g^{\frac{q-1}{2}}$ cannot be $1$. Therefore, the only possibility is that $\omega^\frac{k}{2}$ is equal to $-1$.</p>
<h2>Summary</h2>
<ul>
<li>If $\omega$ is a primitive $k$-th root of unity in a finite field $\mathbb{F}_q$, then $\omega^{\frac{k}{2}} \equiv -1$ for even $k$.</li>
<li>Using this property, we have that $- \omega^i = \omega^{\frac{k}{2}+i}$ or stated equivalently, $\omega^i$ is the additive inverse of $\omega^{k/2+i}$.</li>
</ul>
<p>The <a href="https://rareskills.io/posts/roots-of-unity-unit-circle">following chapter</a> will introduce a visualization that makes these points easier to remember.</p>
<div style='page-break-after: always;'></div>

<h1>Visual representation of the roots of unity</h1>
<p>Source: https://rareskills.io/post/roots-of-unity-unit-circle</p>
<h1>Visual representation of the roots of unity</h1>
<p>The property that if $\omega$ is a $k$-th root of unity, then $\omega^i$ and $\omega^{i+k/2}$ are additive inverses may seem a little abstract — this chapter introduces a visual that makes this concept easier to remember.</p>
<p>Recall that the $k$-th roots of unity are generated by taking the primitive $k$-th root of unity $\omega$ and raising it to successive powers. For example, if $k=4$, we compute the 4-th roots of unity as</p>
<ul>
<li>$\omega^0=1$</li>
<li>$\omega^1=\omega$</li>
<li>$\omega^2=\omega^2$</li>
<li>$\omega^3=\omega^3$</li>
</ul>
<p>If we were to keep going, the exponents would wrap around modulo 4:</p>
<ul>
<li>$\omega^4=1$ (by definition of being a 4-th root of unity)</li>
<li>$\omega^5=\omega^4\omega=1\cdot\omega =\omega$</li>
<li>$\omega^6=\omega^4\omega^2=1\cdot\omega^2=\omega^2$</li>
<li>$\omega^7=\omega^4\omega^3=1\cdot\omega^3=\omega^3$</li>
<li>$\omega^8=\omega^4\omega^4=1$</li>
<li>$\omega^9=\omega^4\omega^4\omega=\omega$</li>
</ul>
<p>and so on.</p>
<p>Note that the exponent “wraps around” at each multiple of four. Thus, for any $\omega^i$ and $\omega^j$, $\omega^i\omega^j=\omega^{i+j\pmod k}$. Since $k=4$ in our example, we have that $\omega^i\omega^j=\omega^{i+j\pmod 4}$</p>
<p>Now recall that addition modulo $k$ can be represented as a “clock.” Here, our clock consists of the “hour markers” 0, 1, 2, 3, which are the exponents of $\omega$</p>
<p><img alt="The numbers 0 to 3 evenly spaced around a circle" src="assets/image3.jpg" /></p>
<p>One way to think about this is</p>
<p>“adding two numbers $i$ and $j$ on the clock is equivalent to multiplying the roots of unity that have exponents $i$ and $j$ i.e. $\omega^i$ and $\omega^j$.”</p>
<p>If we take any root of unity and add 1 to the exponent, this is equivalent to multiplying that root of unity by $\omega^1$ (or just $\omega$). For example, multiplying $\omega^2$ by $\omega$ is the same as adding one to the exponent to get $\omega^3$.</p>
<p>Therefore, multiplying a $k$-th root of unity by $\omega$ or adding 1 to the exponent is the same as taking $1/k$-th step around the circle.</p>
<p>For example, if we multiply $\omega^2$ by $\omega,$ we get $\omega^3$, which equals moving forward 1/4-th step:</p>
<p><img alt="A diagram showing the equivalence between multiplying by omega and taking a step around the unit circle." src="assets/image1.jpg" /></p>
<p>We can think of generating the roots of unity by starting with $\omega^0$ and repeatedly adding 1 to the exponent to produce $\omega^1, \omega^2, \omega^3$, and so on until we reach $\omega^{k}$ at which point the results will wrap around modulo $k$. This is exactly the same as taking $k$ steps around the circle.</p>
<h2>Visualizing congruences with the unit circle</h2>
<p>Let’s expand this circle to include more congruences.</p>
<p><img alt="Showing that omega exponents are addition modulo 4" src="assets/image2.jpg" /></p>
<p>If we multiply $\omega^3$ and $\omega^2$, we will get $\omega^5$, which is congruent to $\omega$, exactly as the chart suggests:</p>
<p><img alt="An example of adding exponents on the unit circle is equivalent to multiplying the roots of unity." src="assets/image4.jpg" /></p>
<p>Another way to think about this is starting at $\omega^2$ and taking 3 steps forward:</p>
<p><img alt="An animation showing three steps along the unit circle from omega^2 to omega^1" src="assets/image9.gif" /></p>
<h2>Points k/2 steps apart</h2>
<p>Since it takes $k$ steps to “walk around” the circle, $k/2$ steps takes you from a point to the opposite point.</p>
<p>Now observe in the circle of $k=4$ that opposite points are additive inverses of each other (they sum to zero). Recall that $-1\equiv\omega^{k/2}$. Since k = 4, $\omega^2 =-1.$</p>
<p><img alt="additive inverses are on opposite sides" src="assets/image5.jpg" /></p>
<p>In the $k=4$ example, we have that</p>
<ul>
<li>$\omega^0+\omega^2\equiv1+(-1)=0$</li>
<li>$\omega^1+\omega^3\equiv\omega+(-\omega)=0$</li>
</ul>
<p>Note that we are now <em>adding</em> the roots of unity together, not multiplying them, so the addition of exponents rule does not apply! Don’t confuse $\omega^0+\omega^2$ with $\omega^0\cdot\omega^2$! Roots of unity are <a href="https://rareskills.io/post/finite-field">finite field</a> elements, and fields have 2 operations: addition and multiplication.</p>
<h2>Examples with other values of k</h2>
<p>If the circle is partitioned into $k$ segments, then taking $k/2$ steps takes you to the opposite side. In each of the cases shown here, we see that opposite points are additive inverses.</p>
<h3>k = 8</h3>
<p><img alt="unit circle with the 8-th roots of unity" src="assets/image6.jpg" /></p>
<h3>k = 6</h3>
<p><img alt="unit circle with the 6-th roots of unity" src="assets/image8.jpg" /></p>
<h3>k = 16</h3>
<p><img alt="roots of unity with the 16-th roots of unity" src="assets/image7.jpg" /></p>
<h2>Summary</h2>
<p>To remember that $\omega^i+\omega^{i+k/2}$ are additive inverses (their sum is zero), we draw a circle with $k$ points where each step is a multiplication by $\omega.$ The opposite points will be additive inverses.</p>
<p>The circle diagram will also be very useful for visualizing subgroups of the roots of unity as well as square roots — we will show those visualizations in the upcoming chapters.</p>
<div style='page-break-after: always;'></div>

<h1>Vandermonde Matrices</h1>
<p>Source: https://rareskills.io/post/vandermonde-matrix</p>
<h1>Vandermonde Matrices</h1>
<p>A <strong>Vandermonde matrix</strong> is a matrix that converts a polynomial from its coefficient representation into its value representation at a set of points.</p>
<p>For a polynomial $f(x) = a_0 +a_1x + a_2x^2+\dots+a_{k-1}x^{k-1}$ with its <a href="https://rareskills.io/post/polynomial-multiplication-point-form#ways-to-represent-a-polynomial">coefficient representation</a>:</p>
<p>$$<br />
\mathbf{a} =<br />
\begin{bmatrix}<br />
a_0\<br />
a_1\<br />
a_2\<br />
\vdots\<br />
a_{k-1}<br />
\end{bmatrix}$$</p>
<p>The Vandermonde matrix evaluates it at $k$ distinct points as a single operation.</p>
<h2>Evaluating a polynomial as a matrix product</h2>
<p>For simplicity, we assume $k=4$, then we have a polynomial of degree $k-1 =3$.</p>
<h3>Evaluation at single point</h3>
<p>The evaluation of polynomial $f(x)$ at the point $x_0$ is:</p>
<p>$$<br />
f(x_0) = a_0 +a_1\cdot x_0 + a_2\cdot x_0^2+a_{3}\cdot x_0^{3}$$</p>
<p>This can be written as matrix product: multiplying a $1\times 4$ matrix containing the successive powers of $x_0$ by the vector of polynomial coefficients, as follows:</p>
<p>$$<br />
\begin{bmatrix}<br />
f(x_0)<br />
\end{bmatrix}<br />
= \begin{bmatrix} 1 &amp; x_0^1 &amp; x_0^2 &amp; x_0^{3}<br />
\end{bmatrix}<br />
\cdot<br />
\begin{bmatrix}<br />
a_0\<br />
a_1\<br />
a_2\<br />
a_{3}<br />
\end{bmatrix}$$</p>
<h3>Evaluation at two points</h3>
<p>To evaluate at two points, $x_0$ and $x_1$, we could express these as two separate matrix products. Instead, we stack these row vectors into a $2 \times 4$ matrix:</p>
<p>$$<br />
\begin{bmatrix}<br />
f(x_0) \<br />
f(x_1)<br />
\end{bmatrix}<br />
=<br />
\begin{bmatrix} 1 &amp; x_0^1 &amp; x_0^2 &amp; x_0^{3}\<br />
1 &amp; x_1^1 &amp; x_1^2 &amp; x_1^{3}\<br />
\end{bmatrix}<br />
\cdot<br />
\begin{bmatrix}<br />
a_0\<br />
a_1\<br />
a_2\<br />
a_{3}<br />
\end{bmatrix}$$</p>
<p>Where each row contains the successive powers of $x_0$ and $x_1$, respectively.</p>
<p>Therefore, evaluating the polynomial at two points is equivalent to multiplying a $2 \times k = 2 \times 4$ matrix by the coefficient vector.</p>
<h3>Evaluation at $4$ points</h3>
<p>If we extend our points to $4$ points, then with $k=4$ (already assumed), the resulting system of equations is equivalent to multiplying a $k\times k = 4\times 4$ matrix by the vector of coefficients:</p>
<p>$$<br />
\begin{bmatrix}<br />
f(x_0) \<br />
f(x_1)\<br />
f(x_2)\<br />
f(x_{3})<br />
\end{bmatrix}<br />
=<br />
\begin{bmatrix}<br />
1 &amp; x_0 &amp; x_0^2 &amp; x_0^{3} \<br />
1 &amp; x_1 &amp; x_1^2 &amp; x_1^{3} \<br />
1 &amp; x_2 &amp; x_2^2 &amp; x_2^{3} \<br />
1 &amp; x_{3} &amp; x_{3}^2 &amp; x_{3}^{3}<br />
\end{bmatrix}<br />
\cdot<br />
\begin{bmatrix}<br />
a_0\<br />
a_1\<br />
a_2\<br />
a_{3}<br />
\end{bmatrix}$$</p>
<p>This matrix is called a $4\times 4$ <strong>Vandermonde matrix</strong> and is denoted by $\mathbf{V}$.</p>
<p>The equation above is compactly written below as</p>
<p>$$<br />
\mathbf{p} = \mathbf{V}\cdot\mathbf{a}$$</p>
<p>where $\mathbf{a}$ is the vector of the polynomial’s coefficients and $\mathbf{p}$ is the vector of its point values.</p>
<h2>Evaluating the polynomial at the 4th roots of unity as a matrix product</h2>
<p>Now, consider evaluating the polynomial $f(x)$ at the 4th <a href="https://rareskills.io/post/roots-of-unity-finite-field">roots of unity</a>, ${\omega^0, \omega^1, \omega^{2}, \omega^{3}}$, instead of at arbitrary $4$ points. We get the <strong>Vandermonde matrix</strong> $\mathbf{V}$ as:</p>
<p>$$<br />
\mathbf{V} =<br />
\begin{bmatrix}<br />
1 &amp; 1^1 &amp; 1^2 &amp; 1^3 \<br />
1 &amp; \omega &amp; \omega^2 &amp; \omega^{3} \<br />
1 &amp; (\omega^2) &amp; (\omega^2)^2 &amp; (\omega^2)^{3} \<br />
1 &amp; (\omega^3) &amp; (\omega^3)^2 &amp; (\omega^3)^{3}<br />
\end{bmatrix}$$</p>
<p>We can simplify every term that is $\omega^{\frac{k}{2}}$ or greater, by leveraging the properties that $\omega^{\frac{k}{2}} = \omega^2 = -1$ and $\omega^k=1$ as follows:</p>
<ul>
<li>$\omega^2 \equiv -1$ imply that $(\omega^2)^2 \equiv (-1)^2 = 1$ and $(\omega^2)^3 \equiv (-1)^3 = -1$.</li>
<li>$\omega^3 = \omega^2\cdot\omega\equiv -1\cdot\omega = -\omega$ imply that:</li>
<li>$(\omega^3)^2 \equiv (-\omega)^2 = \omega^2\equiv -1$ and</li>
<li>$(\omega^3)^3 \equiv (-\omega)^3 = -\omega^3 =-(-\omega) =\omega$.</li>
</ul>
<p>We now substitute these simplifications into the matrix:</p>
<p>$$<br />
\mathbf{V} =<br />
\begin{bmatrix}<br />
1 &amp; 1^1 &amp; 1^2 &amp; 1^3 \<br />
1 &amp; \omega &amp; \omega^2\equiv-1 &amp; \omega^{3}\equiv-\omega \<br />
1 &amp; (\omega^2)\equiv-1 &amp; (\omega^2)^2\equiv1 &amp; (\omega^2)^{3}\equiv-1 \<br />
1 &amp; (\omega^3)\equiv-\omega &amp; (\omega^3)^2\equiv-1 &amp; (\omega^3)^{3}\equiv\omega<br />
\end{bmatrix}$$</p>
<p>Therefore, the matrix simplifies to the following pattern:</p>
<p>$$<br />
\mathbf{V} =<br />
\begin{bmatrix}<br />
1 &amp; 1 &amp; 1 &amp; 1 \<br />
1 &amp; \omega &amp; -1 &amp; -\omega \<br />
1 &amp; -1 &amp; 1 &amp; -1 \<br />
1 &amp; -\omega &amp; -1 &amp; \omega<br />
\end{bmatrix}$$</p>
<p>For a concrete example, $\omega=13$ is a primitive 4th root of unity in the finite field $\mathbb{F}_{17}$ (where arithmetic is modulo 17), and the Vandermonde matrix is:</p>
<p>$$<br />
\mathbf{V} =<br />
\begin{bmatrix}<br />
1 &amp; 1 &amp; 1 &amp; 1 \<br />
1 &amp; 13 &amp; 16 &amp; 4 \<br />
1 &amp; 16 &amp; 1 &amp; 16 \<br />
1 &amp; 4 &amp; 16 &amp; 13<br />
\end{bmatrix}$$</p>
<h2><strong>Conclusion</strong></h2>
<p>Evaluating a polynomial of degree $k-1$ at $k$ points is equivalent to multiplying a $k \times k$ Vandermonde $\mathbf{V}$ matrix by the coefficient vector $\mathbf{a}$, formalized by the equation $\mathbf{V}\cdot\mathbf{a} = \mathbf{p}$.</p>
<div style='page-break-after: always;'></div>

<h1>The square of a k-th root of unity is a k/2-th root of unity</h1>
<p>Source: https://rareskills.io/post/roots-of-unity-squared</p>
<h1>The square of a k-th root of unity is a k/2-th root of unity</h1>
<p>If we take the set of $k$-th roots of unity (with $k$ even) and square each element, the resulting set will be a set of half the size. The new set will be the $\frac{k}{2}$-th roots of unity.</p>
<p>For example, suppose $k = 6$. The 6th roots of unity would be</p>
<p>$$<br />
\set{1,\omega,\omega^2,\omega^3,\omega^4,\omega^5}$$</p>
<p>If we square each element, we get the following set. Some elements have exponents greater than or equal to $k$, but we will handle that in the next step.</p>
<p>$$<br />
\set{1^2,\omega^2,\omega^4,\omega^6,\omega^8,\omega^{10}}$$</p>
<p>We can then factor the exponents as follows:</p>
<p>$$<br />
\set{1^2,\omega^2,\omega^4,\omega^6,(\omega^6)\omega^2,(\omega^{6})\omega^4}$$</p>
<p>Since $\omega$ is a 6-th root of unity, $\omega^6\equiv1$ so we have:</p>
<p>$$<br />
\set{1^2,\omega^2,\omega^4,1,(1)\omega^2,(1)(\omega^4)}$$</p>
<p>Removing multiplication by $1$, we get</p>
<p>$$<br />
\set{1^2,\omega^2,\omega^4,1,\omega^2,\omega^4}$$</p>
<p>Now replace all the duplicate terms with a single element:</p>
<p>$$<br />
\set{1,\omega^2,\omega^4}$$</p>
<p>The new set is half the size of the original, and each element is a 3-rd root of unity:</p>
<ul>
<li>$1^3\equiv1$</li>
<li>$(\omega^2)^3=\omega^6\equiv1$</li>
<li>$(\omega^4)^3=\omega^{12}=\omega^6\omega^6\equiv1\cdot1=1$</li>
</ul>
<p>If we plot the 6-th roots of unity on a circle, we can see that squaring “removes” every other element. We started with $\set{1,\omega,\omega^2,\omega^3,\omega^4,\omega^5}$ and ended with $\set{1,\omega^2,\omega^4}$</p>
<p><img alt="A diagram showing that squaring the 6-th roots of unity results in the 3-rd roots of unity" src="assets/image1.jpg" /></p>
<p>To reiterate, if we take the set of $k$-th roots of unity, and $k$ is even, then square each element, we get a set of half the size with each element being the $\frac{k}{2}$-th root of unity.</p>
<p>Some more examples:</p>
<ul>
<li>If $k = 10$ and we square each of the 10-th roots of unity, we get a set of size five which are the fifth roots of unity.</li>
<li>If $k = 8$ and we square each of the 8-th roots of unity, we get a set of size four which is the fourth roots of unity.</li>
<li>If $k = 4$ and we square each of the 4-th roots of unity, we get a set of size two which is the 2-nd roots of unity.</li>
<li>If $k = 2$ and we square each of the 2-nd roots of unity, we get a set of size 1 which is just the element 1.</li>
</ul>
<p>The last point is easily illustrated. The second roots of unity are square roots of 1, which are always $\set{1,-1}\equiv\set{1,\omega^{k/2}}$. Squaring 1 results in 1 and squaring -1 results in 1. Equivalently, $(\omega^{k/2})^2=\omega^k\equiv1$.</p>
<h2>Example of squaring the 8-th roots of unity</h2>
<p>Consider the subgroup of 8th roots of unity $\langle 9\rangle = {1, 9, 13, 15, 16, 8, 4, 2}$ in the finite field $\mathbb{F}_{17}$. We square all elements of this subgroup as follows:</p>
<p>$$<br />
\begin{aligned}<br />
&amp;1^2 = 1\pmod{17},\<br />
&amp;9^2 \equiv 13\pmod{17},\<br />
&amp;13^2 \equiv 16\pmod{17} ,\<br />
&amp;15^2 \equiv 4\pmod{17},\<br />
&amp;16^2 \equiv 1\pmod{17},\<br />
&amp;8^2 \equiv 13\pmod{17},\<br />
&amp;4^2 \equiv 16\pmod{17},\<br />
&amp;2^2 = 4\pmod{17}.<br />
\end{aligned}$$</p>
<p>The set obtained after squaring is ${1,13,16,4}$, which is precisely the subgroup of 4th roots of unity.</p>
<p>Here is a visualization of the roots of unity before and after squaring. We started with the set $\set{1, 9, 13, 15, 16, 8, 4, 2}$ and ended with the set $\set{1,13,16,4}$</p>
<p><img alt="A diagram showing that squaring the 8-th roots of unity results in the 4-th roots of unity" src="assets/image2.jpg" /></p>
<h2>k must be even</h2>
<p>If $k$ is odd, then there is no such thing as “half of the group” as an odd-sized set cannot be divided into two. For the purposes of NTT, we only deal with even-sized $k$, so we aren’t interested in the case where $k$ is odd.</p>
<h2>Proof of the claim that the new set is half the size</h2>
<p>Let $\omega$ be a primitive $k$-th root of unity with $k$ even. Let $\langle\omega\rangle$ be the subgroup generated by $\omega$ of order $k$. We claim that $|\set{\omega^2|\omega\in\langle\omega\rangle}|=k/2$.</p>
<p>The proof is actually quite simple and intuitive.</p>
<p>We established in an earlier chapter that $\omega^{i}$ and $\omega^{i+k/2}$ are additive inverses. Since $k$ is even, we can partition the group into two sets, the first one being $0…(k/2-1)$ and the second being $k/2…k-1$:</p>
<p>$$<br />
\set{\omega^0,\omega^1,\omega^2,…}\quad\set{\omega^{k/2},\omega^{k/2+1},\omega^{k/2+2},…}$$</p>
<p>Those elements are congruent to the following representation:</p>
<p>$$<br />
\set{\omega^0,\omega^1,\omega^2,…}\quad\set{-\omega^0,-\omega^1,-\omega^2,…}$$</p>
<p>If we apply $f(x)=x^2$ to both sets, we get two sets with identical content and size $k/2$</p>
<p>$$<br />
\set{1,\omega^2,\omega^4,…}\quad\set{1,\omega^2,\omega^4,…}$$</p>
<p>Since the two sets are identical, the union of the two sets will be the same size, which is $k/2$.</p>
<h2>Proof that squaring a $k$-th root of unity produces a $k/2$-th root of unity</h2>
<p>Suppose $a$ is a $k$-th root of unity. We aim to show that $a^2$ is a $\frac{k}{2}$-th root of unity, that is:</p>
<p>$$<br />
(a^2)^{\frac{k}{2}}\equiv 1$$</p>
<p>Let’s simplify $(a^2)^{\frac{k}{2}}$:</p>
<p>$$<br />
(a^2)^{\frac{k}{2}} = a^{k}$$</p>
<p>Since $a^k\equiv1$ (because $a$ is a $k$-th root of unity), it follows that $(a^2)^{\frac{k}{2}}\equiv 1$.</p>
<p>Therefore, $a^2$ is indeed a $\frac{k}{2}$-th root of unity.</p>
<div style='page-break-after: always;'></div>

<h1>Roots of Unity raised to the k/2 power equals 1 or -1</h1>
<p>Source: https://rareskills.io/post/roots-of-unity-raised-k-over-2</p>
<h1>Roots of Unity raised to the k/2 power equals 1 or -1</h1>
<p>Any $k$-th root of unity with even $k$ raised to the $k/2$ power will result in 1 or -1.</p>
<p>This should not be confused with the similar-looking concepts that $\omega^{k/2}\equiv-1$ or that roots of unity $\omega^{i}$ and $\omega^{i+k/2}$ are additive inverses of each other.</p>
<p>Let’s use the primitive 8-th roots of unity as an example with generator (primitive 8-th root of unity) $\omega$:</p>
<ul>
<li>$(1)^{k/2}=1$</li>
<li>$(\omega)^{k/2}\equiv-1$</li>
<li>$(\omega^2)^{k/2}\equiv\omega^{2k/2}\equiv\omega^k\equiv1$</li>
<li>$(\omega^3)^{k/2}\equiv\omega^{3k/2}\equiv(\omega^{k/2})^{3}\equiv(-1)^3\equiv-1$</li>
<li>$(\omega^4)^{k/2}\equiv\omega^{4k/2}\equiv\omega^{2k}\equiv1$</li>
<li>$(\omega^5)^{k/2}\equiv\omega^{5k/2}\equiv(\omega^{k/2})^{5}\equiv(-1)^5\equiv-1$</li>
<li>$(\omega^6)^{k/2}\equiv\omega^{6k/2}\equiv\omega^{3k}\equiv1$</li>
<li>$(\omega^7)^{k/2}\equiv\omega^{7k/2}\equiv(\omega^{k/2})^{7}\equiv(-1)^7\equiv-1$</li>
</ul>
<p>As an exercise for the reader, we recommend taking the 6-th roots of unity, raising each element to the 3rd power ($k/2$) and seeing that the results are $\set{1,-1}$.</p>
<p>Looking at the evaluations above, we see a pattern that the even powered roots of unity plugged into $f(x)=x^{k/2}$ evaluate to 1 and the odd-powered roots of unity plugged into $f(x)=x^{k/2}$ evaluate to -1. A proof of this is in the appendix. Meanwhile, let’s make the central claim of the chapter:</p>
<p><strong>Any k-th root of unity raised to $k/2$ where $k$ is even results in 1 or -1. Specifically, let $\omega$ be the primitive $k$-th root of unity and let the root of unity in question be $\omega^s$. If $s$ is even, $(\omega^s)^{k/2}$ will evaluate to 1 and if $s$ is odd, then $(\omega^s)^{k/2}$ will evaluate to -1.</strong></p>
<p><strong>A side-effect of this claim is that terms in a polynomial with the power $x^{k/2}$ can be evaluated almost for free if evaluated on a root of unity.</strong></p>
<p>Suppose for example that we have a polynomial $f(x)=x^4$ that we want to evaluate on 8 points. Now suppose we set the 8 points to be the 8-th roots of unity. Normally, we’d have to loop through $\set{1, \omega,…,\omega^7}$ and evaluate $f(x)$ on each point. However, we don’t need to actually exponentiate each point of evaluation — we just check if the power of the root of unity is even or odd!</p>
<p>In fact, we can shortcut the process entirely. Let’s treat $\set{1, \omega,…,\omega^7}$ as an array with length 8. We can return 1 or -1 based on whether the array index is even or odd, and completely ignore the exponent. In other words, $f(x)=x^4$ will evaluate to</p>
<p>$$<br />
[1,-1,1,-1,1,-1,1,-1]$$</p>
<p>If the polynomial has a coefficient other than one, for example $f(x)=ax^4$, the evaluation depends just on whether we are on an even or odd index:</p>
<p>$$<br />
[a,-a,a,-a,a,-a,a,-a]$$</p>
<p>But what about polynomials that aren’t of the form $f(x)=x^{k/2}$? Polynomials can be factored to introduce as many $x^{k/2}$ terms as possible. For example, consider the polynomial</p>
<p>$$<br />
f(x)=a_0+a_1x+a_2x^2+a_3x^3+a_4x^4+a_5x^5+a_6x^6+a_7x^7$$</p>
<p>Only the term $a_4x^4$ is of the form $x^{k/2}$. However, suppose we factor the polynomial as follows:</p>
<p>$$<br />
f(x)=(a_0+a_4x^4)+(a_1x+a_5x^5)+(a_2x^2+a_6x^6)+(a_3x^3+a_7x^7)$$<br />
$$<br />
f(x)=(a_0+a_4x^4)+x(a_1+a_5x^4)+x^2(a_2+a_6x^4)+x^3(a_3+a_7x^4)$$</p>
<p>This polynomial is much easier to evaluate since we know in advance when the $x^4$ terms will evaluate to 1 or -1.</p>
<p>However, we don’t yet have a nice trick to handle the lower powers of $x$. This will be the subject of upcoming chapters.</p>
<h2>Summary</h2>
<p>Raising a $k$-th root of unity $\omega^s$ to the $k/2$ power results in 1 if $s$ is even and -1 if $s$ is odd. If we evaluate a polynomial on the $k$-th roots of unity, the terms with power $k/2$ can be automatically computed simply by knowing if the root of unity we are evaluating on is an even power or odd power. Therefore, it is desirable to factor the polynomial so that we maximize the amount of $x^{k/2}$ terms.</p>
<h2>Appendix — Proof that $(\omega^s)^{k/2}$ is 1 if $s$ is even and -1 if $s$ is odd for even $k$</h2>
<p>$\omega^s$ and $\omega^{s+k/2}$ are additive inverses of each other. Since $\omega^0=1$ and $\omega^{0+k/2}=\omega^{k/2}$, $\omega^{k/2}$ must be the additive inverse of $1$ and hence $\omega^{k/2}\equiv-1$.</p>
<p>Now we take $\omega^{k/2}$ (which is -1) and raise it to $s$</p>
<p>$$<br />
\left(\omega^{k/2}\right)^s$$</p>
<p>Note that $(-1)^s$ can only be 1 or -1. Specifically, if $s$ is even, then $(-1)^s=1$ and if $s$ is odd, then $(-1)^s=-1$. Hence, if $s$ is even, the outcome of our expression is 1, and if $s$ is odd, then the outcome is -1.</p>
<p>Our expression can be rewritten as:</p>
<p>$$<br />
\left(\omega^{k/2}\right)^s=\left(\omega^{(k/2)s}\right)=\left(\omega^{s(k/2)}\right)=\left(\omega^s\right)^{k/2}$$</p>
<p>Since the algebraic identity of the expression has not changed, we can still say if $s$ is even, then $(\omega^s)^{k/2}=1$ and if $s$ is odd, $(\omega^s)^{k/2}=-1$.</p>
<p>Therefore, we have proved the original statement that $(\omega^s)^{k/2}=1$ when $s$ is even and $(\omega^s)^{k/2}=-1$ when $s$ is odd.</p>
<div style='page-break-after: always;'></div>

<h1>Square Roots of Roots of Unity</h1>
<p>Source: https://rareskills.io/post/roots-of-unity-square-roots</p>
<h1>Square Roots of Roots of Unity</h1>
<p>The square root of a number $x$ is $y$ such that $y^2=x$. When $x$ is of the form $x^m$ and $m$ is even, then the square root is easy to compute: it’s simply $x^{m/2}$. This follows from the power rule of exponents:</p>
<p>$$<br />
x^\frac{m}{2}\cdot x^\frac{m}{2}=x^{\frac{m}{2}+\frac{m}{2}}=x^\frac{2m}{2}=x^m$$</p>
<p>If we restrict exponents to be integers, then $x^{m}$ has a square root if and only if $m$ is even. Therefore, $x^\frac{m}{2}$ is a square root of $x^m$.</p>
<p>Square roots have two solutions. For example, the integer square root of 4 is 2 and -2. Thus, we also know that if $x^{m/2}$ is a square root of $x^m$, then $-x^{m/2}$ is also a square root. We can verify this algebraically as</p>
<p>$$<br />
(-x^\frac{m}{2})(-x^\frac{m}{2})=(-1)(-1)(x^\frac{m}{2})(x^\frac{m}{2})=x^\frac{m}{2}x^\frac{m}{2}=x^m$$</p>
<h3>Examples of computing the square roots of numbers in exponent form</h3>
<p><strong>Example 1:</strong> What is the square root of $17^{52}$ ?</p>
<p>The exponent is even, so we can divide the exponent by two. The answer is $17^{26}$ and $-17^{26}$.</p>
<p><strong>Example 2:</strong> What is the square root of $13^{8k}$?</p>
<p>Because $k$ is multiplied by 8, it doesn’t matter if $k$ is even or not because the product will be even, so we know the exponent can be divided by two. Half of $8k$ is $4k$ so the answer is $13^{4k}$ and $-13^{4k}$</p>
<p><strong>Example 3:</strong> What is the square root of $a^{2c}$?</p>
<p>Again, we don’t need to know $a$ or $c$. The exponent is guaranteed to be even due to the multiplication by two. Dividing the exponent by 2, we get $c$, so the square roots are $a^c$ and $-a^c$.</p>
<h3>Exponent rule of square roots</h3>
<p>The value $a^{m}$ has square roots $a^{m/2}$ and $-a^{m/2}$. The exponent of the square roots will be integers if and only if $m$ is even.</p>
<h2>Applying the exponent rule of square roots to roots of unity</h2>
<p>As we’ve seen several times already, the roots of unity are written as powers of a primitive root of unity. Here is the multiplicative subgroup of the 8-th roots of unity:</p>
<p>$$<br />
\set{\omega^0\equiv1,\omega^1,\omega^2,\omega^3,\omega^4\equiv-1,\omega^5,\omega^6,\omega^7}$$</p>
<p>Recall the identity that $-\omega^i=\omega^{k/2+i}$.</p>
<p>Since $k=8$ in our example, $k/2=4$ so $\omega$ and $\omega^{5}$ are $k/2$ apart. Thus, we can also write the 8-th roots of unity as</p>
<p>$$<br />
\set{1,\omega,\omega^2,\omega^3,-1,-\omega,-\omega^2,-\omega^3}$$</p>
<p>Based on the exponent rule of square roots, only the even powers of omega have square roots (we consider 0 to be even in this context):</p>
<p>$$<br />
\set{\boxed{\omega^0\equiv1},\omega^1,\boxed{\omega^2},\omega^3,\boxed{\omega^4\equiv-1},\omega^5,\boxed{\omega^6},\omega^7}$$</p>
<p>We can compute their square roots as follows:</p>
<ul>
<li>$\sqrt{\omega^0}=1, -1$ or equivalently $\omega^0$ and $\omega^4$</li>
<li>$\sqrt{\omega^2}=\omega,-\omega$ or equivalently $\omega$ and $\omega^5$</li>
<li>$\sqrt{\omega^4}=\omega^2,-\omega^2$ or equivalently $\omega^2$ and $\omega^6$</li>
<li>$\sqrt{\omega^6}=\omega^3,-\omega^3$ or equivalently $\omega^3$ and $\omega^7$</li>
</ul>
<p>If we visualize the 8-th roots of unity on the circle, we note that only the members of the red subgroups (the even powers, or equivalently the powers of $\omega^2$) have square roots:</p>
<p><img alt="A diagram showing that the square roots of the 4-th roots of unity are the 8-th roots of unity." src="assets/image1.jpg" /></p>
<p>The following diagram shows how each square root evaluation results in two opposite points on the circle:</p>
<p><img alt="A diagram showing square roots on the unit circle for 1, -1, omega^2 and omega^6" src="assets/image2.jpg" /></p>
<h2>Taking the square root of the k-th roots of unity produces the 2k-th roots of unity (if they exist)</h2>
<p>In a previous chapter, we saw that squaring the roots of unity halves the size of the set (assuming the set is even in size). Taking the square root of the roots of unity doubles the size of the set. For example, consider the 8-th roots of unity generated by $\omega$ as shown above:</p>
<p>$$<br />
\set{1,\omega,\omega^2,\omega^3,\omega^4,\omega^5,\omega^6,\omega^7}$$</p>
<p>If we square each element, we get the set:</p>
<p>$$<br />
\set{1,\omega^2,\omega^4,\omega^6}$$</p>
<p>Now, if we take the square root of each element in this new set, we get the original 8-th roots of unity as shown in the section above:</p>
<ul>
<li>$\sqrt{\omega^0}=1, -1$ or equivalently $\omega^0$ and $\omega^4$</li>
<li>$\sqrt{\omega^2}=\omega,-\omega$ or equivalently $\omega$ and $\omega^5$</li>
<li>$\sqrt{\omega^4}=\omega^2,-\omega^2$ or equivalently $\omega^2$ and $\omega^6$</li>
<li>$\sqrt{\omega^6}=\omega^3,-\omega^3$ or equivalently $\omega^3$ and $\omega^7$</li>
</ul>
<p>This is not a profound observation: the square and square roots are opposite operations so naturally the square root should “undo” what the square does and vice versa.</p>
<p>However, this does open up an optimization that we will later leverage. One can square the roots of unity repeatedly to shrink the set down, carry out some operation, then use the square root to “raise” the result back up to the original set. We will introduce the mechanics in the upcoming chapters, but for now, the reader must have the following concept absolutely mastered:</p>
<p>Squaring $k$-th roots of unity reduces the set to the $k/2$-th roots of unity. Taking the square roots of the $k/2$-th roots of unity produces the $k$-th roots of unity and doubles the set size.</p>
<h2>Summary</h2>
<ul>
<li>Only even powers of the roots of unity have square roots</li>
<li>The square roots of $\omega^m$ are $\omega^{m/2}$ and $-\omega^{m/2}$</li>
<li>Based on an earlier chapter, we know $-\omega^{m/2}\equiv\omega^{m/2+k/2}$</li>
<li>Visualizing square roots on a circle. Since square roots are always additive inverses of each other, the square root of a root of unity is on “opposite sides” of the circle.</li>
</ul>
<h2>Practice Problems</h2>
<ol>
<li>Let $\omega$ be the primitive 4-th root of unity. What are the square roots of $-\omega^2$?</li>
<li>Let $\omega$ be the primitive 32-th root of unity. What are the square roots of $-\omega^{16}$?</li>
<li>Let $\omega$ be the primitive 16-th root of unity. What are the square roots of $-1$?</li>
</ol>
<div style='page-break-after: always;'></div>

<h1>The Image Preservation Theorem for Multivalued Functions</h1>
<p>Source: https://rareskills.io/post/image-preservation-theorem</p>
<h1>The Image Preservation Theorem for Multivalued Functions</h1>
<p>We’ll start this chapter on an unusual note — the NTT algorithm is quite simple and can be implemented in less than 20 lines of code. However, the key idea that makes it work, oddly enough, does not have a formal mathematical name. So we are going to take the liberty to give what we believe to be the core theorem behind the algorithm a (subjectively) catchy name:</p>
<p>The Image Preservation Theorem for Multivalued Functions</p>
<p>To explain the Image Preservation Theorem, we’ll go over one last (very simple) concept about roots of unity, then introduce the theorem.</p>
<h2>$k$-th roots of nested square roots</h2>
<p>The literal definition for “k-th root of unity” is that the root of unity $a$ satisfies $a^k\equiv1$. Stated another way:</p>
<p>$$<br />
a\equiv\sqrt[k]{1}$$</p>
<p>Therefore, it should be no surprise that if $\omega$ is the primitive 4-th root of unity then</p>
<p>$$<br />
\sqrt[4]{1}=\set{1,\omega,-1,-\omega}$$</p>
<p>Now let’s do the same operation if $\omega$ is the primitive 8-th root of unity. The result would be</p>
<p>$$<br />
\sqrt[4]{1}=\set{1,\omega^2,\omega^4,\omega^6}$$</p>
<p>and the 8-th root of 1 generates all the 8-th roots of unity:</p>
<p>$$<br />
\sqrt[8]{1}=\set{1,\omega,\omega^2,\omega^3,\omega^4,\omega^5,\omega^6,\omega^7}$$</p>
<p>These observations are simply a matter of definition. Of course, we do not want to <em>directly</em> compute the square root of 1. It’s much easier to first find the primitive $k$-th root of unity first, then generate the set of answers for $\sqrt[8]{1}$. For example:</p>
<pre style='font-family: Arial'><code class="language-solidity">import galois
Fq = 17
k = 8
assert (Fq - 1) % k == 0, &quot;no such subgroup&quot;

GF = galois.GF(Fq)

pr = GF.primitive_root_of_unity(k)

roots = []
for i in range(0,k):
    roots.append(pr**i)
print(roots)
</code></pre>
<p>We can also observe that:</p>
<p>$$<br />
\sqrt[4]{1}=\sqrt{\sqrt{1}}$$<br />
$$<br />
\sqrt[8]{1}=\sqrt{\sqrt{\sqrt{1}}}$$<br />
$$<br />
\sqrt[16]{1}=\sqrt{\sqrt{\sqrt{\sqrt{1}}}}$$</p>
<p>and so on. As noted above, directly computing the $\sqrt[k]{1}$ is not efficient. Instead, we consider the following diagram, where each arrow down is a square root of the number above it:</p>
<p><img alt="A binary tree showing the recursive square root of 1 for the 8-th roots of unity" src="assets/image1.jpg" /></p>
<p>We can compute the 8-th roots of unity by starting with 1 and repeatedly taking square roots.</p>
<p>For arbitrary $k$, the diagram is as follows.</p>
<p><img alt="A binary tree showing the recursive square root of 1 for the k-th roots of unity" src="assets/image2.jpg" /></p>
<p>Here is a walkthrough of the square root computations:</p>
<p>The square root of 1 is $\set{1,\omega^{k/2}}$. $\omega^{k/2}$ is congruent to -1</p>
<p>Recall from the previous chapter that the square root of $\omega^m$ is $\omega^{m/2}$ and $-\omega^{m/2}$ (assuming $m$ is even).</p>
<p>Thus, the square root of $\omega^{k/2}$ is $\set{\omega^{k/4},-\omega^{k/4}}$.</p>
<p>Also recall that the additive inverse of $\omega^i$ is $\omega^{i+k/2}$.</p>
<p>So to compute $-\omega^{k/4}$ without the negative sign, we compute $\omega^{k/4+k/2}=\omega^{k/4+2k/4}=\omega^{3k/4}$. So the square root of $\omega^{k/2}$ is $\set{\omega^{k/4},\omega^{3k/4}}$.</p>
<p>The square root of $\omega^{k/4}$ is ${\omega^{k/8},-\omega^{k/8}}$. Again, to get rid of the negative sign, we compute $\omega^{k/8+k/2}=\omega^{5k/8}$</p>
<p>Why the square root of $\omega^{3k/4}=\set{\omega^{3k/8},\omega^{7k/8}}$ is left as an exercise for the reader.</p>
<h2>The height of the tree is $\log_2n$</h2>
<p>Computing the $k$-th roots of unity by repeatedly taking the square roots of 1 creates a “tree” that is $\log_2(n)$ in height.</p>
<h2>Conceptualizing intermediate states of the tree</h2>
<p>The set</p>
<p>$$<br />
\sqrt{\sqrt{\sqrt{1}}}$$</p>
<p>is mathematically equivalent to the 8-th roots of unity. The set is also equivalent:</p>
<p>$$<br />
\left{\sqrt{\sqrt{1}},\sqrt{\sqrt{-1}}\right}$$</p>
<p>Which is also equivalent to:</p>
<p>$$<br />
\set{\sqrt{1},\sqrt{-1},\sqrt{\omega^2},\sqrt{-\omega^2}}$$</p>
<p>In other words:</p>
<p>$$<br />
\sqrt{\sqrt{\sqrt{1}}}=\left{\sqrt{\sqrt{1}},\sqrt{\sqrt{-1}}\right}=\set{\sqrt{1},\sqrt{-1},\sqrt{\omega^2},\sqrt{-\omega^2}}=\set{1,…,\omega^7}$$</p>
<h2>Quick aside — multivalued functions and images</h2>
<p>Admittedly, the observations we made above are trivial extensions of the definition of $k$-th roots. The more interesting implications of these observations will be shown in the following section. However, it will be helpful to introduce some mathematical terminology first:</p>
<p><strong>Image of a function</strong> — if we have a set of points that we evaluate a function on, then the set of evaluations is the <em>image</em>. For example, if the function is $f(x)=2x$ and the domain we evaluate $f$ on is $\set{1,2}$ then the image will be $\set{2,4}$. The term <em>range</em> of a function means the set of values a function <em>could</em> output. In our context, the <em>image of the function refers to the</em> actual set of outputs for a given set of inputs.</p>
<p><strong>Multivalued function</strong> — a function that may return more than one evaluation for a single point in the domain. For example, $\sqrt{x}$ evaluated on 0 returns 0, but $\sqrt{x}$ evaluated on 1 returns $\set{1, -1}$.</p>
<p>Note: The multivalued function $f(x)=\sqrt{x}$ has an image larger than its domain.</p>
<p>With these definitions in mind, the reader should understand the following statements:</p>
<p>“The image of $f(x)=x+1$ on the domain $\set{0,2}$ is $\set{1,3}$”</p>
<p>“The image of the multivalued function $f(x)=\sqrt{x}$ on the domain $\set{1}$ is $\set{1,-1}$”</p>
<p>Now we get to a more interesting set of observations.</p>
<h3>The image of $f(x)=x$ evaluated on $\set{1,…,\omega^7}$ is the same as $g(x)=\sqrt[8]{x}$ evaluated on $\set{1}$ (k = 8)</h3>
<p>This claim is a simple rephrasing of the concepts shown above, so we won’t elaborate further. The interesting part is how we can generalize this to other functions.</p>
<p>The nuance here is that $g(x)$ is a multivalued function that returns eight elements.</p>
<h3>The image of $f(x)=ax$ evaluated on the 4-th roots of unity is the same as the multivalued function $g(x)=a\sqrt[4]{x}$ evaluated on $\set{1}$ (k = 4)</h3>
<p>Remember, $\sqrt[4]{1}=\set{1,\omega,\omega^2,\omega^3}$, so $a\sqrt[4]{1}=\set{a,a\omega,a\omega^2,a\omega^3}$. This is the same image as evaluating $f$ on each root of unity one-by-one. Since $f(x)=ax$</p>
<ul>
<li>$f(1)=a$</li>
<li>$f(\omega)=a\omega$</li>
<li>$f(\omega^2)=a\omega^2$</li>
<li>$f(\omega^3)=a\omega^3$</li>
</ul>
<p><strong>Exercise for the reader:</strong> Let $f(x)=ax+c$. What is the image of $f(x)$ on the 4-th roots of unity? What is the image of the multivalued function $g(x)=a\sqrt[4]{x}+b$ on the domain $\set{1}$?</p>
<p>In the following chapters, we will show how to handle more complex cases like $f(x)=x^2+x^3$, but for now, we show how to actually compute multivalued functions for the simple cases we showed here. But first, let’s give a formal definition to the observation we’ve made so far about image equivalence.</p>
<h3>The image of $f(x)=x$ evaluated on the 4-th roots of unity is the same as the multivalued function $g(x)=\sqrt{x}$ evaluated on the second roots of unity $\set{1,-1}$ (k = 4)</h3>
<p>This example is very similar to the above, except that we don’t “target” the domain $\set{1}$ but rather $\set{1,-1}$.</p>
<p>We have that</p>
<ul>
<li>$g(1)=\sqrt{1}=\set{1,-1}$</li>
<li>$g(-1)=\sqrt{-1}=\set{\omega,-\omega}$</li>
</ul>
<p>This is the same image as evaluating $f$ on $\set{1,\omega,-1,-\omega}$</p>
<p>In summary, if we “shrink” the domain by a factor of $r$ and replace every $x$ in $f(x)$ with $\sqrt[r]{x}$ to create a new multivalued function $g$, the images of $f$ and $g$ are identical.</p>
<p>We’ll now formally define the concept we’ve been illustrating</p>
<h2>The core theorem of NTT: Image Preservation Theorem for Multivalued Functions</h2>
<p>Let $\omega$ be the primitive $k$-th root of unity and $\langle\omega\rangle=\set{1,\omega,\omega^2,…}$ be the $k$-th roots of unity defined in $\mathbb{F}_q$.</p>
<p>Let $k$ be a power of 2.</p>
<p>Let $f(x)$ be a polynomial in $\mathbb{F}_q$. Let $r$ be a power of 2 less than or equal to $k$.</p>
<p>Let $g(x)$ be a multivalued function created by replacing every $x$ in $f(x)$ with $\sqrt[r]{x}$.</p>
<p>Let the domain $\langle\omega\rangle^r$ be $\set{\omega^r|\omega\in \langle\omega\rangle}$. Note that if $r=k$ then $\langle\omega\rangle^r=\set{1}$.</p>
<p>The image of $f$ on $\langle\omega\rangle$ exactly equals the image of $g$ on $\langle\omega\rangle^r$.</p>
<p><strong>It is of utmost importance that the reader understand the theorem above, as it is the key concept that the Number Theoretic Transform relies on!</strong></p>
<p>Here are some examples to enforce the concept:</p>
<ul>
<li>The image of $f(x)=x$ on the 4-th roots of unity is equal to the image of $g(x)=\sqrt{x}$ on the second roots of unity (as shown in the previous section).</li>
<li>The image of $f(x) =x$ on 16th roots of unity is equal to image of $g(x)=\sqrt{x}$ on 8th roots of unity</li>
<li>The image of $f(x) =x$ on k-th roots of unity is equal to image of $g_1(x)=\sqrt{x}$ on k/2-th roots of unity</li>
<li>The image of $g_1(x)=\sqrt{x}$ on k/2-th roots of unity is equal to image of $g_2(x)=\sqrt[4]{x}$ on k/4-th roots of unity</li>
</ul>
<p>The following section shows examples where $f(x)$ is slightly more complex.</p>
<h3>Image Preservation Theorem Examples for $f(x)=ax+b$</h3>
<p>Let $f(x)=ax+b$ and the domain be the 4-th roots of unity $\set{1,\omega,\omega^2,\omega^3}$.</p>
<p>Let $f_1$ be the multivalued function $a\sqrt{x}+b$ and the domain be$\set{1,-1}$ or equivalently $\set{1,-\omega^2}$.</p>
<p>Let $f_2$ be the multivalued function $a\sqrt[4]{x}+b$ and the domain be $\set{1}$</p>
<p>The images of $f$, $f_1$, and $f_2$ are identical: $\set{a+b,a\omega+b,a\omega^2+b,a\omega^3+b}$</p>
<h2>Connection to NTT</h2>
<p>Remember, the goal of NTT is to evaluate $f(x)$ on the $k$-th roots of unity in $\mathcal{O}(n\log n)$ time. In other words, we want to compute the <em>image</em> of $f$ on the $k$-th roots of unity.</p>
<p>You can probably guess where this is going.</p>
<p>Computing the image of $f(x)$ on the $k$-th roots of unity is equivalent to computing the image of a new function $g(x)$ defined such that for each $x$ in $f$, $x\rightarrow\sqrt[k]{x}$ evaluated on $\set{1}$.</p>
<p>However, this leaves an open question:</p>
<p>Expanding $\sqrt[k]{x}$ into $\set{1,…,\omega^{k-1}}$ doesn’t directly lead to a speedup. Algorithmically, expanding a $\sqrt[k]{1}$ into the $k$-th roots of unity is the same as evaluating $f(x)$ on the $k$ points. How does this alternate way of evaluating $f(x)$ help us?</p>
<p>In the next chapter, we will demonstrate how to evaluate multi-valued functions using square root expansion and explain how this approach can prevent redundant computation.</p>
<div style='page-break-after: always;'></div>

<h1>Evaluating multivalued functions by square root expansion</h1>
<p>Source: https://rareskills.io/post/square-root-multivalued-functions</p>
<h1>Evaluating multivalued functions by square root expansion</h1>
<p>In the previous chapter on Image Preservation of Multivalued Functions we saw that instead of evaluating $f(x)$ on the $k$-th roots of unity, we can transform $f(x)$ into a multivalued function and evaluate it on the domain $\set{1}$.</p>
<h2>Unwrapping Square Roots</h2>
<p>It’s much easier to think of the 8-th root of 1 as nested square roots:</p>
<p>$$<br />
\sqrt[8]{x}=\sqrt{\sqrt{\sqrt{x}}}$$</p>
<p>We now show how to evaluate:</p>
<p>$$<br />
\sqrt{\sqrt{\sqrt{1}}}$$</p>
<p>using square root expansion.</p>
<p>We know that $\sqrt{1}$ evaluates to $\set{1,-1}$ so we replace the inner-most square root of 1 with its evaluations:</p>
<p><img alt="A diagram showing the that a times the 8-th root of 1 is equal to a times the fourth root of 1 and a times the fourth root of -1" src="assets/image1.png" /></p>
<p>This “removes” one layer of square roots. Next, we evaluate $\sqrt{1}$ and $\sqrt{-1}$. Since we are working with the 8-th roots of unity, $-1\equiv\omega^4$ so $\sqrt{-1}={\omega^2,-\omega^2}$</p>
<p><img alt="A diagram showing how the fourth roots can be expanded into two square roots" src="assets/image2.png" /></p>
<p>Then, we evaluate each of the remaining square roots:</p>
<p><img alt="A diagram showing how four square roots evaluate to the 8th roots of unity" src="assets/image7.png" /></p>
<p>Observe that the “leaves” of the evaluation tree are exactly $f$ evaluated on an 8-th root of unity.</p>
<h2>Evaluating $f(x)=x^2$ on the 8-th roots of unity</h2>
<p>The nice thing about square root expansion is that for most powers of $x$, the square roots “disappear early” as this example shows. We replace $x$ with $\sqrt[8]{x}$, but since $x$ is squared, we have end up with $\sqrt[4]{x}$ or</p>
<p>$$<br />
\sqrt{\sqrt{x}}$$</p>
<p>Here is the evaluation tree from repeatedly expanding the square roots until we have eight evaluations. In the final row, when there are no square roots, we simply copy the values down.</p>
<p><img alt="An evaluation tree for the 8-th roots of unity evaluated on x^2 using square root expansion" src="assets/image5.jpg" /></p>
<p>Now observe what happens if we evaluate the 8-th roots of unity $\set{1,…,\omega^7}$ directly on $f(x)=x^2$ — the outcome is exactly the same.</p>
<p>$$<br />
\begin{align*}<br />
f(1)&amp;=(1)^2&amp;=1\<br />
f(-1)&amp;=(-1)^2&amp;=1\<br />
f(\omega^2)&amp;=(\omega^2)^2&amp;=-1\<br />
f(-\omega^2)&amp;=(-\omega^2)^2&amp;=-1\<br />
f(\omega)&amp;=(\omega)^2&amp;=\omega^2\<br />
f(-\omega)&amp;=(-\omega)^2&amp;=\omega^2\<br />
f(\omega^3)&amp;=(\omega^3)^2&amp;=-\omega^2\<br />
f(-\omega^3)&amp;=(-\omega^3)^2&amp;=-\omega^2\<br />
\end{align*}$$</p>
<h2>Evaluating $f(x)=x^4$ on the 8-th roots</h2>
<p>Again, we replace $x$ with $\sqrt[8]{x}$ which gives us $\sqrt{x}$. Since we only have one square root, we will only expand the square root once, then simply copy the results down.</p>
<p><img alt="A diagram showing the square root expansion of 1 to the 8-th roots" src="assets/image4.jpg" /></p>
<p>We saw in an earlier chapter that $x^{k/2}$ is 1 if evaluated on an even power of $\omega$ and -1 otherwise. The result here matches the expected outcome.</p>
<h2>Evaluating $f(x)=ax+bx^5$ on the 8-th roots of unity</h2>
<p>If we replace $x$ with $\sqrt[8]{x}$, we get a slightly awkward result:</p>
<p>$$<br />
f(x)=a\sqrt[8]{x}+bx^{5/8}$$</p>
<p>However, if we factor $x$ out first to turn $f(x)$ into $f(x)=x(a+bx^4)$, the new form becomes a lot more manageable when we substitute $\sqrt[8]{x}$ for $x$:</p>
<p>$$<br />
g(x)=\sqrt[8]{x}(a+b\sqrt{x})$$</p>
<p>The first square root will disappear after the first evaluation and $(a+b\sqrt{x})$ will become a constant for most of the tree.</p>
<p>Below is a diagram of expanding the square roots. At each level, we unwrap (evaluate) one square root. The square roots in blue represent the ones evaluated at each step. As a general rule, evaluate the innermost square root if it is nested:</p>
<p><img alt="A diagram showing the square root expansion of a + bx^4" src="assets/image6.jpg" /></p>
<p>Now let’s compare the result to evaluating $f(x)=ax+bx^5$ on the 8-th roots of unity one-by-one:</p>
<p>$$<br />
\begin{array}{rcll}<br />
f(1) &amp;=&amp; a(1) + b(1)^5 &amp;= a + b\[4pt]<br />
f(-1\equiv\omega^4) &amp;=&amp; a\omega^4 + b\omega^{20} = a(-1) + b(-1) &amp;= -a – b\[4pt]<br />
f(\omega^2) &amp;=&amp; a\omega^2 + b\omega^{10} = a\omega^2 + b\omega^2 &amp;= (a + b)\omega^2\[4pt]<br />
f(-\omega^{2}\equiv\omega^6) &amp;=&amp; a\omega^6 + b\omega^{30} = a(-\omega^2) + b(-\omega^2) &amp;= -(a + b)\omega^2\[4pt]<br />
f(\omega) &amp;=&amp; a\omega + b\omega^5 &amp;= a\omega – b\omega \[4pt]<br />
f(-\omega\equiv\omega^5) &amp;=&amp; a\omega^5 + b\omega^{25} = a(-\omega) + b\omega &amp;= -a\omega + b\omega = (b – a)\omega\[4pt]<br />
f(\omega^3) &amp;=&amp; a\omega^3 + b\omega^{15} = a\omega^3 + b\omega^7 &amp;= a\omega^3 – b\omega^3 = (a – b)\omega^3\[4pt]<br />
f(-\omega^3\equiv\omega^7) &amp;=&amp; a\omega^7 + b\omega^{35} = a(-\omega^3) + b(-\omega^3) &amp;= -(a + b)\omega^3<br />
\end{array}<br />
$$</p>
<p>Evaluating the terms using the method above requires 8 additions and 16 multiplications.</p>
<p>However, using square root expansion, we only need 2 additions and 10 multiplications. Whenever a square root is expanded into its two solutions, we multiply it by the adjacent term. So whenever the “final” square root is unwrapped, it results in two multiplications. These are highlighted in red below. The additions are highlighted in blue.</p>
<p><img alt="A diagram counting the number of additions and multiplications in the square root expansion." src="assets/image3.jpg" /></p>
<p>Note that “computing the square root” is completely deterministic. We know it always follows the pattern of $\set{1}$, 2nd roots of unity, 4-th roots of unity, 8-th roots of unity, and so on. Thus, there is no need to explicitly compute the square roots.</p>
<h2>Easy terms and hard terms</h2>
<p>We can see that terms $x^{k/2}$ are easiest to evaluate because they only require 1 evaluation, and the rest is just copying values down the tree.</p>
<p>On the other hand, $x$ with no power is the “hardest” to evaluate because we need to do a square root expansion at every step down the tree.</p>
<p>The nice thing about the function $f(x)=a+bx^{k/2}$, which becomes the multivalued function $g(x)=(a+b\sqrt{x})$ on the $k$-th roots of unity is that we fully evaluate the square root at the second level of the tree, and simply copy down the sum of $a+b$ for the rest of the evaluation.</p>
<p>In fact, any polynomial can be written to “maximize” the amount of $a+bx^{k/2}$ terms and “minimize” the amount of $x$ terms.</p>
<p>Suppose we have a 7-degree polynomial that we want to evaluate on the 8-th roots of unity.</p>
<p>$$<br />
f(x)= a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5 + a_6x^6 + a_7x^7$$</p>
<p>To maximize the number of $x^4$ terms and have only one $x$ term, we factor it as follows:</p>
<p>$$<br />
f(x)= a_0 + a_2x^2 + a_4x^4 + a_6x^6 + a_1x + a_5x^5 + a_3x^3+ a_7x^7$$<br />
$$<br />
f(x)= a_0 + a_4x^4 + (a_2x^2 + a_6x^6) + (a_1x + a_5x^5) + (a_3x^3+ a_7x^7)$$<br />
$$<br />
f(x)= a_0 + a_4x^4 + x^2(a_2 + a_6x^4) + x(a_1 + a_5x^4) + x^3(a_3x+ a_7x^4)$$<br />
$$<br />
f(x)= a_0 + a_4x^4 + x^2(a_2 + a_6x^4) + x((a_1 + a_5x^4) + x^2(a_3x+ a_7x^4))$$</p>
<p><strong>Exercise:</strong> How should the polynomial $f(x)=a_0+a_1x+a_2x^2+a_3x^3$ evaluated on the 4th roots of unity be factored to have only one $x$ term and as many $x^2$ terms as possible? Remember, $x^{k/2}$ is $x^2$ in this case.</p>
<p>In the next chapter, we will show how to evaluate a general degree 4 and degree 8 polynomial using square root expansion, which also happens to be exactly the NTT algorithm.</p>
<div style='page-break-after: always;'></div>

<h1>NTT Algorithm By Hand</h1>
<p>Source: https://rareskills.io/post/ntt-by-hand</p>
<h1>NTT Algorithm By Hand</h1>
<p>The NTT (Number Theoretic Transform) algorithm converts a polynomial in a finite field from coefficient form to point form.</p>
<p>If a polynomial has degree $d$ then we evaluate it on the $k$-th roots of unity where $k\gt d.$</p>
<p>Rather than evaluating the polynomial $f(x)$ on each point in the set of $k$-th roots of unity, $\set{1,\omega,\omega^2,…,\omega^{k-1}}$, we use the <a href="https://rareskills.io/post/image-preservation-theorem">image preservation theorem for multivalued functions</a> to <a href="https://rareskills.io/post/square-root-multivalued-functions">evaluate the multivalued function</a> created by substituting $x$ in $f$ with $\sqrt[k]{x}$ on the domain $\set{1}$. We then iteratively expand the evaluations of the square root from $\set{1}$ to $\set{1,\omega^{k/2}}$ to $\set{1,\omega^{k/4},\omega^{k/2},-\omega^{k/4}}$ and so on until the evaluation is expanded to the $k$-th roots of unity.</p>
<p>The runtime of this method is $\mathcal{O}(n \log n)$.</p>
<h2>Evaluating $f(x)=a_0+a_1x+a_2x^2+a_3x^3$ on the 4-th roots of unity</h2>
<p>First, we factor the function to maximize the occurrences of $x^2$, since $2=k/2$ and $x^{k/2}$ is easy to evaluate on a root of unity (it only results in $\set{1,-1}$ depending on if the power of the root of unity is even or odd).</p>
<p>This creates the following function:</p>
<p>$$<br />
f(x)=a_0+a_2x^2+x(a_1+a_3x^2)$$</p>
<p>Next, we transform $f$ so that $x\rightarrow\sqrt[4]{x}$ which gives us</p>
<p>$$<br />
f(x)=a_0+a_2\sqrt{x}+\sqrt[4]{x}(a_1+a_3\sqrt{x})$$</p>
<p>Here is the square root expansion diagram:</p>
<p><img alt="NTT of a degree four polynomial" src="assets/image3.jpg" /></p>
<p>Now we compare the result to evaluating $f(x)$ on the 4-th roots of unity one-by-one:</p>
<p>$$<br />
\begin{align*}<br />
f(1) &amp;= a_0 &amp;+&amp; a_1(1) &amp;+&amp; a_2(1)^2 &amp;+&amp; a_3(1)^3\<br />
f(-1) &amp;= a_0 &amp;+&amp; a_1(-1) &amp;+&amp; a_2(-1)^2 &amp;+&amp; a_3(-1)^3\<br />
f(\omega) &amp;= a_0 &amp;+&amp; a_1(\omega) &amp;+&amp; a_2(\omega)^2 &amp;+&amp; a_3(\omega)^3\<br />
f(-\omega) &amp;= a_0 &amp;+&amp; a_1(-\omega) &amp;+&amp; a_2(-\omega)^2 &amp;+&amp; a_3(-\omega)^3<br />
\end{align*}$$</p>
<p>We have that $\omega^2=(-\omega^2)=-1$ and $\omega^3=-\omega$ and $(-\omega)^3=(-1)^3(\omega)^3=-\omega^3=-(-\omega)=\omega$. By substitution, we have:</p>
<p>$$<br />
\begin{align*}<br />
f(1) &amp;= a_0 &amp;+&amp; a_1 &amp;+&amp; a_2 &amp;+&amp; a_3\<br />
f(-1) &amp;= a_0 &amp;-&amp; a_1 &amp;+&amp; a_2 &amp;-&amp; a_3\<br />
f(\omega) &amp;= a_0 &amp;+&amp; a_1\omega &amp;-&amp; a_2 &amp;-&amp; a_3\omega\<br />
f(-\omega) &amp;= a_0 &amp;-&amp; a_1\omega &amp;-&amp; a_2 &amp;+&amp; a_3\omega<br />
\end{align*}$$</p>
<p><strong>Exercise:</strong> Use the above method to evaluate $a_0+a_1x+a_2x^2$ on the 4-th roots of unity. Hint: use the example above and set $a_3=0$.</p>
<p>The height of the tree is $\log n$ and we do $\mathcal{O}(n)$ operations on each row, so the runtime is $\mathcal{O}(n \log n)$.</p>
<h2>Evaluating $f(x)=a_0+a_1x+…+a_7x^7$ on the 8-th roots of unity</h2>
<p>First, we rearrange the polynomial to maximize the number of $x^4$ terms (since k = 8). This gives us:</p>
<p>$$<br />
f(x)=a_0+a_4x^4+x^2(a_2+a_6x^4)+x((a_1+a_5x^4)+x^2(a_3+a_7x^4))$$</p>
<p>Now we substitute $x\rightarrow\sqrt[8]{x}$ to get our multivalued function $g(x)$</p>
<p>$$<br />
g(x)=a_0+a_4\sqrt{x}+\sqrt[4]{x}(a_2+a_6\sqrt{x})+\sqrt[8]{x}((a_1+a_5\sqrt{x})+\sqrt[4]{x}(a_3+a_7\sqrt{x}))$$</p>
<p>Since drawing the evaluation tree in one image would be quite large, we’ll draw the left side of the tree where we evaluate $\sqrt{1}=1$ and show the diagram for that first:</p>
<p><img alt="NTT of an 8 degree polynomial only showing the left side of the evaluation tree." src="assets/image1.jpg" /></p>
<p>From the image above, we have that</p>
<p>$$<br />
\begin{align*}<br />
f(1)=((a_0+a_4)+(a_2+a_6))+((a_1+a_5)+(a_3+a_7))\<br />
f(-1)=((a_0+a_4)+(a_2+a_6))+((a_1+a_5)+(a_3+a_7))\<br />
f(\omega^2)=((a_0+a_4)-(a_2+a_6))+\omega((a_1+a_5)-(a_3+a_7))\<br />
f(-\omega^2)=((a_0+a_4)-(a_2+a_6))-\omega((a_1+a_5)-(a_3+a_7))\<br />
\end{align*}$$</p>
<p>Now we expand the right side of the tree where $\sqrt{x}=-1$:</p>
<p><img alt="NTT evaluation of an 8 degree polynomial on the right side of the evaluation tree" src="assets/image2.jpg" /></p>
<p>From that result, we have that:</p>
<p>$$<br />
\begin{align*}<br />
f(\omega)=((a_0-a_4)+\omega^2(a_2-a_6))+\omega((a_1-a_5)+\omega^2(a_3-a_7))\<br />
f(-\omega)=((a_0-a_4)+\omega^2(a_2-a_6))-\omega((a_1-a_5)+\omega^2(a_3-a_7))\<br />
f(\omega^3)=((a_0-a_4)-\omega^2(a_2-a_6))+\omega^3((a_1-a_5)-\omega^2(a_3-a_7))\<br />
f(-\omega^3)=((a_0-a_4)-\omega^2(a_2-a_6))-\omega^3((a_1-a_5)-\omega^2(a_3-a_7))<br />
\end{align*}$$</p>
<p>Combining the evaluations and distributing the omega terms, we have:</p>
<p>$$<br />
\begin{align*}<br />
f(1) &amp;= a_0 &amp;+ a_4 &amp;+ a_2 &amp;+ a_6 &amp;+ a_1 &amp;+ a_5 &amp;+ a_3 &amp;+ a_7\f(-1) &amp;= a_0 &amp;+ a_4 &amp;+ a_2 &amp;+ a_6 &amp;- a_1 &amp;- a_5 &amp;- a_3 &amp;- a_7\f(\omega^2) &amp;= a_0 &amp;+ a_4 &amp;- a_2 &amp;- a_6 &amp;+ a_1\omega^2 &amp;+ a_5\omega^2 &amp;- a_3\omega^2 &amp;- a_7\omega^2\f(-\omega^2)&amp;= a_0 &amp;+ a_4 &amp;- a_2 &amp;- a_6 &amp;- a_1\omega^2 &amp;- a_5\omega^2 &amp;+ a_3\omega^2 &amp;+ a_7\omega^2\f(\omega) &amp;= a_0 &amp;- a_4 &amp;+ a_2\omega^2 &amp;- a_6\omega^2 &amp;+ a_1\omega &amp;- a_5\omega &amp;+ a_3\omega^3 &amp;- a_7\omega^3\f(-\omega) &amp;= a_0 &amp;- a_4 &amp;+ a_2\omega^2 &amp;- a_6\omega^2 &amp;- a_1\omega &amp;+ a_5\omega &amp;- a_3\omega^3 &amp;+ a_7\omega^3\f(\omega^3) &amp;= a_0 &amp;- a_4 &amp;- a_2\omega^2 &amp;+ a_6\omega^2 &amp;+ a_1\omega^3 &amp;- a_5\omega^3 &amp;+ a_3\omega &amp;- a_7\omega\f(-\omega^3)&amp;= a_0 &amp;- a_4 &amp;- a_2\omega^2 &amp;+ a_6\omega^2 &amp;- a_1\omega^3 &amp;+ a_5\omega^3 &amp;- a_3\omega &amp; a_7\omega\end{align*}$$</p>
<p>Next, we put the coefficients in ascending order:</p>
<p>$$<br />
\begin{align*}<br />
f(1) &amp;= a_0 &amp;+ a_1 &amp;+ a_2 &amp;+ a_3 &amp;+ a_4 &amp;+ a_5 &amp;+ a_6 &amp;+ a_7\<br />
f(-1) &amp;= a_0 &amp;- a_1 &amp;+ a_2 &amp;- a_3 &amp;+ a_4 &amp;- a_5 &amp;+ a_6 &amp;- a_7\<br />
f(\omega^2) &amp;= a_0 &amp;+ a_1\omega^2 &amp;- a_2 &amp;- a_3\omega^2 &amp;+ a_4 &amp;+ a_5\omega^2 &amp;- a_6 &amp;- a_7\omega^2\<br />
f(-\omega^2)&amp;= a_0 &amp;- a_1\omega^2 &amp;- a_2 &amp;+ a_3\omega^2 &amp;+ a_4 &amp;- a_5\omega^2 &amp;- a_6 &amp;+ a_7\omega^2\<br />
f(\omega) &amp;= a_0 &amp;+ a_1\omega &amp;+ a_2\omega^2 &amp;+ a_3\omega^3 &amp;- a_4 &amp;- a_5\omega &amp;- a_6\omega^2 &amp;- a_7\omega^3\<br />
f(-\omega) &amp;= a_0 &amp;- a_1\omega &amp;+ a_2\omega^2 &amp;- a_3\omega^3 &amp;- a_4 &amp;+ a_5\omega &amp;- a_6\omega^2 &amp;+ a_7\omega^3\<br />
f(\omega^3) &amp;= a_0 &amp;+ a_1\omega^3 &amp;- a_2\omega^2 &amp;+ a_3\omega &amp;- a_4 &amp;- a_5\omega^3 &amp;+ a_6\omega^2 &amp;- a_7\omega\<br />
f(-\omega^3)&amp;= a_0 &amp;- a_1\omega^3 &amp;- a_2\omega^2 &amp;- a_3\omega &amp;- a_4 &amp;+ a_5\omega^3 &amp;+ a_6\omega^2 &amp;+ a_7\omega<br />
\end{align*}$$</p>
<p>Now let’s rearrange the evaluations to go from $f(1)$, $f(\omega)$, … , $f(\omega^7)$</p>
<p>$$<br />
\begin{align*}<br />
f(1) &amp;= a_0 &amp;+ a_1 &amp;+ a_2 &amp;+ a_3 &amp;+ a_4 &amp;+ a_5 &amp;+ a_6 &amp;+ a_7\<br />
f(\omega) &amp;= a_0 &amp;+ a_1\omega &amp;+ a_2\omega^2 &amp;+ a_3\omega^3 &amp;- a_4 &amp;- a_5\omega &amp;- a_6\omega^2 &amp;- a_7\omega^3\<br />
f(\omega^2) &amp;= a_0 &amp;+ a_1\omega^2 &amp;- a_2 &amp;- a_3\omega^2 &amp;+ a_4 &amp;+ a_5\omega^2 &amp;- a_6 &amp;- a_7\omega^2\<br />
f(\omega^3) &amp;= a_0 &amp;+ a_1\omega^3 &amp;- a_2\omega^2 &amp;+ a_3\omega &amp;- a_4 &amp;- a_5\omega^3 &amp;+ a_6\omega^2 &amp;- a_7\omega\<br />
f(-1) &amp;= a_0 &amp;- a_1 &amp;+ a_2 &amp;- a_3 &amp;+ a_4 &amp;- a_5 &amp;+ a_6 &amp;- a_7\<br />
f(-\omega) &amp;= a_0 &amp;- a_1\omega &amp;+ a_2\omega^2 &amp;- a_3\omega^3 &amp;- a_4 &amp;+ a_5\omega &amp;- a_6\omega^2 &amp;+ a_7\omega^3\<br />
f(-\omega^2)&amp;= a_0 &amp;- a_1\omega^2 &amp;- a_2 &amp;+ a_3\omega^2 &amp;+ a_4 &amp;- a_5\omega^2 &amp;- a_6 &amp;+ a_7\omega^2\<br />
f(-\omega^3)&amp;= a_0 &amp;- a_1\omega^3 &amp;- a_2\omega^2 &amp;- a_3\omega &amp;- a_4 &amp;+ a_5\omega^3 &amp;+ a_6\omega^2 &amp;+ a_7\omega<br />
\end{align*}$$</p>
<p>If we compare this to the <a href="https://rareskills.io/post/vandermonde-matrix">Vandermonde matrix</a> for the 8-th roots of unity, we can see we correctly computed the powers of $\omega$.</p>
<p>$$<br />
\mathbf{V} =\begin{bmatrix}<br />
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \<br />
1 &amp; \omega &amp; \omega^{2} &amp; \omega^{3} &amp; -1 &amp; -\omega &amp; -\omega^{2} &amp; -\omega^{3} \<br />
1 &amp; \omega^{2} &amp; -1 &amp; -\omega^{2} &amp; 1 &amp; \omega^{2} &amp; -1 &amp; -\omega^{2} \<br />
1 &amp; \omega^{3} &amp; -\omega^{2} &amp; \omega &amp; -1 &amp; -\omega^{3} &amp; \omega^{2} &amp; -\omega \<br />
1 &amp; -1 &amp; 1 &amp; -1 &amp; 1 &amp; -1 &amp; 1 &amp; -1 \<br />
1 &amp; -\omega &amp; \omega^{2} &amp; -\omega^{3} &amp; -1 &amp; \omega &amp; -\omega^{2} &amp; \omega^{3} \<br />
1 &amp; -\omega^{2} &amp; -1 &amp; \omega^{2} &amp; 1 &amp; -\omega^{2} &amp; -1 &amp; \omega^{2} \<br />
1 &amp; -\omega^{3} &amp; -\omega^{2} &amp; -\omega &amp; -1 &amp; \omega^{3} &amp; \omega^{2} &amp; \omega<br />
\end{bmatrix}$$</p>
<h3>Vandermonde matrix computation</h3>
<p>The Vandermonde matrix above was derived as follows. Each row is the powers of $x$ for $f(x)=a_0+a_1x+…+a_7x^7$ on $x=\set{1,\omega,\omega^2,…,\omega^7}$</p>
<p>$$<br />
\mathbf{V}=<br />
\begin{bmatrix}<br />
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br />
1 &amp; \omega &amp; \omega^{2} &amp; \omega^{3} &amp; \omega^{4} &amp; \omega^{5} &amp; \omega^{6} &amp; \omega^{7}\<br />
1 &amp; \omega^{2} &amp; \omega^{4} &amp; \omega^{6} &amp; \omega^{8} &amp; \omega^{10} &amp; \omega^{12} &amp; \omega^{14}\<br />
1 &amp; \omega^{3} &amp; \omega^{6} &amp; \omega^{9} &amp; \omega^{12} &amp; \omega^{15} &amp; \omega^{18} &amp; \omega^{21}\<br />
1 &amp; \omega^{4} &amp; \omega^{8} &amp; \omega^{12} &amp; \omega^{16} &amp; \omega^{20} &amp; \omega^{24} &amp; \omega^{28}\<br />
1 &amp; \omega^{5} &amp; \omega^{10} &amp; \omega^{15} &amp; \omega^{20} &amp; \omega^{25} &amp; \omega^{30} &amp; \omega^{35}\<br />
1 &amp; \omega^{6} &amp; \omega^{12} &amp; \omega^{18} &amp; \omega^{24} &amp; \omega^{30} &amp; \omega^{36} &amp; \omega^{42}\<br />
1 &amp; \omega^{7} &amp; \omega^{14} &amp; \omega^{21} &amp; \omega^{28} &amp; \omega^{35} &amp; \omega^{42} &amp; \omega^{49}<br />
\end{bmatrix}$$</p>
<p>Next, we factor out multiples of 8:</p>
<p>$$<br />
\mathbf{V}=<br />
\begin{bmatrix}<br />
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br />
1 &amp; \omega &amp; \omega^{2} &amp; \omega^{3} &amp; \omega^{4} &amp; \omega^{5} &amp; \omega^{6} &amp; \omega^{7}\<br />
1 &amp; \omega^{2} &amp; \omega^{4} &amp; \omega^{6} &amp; \omega^{8} &amp; \omega^{8}\omega^{2} &amp; \omega^{8}\omega^{4} &amp; \omega^{8}\omega^{6}\<br />
1 &amp; \omega^{3} &amp; \omega^{6} &amp; \omega^{8}\omega &amp; \omega^{8}\omega^{4} &amp; \omega^{8}\omega^{7} &amp; \omega^{16}\omega^{2} &amp; \omega^{16}\omega^{5}\<br />
1 &amp; \omega^{4} &amp; \omega^{8} &amp; \omega^{8}\omega^{4} &amp; \omega^{16} &amp; \omega^{16}\omega^{4} &amp; \omega^{24} &amp; \omega^{24}\omega^{4}\<br />
1 &amp; \omega^{5} &amp; \omega^{8}\omega^{2} &amp; \omega^{8}\omega^{7} &amp; \omega^{16}\omega^{4} &amp; \omega^{24}\omega &amp; \omega^{24}\omega^{6} &amp; \omega^{32}\omega^{3}\<br />
1 &amp; \omega^{6} &amp; \omega^{8}\omega^{4} &amp; \omega^{16}\omega^{2} &amp; \omega^{24} &amp; \omega^{24}\omega^{6} &amp; \omega^{32}\omega^{4} &amp; \omega^{40}\omega^{2}\<br />
1 &amp; \omega^{7} &amp; \omega^{8}\omega^{6} &amp; \omega^{16}\omega^{5} &amp; \omega^{24}\omega^{4} &amp; \omega^{32}\omega^{3} &amp; \omega^{40}\omega^{2} &amp; \omega^{48}\omega\<br />
\end{bmatrix}$$</p>
<p>Removing the factors of 8 we have:</p>
<p>$$<br />
\mathbf{V} =<br />
\begin{bmatrix}<br />
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\<br />
1 &amp; \omega &amp; \omega^{2} &amp; \omega^{3} &amp; \omega^{4} &amp; \omega^{5} &amp; \omega^{6} &amp; \omega^{7}\<br />
1 &amp; \omega^{2} &amp; \omega^{4} &amp; \omega^{6} &amp; 1 &amp; \omega^{2} &amp; \omega^{4} &amp; \omega^{6}\<br />
1 &amp; \omega^{3} &amp; \omega^{6} &amp; \omega &amp; \omega^{4} &amp; \omega^{7} &amp; \omega^{2} &amp; \omega^{5}\<br />
1 &amp; \omega^{4} &amp; 1 &amp; \omega^{4} &amp; 1 &amp; \omega^{4} &amp; 1 &amp; \omega^{4}\<br />
1 &amp; \omega^{5} &amp; \omega^{2} &amp; \omega^{7} &amp; \omega^{4} &amp; \omega &amp; \omega^{6} &amp; \omega^{3}\<br />
1 &amp; \omega^{6} &amp; \omega^{4} &amp; \omega^{2} &amp; 1 &amp; \omega^{6} &amp; \omega^{4} &amp; \omega^{2}\<br />
1 &amp; \omega^{7} &amp; \omega^{6} &amp; \omega^{5} &amp; \omega^{4} &amp; \omega^{3} &amp; \omega^{2} &amp; \omega<br />
\end{bmatrix}$$</p>
<p>After replacing $\omega^4=-1$, $\omega^5=-\omega$, $\omega^6=-\omega^2$, $\omega^7=-\omega^3$ we have the original Vandermonde matrix:</p>
<p>$$<br />
\mathbf{V} =\begin{bmatrix}<br />
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \<br />
1 &amp; \omega &amp; \omega^{2} &amp; \omega^{3} &amp; -1 &amp; -\omega &amp; -\omega^{2} &amp; -\omega^{3} \<br />
1 &amp; \omega^{2} &amp; -1 &amp; -\omega^{2} &amp; 1 &amp; \omega^{2} &amp; -1 &amp; -\omega^{2} \<br />
1 &amp; \omega^{3} &amp; -\omega^{2} &amp; \omega &amp; -1 &amp; -\omega^{3} &amp; \omega^{2} &amp; -\omega \<br />
1 &amp; -1 &amp; 1 &amp; -1 &amp; 1 &amp; -1 &amp; 1 &amp; -1 \<br />
1 &amp; -\omega &amp; \omega^{2} &amp; -\omega^{3} &amp; -1 &amp; \omega &amp; -\omega^{2} &amp; \omega^{3} \<br />
1 &amp; -\omega^{2} &amp; -1 &amp; \omega^{2} &amp; 1 &amp; -\omega^{2} &amp; -1 &amp; \omega^{2} \<br />
1 &amp; -\omega^{3} &amp; -\omega^{2} &amp; -\omega &amp; -1 &amp; \omega^{3} &amp; \omega^{2} &amp; \omega<br />
\end{bmatrix}$$</p>
<p><strong>Exercise:</strong> Evaluate $f(x)=a_0+a_1x+a_2x^2+a_3x^3+a_4x^4+a_5x^5+a_6x^6$ on the 8-th roots of unity. Again, note that you can set $a_7=0$.</p>
<p><strong>Exercise:</strong> Evaluate $f(x)=3 +2x+9x^2+x^3$ on the 4-th roots of unity in $\mathbb{F}_q$ where $q=17$. Use Python to find a primitive 4-th root of unity as a starting point.</p>
<h2>Summary</h2>
<p>Evaluating a polynomial on the $k$-th roots of unity using square root expansion has the same evaluations as evaluating the polynomial on the roots of unity one at a time. This holds due to the Image Preservation Theorem for Multivalued Functions as we are simply evaluating the multivalued function on the domain $\set{1}$.</p>
<p>This method saves computation cost because at each step, half of the square roots are evaluated and multiplied the coefficient or sum of coefficients they are paired with. For the remaining evaluations, the results are simply copied down instead of being reevaluated.</p>
<div style='page-break-after: always;'></div>

<h1>FFT Friendly Finite Fields</h1>
<p>Source: https://rareskills.io/post/fft-friendly-finite-fields</p>
<h1>FFT Friendly Finite Fields</h1>
<p>In order to carry out the FFT algorithm in a finite field (the Number Theoretic Transform), there needs to be $k$-th roots of unity such that $k$ is a power of 2.</p>
<p>Ideally, we want a large power of 2 so that we can multiply large polynomials. There are several commonly used finite fields (almost all of which have a catchy name associated with them). This article lists some of the more common ones.</p>
<p>As a quick definition, the <em>characteristic</em> of a field is the prime number we take the modulus with, so in the finite field $\mathbb{F}_q$, $q$ is the characteristic. (There are some nuances with this definition if we consider finite field extensions, but we do not want to get into that right now. For our purposes, $q$ is a prime number and the characteristic of the finite field).</p>
<p>As we saw in the <a href="https://rareskills.io/post/fundamental-theorem-cyclic-groups">Fundamental Theorem of Finite Cyclic Groups</a>, a subgroup exists if the order of the subgroup divides the order of the group.</p>
<p>Therefore, the field is FFT-friendly, then there exists some $k$ such that $k|(q-1)$ and $k$ is a large power of 2. In other words, $q-1$ is divisible by a large power of 2.</p>
<p>The list here is not complete — we only include relatively well-known ones as of the time of publication.</p>
<h2>List of FFT-friendly fields</h2>
<h3>Goldilocks Field</h3>
<p>The Goldilocks field has characteristic $q=2^{64}-2^{32}+1$ and a $2^{32}$-th root of unity.</p>
<p>The following Python code asserts that a <a href="https://rareskills.io/post/multiplicative-subgroups">multiplicative subgroup</a> of order $2^{32}$ exists in this field.</p>
<pre style='font-family: Arial'><code class="language-solidity">q = 2**64 - 2**32 + 1
k = 2**32
assert (q - 1) % k == 0
</code></pre>
<p>Since the characteristic is smaller than 64 bits, the elements can be stored in a single word on most modern hardware (which is usually 64 bits).</p>
<p>However, multiplying two 64-bit numbers together temporarily requires 128 bits, which necessitates an additional register for multiplication.</p>
<p>This motivates the use of a smaller field that uses only 32 bits.</p>
<h3>Baby Bear Field</h3>
<p>The Baby Bear field uses a characteristic $2^{31} – 2^{27} + 1$. It has a $2^{27}$-th root of unity.</p>
<pre style='font-family: Arial'><code class="language-solidity">q = 2**31 - 2**27 + 1
k = 2**27
assert (q - 1) % k == 0
</code></pre>
<p>The name “Baby Bear” is a riff off of <a href="https://en.wikipedia.org/wiki/Goldilocks_and_the_Three_Bears">Goldilocks’ fairytale</a>, where the story emphasizes the Baby Bear’s small size compared to the main character of the story (Goldilocks).</p>
<p>Since the field fits in 32 bits, a 64-bit computer can multiply two elements together in a single word.</p>
<h3>Teddy Bear Field</h3>
<p>The Teddy Bear Field uses a characteristic $2^{32}-2^{30}+1$ and has a $2^{30}$-th root of unity.</p>
<pre style='font-family: Arial'><code class="language-solidity">q = 2**32 - 2**30 + 1
k = 2**30
assert (q - 1) % k == 0
</code></pre>
<p>Compared to the Baby Bear field, it fits inside 32 bits, but has a larger multiplicative subgroup that is eight times as large.</p>
<p>The Teddy Bear field was introduced by Ingonyama in this <a href="https://cdn.prod.website-files.com/63970f25aa7b42e284492d52/6841984a01eabc37a492624c_polar_bear_teddy_bear_prime_fields.pdf">paper</a>.</p>
<h3>Koala Bear Field</h3>
<p>The Koala Bear Field is another field whose characteristic $2^{31} – 2^{24} + 1$ fits in 32 bits and has a $2^{24}$-th root of unity.</p>
<pre style='font-family: Arial'><code class="language-solidity">q = 2**31 - 2**24 + 1
k = 2**24
assert (q - 1) % k == 0
</code></pre>
<h3>BN-128 field</h3>
<p>The BN-128 field is used for its support of <a href="https://rareskills.io/post/bilinear-pairing">elliptic curve pairings</a>, but it also has a relatively large multiplicative subgroup that is of order $2^{28}$.</p>
<pre style='font-family: Arial'><code class="language-solidity"># curve_order = 21888242871839275222246405745257275088548364400416034343698204186575808495617
from py_ecc.bn128 import curve_order
k = 2**28
assert (curve_order - 1) % k == 0
</code></pre>
<h3>The STARK Field</h3>
<p>The Cairo VM used by <a href="https://rareskills.io/cairo-tutorial">Starknet</a> has a characteristic $2^{251}+17\cdot2^{192}+1$ and has a very large $2^{192}$-th root of unity.</p>
<pre style='font-family: Arial'><code class="language-solidity">q = 2**251 + 17*2**192 + 1
k = 2**192
assert (q - 1) % k == 0
</code></pre>
<h3>The BLS12-381</h3>
<p>The BLS12-381 is another pairing-friendly elliptic curve whose curve order has a $2^{32}$-th root of unity.</p>
<pre style='font-family: Arial'><code class="language-solidity"># curve_order = 52435875175126190479447740508185965837690552500527637822603658699938581184513
from py_ecc.bls12_381 import curve_order
k = 2**32
assert (curve_order - 1) % k == 0
</code></pre>
<h2>Library Support</h2>
<p>The <a href="https://github.com/Plonky3/Plonky3">Plonky3 library</a> has libraries for Goldilocks, Baby Bear, and Koala Bear.</p>
<div style='page-break-after: always;'></div>